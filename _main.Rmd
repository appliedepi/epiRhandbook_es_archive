---
knit: "bookdown::render_book"
title: "EpiRhandbook en español"  
description: "EpiRhandbook es un manual de referencia de R aplicado a la epidemiología y la salud pública. "
author: "El equipo del manual"
date: "`r Sys.Date()`"
#url: 'https://github.com/nsbatra/Epi_R_handbook'
#twitter-handle: 
#cover-image: images/R_Handbook_Logo.png
site: bookdown::bookdown_site
# output: bookdown::gitbook:
#      config:
#           sharing:
#                twitter: yes
#                facebook: yes
#                whatsapp: yes
#                github: yes
documentclass: book
---





#  {-}

```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "Epi R Handbook banner beige 1500x500.png"))
# library(knitr)
# opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<meta name="description" content="EpiRhandbook es un manual de referencia de R aplicado a la epidemiología y la salud pública. ">

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

<!-- <span style="color: red;">**THIS IS A DRAFT.  REVIEWERS GIVE FEEDBACK AT THIS [LINK](https://forms.gle/4RNdRRLGx67xW9yq9)**.</span> -->

<!-- <span style="color: darkgreen;">**DO YOU LIKE THIS HANDBOOK? SHOULD SOMETHING BE CHANGED? PLEASE TELL US!**</span> -->

<!-- <form target="_blank" action="https://forms.gle/A5SnRVws7tPD15Js9"> -->
<!--     <input type="submit" value="FEEDBACK" /> -->
<!-- </form> -->

<!-- ======================================================= -->
<!-- ## An R reference manual for applied epidemiology and public health {.unnumbered} -->


<!-- <span style="color: brown;">**The Epi R Handbook is an R reference manual for applied epidemiology and public health.**</span> -->

<!-- ## About this handbook   -->

## R para epidemiología aplicada y salud pública {-}  

**Utilización**: Este manual ha sido utilizado más de **1 millón de veces por 300.000 personas** en todo el mundo.


**Objetivo:** Servir como breve guía de referencia para escribir código en R (en línea y [**versión sin conexión**](#download-handbook-and-data) con ejemplos detallados que aborden problemas epidemiológicos.  

**¿Está empezando con R?** Pruebe nuestros **[tutoriales interactivos gratuitos](https://www.appliedepi.org/tutorial/)** o **[el curso de introducción](https://www.appliedepi.org/live/)** sincrónico, utilizado por los CDC de EE.UU., la OMS, y más de 75 agencias de salud y programas de formación de Epi de campo.  

<p><strong>Idiomas:</strong> <a href="https://epirhandbook.com/en/">Inglés</a>, <a href="https://epirhandbook.com/vn/">Vietnamita (Tiếng Việt)</a>, <a href="https://epirhandbook.com/tr/">Turco (Türkçe)</a></p>

<!-- * Use practical epi examples - cleaning case linelists, making transmission chains and epidemic curves, automated reports and dashboards, modeling incidence and making projections, demographic pyramids and rate standardization, record matching, outbreak detection, survey analysis, survival analysis, GIS basics, contact tracing, phylogenetic trees...   -->



<!-- **How is this different than other R books?**   -->

<!-- * It is community-driven - *written for epidemiologists by epidemiologists* in their spare time and leveraging experience in local, national, academic, and emergency settings   -->

<!-- Dual-column created based on the rmarkdown cookbook here: https://bookdown.org/yihui/rmarkdown-cookbook/multi-column.html -->



<br>


:::: {style="display: flex;"}

::: {}
```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "Applied_Epi_logo.png"))
```
:::


::: {.col data-latex="{0.05\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::

::: {}
<span style="color: black;">**Escrito y traducido por profesionales de la epidemiología, para profesionales de la epidemiología**</span>

Somos epi's de campo de todo el mundo, escribiendo en nuestro tiempo libre para ofrecer este recurso a la comunidad. Tu apoyo y comentarios son muy bienvenidos:

* Envía un email a **contact@appliedepi.org**, un tweet **[\@appliedepi](https://twitter.com/appliedepi)**, o en LinkedIn  
* Envía problemas a nuestro **[Repositorio Github](https://github.com/epirhandbook/Epi_R_handbook)**  


:::

::::

<form target="_blank" action="https://www.paypal.com/donate" method="post" target="_top">
<input type="hidden" name="hosted_button_id" value="YTEZELC8VBXV6" />
<input type="image" src="https://github.com/appliedepi/epiRhandbook_eng/raw/master/images/donate_button_long.png" border="0" name="submit" title="PayPal - The safer, easier way to pay online!" alt="Donate with PayPal button" />
<img alt="" border="0" src="https://www.paypal.com/en_US/i/scr/pixel.gif" />
</form>


<!-- ======================================================= -->
## Cómo utilizar este manual {#how-to-use-this-handbook .unnumbered} 

* Navega por las páginas del índice o utiliza el cuadro de búsqueda
* Clica en los iconos "Copy" para copiar el código  
* Puedes seguir paso a paso las lecciones utilizando nuestros [datos de ejemplo][Download handbook and data]  

**Versión sin conexión**  

Consulta las instrucciones en la página de [Descargar el Manual y los datos](#download-handbook-and-data).





<!-- ======================================================= -->
## Agradecimientos {#acknowledgements .unnumbered}  

Este manual ha sido elaborado mediante la colaboración de profesionales de la epidemiología de todo el mundo, basándonos en nuestra experiencia en organismos sanitarios locales, estatales, provinciales y nacionales, la Organización Mundial de la Salud (OMS), Médicos Sin Fronteras (MSF), sistemas hospitalarios e instituciones académicas.

Este manual **no** es un producto aprobado por ninguna organización específica. Aunque nos esforzamos por ser precisos, no ofrecemos ninguna garantía sobre el contenido de este libro.


### Colaboradores {-}  

**Redactor jefe:** [Neale Batra](https://www.linkedin.com/in/neale-batra/) 

**Equipo central del proyecto:** [Neale Batra](https://www.linkedin.com/in/neale-batra/), [Alex Spina](https://github.com/aspina7), [Amrish Baidjoe](https://twitter.com/Ammer_B), Pat Keating, [Henry Laurenson-Schafer](https://github.com/henryls1), [Finlay Campbell](https://github.com/finlaycampbell)  

**Autores**: [Neale Batra](https://www.linkedin.com/in/neale-batra/), [Alex Spina](https://github.com/aspina7), [Paula Blomquist](https://www.linkedin.com/in/paula-bianca-blomquist-53188186/), [Finlay Campbell](https://github.com/finlaycampbell), [Henry Laurenson-Schafer](https://github.com/henryls1), [Isaac Florence](www.Twitter.com/isaacatflorence), [Natalie Fischer](https://www.linkedin.com/in/nataliefischer211/), [Aminata Ndiaye](https://twitter.com/aminata_fadl), [Liza Coyer]( https://www.linkedin.com/in/liza-coyer-86022040/), [Jonathan Polonsky](https://twitter.com/jonny_polonsky), [Yurie Izawa](https://ch.linkedin.com/in/yurie-izawa-a1590319), [Chris Bailey](https://twitter.com/cbailey_58?lang=en), [Daniel Molling](https://www.linkedin.com/in/daniel-molling-4005716a/), [Isha Berry](https://twitter.com/ishaberry2), [Emma Buajitti](https://twitter.com/buajitti), [Mathilde Mousset](https://mathildemousset.wordpress.com/research/), [Sara Hollis](https://www.linkedin.com/in/saramhollis/), Wen Lin  

**Revisores**: Pat Keating, Annick Lenglet, Margot Charette, Danielly Xavier, Esther Kukielka, Michelle Sloan, Aybüke Koyuncu, Rachel Burke, Kate Kelsey, [Berhe Etsay](https://www.linkedin.com/in/berhe-etsay-5752b1154/), John Rossow, Mackenzie Zendt, James Wright, Laura Haskins, [Flavio Finger](ffinger.github.io), Tim Taylor, [Jae Hyoung Tim Lee](https://www.linkedin.com/in/jaehyoungtlee/), [Brianna Bradley](https://www.linkedin.com/in/brianna-bradley-bb8658155), [Wayne Enanoria](https://www.linkedin.com/in/wenanoria), Manual Albela Miranda, [Molly Mantus](https://www.linkedin.com/in/molly-mantus-174550150/), Pattama Ulrich, Joseph Timothy, Adam Vaughan, Olivia Varsaneux, Lionel Monteiro, Joao Muianga  

**Equipo de traducción al español**: [Juan Carlos Fernández-Merino](https://www.linkedin.com/in/juan-carlos-fernandez-merino-2596b522/), [Juan Francisco Monteagudo](https://www.linkedin.com/in/juan-francisco-monteagudo-pérez-05889064/), [Ximena Tolosa](https://www.linkedin.com/in/ximena-tolosa-miph-mae-phd/), [Luis Hernando Aguilar Ramirez](https://www.linkedin.com/in/luishernando/), [Ignacio Castro Aguirre](https://www.linkedin.com/in/ignacio-castro-aguirre-1b14ab41/), Esther Kukielka, [Cristina Torró](https://www.linkedin.com/in/cristina-torro-8aab0111b/), [Ana Fernández-Ayuso](https://www.linkedin.com/in/ana-fern%C3%A1ndez-ayuso-17b39b57/).


**Illustraciones**: Calder Fong  


<!-- **Editor-in-Chief:** Neale Batra  -->

<!-- **Project core team:** Neale Batra, Alex Spina, Amrish Baidjoe, Pat Keating, Henry Laurenson-Schafer, Finlay Campbell   -->

<!-- **Authors**: Neale Batra, Alex Spina, Paula Blomquist, Finlay Campbell, Henry Laurenson-Schafer, [Isaac Florence](www.Twitter.com/isaacatflorence), Natalie Fischer, Aminata Ndiaye, Liza Coyer, Jonathan Polonsky, Yurie Izawa, Chris Bailey, Daniel Molling, Isha Berry, Emma Buajitti, Mathilde Mousset, Sara Hollis, Wen Lin   -->

<!-- **Reviewers**: Pat Keating, Mathilde Mousset, Annick Lenglet, Margot Charette, Isha Berry, Paula Blomquist, Natalie Fischer, Daniely Xavier, Esther Kukielka, Michelle Sloan, Aybüke Koyuncu, Rachel Burke, Daniel Molling, Kate Kelsey, Berhe Etsay, John Rossow, Mackenzie Zendt, James Wright, Wayne Enanoria, Laura Haskins, Flavio Finger, Tim Taylor, Jae Hyoung Tim Lee, Brianna Bradley, Manual Albela Miranda, Molly Mantus, Priscilla Spencer, Pattama Ulrich, Joseph Timothy, Adam Vaughan, Olivia Varsaneux, Lionel Monteiro, Joao Muianga   -->


### Financiación y apoyo {-}  
Este libro ha sido principalmente un esfuerzo voluntario que ha requerido miles de horas de trabajo.  
El manual recibió apoyo financiaciero de [TEPHINET](https://www.tephinet.org/), la red mundial de Programas de Formación en Epidemiología de Campo (FETP) a través de una subvención para el desarrollo de capacidades de emergencia COVID-19.

La Red de Antiguos Alumnos de ([EAN](https://epietalumni.net/)) proporcionó apoyo administrativo, con un agradecimiento especial a Annika Wendland. EPIET es el Programa Europeo de Formación en Epidemiología de Intervención.

Un agradecimiento especial a Médicos Sin Fronteras (MSF) Centro Operativo de Ámsterdam (OCA) por su apoyo durante la elaboración de este manual.

*Esta publicación fue respaldada por el Acuerdo de Cooperación número NU2GGH001873, financiado por los Centros para el Control y la Prevención de Enfermedades a través de TEPHINET, un programa de The Task Force for Global Health. Su contenido es responsabilidad exclusiva de los autores y no representa necesariamente las opiniones oficiales de los Centros para el Control y la Prevención de Enfermedades, el Departamento de Salud y Servicios Humanos, The Task Force for Global Health, Inc. o TEPHINET.*



### Inspiración {-}  

Hay multitud de tutoriales y viñetas que aportaron conocimientos para el desarrollo del contenido del manual y se acreditan en sus respectivas páginas.

De manera más general, las siguientes fuentes han servido de inspiración para este manual:

[El proyecto "R4Epis"](https://r4epis.netlify.app/) (una colaboración entre MSF y RECON) 

[R Epidemics Consortium (RECON)](https://www.repidemicsconsortium.org/)  

[El libro R for Data Science (R4DS)](https://r4ds.had.co.nz/), en español en este [enlace](https://es.r4ds.hadley.nz/)

[bookdown: Creación de libros y documentos técnicos con R Markdown](https://bookdown.org/yihui/bookdown/)  

[Netlify](https://www.netlify.com) alberga este sitio web  


<!-- ### Image credits {-}   -->

<!-- Images in logo from US CDC Public Health Image Library) include [2013 Yemen looking for mosquito breeding sites](https://phil.cdc.gov/Details.aspx?pid=19623), [Ebola virus](https://phil.cdc.gov/Details.aspx?pid=23186), and [Survey in Rajasthan](https://phil.cdc.gov/Details.aspx?pid=19838).   -->


## Condiciones de uso y contribución {#terms-of-use-and-contribution .unnumbered}  

### Licencia {.unnumbered} 

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />Esta obra está bajo una <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Licencia Internacional Creative Commons Attribution-NonCommercial-ShareAlike 4.0</a>.


Los cursos académicos y los programas de formación en epidemiología pueden utilizar este manual con sus estudiantes. Si tienes preguntas sobre el uso que se le va a dar, envía un correo electrónico a **epiRhandbook@gmail.com**.

### Cita sugerida {.unnumbered}

Batra, Neale, et al. Manual de R para Epidemiología. 2021. <a rel="license" href="https://zenodo.org/badge/231610102.svg"><img alt="DOI" style="border-width:0" src="https://zenodo.org/badge/231610102.svg" /></a><br />

### Contribución {.unnumbered}  

Si quieres hacer una contribución de contenido, por favor, ponte en contacto con nosotros primero a través de Github o por correo electrónico. Estamos implementando un calendario de actualizaciones y estamos creando una guía para colaboradores.

Ten en cuenta que el proyecto epiRhandbook se publica con un [Código de Conducta del Colaborador](https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html) . Al contribuir a este proyecto, te comprometes a respetar sus términos.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:index.Rmd-->

# (PART) Acerca de este libro {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_about_book.Rmd-->

# Notas editoriales y técnicas {#editorial-and-technical-notes}

En esta página describimos la filosofía, el estilo y las decisiones editoriales elegidas para la elaboración de este manual.


## Enfoque y estilo {#approach-and-style}

El público potencial de este libro es amplio. Seguramente será utilizado tanto por personas muy noveles con R, como por usuarios experimentados buscando los mejores consejos y prácticas. Por lo tanto, este debe ser accesible y conciso a la vez. Por ello, nuestro enfoque fue proporcionar *la información suficiente* para que alguien muy nuevo en R pueda aplicar y seguir el código.

Otros puntos:

* Se trata de un libro de referencia de códigos acompañado de ejemplos relativamente breves, *no* de un libro de texto completo sobre R o ciencia de datos
* Este es un *manual de R* para su uso dentro de la epidemiología aplicada - no un manual sobre los métodos o ciencia de la epidemiología aplicada
* Se trata de un documento vivo: los paquetes de R óptimos para una tarea determinada cambian a menudo, por lo que agradecemos que exista debate sobre cuáles son los más empleados en el manual


### Paquetes de R {.unnumbered}

**Muchas opciones**

Uno de los aspectos más difíciles de aprender en R es saber qué paquete utilizar para una tarea determinada. Es muy común pelearse con una tarea para luego darse cuenta de que ¡hay un paquete de R que hace todo eso en una línea de código!.

En este manual, tratamos de ofrecerte al menos dos maneras de completar cada tarea: un método probado y comprobado (probablemente en R **base** o **tidyverse**) y un paquete especial de R que está hecho a medida para ese propósito. Queremos que tengas un par de opciones en caso de que no puedas descargar un paquete determinado o de que éste no te funcione.

A la hora de elegir los paquetes a utilizar, hemos dado prioridad a los paquetes y enfoques de R que han sido probados y aprobados por la comunidad, que minimizan el número de paquetes utilizados en una sesión de trabajo típica, que son estables (no cambian con frecuencia) y que realizan la tarea de forma sencilla y limpia.

En general, este manual da prioridad a los paquetes y funciones de R de **tidyverse**. Tidyverse es una colección de paquetes de R diseñados para ciencia de datos que comparten la gramática y estructuras de datos subyacentes. Todos los paquetes tidyverse pueden instalarse o cargarse a través del paquete **tidyverse**. Más información en el [sitio web de tidyverse](https://www.tidyverse.org/).

Cuando es aplicable, también ofrecemos opciones de código usando R **base** - los paquetes y funciones que vienen con R en la instalación. Esto se debe a que somos conscientes de que parte de la audiencia de este libro podría no tener una buena conexión a internet para descargar paquetes adicionales. 

**Vinculación explícita de las funciones a los paquetes**

Es frustrante cuando en algunos tutoriales de R, se muestra una función (en código), pero no se sabe bien de qué paquete es. En este libro intentamos evitar esta situación.

En el texto explicativo, los nombres de los paquetes se escriben en negrita (por ejemplo, **dplyr**) y las funciones se escriben así: `mutate()`. Nos esforzaremos en dejar claro el paquete del que proviene una función, ya sea haciendo referencia al paquete en el texto o especificando el paquete en el código mediante esta sintaxis: `dplyr::mutate()`. Puede parecer redundante, pero lo hacemos a propósito.

Consulta la página sobre [fundamentos de R](#r-basics) para saber más sobre los paquetes y las funciones.


### Código de estilo {.unnumbered}

En el manual, utilizamos con frecuencia "líneas nuevas", haciendo que nuestro código parezca "largo". Lo hacemos por varias razones:

* De esta forma se pueden escribir comentarios explicativos con `#`, los cuales están situados adyacentes a cada línea de código 
* En general, el código más largo (en vertical) es más fácil de leer 
* Es más fácil de leer en una pantalla estrecha (no es necesario desplazarse lateralmente) 
* Con las sangrías, puede ser más fácil saber qué argumentos pertenecen a cada función

Como resultado, el código que *podría* estar escrito:
 

```{r, eval=F}
linelist %>% 
  group_by(hospital) %>%  # filas agrupadas por hospital
  slice_max(date, n = 1, with_ties = F) # si hay un empate (de fecha), tomar la primera fila
```

...pero se escribe así:

```{r, eval=F}
linelist %>% 
  group_by(hospital) %>% # group rows by hospital
  slice_max(
    date,                # mantener la fila por grupo con el valor máximo de la fecha 
    n = 1,               # mantener sólo la fila más alta
    with_ties = F)       # si hay un empate (de fecha), tomar la primera fila
```

El código de R generalmente no se ve afectado por nuevas líneas o sangrías. Al escribir el código, si se inicia una nueva línea después de una coma, se aplicarán patrones de sangría automáticos.

También utilizamos muchos espacios (por ejemplo, `n = 1` en lugar de `n=1`) porque es más fácil de leer. ¡Sé amable con la gente que lee tu código!



### Nomenclatura {.unnumbered}  

En este manual, generalmente hacemos referencia a "columnas" y "filas" en lugar de "variables" y "observaciones". Como se explica en este manual sobre ["datos ordenados"](https://tidyr.tidyverse.org/articles/tidy-data.html), la mayoría de los conjuntos de datos estadísticos epidemiológicos se componen estructuralmente de filas, columnas y valores.

*Las variables contienen* los valores que miden el mismo atributo subyacente (como el grupo de edad, el resultado o la fecha de inicio). Las *observaciones contienen* todos los valores medidos en la misma unidad (por ejemplo, una persona, un lugar o una muestra de laboratorio). Por lo tanto, estos aspectos pueden ser más difíciles de definir de forma tangible.

En los conjuntos de datos "ordenados", cada columna es una variable, cada fila es una observación y cada celda es un único valor. Sin embargo, algunos conjuntos de datos que se encuentran no se ajustan a este molde: unos datos de formato "amplio" puede tener una variable dividida en varias columnas (véase un ejemplo en la página [Pivotar datos](#pivoting-data)). Del mismo modo, las observaciones pueden estar divididas en varias filas.

La mayor parte de este manual trata sobre la gestión y la transformación de datos, por lo que las referencias a las estructuras de datos concretas de filas y columnas son más relevantes que las observaciones y las variables más abstractas. Las excepciones se dan sobre todo en las páginas sobre análisis de datos, en las que verás más referencias a las variables y las observaciones.


### Nota {.unnumbered} 

Here are the types of notes you may encounter in the handbook:  

<span style="color: black;">**_NOTA:_** Esto es una nota</span>  
<span style="color: darkgreen;">**_CONSEJO:_** Esto es un consejo.</span>  
<span style="color: orange;">**_PRECAUCIÓN:_** Esto es una nota de precaución.</span>  
<span style="color: red;">**_PELIGRO_** Esto es un aviso (warning).</span>  


## Decisiones editoriales {#editorial-decisions}

A continuación, hacemos un seguimiento de las decisiones editoriales importantes en torno a la elección de paquetes y funciones. Si no estás de acuerdo o quieres ofrecer una nueva herramienta para que la consideremos, únete o inicia una conversación en nuestra [página de Github](https://github.com/appliedepi/epirhandbook_eng).

**Tabla de paquetes, funciones y otras decisiones editoriales** 


Asunto            |     Considerado     |   Resultado            |    Breve explicación 
----------------- | --------------------|------------------------|-----------------------------------------------
Enfoque general de codificación|**tidyverse**, **data.table**, **base**|**tidyverse**, con una página sobre **data.table**, y menciones de alternativas de R **base** para los lectores sin internet|legibilidad de **tidyverse**, universalidad, más enseñado
Carga de paquetes|`library()`,`install.packages()`, `require()`, **pacman**|**pacman**|Acorta y simplifica el código para la mayoría de los casos de instalación/carga de paquetes múltiples
Importación y exportación|**rio**, muchos otros paquetes|**rio**|Facilidad para muchos tipos de archivos
Agrupación para las estadísticas de síntesis|**dplyr** `group_by()`, **stats** `aggregate()`|**dplyr** `group_by()`|Consecuente con el énfasis en **tidyverse**
Pivotar tablas|**tidyr** (funciones de pivote), **reshape2** (melt/cast), **tidyr** (spread/gather)|**tidyr** (funciones de pivote)|**reshape2** se ha retirado, **tidyr** utiliza funciones pivot a partir de la v1.0.0
Limpiar los nombres de las columnas|**linelist**, **janitor**|**janitor**|Se hace hincapié en la consolidación de los paquetes
Semanas epidemiológicas. Epiweeks |**lubridate**, **aweek**, **tsibble**, **zoo**|Normalmente **lubridate**, los otros para casos específicos|La flexibilidad, la coherencia y las perspectivas de mantenimiento de los paquetes de **lubridate**
Etiquetas ggplot|`labs()`, `ggtitle()`/`ylab()`/`xlab()` |`labs()` |Todas las etiquetas en un solo lugar, la simplicidad  
Convertir en factor |`factor()`, **forcats**|**forcats**|Sus diversas funciones también se convierten en factor en el mismo comando
Curvas epidémicas|**incidence**, **ggplot2**, **EpiCurve**|**incidence2** por rapidez`, **ggplot2** para tareas detalladas|fiabilidad
Concatenación|`paste()`, `paste0()`, `str_glue()`, `glue()`|`str_glue()`|Sintaxis más sencilla que las funciones de pegado; dentro de **stringr**


## Revisiones importantes  {#major-revisions}


Fecha          |Cambios mayores       
---------------| ------------------------------------------    
10 Mayo 2021    |Lanzamiento de la versión 1.0.0    


## Información de la sesión (R, RStudio, paquetes) {#session-info-r-rstudio-packages}

A continuación se presenta la información sobre las versiones de R, RStudio y los paquetes de R utilizados en esta versión del Manual.


```{r}
sessioninfo::session_info()
```




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/editorial_style.Rmd-->

# Descargando el manual y los datos {#download-handbook-and-data}


<!-- Note to self: If you want to create a download link to Github, right-click the "View Raw" button on Github, copy the address, and use that in the HTML below. -->




## Descargar el manual sin conexión {#download-offline-handbook}

Puedes descargar la versión sin conexión de este manual. Éste es un archivo HTML que puedes ver en tu navegador web sin acceder a Internet. Si estás pensando en utilizar este manual sin conexión, debes tener en cuenta algunas cosas:

* Al abrir el archivo, las imágenes y el índice pueden tardar uno o  dos minutos en cargarse.
* Este manual tiene un diseño ligeramente diferente: una página muy larga con el índice a la izquierda. Para buscar términos específicos utiliza Ctrl+f (Cmd-f)
* Consulta la página de [Paquetes recomendados](#suggested-packages-1) para ayudarte a instalar los paquetes de R adecuados antes de que pierdas la conectividad a Internet
* Instala nuestro paquete R **epirhandbook** que contiene todos los datos del ejemplo (el proceso de instalación se describe a continuación)

**Hay dos maneras de descargar el manual:**


### Utilizando el enlace de descarga {.unnumbered}  

Para acceder rápidamente, **clica con el botón derecho** [en este enlace](https://github.com/appliedepi/epirhandbook_eng/raw/master/offline_long/Epi_R_Handbook_offline.html) **y selecciona "Guardar enlace como"**.

Si es un Mac, utiliza Cmd+clic. Si es un móvil, mantén clicado el enlace y selecciona "Guardar enlace". El manual se descargará en el dispositivo. Si aparece una pantalla con código HTML sin procesar, asegúrate de haber seguido las instrucciones anteriores o prueba la opción 2.


### Utilizando nuestro paquete R {.unnumbered}  

Ofrecemos un paquete llamado **epirhandbook**. Este incluye la función `download_book()` que descarga el archivo del manual desde nuestro repositorio de Github a tu ordenador.

Este paquete también contiene una función `get_data()` que descarga todos los datos del ejemplo en tu ordenador.

Ejecuta el siguiente código para instalar nuestro paquete R **epirhandbook** desde el [repositorio de Github *appliedepi*](https://github.com/appliedepi/epirhandbook). Este paquete no está en CRAN, así que utiliza la función especial `p_install_gh()` para instalarlo desde Github.


```{r, eval=F}
# instala la última versión del paquete Epi R Handbook
pacman::p_install_gh("appliedepi/epirhandbook")
```

Ahora, puedes cargar el paquete para utilizarlo en la sesión actual de R:

```{r, eval=F}
# carga el paquete para utilizarlo
pacman::p_load(epirhandbook)
```

A continuación, ejecuta la función del paquete `download_book()` (con los paréntesis vacíos) para descargar el manual en tu ordenador. Suponiendo que estés en RStudio, aparecerá una ventana que te permitirá seleccionar una ubicación para guardarlo.

```{r, eval=F}
# descarga el manual offline en tu computadora
download_book()
```
## Descarga los datos para seguir el manual {#download-data-to-follow-along}

Para "seguir" las páginas del manual, puedes descargar los datos y los resultados de los ejemplos.

### Utiliza nuestro paquete para R {.unnumbered}  

El método más sencillo para descargar todos los datos es instalar nuestro paquete **epirhandbook**. Contiene una función `get_data()` que guarda todos los datos del ejemplo en una carpeta de tu elección en tu ordenador.

Para instalar nuestro paquete **epirhandbook**, ejecuta el siguiente código. Este paquete no está en CRAN, así que utiliza la función `p_install_gh()` para instalarlo. La entrada hace referencia a nuestra organización de Github ("*appliedepi*") y al paquete **epirhandbook**.

**epirhandbook** package.  

```{r, eval=F}
# instala la última versión del paquete Epi R Handbook
pacman::p_install_gh("appliedepi/epirhandbook")
```


Ahora, carga el paquete para utilizarlo en tu sesión actual de R: 

```{r, eval=F}
# carga el paquete para utilizarlo
pacman::p_load(epirhandbook)
```

A continuación, utiliza la función `get_data()` del paquete para descargar los datos de ejemplo en tu ordenador. Ejecuta get_data("all") para obtener *todos los datos de ejemplo*, o escribe un nombre de archivo específico y una extensión entre comillas para recuperar sólo un archivo.

Los datos ya se han descargado con el paquete, y sólo hay que transferirlos a una carpeta del ordenador. Aparecerá una ventana emergente para seleccionar la ubicación de la carpeta de almacenamiento. Te sugerimos que crees una nueva carpeta de "datos", ya que hay unos 30 archivos (incluidos los datos de ejemplo y los resultados de ejemplo).

```{r, eval=F}
# descarga todos los datos de ejemplos en tu computadora
get_data("all")

# descarga solo los datos del ejemplo listado limpio
get_data(file = "linelist_cleaned.rds")

```


```{r, eval=F}
# descarga solo un archivo específoco en tu computadora
get_data("linelist_cleaned.rds")
```

Una vez que hayas utilizado `get_data()` para guardar un archivo en tu ordenador, tendrás que importarlo a R. Consulta la página de [importación y exportación](#import-and-export) para más detalles.

Si lo deseas, puedes revisar todos los datos utilizados en este manual en la **[carpeta "data"](https://github.com/appliedepi/epirhandbook_eng/tree/master/data)** de nuestro repositorio de Github.



### Descargar uno por uno {.unnumbered}  

Esta opción implica la descarga de los datos archivo por archivo desde nuestro repositorio de Github a través de un enlace o un comando de R específico para el archivo. Algunos tipos de archivos permiten un botón de descarga, mientras que otros pueden descargarse mediante un comando de R.


#### Linelist {.unnumbered}

Se trata de un brote de ébola ficticio, ampliado por el equipo del manual a partir de los datos `ebola_sim` de las prácticas del paquete **Outbreaks**.

* [Clica para descargar Linelist (con los casos) "en bruto" -raw- (.xlsx)](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_raw.xlsx). Este listado de casos "en bruto" es una hoja de cálculo de Excel con datos desordenados. Utilízala para seguir la página de [limpieza de datos y funciones básicas](#cleaning-data-and-core-functions).

* [Clica para descargar](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds) Linelist "en limpio". Utiliza este archivo para todas las demás páginas de este manual que utilizan el listado de casos. Un archivo .rds es un tipo de archivo específico de R que conserva los tipos de columnas. Esto asegura que sólo tendrás que hacer una limpieza mínima después de importar los datos a R.


*Otros archivos relacionados:*

* [Clica para descargar](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.xlsx) el listado de casos "en limpio" como archivo Excel.

* Parte de la página de limpieza utiliza un "diccionario de limpieza" (archivo .csv). Puedes cargarlo directamente en R ejecutando los siguientes comandos:

```{r, eval=F}
pacman::p_load(rio) # install/load the rio package

# importa el archivo directamente desde Github
cleaning_dict <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/cleaning_dict.csv")
```


#### Recuento de datos de malaria {#data_malaria .unnumbered}  

Estos datos son recuentos ficticios de casos de malaria por grupos de edad, centro y día. Un archivo .rds es un tipo de archivo específico de R que conserva los tipos de columnas. Esto asegura que sólo tendrás que hacer una limpieza mínima después de importar los datos a R.

[Clica para descargar los datos del recuento de casos de malaria (archivo .rds) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/malaria_facility_count_data.rds)


#### Datos en escala Likert {.unnumbered}  

Se trata de datos ficticios de una encuesta tipo Likert, utilizados en la página sobre [Pirámides de población y escalas de Likert](#demographic-pyramids-and-likert-scales). Puedes cargar estos datos directamente en R ejecutando los siguientes comandos:

```{r, eval=F}
pacman::p_load(rio) # instala/carga el paquete rio

# importa el fichero directamente de Github
likert_data <- import("https://raw.githubusercontent.com/appliedepi/epirhandbook_eng/master/data/likert_data.csv")
```


#### Flexdashboard {.unnumbered}  

A continuación se encuentran los enlaces al archivo asociado a la página sobre [Dashboards con R Markdown](#dashboards-with-r-markdown):

* Para descargar el código de R Markdown para el dashboard (panel de control) del brote, clica con el botón derecho en este [enlace](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/flexdashboard/outbreak_dashboard.Rmd) (Cmd+clic para Mac) y luego "Guardar enlace como".

* Para descargar el código HTML del dashboard, clica con el botón derecho en este [enlace](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/flexdashboard/outbreak_dashboard_test.html) (Cmd+clic para Mac) y luego "Guardar enlace como".

#### Rastreo de contactos {.unnumbered} 

La página de [rastreo de contactos](#contact-tracing-1) muestra el análisis de los datos de rastreo de contactos, utilizando como ejemplo datos de [Go.Data](https://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting). Los datos utilizados en la página pueden descargarse como archivos .rds clicando en los siguientes enlaces:

[Clica para descargar los datos de la investigación de casos (archivo .rds) ](https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/cases_clean.rds?raw=true)  

[Clica para descargar los datos de registro de los contactos (archivo .rds) ](https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/contacts_clean.rds?raw=true)  

[Clica para descargar los datos de seguimiento de los contactos (archivo .rds) ](https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/followups_clean.rds?raw=true) 


<span style="color: black;">**_NOTA:_** Los datos de rastreo de contactos estructurados de otro software (por ejemplo, KoBo, DHIS2 Tracker, CommCare) pueden tener un aspecto diferente. Si quieres contribuir con una muestra de datos alternativos o con contenido para esta página, [por favor, ponte en contacto, con nosotros](#contact_us).</span> 

<span style="color: darkgreen;">**_CONSEJO:_** Si estás utilizando Go.Data y quieres conectarte a tu instancia de la API, consulta la página de importación y exportación [(sección API)](#import_api) y la [Comunidad de Prácticas de Go.Data](https://community-godata.who.int/).</span>


#### SIG (GIS) {.unnumbered}  

Los archivos geográficos tipo shapefile tienen varios archivos subcomponentes, cada uno con una extensión de archivo diferente. Un archivo tendrá la extensión ".shp", pero otros tienen la extensión ".dbf", ".prj", etc.

La página de [Conceptos básicos de los SIG](#gis-basics) contiene enlaces al sitio web de *Humanitarian Data Exchange*, donde se pueden descargar los shapefiles directamente como archivos comprimidos.

Por ejemplo, los datos de los puntos de las instalaciones sanitarias se pueden descargar de [aquí](https://data.humdata.org/dataset/hotosm_sierra_leone_health_facilities). Descarga "hotosm_sierra_leone_health_facilities_points_shp.zip". Una vez guardado en tu ordenador, "descomprime" la carpeta. Ahí vas a encontrar varios archivos con diferentes extensiones (por ejemplo, ".shp", ".prj", ".shx") todos ellos deben guardarse en la misma carpeta. A continuación, para importar en R, proporciona la ruta completa y el nombre del archivo ".shp" a `st_read()` del paquete **sf** (como se describe en la página de [Conceptos básicos de los SIG](#gis-basics)).

Si sigues la opción 1 para descargar todos los datos de ejemplo (a través de nuestro paquete **epirhandbook**), todos los shapefiles están incluidos en el paquete.

También puedes descargar los shapefiles de la carpeta "data" de R Handbook Github (véase la subcarpeta "gis"). Sin embargo, ten en cuenta que tendrás que descargar *cada* subfichero individualmente en tu ordenador. En Github, clica en cada archivo individualmente y descárgalo clicando en el botón "Download". A continuación, puedes ver cómo el shapefile "sle_adm3" consta de muchos archivos, cada uno de los cuales tendría que ser descargado de Github.

```{r out.height = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "download_shp.png"))
```


#### Árboles filogenéticos {.unnumbered}  

Mira la página sobre [árboles filogenéticos](#phylogenetic-trees-1). El archivo Newick con el árbol filogenético construido a partir de la secuenciación del genoma completo de 299 muestras de Shigella sonnei y los datos de las muestras correspondientes (convertidos en un archivo de texto). Las muestras belgas y los datos resultantes han sido proporcionados amablemente por el NRC belga para Salmonella y Shigella en el marco de un proyecto dirigido por un fellow del programa ECDC EUPHEM, y también se publicarán en un manuscrito. Los datos internacionales están disponibles en bases de datos públicas (ncbi) y han sido publicados previamente.

* Para descargar el archivo del árbol filogenético "Shigella_tree.txt", clica con el botón derecho en este [enlace](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/phylo/Shigella_tree.txt) (Cmd+clic para Mac) y selecciona "Guardar enlace como".

* Para descargar el archivo "sample_data_Shigella_tree.csv" con información adicional sobre cada muestra, clica con el botón derecho en este [enlace](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/phylo/sample_data_Shigella_tree.csv) (Cmd+clic para Mac) y selecciona "Guardar enlace como".

* Para ver el nuevo árbol de subconjuntos creado, clica con el botón  derecho en este [enlace](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/phylo/Shigella_subtree_2.txt)  (Cmd+clic para Mac) y selecciona "Guardar enlace como". El archivo  .txt se descargará en tu ordenador.

Tras la descarga, se pueden importar los archivos .txt con la función `read.tree()` del paquete **ape**, como se explica en la página.

```{r, eval=F}
ape::read.tree("Shigella_tree.txt")
```


#### Estandarización {.unnumbered}  

Consulta la página de [tasas estandarizadas](#standardised-rates). Puedes cargar los datos directamente desde nuestro repositorio de Github en Internet en tu sesión de R con los siguientes comandos:


```{r, eval=F}
# instala/carga el paquete rio
pacman::p_load(rio) 

##############
# Country A
##############
# importa demographics del país A directamente desde Github
A_demo <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/country_demographics.csv")

# importa defunciones del país A directamente desde Github
A_deaths <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/deaths_countryA.csv")

##############
# Country B
##############
# importa demographics del país B directamente desde Github
B_demo <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/country_demographics_2.csv")

# importa defunciones del país B directamente desde Github
B_deaths <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/deaths_countryB.csv")


##########################
# Población de referencia#
##########################
# importa demographics del país B directamente desde Github
standard_pop_data <- import("https://github.com/appliedepi/epirhandbook_eng/raw/master/data/standardization/world_standard_population_by_sex.csv")
```



#### Series temporales y detección de brotes {#data_outbreak .unnumbered}  

Véase la página sobre [series temporales y detección de brotes](#time-series-and-outbreak-detection). Utilizamos los casos de campylobacter notificados en Alemania entre 2002 y 2011, disponibles en el paquete R **surveillance**. (*nb.* este conjunto de datos ha sido adaptado del original, en el sentido de que se han eliminado 3 meses de datos de finales de 2011 para fines de demostración).

[Clica para descargar Campylobacter en Alemania (.xlsx) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/campylobacter_germany.xlsx)

También utilizamos datos climáticos de Alemania de 2002 a 2011 (temperatura en grados centígrados y lluvia caída en milímetros). Estos datos se descargaron de los datos del reanálisis por satélite Copernicus de la UE utilizando el paquete **ecmwfr**. Tendrás que descargarlos todos e importarlos con `stars::read_stars()` como se explica en la página de series temporales.

[Clica para descargar el tiempo de Alemania 2002 (archivo .nc) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2002.nc)

[Clica para descargar el tiempo de Alemania 2003 (archivo .nc) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2003.nc)

[Clica para descargar el tiempo en Alemania 2004 (archivo .nc) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2004.nc)

[Clica para descargar el tiempo en Alemania 2005 (archivo .nc) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2005.nc)

[Clica para descargar el tiempo en Alemania 2006 (archivo .nc) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2006.nc)

[Clica para descargar el tiempo de Alemania 2007 (archivo .nc) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2007.nc)

[Clica para descargar el tiempo de Alemania 2008 (archivo .nc) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2008.nc)

[Clica para descargar el tiempo en Alemania 2009 (archivo .nc) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2009.nc)

[Clica para descargar el tiempo en Alemania 2010 (archivo .nc) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2010.nc)

[Clica para descargar el tiempo en Alemania 2011 (archivo .nc) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/time_series/weather/germany_weather2011.nc)


#### Análisis de encuestas {#data_survey .unnumbered}  

Para el capítulo sobre [análisis de encuestas](#survey-analysis) utilizamos datos ficticios de encuestas de mortalidad basados en las plantillas de encuestas de MSF OCA. Estos datos ficticios se generaron como parte del [proyecto "R4Epis"](https://r4epis.netlify.app/).

[Clica para descargar los datos de la encuesta ficticia (.xlsx) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/surveys/survey_data.xlsx)

[Clica para descargar el diccionario de datos de la encuesta ficticia (.xlsx) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/surveys/survey_dict.xlsx)

[Clica para descargar los datos de la población de la encuesta ficticia (.xlsx) ](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/surveys/population.xlsx)


#### Shiny {#data_shiny .unnumbered}  

El capítulo sobre [Dashboards con Shiny](#dashboards-with-shiny) demuestra la construcción de una sencilla aplicación para mostrar datos sobre la malaria.

Para descargar los archivos R que producen la aplicación Shiny:

Puedes [clicar aquí para descargar el archivo app.R que contiene tanto la interfaz de usuario como el código del servidor para la aplicación Shiny.](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/malaria_app/app.R)

Puedes [clicar aquí para descargar el archivo facility_count_data.rds](https://github.com/appliedepi/epirhandbook_eng/blob/master/data/malaria_app/data/facility_count_data.rds) que contiene datos sobre la malaria para la aplicación Shiny. Ten en cuenta que puede ser necesario almacenarlo dentro de una carpeta "data" para que las rutas de los archivos here() funcionen correctamente.

Puedes [clicar aquí para descargar el archivo global.R](https://github.com/appliedepi/epirhandbook_eng/blob/master/data/malaria_app/global.R) que debe ejecutarse antes de que se abra la aplicación, como se explica en dicho capítulo.

Puedes [clicar aquí para descargar el archivo plot_epicurve.R](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/malaria_app/funcs/plot_epicurve.R) que es originado por global.R. Ten en cuenta que puede necesitar almacenarlo dentro de una carpeta "funcs" para que las rutas de los archivos here() funcionen correctamente.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/data_used.Rmd-->

# (PART) Aspectos básicos {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_basics.Rmd-->

# Fundamentos de R {#r-basics}

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "basics_header_close.png"))
```

Bienvenido.

Esta página repasa los aspectos esenciales de R. No pretende ser un tutorial exhaustivo, pero proporciona los fundamentos y puede ser útil para refrescar la memoria. La sección de [Recursos para el aprendizaje](#learning) enlaza con tutoriales más completos.

Partes de esta página han sido adaptadas con permiso del [proyecto R4Epis](https://r4epis.netlify.app/).

Consulta la página sobre [Transición a R](#transition-to-r) para obtener consejos sobre cómo cambiar a R desde STATA, SAS o Excel.

```{r, echo=F}
# importar linelist_leaned con los datos de ebola ya limpios
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
pacman::p_load(apyramid)
```




<!-- ======================================================= -->
## ¿Por qué utilizar R? {#why-use-r}

Como se indica en el [sitio web de R project](https://www.r-project.org/about.html), éste es un lenguaje de programación y un entorno para la computación estadística y gráficos. Es muy versátil, ampliable y dirigido por la comunidad.

**Coste**

El uso de R es gratuito. Hay una fuerte ética en la comunidad de material libre y de código abierto.

**Reproducibilidad**

La gestión y el análisis de los datos a través de un lenguaje de programación (en comparación con Excel u otra herramienta principalmente manual) mejora **la reproducibilidad**, facilita la **detección de errores** y alivia la carga de trabajo.

**Comunidad**

La comunidad de usuarios de R es enorme y colaborativa. Cada día se desarrollan nuevos paquetes y herramientas para abordar problemas cotidianos, que son examinados por la comunidad de usuarios. Por ejemplo, [R-Ladies](https://rladies.org/) es una asociación mundial cuya misión es promover la diversidad de género en la comunidad de R, siendo una de las mayores asociaciones de usuarios de R. Es probable que tengas un grupo cerca.


## Términos clave {#key-terms}

**RStudio** - RStudio es una interfaz gráfica de usuario (GUI) para facilitar el uso de **R.** Lee más en la sección [RStudio](#rstudio).

**Objetos** - Todo lo que se almacena en R - conjuntos de datos, variables, una lista de nombres de pueblos, un número total de población, incluso resultados como gráficos - son *objetos* a los que se les *asigna un nombre* y *pueden ser referenciados* en comandos posteriores. Lee más en la sección [Objetos](#objects).

**Funciones** - Una función es una operación de código que acepta entradas y devuelve una salida transformada. Lee más en la sección [Funciones](#functions).

**Paquetes** - Un paquete de R es un conjunto de funciones que se pueden compartir. Lee más en la sección [Paquetes](#packages).

**Scripts** - Un script es el archivo de documento que contiene tus comandos. Lee más en la sección [Scripts](#scripts)


## Recursos para aprender {#learning}  

### Recursos en RStudio {.unnumbered} 

**Documentación de ayuda**

Busca en la pestaña "Help" de RStudio la documentación sobre los paquetes de R y funciones específicas. Esto está dentro del panel que también contiene Archivos, Gráficos y Paquetes (normalmente en el panel inferior derecho). También puedes escribir el nombre de un paquete o función en la consola de R después de un signo de interrogación para abrir la página de ayuda correspondiente. No incluyas paréntesis.

Por ejemplo: ?filter o ?diagrammeR.

**Tutoriales interactivos**

Hay varias formas de aprender R de forma interactiva *dentro de* RStudio.

El propio RStudio ofrece un Tutorial que se encuentra en el paquete de R [**[learnr]{.underline}**](https://blog.rstudio.com/2020/02/25/rstudio-1-3-integrated-tutorials/). Simplemente instala este paquete y abre un tutorial a través de la nueva pestaña "Tutorial" en el panel superior derecho de RStudio (que también contiene las pestañas Environment e History).

El paquete de R [**swirl**](https://swirlstats.com/) ofrece cursos interactivos en la consola de R. Instala y carga este paquete, luego ejecuta el comando `swirl()` (paréntesis vacío) en la consola de R. Verá que aparecen indicaciones en la consola. Responde escribiendo en la consola. Te guiará a través de un curso de tu elección.


### Hojas de referencia {.unnumbered}

Hay muchas "hojas de referencias o trucos" (Cheatsheets) en PDF disponibles en el [sitio web de RStudio](https://rstudio.com/resources/cheatsheets/), por ejemplo:

* Factores con el paquete **forcats**
* Fechas y horarios con el paquete **lubridate**
* Cadenas con el paquete **stringr**
* Operaciones iterativas con el paquete **purrr**
* Importación de datos
* Transformación de datos con el paquete **dplyr**
* **R Markdown** (para crear documentos como PDF, Word, Powerpoint...)
* **Shiny** (para crear aplicaciones web interactivas)
* Visualización de datos con el paquete **ggplot2**
* Cartografía (GIS)
* Mapas interactivos con el paquete **leaflet**
* Python con R (paquete **reticulate**)

En este enlace puedes encontrar un recurso en línea, específicamente para los [usuarios de Excel](https://jules32.github.io/r-for-excel-users/)


### Twitter {.unnumbered}

R tiene una vibrante comunidad en Twitter en la que puedes aprender trucos, atajos y noticias: sigue estas cuentas:

* Síguenos! [@epiRhandbook](https://twitter.com/epirhandbook)
* R Function A Day [@rfuntionaday](https://twitter.com/rfunctionaday) (Es un recurso *increíble)*
* R para ciencia de datos [@rstats4ds](https://twitter.com/rstats4ds?lang=en)
* RStudio [@RStudio](https://twitter.com/rstudio?lang=en)
* Trucos de RStudio [@rstudiotips](https://twitter.com/rstudiotips)
* R-Bloggers [@Rbloggers](https://twitter.com/Rbloggers)
* R-ladies [@RLadiesGlobal](https://twitter.com/RLadiesGlobal)
* Hadley Wickham [@hadleywickham](https://twitter.com/hadleywickham?ref_src=twsrc^google|twcamp^serp|twgr^author)

También:

**#epitwitter** y **#rstats**



### Recursos gratuitos en línea {.unnumbered}

Un texto definitivo es el libro [R for Data Science](https://r4ds.had.co.nz/) de Garrett Grolemund y Hadley Wickham

El sitio web del proyecto [R4Epis](https://r4epis.netlify.app/) tiene como objetivo "desarrollar herramientas estandarizadas de limpieza de datos, análisis y elaboración de informes para cubrir los tipos comunes de brotes y estudios realizados en la población en un entorno de respuesta de emergencia de MSF". Se pueden encontrar materiales de formación sobre los fundamentos de R, plantillas para informes de RMarkdown sobre brotes y encuestas, y tutoriales para ayudar a configurarlos.



### Idiomas distintos del inglés {.unnumbered}  

[Materiales de RStudio en Español](https://www.rstudio.com/collections/espanol/)

[R for Data Science en español](https://es.r4ds.hadley.nz/)

[Introduction à R et au tidyverse (Francais)](https://juba.github.io/tidyverse/index.html)  


<!-- ======================================================= -->
## Instalación {#Installation}

### R y  RStudio {.unnumbered}  

**Cómo instalar R**

Visita este sitio web [https://www.r-project.org/](https://www.r-project.org/) y descarga la última versión de R adecuada a tu ordenador.

**Cómo instalar RStudio**

Visita este sitio web [https://rstudio.com/products/rstudio/download/](https://rstudio.com/products/rstudio/download/) y descarga la última versión gratuita de RStudio para escritorio adecuada para tu ordenador.

**Permisos **

Ten en cuenta que debes instalar R y RStudio en una unidad donde tengas permisos de lectura y escritura. De lo contrario, la capacidad para instalar paquetes de R (algo frecuente) se verá afectada. Si tienes problemas, intenta abrir RStudio con el botón derecho en el icono y seleccionando "Ejecutar como administrador". Puedes encontrar otros consejos en la página [R en unidades de red](#r-on-network-drives).

**Cómo actualizar R y RStudio**

Tu versión de R se muestra al inicio de la consola de R. También puede ejecutar `sessionInfo()`.

Para actualizar R, puedes ir al sitio web mencionado anteriormente y vuelva a instalar R. También puede utilizar el paquete **installr** (en Windows) ejecutando `installr::updateR()`. Esto abrirá cuadros de diálogo para ayudarle a descargar la última versión de R y actualizar sus paquetes a la nueva versión de R. Puedes encontrar más detalles en la [documentación](https://www.r-project.org/nosvn/pandoc/installr.html) de **installr**.

Ten en cuenta que la versión antigua de R seguirá existiendo en tu ordenador. Puedes ejecutar temporalmente una versión anterior (una "instalación" más antigua) de R clicando en "Herramientas" -> "Opciones globales" en RStudio y eligiendo una versión de R. Esto puede ser útil si quieres utilizar un paquete que no ha sido actualizado para funcionar en la versión más reciente de R.

Para actualizar RStudio, puede ir a la página web anterior y volver a descargar RStudio. Otra opción es clicar en "Ayuda" -> "Buscar actualizaciones" dentro de RStudio, pero esto puede no mostrar las últimas actualizaciones.

Para ver qué versiones de R, RStudio o paquetes se utilizaron cuando se hizo este Manual, consulta la página de [Notas técnicas y editoriales](#editorial-and-technical-notes).


### Otros programas que *puedes* necesitar instalar {.unnumbered} 

* TinyTeX (*para compilar un documento RMarkdown en PDF*)
* Pandoc (*para compilar documentos RMarkdown*)
* RTools (*para construir paquetes para R*)
* phantomjs (*para guardar imágenes fijas de redes animadas, como cadenas de transmisión*)


#### TinyTex {.unnumbered}  

TinyTex es una distribución LaTeX personalizada, útil cuando se trata de producir PDFs desde R.\ Ver [https://yihui.org/tinytex/](https://yihui.org/tinytex/) para más información.

Para instalar TinyTex desde R:

```{r, eval=F}
install.packages('tinytex')
tinytex::install_tinytex()
# para desinstalar TinyTeX, ejecuta tinytex::uninstall_tinytex()
```


#### Pandoc {.unnumbered}

Pandoc es un conversor de documentos, un software separado de R. **Viene incluido con RStudio y no debería ser necesario descargarlo.** Ayuda en el proceso de conversión de documentos Rmarkdown a formatos como pdf y añade funcionalidades complejas.


#### RTools {.unnumbered}  

RTools es una colección de software para construir paquetes para R

Se instala desde este sitio web:[https://cran.r-project.org/bin/windows/Rtools/](https://cran.r-project.org/bin/windows/Rtools/)  


#### phantomjs {.unnumbered}  

Esto se utiliza a menudo para hacer "capturas de pantalla" de las páginas web. Por ejemplo, cuando se hace una cadena de transmisión con el paquete **epicontacts**, se produce un archivo HTML que es interactivo y dinámico. Si deseas una imagen estática, puede ser útil utilizar el paquete [**[webshot**](https://wch.github.io/webshot/articles/intro.html) para automatizar este proceso. Para ello se necesita el programa externo "phantomjs". Puedes instalar phantomjs a través del paquete **webshot** con el comando `webshot::install_phantomjs()`.


<!-- ======================================================= -->
## RStudio {#rstudio}

### Orientación de RStudio {.unnumbered}  

**Primero, abre RStudio.** Como sus iconos pueden ser muy similares, asegúrate de que estás abriendo *RStudio* y no R.

Para que RStudio funcione, también debes tener instalado R en el ordenador (consulta las instrucciones de instalación más arriba).

**RStudio** es una interfaz (GUI) para facilitar el uso de **R**. Puedes pensar que R es el motor de un vehículo, que hace el trabajo crucial, y RStudio es la carrocería del vehículo (con asientos, accesorios, etc.) que te ayuda a usar el motor para avanzar. Puedes ver la hoja de trucos completa de la interfaz de usuario de RStudio (PDF) [aquí](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf)

Por defecto, RStudio muestra cuatro paneles rectangulares.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "RStudio_overview.png"))
```


<span style="color: black;">**_CONSEJO:_** Si tu RStudio sólo muestra un panel izquierdo es porque aún no tiene ningún script abierto.</span>


**El panel Source**

Este panel de código Fuente u origen, por defecto en la parte superior izquierda, es un espacio para editar, ejecutar y guardar tus [scripts](#scripts). Los scripts contienen los comandos que desea ejecutar. Este panel también puede mostrar conjuntos de datos (data frames) para su visualización.

Para los usuarios de Stata, este panel es similar a las ventanas de Do-file y del Editor de Datos.

**El panel Console**

La consola de R es el hogar del "motor" de R es, por defecto,  el panel izquierdo o inferior izquierdo en R Studio. Aquí es donde se ejecutan realmente los comandos y aparecen las salidas no gráficas y los mensajes de error/advertencia. Puedes introducir y ejecutar directamente comandos en la Consola de R, pero ten en cuenta que estos comandos no se guardan como cuando se ejecutan comandos desde un script.

Si estás familiarizado con Stata, la consola de R es como la ventana de comandos y también la ventana de resultados.

**El panel Environment**

Este panel de Entorno, por defecto en la parte superior derecha, se utiliza más a menudo para ver breves resúmenes de los [objetos](#objects) en el Entorno R en la sesión actual. Estos objetos pueden incluir conjuntos de datos importados, modificados o creados, parámetros que hayas definido (por ejemplo, una semana epi específica para el análisis), o vectores o listas que hayas definido durante el análisis (por ejemplo, nombres de regiones). Puedes clicar en la flecha situada junto al nombre de un dataframe para ver sus variables.

En Stata, esto es muy similar a la ventana del Gestor de Variables.

Este panel también contiene *History* donde puede ver los comandos ejecutados anteriormente. También tiene una pestaña "Tutorial" donde puedes completar tutoriales interactivos de R si tienes el paquete **learnr** instalado. También tiene una pestaña de "Conexiones" para las conexiones externas, y puede tener un panel "Git" si decides interactuar con Github.

**Panel Files, Plots, Packages, Help, Viewer** Este panel inferior derecho incluye varias pestañas importantes. La pestaña Files (Archivos) permite navegar por las carpetas y puede utilizarse para abrir o eliminar archivos. En la pestaña Plots (Gráficos), se mostrarán todos los gráficos, incluyendo los mapas. Las salidas interactivas o HTML se mostrarán en la pestaña Viewer (Visor). El panel Packages (Paquetes) permite ver, instalar, actualizar, eliminar, cargar/descargar paquetes de R y ver qué versión del paquete tiene. En la [sección de paquetes](#packages) más abajo se puede aprender más sobre los paquetes. Por último, en el panel de Ayuda (Help) se mostrará la documentación y los archivos de ayuda.

Este panel contiene los equivalentes en Stata de las ventanas Plots Manager y Project Manager.

### Configuración de RStudio {.unnumbered}

Cambia la configuración y la apariencia de RStudio en el menú desplegable *Tools (Herramientas)*, seleccionando *Global Options (Opciones globales)*. Allí puede cambiar la configuración por defecto, incluyendo la apariencia/color de fondo.


```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "RStudio_tools_options_1.png"))

knitr::include_graphics(here::here("images", "RStudio_tools_options.png"))
```

**Reiniciar**

Si se cuelga R, se puede reiniciar yendo al menú Sesión y clicando en "Restart R" (Reiniciar R)". Esto evita la molestia de cerrar y abrir RStudio. Al hacer esto, se eliminará todo el entorno de esa sesión de R.

### Atajos de teclado {.unnumbered}

Algunos atajos de teclado muy útiles están abajo. Se pueden ver todos los atajos de teclado para Windows, Max y Linux en la segunda página de esta [hoja de trucos de la interfaz de usuario](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf) de RStudio.


Windows/Linux     |Mac             |Acción                
------------------|----------------|-----------------------------------------
Esc               |Esc             |Interrumpir el comando actual (útil si accidentalmente ejecutó un comando incompleto y no puede evitar ver "+" en la consola de R)
Ctrl+s            |Cmd+s           |Guardar (script)
Tab               |Tab             |Autocompletar
Ctrl + Enter      |Cmd + Enter     |Ejecutar la(s) línea(s) de código actual(es)
Ctrl + Mayús + C  |Cmd + Shift + c |Comentar/descomentar las líneas resaltadas
Alt + *           |Opción + *      |Insertar <-
Ctrl + Shift + m  |Cmd + Shift + m |Insertar %>%
Ctrl + l          |Cmd + l         |Limpiar la consola de R
Ctrl + Alt + b    |Cmd + Opción + b|Ejecutar desde el inicio hasta la línea actual
Ctrl + Alt + t    |Cmd + Opción + t|Ejecutar la sección de código actual (R Markdown)
Ctrl + Alt + i    |Cmd + Shift + r |Insertar un trozo (chunk) de código (en R Markdown)
Ctrl + Alt + c    |Cmd + Opción + c|Ejecutar el código chunk actual (R Markdown)
flechas arriba/abajo en la consola R     |En el mismo |Recorrer los comandos ejecutados recientemente
Shift + flechas arriba/abajo en el script|En el mismo |Seleccionar varias líneas de código
Ctrl + f          |Cmd + f         |Buscar y reemplazar en el script actual
Ctrl + Mayús + f  |Cmd + Shift + f |Buscar en archivos (buscar/reemplazar en muchos scripts)
Alt + l           |Cmd + Opción + l|Plegar el código seleccionado
Shift + Alt + l   |Cmd + Shift + Opción+l   Desplegar el código seleccionado

<span style="color: darkgreen;">**_CONSEJO:_** Utiliza la tecla Tab cuando escribas para activar la función de autocompletar de RStudio. Esto puede evitar errores de ortografía. Pulsa el tabulador mientras escribes para que aparezca un menú desplegable de posibles funciones y objetos, basándose en lo que escrito hasta ese momento.</span>  


<!-- ======================================================= -->
## Funciones {#functions}  

Las funciones son la pieza principal en el uso de R. Las funciones son la forma de realizar tareas y operaciones. Muchas vienen instaladas con R, mientras muchas otras están disponibles para su descarga en *paquetes* (explicados en la sección de [paquetes](#packages)), ¡E incluso, puedes escribir tus propias funciones personalizadas.

Esta sección básica sobre las funciones explica:

* Qué es una función y cómo funciona
* Qué son los *argumentos* de una función
* Cómo obtener ayuda para entender una función

*Una nota rápida sobre la sintaxis:* En este manual, las funciones se escriben en código-texto con paréntesis abiertos, así: `filter()`. Como se explica en la sección de [paquetes](#packages), las funciones se descargan dentro de *los paquetes*. En este manual, los nombres de los paquetes se escriben en **negrita**, como **dplyr**. A veces en el código de ejemplo puede ver el nombre de la función vinculado explícitamente al nombre de su paquete con dos dos puntos (::) como: `dplyr::filter()`. El propósito de esta vinculación se explica en la sección de paquetes.


<!-- ======================================================= -->
### Funciones simples {.unnumbered}  

**Una función es como una máquina que recibe entradas, realiza alguna acción con esas entradas y produce una salida.** El resultado depende de la función.

**Las funciones suelen operar sobre algún objeto colocado dentro de los paréntesis de la función**. Por ejemplo, la función `sqrt()` calcula la raíz cuadrada de un número:


```{r basics_function_sqrt}
sqrt(49)
```

El objeto proporcionado a una función también puede ser una columna de unos datos (véase la sección [Objetos](#objects) para conocer todos los tipos de objetos). Dado que R puede almacenar múltiples conjuntos de datos, tendrá que especificar tanto el set de datos como la columna. Una forma de hacerlo es utilizar la notación `$` para vincular el nombre de los datos y el nombre de la columna (`dataset$column`). En el siguiente ejemplo, la función summary() se aplica a la columna numérica `age` en los datos `linelist`, y la salida es un resumen de los valores numéricos y faltantes de la columna.


```{r basics_functions_summary}
# Muestra estadísticas resumen de la columna 'age' del dataset 'linelist'
summary(linelist$age)
```

<span style="color: black;">**_NOTA:_** Entre bastidores, una función representa un código adicional complejo que ha sido envuelto para el usuario en un comando sencillo.</span>



<!-- ======================================================= -->
### Functions with multiple arguments {.unnumbered} 

Las funciones suelen pedir varias entradas, llamadas ***argumentos***, situadas dentro del paréntesis de la función, normalmente separadas por comas.

* Algunos argumentos son necesarios para que la función funcione correctamente, mientras otros son opcionales
* Los argumentos opcionales tienen una configuración por defecto
* Los argumentos pueden tomar caracteres, números, lógica (TRUE/FALSE) y otras entradas

He aquí una divertida función ficticia, llamada `oven_bake()`, como ejemplo de una función típica (hacer en el horno). Toma un objeto de entrada ("input") (por ejemplo, una base de datos o en este ejemplo "masa") y realiza operaciones en él según lo especificado por los argumentos adicionales (`minutes = ` y `temperature = `). La salida ("output") puede ir a la consola, o guardarse como un objeto utilizando el operador de asignación `<-`.

```{r basics_functions_image, echo=F, out.width = "75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Function_Bread_Example.png"))
```


**En un ejemplo más realista**, el comando `age_pyramid()` que aparece a continuación produce un gráfico de pirámide de edad basado en grupos de edad definidos y una columna de división binaria, como `gender`. La función recibe tres argumentos dentro de los paréntesis, separados por comas. Los valores suministrados a los argumentos establecen `linelist` como los datos (dataframe) a utilizar, `age_cat5` como la columna a contar, y `gender` como la columna binaria a utilizar para dividir la pirámide por color según género.


```{r basics_functions_arguments, include=FALSE, results='hide', message=FALSE, warning=FALSE,}
## crea una variable de grupos de edad (age group especificando los saltos de cada grupo
linelist$age_group <- cut(linelist$age, breaks = c(0, 5, 10, 15, 20, 30, 45, 60))
```

```{r message=FALSE, warning=FALSE,  out.width = "75%", out.height="75%"}
# Crea una pirámide de edad 
age_pyramid(data = linelist, age_group = "age_cat5", split_by = "gender")
```

El comando anterior puede escribirse de forma equivalente a la de más abajo, con un estilo más largo con una nueva línea para cada argumento. Este estilo puede ser más fácil de leer, y más fácil de escribir "comentarios" con `#` para explicar cada segmento de código (¡comentar en el código es considerado una buena práctica!). Para ejecutar este comando más largo puedes seleccionar todo el texto y clicar en "Ejecutar", o simplemente colocar el cursor en la primera línea y luego clicar las teclas Ctrl y Enter simultáneamente.

```{r message=FALSE, warning=FALSE,  out.width = "75%", out.height="75%"}
# Create an age pyramid
age_pyramid(
  data = linelist,        # usa los datos de linelist
  age_group = "age_cat5", # especifica la columna para los grupos de edad
  split_by = "gender"     # usa la columna gender para los dos lados de la pirámide
  )
```

No es necesario especificar la primera mitad de una asignación de argumentos (por ejemplo, `data =`) si los argumentos se escriben en su orden específico (especificado en la documentación de la función). El código siguiente produce exactamente la misma pirámide que la anterior, porque la función espera ese orden de los argumentos: conjunto de datos, la variable para `age_group, y la variable para `split_by` .

```{r, basics_functions_pyramid2, eval = FALSE, warning=FALSE, message=FALSE, , out.width = "75%", out.height="75%", eval=F}
# Esta orden produce exactamente el mismo gráfico que la anterior
age_pyramid(linelist, "age_cat5", "gender")
```

**Un comando `age_pyramid()` más complejo podría incluir argumentos *opcionales* para:**

* Mostrar proporciones en lugar de recuentos (estableciendo `proportional = TRUE` cuando el valor por defecto es `FALSE`)
* Especificar los dos colores a utilizar (`pal = ` es la abreviatura de "paleta" y se suministra con un vector de dos nombres de color. Para saber cómo se hace un vector con la función `c()` puedes consultar la página de [objetos](#objectstructure) .

<span style="color: black;">**_NOTA:_** En los argumentos en los que se especifican ambas partes del argumento (por ejemplo, `proporcional = TRUE`), no importa el orden de estos argumentos.</span>


```{r message=FALSE, warning=FALSE, out.width = "75%", out.height="75%"}
age_pyramid(
  linelist,                    # usa los casos de linelist
  "age_cat5",                  # columna de los grupos de edad
  "gender",                    # dividido por género
  proportional = TRUE,         # porcentajes en vez de números absolutos
  pal = c("orange", "purple")  # colores
  )
```



<!-- ======================================================= -->

### Escribir funciones {.unnumbered}  

R es un lenguaje orientado a las funciones, por lo que deberías sentirte capacitado para escribir tus propias funciones. La creación de funciones aporta varias ventajas:

* Facilitar la programación modular, es decir, la separación del código en partes independientes y manejables.
* Sustituye el repetitivo copiar y pegar, que puede dar lugar a errores
* Dar a las piezas de código nombres fáciles de recordar

En la página [Escribir funciones](#writing-functions-1) se trata en profundidad cómo escribir funciones.


<!-- A function is given a name and defined with the assignment operator `<-` to a special **base** R function called `function()`. Within the parentheses, the arguments that the function will accept are defined. This is followed by curly brackets `{ }`, within which the actual code of the function is written.     -->

```{r, eval=F, echo=F}
my_function <- function( ARGUMENTS HERE ){ CODE HERE }
```

<!-- The arguments should be provided in the syntax `argument = default`, separated by commas.   -->

<!-- Here is an example where we create a function `staff_calc()` to serve as a staffing calculator for COVID-19 case investigation and contact tracing calls.   -->

<!-- The arguments (inputs) and their default values will be:   -->

<!-- * `daily_cases = NULL` The number of new COVID-19 cases per day   -->
<!-- * `contacts_each = 5` The number contacts enumerated for each case   -->
<!-- * `time_case = 0.5`  Number of hours to complete a case investigaton by phone   -->
<!-- * `time_contact = 0.25`  Number of hours to complete a contact follow-up by phone   -->
<!-- * `time_day = 8` The number of hours one staff works per day   -->

<!-- Below, the function is created. The code ends with the special function `return()`, which is what the function produces.    -->

<!-- ```{r message=FALSE, warning=FALSE, out.width = "75%", out.height="75%"} -->
<!-- staff_calc <- function(daily_cases = NULL, contacts_each = 5, -->
<!--                        time_case = 0.5, time_contact = 0.25, time_day = 8){ -->

<!--   # Define total daily hours for calling cases -->
<!--   case_hours <- daily_cases * time_case  -->

<!--   # Define total daily hours for calling contacts -->
<!--   contact_hours <- daily_cases * contacts_each * time_contact -->

<!--   # Calculate number of staff required -->
<!--   staff_required <- (case_hours + contact_hours)/time_day -->

<!--   return(staff_required) -->
<!-- } -->
<!-- ``` -->

<!-- Once this code is run, the function will be defined and will appear in the R Environment. We can run the function. Below all the default values are used and the `daily_cases = ` is set to 150.   -->

```{r eval=F, echo=F, message=FALSE, warning=FALSE, out.width = "75%", out.height="75%"}
staff_calc(daily_cases = 150)
```

```{r, eval=F, echo=F}
case_incidence <- tibble(
  dates = seq.Date(from = as.Date("2020-05-01"), to = as.Date("2020-05-21"), by = 1),
  projected_incidence = c(102,110,50,37,106,190,146,138,135,111,60,43,189,184,185,80,44,97,254,291,288),
  staff_needed = staff_calc(projected_incidence)
)

ggplot(case_incidence, aes(x = dates))+
  geom_line(aes(y = projected_incidence))+
  geom_line(aes(y = staff_needed))
```

<!-- There are many other nuances to understand when writing functions, as discussed in the page [Writing functions].   -->


<!-- ======================================================= -->
<!-- ======================================================= -->
## Paquetes {#packages}  

**Los paquetes contienen funciones.**  

Un paquete de R es un conjunto de código y documentación que se puede compartir y que contiene funciones predefinidas. Los usuarios de la comunidad R desarrollan paquetes todo el tiempo atendiendo a problemas específicos, ¡es probable que alguno pueda ayudarte en tu trabajo! En tu uso de R instalarás y utilizarás cientos de paquetes.

En la instalación, R contiene paquetes **"base"** y funciones que realizan tareas elementales comunes. Pero muchos usuarios de R crean funciones especializadas, que son verificadas por la comunidad de R y que puedes descargar como **paquete** para tu propio uso. En este manual, los nombres de los paquetes se escriben en **negrita**. Uno de los aspectos más desafiantes de R es que a menudo hay muchas funciones o paquetes donde elegir para una tarea determinada.


### Instalar y cargar {.unnumbered}  

*Las funciones* están contenidas en **paquetes** que pueden descargarse ("instalarse") en tu ordenador desde Internet. Una vez descargado un paquete, se almacena en tu "librería". Puedes acceder a las funciones que contiene durante una sesión de R "cargando" el paquete. 

*Piensa en R como tu librería personal*: Cuando se descarga un paquete, tu librería adquiere un nuevo libro de funciones, pero cada vez que quieras utilizar una función de ese libro, debes tomar prestado ("cargar") ese libro de tu librería.

En resumen: para utilizar las funciones disponibles en un paquete de R, hay que realizar dos pasos:

1.  El paquete debe ser **instalado** (una vez), *y*
2.  El paquete debe ser **cargado** (cada sesión de R)


#### Tu librería {.unnumbered}  

Tu "librería" es en realidad una carpeta en tu ordenador, que contiene una carpeta para cada paquete que se ha instalado. Averigua dónde está instalado R en tu ordenador, y busca una carpeta llamada "win-library". Por ejemplo: R\win-library\4.0 (la 4.0 es la versión de R - tendrá una librería diferente para cada versión de R que haya descargado).

Puedes imprimir la ruta del archivo de tu librería introduciendo .libPaths() (paréntesis vacíos). Esto resulta especialmente importante si se trabaja con [R en unidades de red](#r-on-network-drives).


#### Instalar desde CRAN {.unnumbered}  

Lo habitual es que los usuarios de R descarguen paquetes de CRAN. CRAN (Comprehensive R Archive Network) es un almacén público online de paquetes de R que han sido publicados por los miembros de la comunidad R.

¿Te preocupan los virus y la seguridad al descargar un paquete de CRAN? Lee [este artículo](https://support.rstudio.com/hc/en-us/articles/360042593974-R-and-R-Package-Security) sobre el tema.
 

#### Cómo instalar y cargar {.unnumbered}  

En este manual, sugerimos utilizar el paquete **pacman** (abreviatura de "packages manager"). Ofrece una interesante función `p_load()` que instalará un paquete si es necesario *y* lo cargará para su uso en la sesión actual de R.

La sintaxis es bastante sencilla. Sólo hay que listar los nombres de los paquetes dentro de los paréntesis de `p_load()`, separados por comas. Este comando instalará los paquetes **rio**, **tidyverse** y **here** si aún no están instalados, y los cargará para su uso. Esto hace que el enfoque de `p_load()` sea conveniente y conciso si se comparten scripts con otros. Ten en cuenta que los nombres de los paquetes distinguen entre mayúsculas y minúsculas.


```{r}
# Instala (si es necesario) y carga los paquetes a utilizar
pacman::p_load(rio, tidyverse, here)
```

Fíjate que hemos utilizado la sintaxis `pacman::p_load()` que escribe explícitamente el nombre del paquete (**pacman**) antes del nombre de la función (`p_load()`), conectado por dos dos puntos `::`. Esta sintaxis es útil porque también carga el paquete **pacman** (suponiendo que ya esté instalado).

Hay funciones de R **base** alternativas que verás a menudo. La función de R **base** para instalar un paquete es `install.packages()`. El nombre del paquete a instalar debe proporcionarse entre paréntesis *entre comillas*. Si deseas instalar varios paquetes en un solo comando, deben ser listados dentro de un vector de caracteres `c()`.

Nota: este comando *instala* un paquete, pero *no* lo carga para utilizarlo en la sesión actual.

```{r, eval=F}
# instala un solo paquete con R base
install.packages("tidyverse")

# instala múltiples paquetes con R base
install.packages(c("tidyverse", "rio", "here"))
```

La instalación también se puede realizar clicando en el panel "Packages" de RStudio, luego en "Install" y buscando el nombre del paquete deseado.

La función alternativa en para **cargar** un paquete (después de haberlo instalado) es `library()`. Sólo puedes cargar un paquete a la vez (otra razón para usar `p_load()`). Se puede escribir el nombre del paquete con o sin comillas.

```{r, eval=F}
# carga los paqutes para usarlos, con R base
library(tidyverse)
library(rio)
library(here)
```

Para comprobar si un paquete está instalado y/o cargado, puedes mirar en la pestaña de Packages de RStudio. Si el paquete está instalado, se muestra allí con el número de versión. Si su casilla está marcada, está cargado para la sesión actual.


**Instalar desde Github**

A veces, necesitas instalar un paquete que aún no está disponible en CRAN. O tal vez el paquete está disponible en CRAN pero quieres la *versión de desarrollo* con nuevas características que aún no se ofrecen en la versión más estable publicada en CRAN. Éstas suelen estar alojadas en el sitio web [github.com](https://github.com/) en un "repositorio" de código gratuito y de acceso público. Lee más sobre Github en la página del manual sobre [Control de versiones y colaboración con Git y Github](#version-control-and-collaboration-with-).

Para descargar los paquetes de R desde Github, puedes utilizar la función `p_load_gh()` de **pacman**, que instalará el paquete si es necesario, y lo cargará para utilizarlo en tu sesión actual de R. Las alternativas de instalación incluyen el uso de los paquetes **remotes** o **devtools**. Puedes leer más sobre todas las funciones de **pacman** en la [documentación del paquete](https://cran.r-project.org/web/packages/pacman/pacman.pdf).

Para instalar desde Github, tienes que proporcionar más información. Debe proporcionar:

1.  El ID de Github del propietario del repositorio
2.  El nombre del repositorio que contiene el paquete
3.  *(opcional) El nombre de la "rama" (versión de desarrollo específica) que quieras descargar*

En los ejemplos siguientes, la primera palabra entre comillas es el ID de Github del propietario del repositorio, después de la barra es el nombre del repositorio (el nombre del paquete).

```{r, eval=F}
# instalar/cargar el paquete epicontacts desde su repositorio de Github
p_load_gh("reconhub/epicontacts")
```

Si quieres instalar desde una "rama" (versión) distinta de la rama principal, añade el nombre de la rama tras una "@", después del nombre del repositorio.  

```{r, eval=F}
# instala el paquete epicontacts de la rama "timeline" desde Github
p_load_gh("reconhub/epicontacts@timeline")
```

Si no hay diferencia entre la versión de Github y la versión en tu ordenador, no se realizará ninguna acción. Puedes "forzar" una reinstalación usando `p_load_current_gh()` con el argumento `update = TRUE`. Puedes leer más sobre **pacman** en esta [viñeta online](http://trinker.github.io/pacman/vignettes/Introduction_to_pacman.html)


**Instalar desde ZIP o TAR**

Puedes instalar el paquete desde una URL:

```{r, eval=F}
packageurl <- "https://cran.r-project.org/src/contrib/Archive/dsr/dsr_0.2.2.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
```

O bien, descargarlo en tu ordenador en un archivo comprimido:

Opción 1: utilizar `install_local()` del paquete **remotes**

```{r, eval=F}
remotes::install_local("~/Downloads/dplyr-master.zip")
```

Opción 2: utilizando `install.packages()` desde , proporcionando la ruta del archivo ZIP y estableciendo `type = "source"` y `repos = NULL`.

```{r, eval=F}
install.packages("~/Downloads/dplyr-master.zip", repos=NULL, type="source")
```


### Sintaxis del código {.unnumbered}  

Para mayor claridad en este manual, las funciones van a veces precedidas por el nombre de su paquete utilizando el símbolo `::` de la siguiente manera: `nombre_del_paquete::nombre_de_la_función()`

Una vez cargado un paquete para una sesión, este estilo explícito no es necesario. Se puede utilizar simplemente `nombre_de_la_funcion()`. Sin embargo, escribir el nombre del paquete es útil cuando el nombre de una función es común y puede existir en varios paquetes (por ejemplo, `plot()`). Escribir el nombre del paquete también cargará el paquete si no está todavía cargado.

```{r eval=FALSE}
# Este comando usa el paquete "rio" y su función "import()" para importar un dataset
linelist <- rio::import("linelist.xlsx", which = "Sheet1")
```



### Ayuda a la función {.unnumbered} 

Para leer más sobre una función, se puede buscar en la pestaña Ayuda de la parte inferior derecha de RStudio. También se puede ejecutar un comando como `?nombre_de_la_funcion` (escribe el nombre de la función después de un signo de cerrar interrogación) y aparecerá la página de ayuda en la pestaña de ayuda. Por último, intenta buscar otros recursos en Internet.



### Actualizar paquetes {.unnumbered}

Puedes actualizar los paquetes reinstalándolos. También puedes clicar en el botón verde "Update" en la pestaña de paquetes de RStudio para ver qué paquetes tienen nuevas versiones para instalar. Ten en cuenta que tu código antiguo puede necesitar ser actualizado si hay una revisión importante en el funcionamiento de una función.

### Eliminar paquetes {.unnumbered}

Utiliza `p_delete()` de **pacman**, o `remove.packages()` de **utils**. Alternativamente, puedes buscar la carpeta que lo contiene en tu librería y borrarla manualmente.



### Dependencias {.unnumbered}  

Los paquetes a menudo dependen de otros paquetes para funcionar. Estos se llaman dependencias. Si una dependencia no está instalada, entonces el paquete que depende de ella también puede no instalarse.

Se pueden ver las dependencias de un paquete con `p_depends()`, y ver qué paquetes dependen de él con `p_depends_reverse()`



### Funciones enmascaradas {.unnumbered}  

No es raro que dos o más paquetes contengan el mismo nombre de función. Por ejemplo, el paquete **dplyr** tiene una función `filter()`, pero también la tiene el paquete **stats**. La función `filter()` por defecto depende del orden en que estos paquetes se cargan por primera vez en la sesión de R - el último será el predeterminado para el comando `filter()`.

Puedes comprobar el orden en la Pestaña de Entorno de R Studio - clica en el desplegable de "Global Evironment" (Entorno Global) y mira el orden de los paquetes. Las funciones de los paquetes *más bajos* en esa lista desplegable enmascararán las funciones del mismo nombre en los paquetes que aparecen más arriba en la lista desplegable. Cuando se carga por primera vez un paquete, R advertirá en la consola si se está produciendo el enmascaramiento, pero esto puede ser fácil de pasar por alto.  

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "masking_functions.png"))
```

Aquí hay formas de arreglar el enmascaramiento:

1.  Especifique el nombre del paquete en el comando. Por ejemplo, utiliza `dplyr::filter()`
2.  Reordena el orden de carga de los paquetes (por ejemplo, dentro de `p_load()`), e **iniciar una nueva sesión de R**



### Descargar {.unnumbered}  

Para descargar (detach / unload) un paquete, utiliza este comando, con el nombre correcto del paquete y sólo dos puntos. Ten en cuenta que esto puede no resolver el enmascaramiento.

```{r, eval=F}
detach(package:PACKAGE_NAME_HERE, unload=TRUE)
```


### Instalar una versión anterior {.unnumbered}  

Consulta esta [guía](https://support.rstudio.com/hc/en-us/articles/219949047-Installing-older-versions-of-packages) para instalar una versión anterior de un paquete concreto.


### Paquetes recomendados {.unnumbered}

Consulta la página de [Paquetes recomendados](#suggested-packages-1) para obtener una lista de paquetes que recomendamos para la epidemiología del día a día.







<!-- ======================================================= -->
## Scripts {#scripts}

Los scripts son una parte fundamental de la programación. Son documentos que contienen comandos (por ejemplo, funciones para crear y modificar datos, imprimir visualizaciones, etc.). Puedes guardar un script y volver a ejecutarlo más tarde. Almacenar y ejecutar tus comandos desde un script tiene muchas ventajas (frente a teclear los comandos uno a uno en la "línea de comandos" de la consola de R):

* Portabilidad: puedes compartir tu trabajo con otros enviándoles tus scripts
* Reproducibilidad: para que tú y los demás sepan exactamente lo que ha hecho
* Control de versiones: para que puedas hacer un seguimiento de los cambios realizados por ti mismo o por tus colegas
* Comentarios/anotaciones: para explicar a tus compañeros lo que has hecho

### Comentarios {.unnumbered}  

En un script también puede anotar ("comentar") alrededor de su código R. Los comentarios son útiles para explicarte a sí mismo y a otros lectores lo que está haciendo. Puedes añadir un comentario escribiendo el símbolo de almohadilla (#) y escribiendo el comentario después de él. El texto comentado aparecerá en un color diferente al del código R.

Cualquier código escrito después de la # no se ejecutará. Por lo tanto, colocar un # antes del código es también una forma útil de bloquear temporalmente una línea de código ("comentar") si no quieres borrarla). Puedes comentar varias líneas a la vez resaltándolas y clicando Ctrl+Mayús+c (Cmd+Mayús+c en Mac).


```{r, eval = F}
# Un comentario puede ser una línea en sí mismo
# importar datos
linelist <- import("linelist_raw.xlsx") %>%   # un comentariotambién puede venir después del  código
# filter(age > 50)                          # También se puede utilizar para desactivar / quitar una línea de código
  count()

```

* Comenta *lo que* haces *y **por qué** lo haces*.
* Divide tu código en secciones lógicas
* Acompaña tu código con un texto describiendo paso a paso lo que está haciendo (por ejemplo, pasos numerados)

### Estilo {.unnumbered}

It is important to be conscious of your coding style - especially if working on a team. We advocate for the **tidyverse** [style guide](https://style.tidyverse.org/). There are also packages such as **styler** and **lintr** which help you conform to this style.  

A few very basic points to make your code readable to others:  
  * When naming objects, use only lowercase letters, numbers, and underscores `_`, e.g. `my_data`  
  * Use frequent spaces, including around operators, e.g. `n = 1` and `age_new <- age_old + 3`  


### Ejemplo de Script {.unnumbered}  

A continuación se muestra un ejemplo de un breve script de R. ¡Recuerda!, cuanto mejor expliques brevemente el código con los comentarios, ¡más gustará a tus colegas!

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "example_script.png"))
```

<!-- ======================================================= -->
### R markdown {.unnumbered}

Un script de R markdown es un tipo de script de R en el que el propio script *se convierte en* un documento de salida (PDF, Word, HTML, Powerpoint, etc.). Se trata de herramientas increíblemente útiles y versátiles que suelen utilizarse para crear informes dinámicos y automatizados. ¡Hasta este sitio web y este manual se han hecho con scripts de R markdown!

Vale la pena señalar que los usuarios principiantes de R también pueden utilizar R Markdown - ¡no te dejes intimidar! Para saber más, consulta el capítulo del manual sobre [Informes con R Markdown](#reports-with-r-markdown).


<!-- ======================================================= -->
### R notebooks {.unnumbered}

No hay ninguna diferencia entre escribir en Rmarkdown o R notebook. Sin embargo, la ejecución del documento difiere ligeramente. Consulta este [sitio](http://uc-r.github.io/r_notebook) para obtener más detalles.


<!-- ======================================================= -->
### Shiny {.unnumbered}

Las aplicaciones/sitios web de Shiny están contenidos en un script, que debe llamarse `app.R`. Este archivo tiene tres componentes:

1.  Una interfaz de usuario (ui)
2.  Una función de servidor
3.  Una llamada a la función `shinyApp`

Consulta la página del manual sobre [Dashboards con Shiny](#dashboards-with-shiny), o este [Tutorial de Shiny](https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/)

*Hace tiempo, el archivo anterior se dividía en dos archivos (`ui.R` y `server.R`)*


### Plegar código {.unnumbered}  

Puedes contraer porciones de código para facilitar la lectura del script.

Para ello, crea una cabecera de texto con #, escribe tu cabecera y sigue con al menos 4 guiones (-), almohadillas (#) o iguales (=). Cuando hayas hecho esto, aparecerá una pequeña flecha en el "margen" de la izquierda (junto al número de fila). Puedes clicar en esta flecha y en el código de abajo hasta que la siguiente cabecera se pliegue y aparezca un icono de flecha doble en su lugar.

Para expandir el código, clica de nuevo en la flecha del margen o en el icono de la flecha doble. También hay atajos de teclado como se explica en la [sección de RStudio](#rstudio) de esta página.

Al crear cabeceras con #, también activarás el índice de contenidos en la parte inferior del script (véase más abajo) que puedes utilizar para navegar por el script. Se pueden crear subcabeceras añadiendo más símbolos #, por ejemplo # para las primarias, ## para las secundarias y ### para las terciarias.

A continuación se muestran dos versiones de un script de ejemplo. A la izquierda está el original con cabeceras comentadas. A la derecha, se han escrito cuatro guiones después de cada cabecera, haciéndolas plegables. Dos de ellas están plegadas, y se puede ver que la Tabla de Contenidos en la parte inferior muestra cada sección.

```{r, out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "code_folding1.png"))
knitr::include_graphics(here::here("images", "code_folding2.png"))
```

Otras áreas de código que son automáticamente elegibles para plegarlas son las zonas entre corchetes `{ }` como las definiciones de funciones o los bloques condicionales (sentencias if else). Puedes leer más sobre el plegado de código en el [sitio de](https://support.rstudio.com/hc/en-us/articles/200484568-Code-Folding-and-Sections) RStudio.



<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Directorio de trabajo {#working_directory}  

El directorio de trabajo (o Working Directory "WD") es la ubicación de la carpeta raíz utilizada por R para su trabajo - donde R busca y guarda los archivos por defecto. Por defecto, guardará los nuevos archivos y resultados en esta ubicación, y buscará los archivos de datos para importar aquí también.

El directorio de trabajo aparece en texto gris en la parte superior de la consola de RStudio. También puede imprimir el directorio de trabajo actual ejecutando `getwd()` (deja los paréntesis vacíos).

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "working_directory_1.png"))
```


### Enfoque recomendado {.unnumbered}  

**Consulta la página sobre [proyectos de R](#r-projects) para obtener detalles sobre nuestro enfoque recomendado para gestionar tu directorio de trabajo.** Una forma común, eficiente y sin problemas de gestionar tu directorio de trabajo y las rutas de los archivos es combinar estos 3 elementos en un flujo de trabajo [orientado a los proyectos de R](#r-projects):

1.  Un proyecto R para almacenar todos tus archivos (ver página sobre [proyectos R](#r-projects))
2.  El paquete **here** para localizar los archivos (véase la página sobre [importación y exportación](#import-and-export))
3.  El paquete **rio** para importar/exportar archivos (véase la página sobre [importación y exportación](#import-and-export))


<!-- ======================================================= -->
### Mediante comandos {.unnumbered}

Hasta hace poco, a muchas personas que aprendían R se les enseñaba a comenzar sus scripts con un comando `setwd()`. En vez de esto, piensa mejor en un flujo de trabajo [orientado al proyecto R](#r-projects) y lee las [razones para no usar setwd()](https://www.tidyverse.org/blog/2017/12/workflow-vs-script/). En resumen, tu trabajo se convierte en algo específico de tu ordenador, las rutas de archivo utilizadas para importar y exportar archivos se vuelven "frágiles", y esto dificulta gravemente la colaboración y el uso de tu código en cualquier otro ordenador. Hay alternativas fáciles!

Como se ha indicado anteriormente, aunque no recomendamos este enfoque en la mayoría de las circunstancias, puedes utilizar el comando `setwd()` con la ruta del archivo de la carpeta deseada entre comillas, por ejemplo:

```{r, eval=F}
setwd("C:/Documents/R Files/My analysis")
```

<span style="color: red;">**_PELIGRO:_** Establecer un directorio de trabajo con `setwd()` *puede* ser "frágil" si la ruta del archivo es específica de un ordenador. En su lugar, utiliza rutas de archivos relativas a un directorio raíz del proyecto R (con el paquete **here**). </span>  



<!-- ======================================================= -->
### Manualmente {.unnumbered}  

Para establecer el directorio de trabajo manualmente (el equivalente de apuntar y clicar en setwd()), clica en el menú desplegable Session y luego "Set Working Directory" (Establecer el directorio de trabajo) y entonces "Choose Directory" (Elegir el directorio). Esto establecerá el directorio de trabajo para esa sesión específica de R. Nota: si utilizas este enfoque, tendrás que hacerlo manualmente cada vez que abras RStudio.


<!-- ======================================================= -->
### Con un proyecto {.unnumbered}

Si se utiliza un proyecto R, el directorio de trabajo será por defecto la carpeta raíz del proyecto R que contiene el archivo ".rproj". Esto se aplicará si clicas en abrir el proyecto en RStudio (el archivo con extensión ".rproj").


<!-- ======================================================= -->
### Directorio en Rmarkdown {.unnumbered}

En un script de R markdown, el directorio de trabajo por defecto es la carpeta en la que se guarda el archivo Rmarkdown (`.Rmd`). Si se utiliza un proyecto R y el paquete **here**, esto no se aplica y el directorio de trabajo será `here()` como se explica en la página de [proyectos R](#r-projects).

Si quieres cambiar el directorio de trabajo de un Rmarkdown independiente (no en un proyecto R), si utiliza setwd() sólo se aplicará a ese trozo de código específico. Para hacer el cambio para todos párrafos, edite el trozo de configuración para añadir el parámetro root.dir =, como se indica a continuación:

```{r, eval=F}
knitr::opts_knit$set(root.dir = 'desired/directorypath')
```

Es mucho más fácil usar Rmarkdown dentro de un proyecto R y usar el paquete **here**.



<!-- ======================================================= -->
### Escribir la ruta completa {.unnumbered} 

Quizás la fuente más común de frustración para un principiante de R (al menos en una máquina Windows) es escribir una ruta de archivo para importar o exportar datos. Hay una explicación exhaustiva sobre la mejor manera de introducir las rutas de los archivos en la página de [importación y exportación](#import-and-export), pero aquí hay algunos puntos clave:

**Rutas rotas**

A continuación se muestra un ejemplo de una ruta de archivo "absoluta" o de "dirección completa". Es probable que se rompa si se utiliza en otro ordenador. Una excepción es si estás usando una unidad compartida de red.

```
C:/Nombre de usuario/Documento/Software analítico/R/Proyectos/Análisis2019/datos/Marzo2019.csv
```

**Dirección de la barra**

*Si escribes una ruta de archivo, ten en cuenta la dirección de las barras inclinadas (/).* Utiliza barras *inclinadas* (/) para separar los componentes ("data/provincial.csv"). Para los usuarios de Windows, la forma predeterminada en que se muestran las rutas de los archivos es con barras *invertidas* (\) - por lo que tendrás que cambiar la dirección de cada barra. Si utilizas el paquete **here**, tal y como se describe en la página de [proyectos de R,](#r-projects) la dirección de la barra no es un problema.

**Relative file paths**  

Generalmente recomendamos proporcionar rutas de archivo "relativas" - es decir, la ruta *relativa a* la raíz de tu proyecto R. Puedes hacer esto usando el paquete **here** como se explica en la página de [proyectos R](#r-projects). Una ruta de archivo relativa podría ser así:  

```{r, eval=F}
# Import csv linelist from the data/linelist/clean/ sub-folders of an R project
linelist <- import(here("data", "clean", "linelists", "marin_country.csv"))
```

Incluso si se utilizan rutas de archivo relativas dentro de un proyecto R, se pueden utilizar rutas absolutas para importar/exportar datos fuera del proyecto R. 





<!-- ======================================================= -->
## Objetos {#objects}

Todo en R es un objeto, y R es un lenguaje "orientado a objetos". Estas secciones lo explicarán:

* Cómo crear objetos (`<-`)
* Tipos de objetos (por ejemplo, dataframe -conjunto de datos-, vectores...)
* Cómo acceder a las subpartes de los objetos (por ejemplo, las variables de unos datos)
* Tipos de objetos (por ejemplo, numéricos, lógicos, enteros, dobles, caracteres, factores)



<!-- ======================================================= -->
### Todo es un objeto {.unnumbered} 

*Esta sección está adaptada del [proyecto R4Epis](https://r4epis.netlify.app/training/r_basics/objects/).*  Todo lo que se almacena en R -conjuntos de datos (dataframe), variables, una lista de nombres de pueblos, un número total de población, incluso resultados como gráficos- son **objetos** alos que se **asigna un nombre** y a los que **se puede hacer referencia** en comandos posteriores.

Un objeto existe cuando se le ha asignado un valor (véase la sección de asignación más adelante). Cuando se le asigna un valor, el objeto aparece en el Entorno -Environment- (en el panel superior derecho de RStudio). A partir de ese momento se puede operar con él, manipularlo, cambiarlo y redefinirlo.


<!-- ======================================================= -->
### Definición de objetos (`<-`) {.unnumbered}

**Crea objetos *asignándoles un valor* con el operador <-.**
Puedes pensar que el operador de asignación `<-` significa: "se define como". Los comandos de asignación suelen seguir un orden estándar:

**nombre_objeto** <-  **valor** (o proceso/cálculo que produce un valor)

Por ejemplo, es posible que desees registrar la semana de notificación epidemiológica actual como un objeto para su referencia posteriormente en un código En este ejemplo, el objeto `current_week` se crea cuando se le asigna el valor `"2018-W10"` (las comillas hacen que sea un valor de tipo carácter). El objeto `current_week` aparecerá entonces en el panel de Environment de RStudio (arriba a la derecha) y podrá ser referenciado en comandos posteriores.

Ver los comandos de R y su salida en los cuadros de abajo.

```{r basics_objects_assignment}
current_week <- "2018-W10"   # Este comando crea el objeto current_week asignándole un valor
current_week                 # Este comando muestra en la consola el valor actual del objeto current_week
```

<span style="color: black;">**_NOTA:_** Observa que el `[1]` en la consola de resultados simplemente indica que estás viendo el primer elemento de resultados</span>

<span style="color: orange;">**_ATENCIÓN:_** **El valor de un objeto puede sobrescribirse** en cualquier momento ejecutando un comando de asignación para redefinir su valor. Por lo tanto, el **orden de ejecución de los comandos es muy importante**.</span>

El siguiente comando redefinirá el valor de `current_week`:


```{r basics_objects_reassignment}
current_week <- "2018-W51"   # asigna un NUEVO valor al objeto current_week
current_week                 # muestra en la consola el valor actual del objeto current_week
```

**Signos de igualdad `=`**  

También verás signos de igualdad en el código R:

* Un doble signo de igualdad `==` entre dos objetos o valores formula una *pregunta lógica*: "¿es esto igual a aquello?".
* También verás signos de igualdad dentro de las funciones que se utilizan para especificar los valores de los argumentos de la función (lee sobre ellos en las secciones siguientes), por ejemplo `max(edad, na.rm = TRUE)`.
* *Puedes* utilizar un único signo de igualdad `=` en lugar de `<-` para crear y definir objetos, pero se desaconseja hacerlo. Puedes leer por qué se desaconseja [aquí](https://renkun.me/2014/01/28/difference-between-assignment-operators-in-r/).

**Conjuntos de datos (datasets)**

Los conjuntos de datos también son objetos (normalmente "dataframes") y se les debe asignar un nombre cuando se importan. En el código siguiente, se crea el objeto `linelist` y se le asigna el valor de un archivo CSV importado con la función `import()` del paquete **rio**.

```{r basics_objects_dataframes, eval=FALSE}
# se crea linelist y se le asigna los valores del archivo CSV importado
linelist <- import("my_linelist.csv")
```

Puedes obtener más información sobre la importación y la exportación de datos en la sección sobre [importación y exportación](#import-and-export).

<span style="color: orange;">**_ATENCIÓN:_** Una nota rápida sobre la denominación de los objetos:</span>

* Los nombres de los objetos no deben contener espacios, debes utilizar el guión bajo (_) o un punto (.) en lugar de un espacio.
* Los nombres de los objetos distinguen entre mayúsculas y minúsculas (lo que significa que Dataset_A es diferente de dataset_A).
* Los nombres de los objetos deben empezar por una letra (no pueden empezar por un número como 1, 2 o 3).

**Resultados**

Los resutados, como las tablas y los gráficos proporcionan un ejemplo de cómo las salidas pueden guardarse como objetos, o simplemente mostrarse en la consola sin ser guardadas. Una tabla cruzada de género y resultado utilizando la función `table()` de R **base** puede mostrarse directamente en la consola de R (*sin guardarse*).

```{r}
# solamente se muestra en la consola de R
table(linelist$gender, linelist$outcome)
```

Pero la misma tabla puede guardarse como un objeto con nombre. Y luego, opcionalmente, se puede mostrar o imprimir.

```{r}
# save
gen_out_table <- table(linelist$gender, linelist$outcome)

# print
gen_out_table
```

**Columnas**

Las columnas de unos datos también son objetos y pueden definirse, sobrescribirse y crearse como se describe a continuación en la sección sobre Columnas.

Puedes utilizar el operador de asignación de R **base** para crear una nueva columna. A continuación, se crea la nueva columna `bmi` (Índice de masa corporal), y para cada fila el nuevo valor es el resultado de una operación matemática sobre el valor de la fila en las columnas `wt_kg` y `ht_cm`.

```{r, eval=F}
# crea la columna "bmi" utilizando sintaxis de R base
linelist$bmi <- linelist$wt_kg / (linelist$ht_cm/100)^2
```

Sin embargo, en este manual, hacemos hincapié en un enfoque diferente para definir las columnas, que utiliza la función `mutate()` del paquete **dplyr** y la *canalización* con el operador pipe (`%>%`). La sintaxis es más fácil de leer y hay otras ventajas que se explican en la página sobre [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions). Puedes leer más sobre la *canalización* en dicha sección más abajo.

```{r, eval=F} 
# crea la columna "bmi" utilizando sintaxis de dplyr
linelist <- linelist %>% 
  mutate(bmi = wt_kg / (ht_cm/100)^2)
```

<!-- ======================================================= -->
### Estructura de los objetos {.unnumbered}  

**Los objetos pueden ser un solo dato (por ejemplo, `mi_número <- 24`), o pueden consistir en datos estructurados.**

El gráfico siguiente está tomado de [este tutorial de R en línea](http://venus.ifca.unican.es/Rintro/dataStruct.html). Muestra algunas estructuras de datos comunes y sus nombres. No se incluyen en esta imagen los datos espaciales, de los que se habla en la página de [fundamentos del SIG](#gis-basics).

```{r basics_objects_structures, echo=F, out.width = "75%", out.height="50%", fig.align = "center"}
knitr::include_graphics(here::here("images", "R_data_structures.png"))
```  

En epidemiología (y en particular en epidemiología de campo), *lo habitual es* encontrar dataframes (conjuntos de datos) y vectores:

Estructura común |Explicación  |Ejemplo
------------------- | ------------------------------------ | ------------------------  
Vectores | Un contenedor para una secuencia de objetos singulares, todos del mismo tipo (por ejemplo, numérico, carácter). | **Las variables (columnas) en los dataframes son vectores** (por ejemplo, la columna de edad en años age_years).
Dataframes | Vectores (por ejemplo, columnas) unidos que tienen todos el mismo número de filas. | `linelist` es un dataframe.

Ten en cuenta que para crear un vector que "está solo" (no forma parte de un dataframe) se utiliza la función `c()` para combinar los diferentes elementos. Por ejemplo, si se crea un vector de colores de la escala de colores del gráfico: `vector_de_colores <- c("azul", "rojo2", "naranja", "gris")`


<!-- ======================================================= -->
### Tipos de objeto  {.unnumbered}

Todos los objetos almacenados en R tienen un *tipo (class)* que indica a R cómo manejar el objeto. Hay muchos tipos de datos, pero los más comunes son:

 Tipo  | Explicación                                | Ejemplos
------ | ------------------------------------------ |  -----------------------------
Character | Son textos/palabras/frases **"entre comillas"**. No se pueden realizar operaciones matemáticas con estos objetos. | "Los objetos de carácter están entre comillas"
Integer | Números **sólo enteros** (sin decimales) | -5, 14, o 2000
Numeric | Son números y **pueden incluir decimales**. Si están entre comillas se considerarán de tipo de caracteres. | 23.1 o 14
Factor | Se trata de vectores que tienen un **orden determinado** o jerarquía de valores | Una variable de situación económica con valores ordenados
Date | **Una vez que se le dice a R que ciertos datos son Fechas**, estos datos pueden ser manipulados y mostrados de maneras especiales. Para más información, consulta la página sobre el [trabajo con fechas](#working-with-dates-1). | 2018-04-12 o 15/3/1954 o miércoles 4 de enero de 1980
Logical | Los valores deben ser uno de los dos valores especiales TRUE o FALSE (nótese que **no son** "TRUE" y "FALSE" entre comillas) | TRUE o FALSE
data.frame | Un dataframe es la forma en que R almacena un ** conjunto de datos típico**. Consiste en vectores (columnas) de datos unidos, que tienen todos el mismo número de observaciones (filas). | El set de datos AJS de ejemplo denominado `linelist_raw` contiene 68 variables con 300 observaciones (filas) cada una.
tibble | Los tibbles son una variación de los dataframe, la principal diferencia operativa es que muestran de forma más agradable en la consola (muestran las 10 primeras filas y sólo las columnas que caben en la pantalla) | Cualquier conjunto de datos, lista o matriz puede convertirse en un tibble con `as_tibble()`
List | Una lista es como un vector, pero contiene otros objetos que pueden ser de otras tipos diferentes | Una lista puede contener un solo número, un dataframe de datos, un vector e incluso otra lista.

**Puedes comprobar el tipo de un objeto escribiendo su nombre en la función `class()`**. Nota: puede hacer referencia a una columna específica dentro de unos datos utilizando la notación `$` para separar el nombre de los datos y el nombre de la columna.

```{r, echo=TRUE,}
class(linelist) # debe ser de tipo dataframe o tibble
class(linelist$age) # debe ser de tipo numérico
class(linelist$gender) # debe ser de tipo carácter
```

A veces, una columna puede ser convertida automáticamente por R en un tipo diferente. ¡Cuidado con esto! Por ejemplo, si tiene un vector o columna de números, pero se inserta un valor de carácter... toda la columna cambiará al tipo carácter.

```{r}
num_vector <- c(1,2,3,4,5) # define vector como todos números
class(num_vector)          # el vector es de tipo numérico
num_vector[3] <- "three"   # convierte el tercer elemento en de tipo carácter
class(num_vector)          # el vector es ahora de tipo carácter class
```

Un ejemplo común de esto es cuando se manipula unos datos para imprimir una tabla - si se hace una fila de totales y se intenta pegar porcentajes en la misma celda como números (por ejemplo, `23 (40%)`), toda la columna numérica de arriba se convertirá en carácter y ya no se podrá utilizar para cálculos matemáticos. **A veces, tendrás que convertir objetos o columnas a otro tipo.**

Función | Acción
----------------- | --------------------------------------------------------------    
as.character() | Convierte al tipo de carácter
as.numeric() | Convierte al tipo numérico
as.integer() | Convierte al tipo entero
as.date() | Convierte al tipo de fecha - Nota: ver la sección de [fechas](#dates) para más detalles
factor() | Convierte en factor - Nota: la redefinición del orden de los niveles de valor requiere argumentos adicionales

Asimismo, existen funciones de R **base** para comprobar si un objeto ES de un tipo específico, como `is.numeric()`, `is.character()`, `is.double()`, `is.factor()`, `is.integer()`

Aquí hay [más material en línea sobre tipos y estructuras de datos en R](https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/).


<!-- ======================================================= -->
### Columnas/Variables (`$`) {.unnumbered}  

**Una columna en un dataframe es técnicamente un "vector" (véase la tabla anterior)** - una serie de valores que deben ser todos del mismo tipo (ya sea carácter, numérico, lógico, etc).

Un vector puede existir independientemente de un dataframe, por ejemplo, un vector de nombres de columnas que se desea incluir como variables explicativas en un modelo. Para crear un vector "independiente", utiliza la función `c()` como se indica a continuación:

```{r, warning=F, message=F}
# define el vector con valores de caracteres
explanatory_vars <- c("gender", "fever", "chills", "cough", "aches", "vomit")

# muestra los valores de este vector
explanatory_vars
```

**Las columnas de un dataframe también son vectores y pueden ser llamadas, referenciadas, extraídas o creadas utilizando el símbolo `$`.** El símbolo `$` conecta el nombre de la columna con el nombre del dataframe. En este manual, tratamos de utilizar la palabra "columna" en lugar de "variable".

```{r basics_objects_call, eval=F}
# Recuperar la longitud del vector age_years
length(linelist$age) # (age es una columna del dataframe linelist)

```

Al escribir el nombre del dataframe seguido de `$` también verá un menú desplegable de todas las columnas del dataframe. Puedes desplazarte por ellas con la tecla de flecha, seleccionar una con la tecla Intro y evitar errores ortográficos.


```{r echo=F, out.width = "100%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Calling_Names.gif"))
```  

<span style="color: darkgreen;">**_CONSEJO AVANZADO:_** Algunos objetos más complejos (por ejemplo, una lista, o un objeto `epicontacts`) pueden tener múltiples niveles a los que se puede acceder a través de múltiples signos de dólar. Por ejemplo `epicontacts$linelist$date_onset`</span>


<!-- ======================================================= -->
### Acceso/índice con corchetes (`[ ]`) {.unnumbered} 

Es posible que tengas que mirar sólo partes de los objetos, lo que también se llama "indexación", que a menudo se hace utilizando los corchetes `[ ]`. El uso de `$` en un dataframe para acceder a una columna también es un tipo de indexación.

```{r}
my_vector <- c("a", "b", "c", "d", "e", "f")  # definir el vector
my_vector[5]                                  # mostrar el 5º elemento
```

Los corchetes también sirven para devolver partes específicas de un resultado, como la salida de una función `summary()`:

```{r}
# resumen completo 
summary(linelist$age)

# Sólo el segundo elemento del resumen, nombre (usando solo conchetes simples)
summary(linelist$age)[2]

# Sólo el segundo elemento, sin nombre (usando dobles corchetes)
summary(linelist$age)[[2]]

# Extrae un elemento por nombre, sin mostrar el nombre
summary(linelist$age)[["Median"]]

```

Los corchetes también funcionan en los dataframes para ver filas y columnas específicas. Puedes hacerlo utilizando la sintaxis `dataframe[filas, columnas]`:

```{r basics_objects_access, eval=F}
# Ver una fila específica (2) del dataset, con todas las columnas (¡no olvides la coma!)
linelist[2,]

# Ver todas las filas, pero solo una columna
linelist[, "date_onset"]

# Ver los valores de la fila 2 y las columnas 5 hasta la 10
linelist[2, 5:10] 

# Ver los valores desde la fila 2 y las columnas 5 hasta la 10 y 18
linelist[2, c(5:10, 18)] 

# Ver las filas 2 a la 20 y sólo unas columnas especificadas
linelist[2:20, c("date_onset", "outcome", "age")]

# Ver filas y columnas basado en ciertos criterios
# *** ¡Observa que el nombre del dataframe también debe estar entre los criterios!
linelist[linelist$age > 25 , c("date_onset", "outcome", "age")]

# Usa View() para ver el resultado en el Visor de Rstudio (más fácil de leer) 
# *** Fíjate que la "V" de la función View() está en mayúsculas
View(linelist[2:20, "date_onset"])

# Guarda como objeto nuevo
new_table <- linelist[2:20, c("date_onset")] 
```

Ten en cuenta que también puedes lograr la indexación de filas/columnas anterior en dataframes y tibbles utilizando sintaxis de **dplyr** (funciones `filter()` para filas, y `select()` para columnas). Puedes leer más sobre estas funciones básicas en la página de [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions).

Para filtrar en base al "número de fila", puedes utilizar la función `row_number()` de **dplyr** (con paréntesis vacíos) como parte de una sentencia lógica de filtrado. A menudo se utiliza el operador `%in%` y un rango de números como parte de esa sentencia lógica, como se muestra a continuación. Para ver las *primeras* N filas, también puede utilizar la función especial `head()` de **dplyr**.

```{r, eval=F}
# Ver las primeras 100 filas
linelist %>% head(100)

# Mostrar sólo la fila 5
linelist %>% filter(row_number() == 5)

# Ver las filas 2 hasta la 20, y tres columnas específicas (No sonnecesarias las comillas pasa los nombres de columna)
linelist %>% filter(row_number() %in% 2:20) %>% select(date_onset, outcome, age)
```

Cuando se indexa un objeto del tipo **list**, los corchetes simples siempre devuelven con el tipo list, incluso si sólo se devuelve un único objeto. Los corchetes dobles, sin embargo, pueden utilizarse para acceder a un solo elemento y devolver un tipo diferente al de la lista. Los corchetes también pueden escribirse uno tras otro, como se demuestra a continuación.

Esta [explicación visual de la indexación de listas, con pimenteros](https://r4ds.had.co.nz/vectors.html#lists-of-condiments), es divertida y útil.


```{r}
# define demo list
my_list <- list(
  # El primer elemento en la lista es un vector de carácter
  hospitals = c("Central", "Empire", "Santa Anna"),
  
  # El segundo elemento en la lista es un data frame de direcciones
  addresses   = data.frame(
    street = c("145 Medical Way", "1048 Brown Ave", "999 El Camino"),
    city   = c("Andover", "Hamilton", "El Paso")
    )
  )
```

Este es el aspecto de la lista cuando se muestra en la consola. Mira cómo hay dos elementos con nombre:

* `hospital`, un vector de caracteres
* `addresses`, un data frame de direcciones

```{r}
my_list
```

Ahora extraeremos información, utilizando varios métodos:

```{r}
my_list[1] # este devuelve el elemento de tipo "list" - también se muestra el nombre del elemento

my_list[[1]] # este devuelve sólo el vector de carácter (sin nombrarlo) 

my_list[["hospitals"]] # también puedes indexar por nombre del elemento list

my_list[[1]][3] # este devuelve el tercer elemento del vector de carácter "hospitals"

my_list[[2]][1] # este devuelve la primera columna ("street") del data frame address

```



<!-- ======================================================= -->
### Eliminar objetos {.unnumbered} 

Puedes eliminar objetos individuales de tu entorno R poniendo el nombre en la función `rm()` (sin comillas):

```{r, eval=F}
rm(object_name)
```

Puedes eliminar todos los objetos (limpiar tu espacio de trabajo) ejecutando:

```{r, eval=F}
rm(list = ls(all = TRUE))
```



<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Piping (`%>%`)  {#piping}

**Dos aproximaciones para trabajar con objetos:**  

1) **Pipes/tidyverse** - pipes envía un objeto de una función a otra función - énfasis en la *acción*, no en el objeto  
2) **Definir objetos intermedios** - se re-define un objeto una y otra vez - el énfasis está en el objeto  


<!-- ======================================================= -->
### **Pipes (%>%)** {.unnumbered}

**Hay dos enfoques generales para trabajar con objetos:**

1.  **Pipes/tidyverse** - los pipes envían un objeto de función a función - el énfasis está en la *acción*, no en el objeto

2.  **Definir objetos intermedios** - un objeto se redefine una y otra vez - el énfasis está en el objeto

**Pipes**

**Explicado de forma sencilla, el operador pipe (`%>%`) pasa un resultado intermedio de una función a la siguiente.**\ Puedes pensar en él como si dijera "entonces". Muchas funciones pueden enlazarse con `%>%`.

* **Los pipes hacen hincapié en una secuencia de acciones, no en el objeto sobre el que se realizan las acciones**
* Los pipes son mejores cuando hay que realizar una secuencia de acciones en un objeto
* Los pipes provienen del paquete **magrittr**, que se incluye automáticamente en los paquetes **dplyr** y **tidyverse**
* Los pipes pueden hacer que el código sea más limpio y fácil de leer, más intuitivo

Más información sobre este enfoque en la [guía de estilo de tidyverse](https://style.tidyverse.org/pipes.html)

He aquí un ejemplo para comparar, utilizando funciones ficticias para "hornear un pastel". Primero, el método pipe:

```{r piping_example_pipe, eval=F}
# Un ejemplo falso de cómo hornear un pastel utilizando sintaxis con pipes

cake <- flour %>%       # para definir pastel, empezar con harina, y luego...
  add(eggs) %>%   # añadir huevos
  add(oil) %>%    # añadir aceite
  add(water) %>%  # añadir agua
  mix_together(         # mezclarlos juntos
    utensil = spoon,
    minutes = 2) %>%    
  bake(degrees = 350,   # hornear
       system = "fahrenheit",
       minutes = 35) %>%  
  let_cool()            # dejar que se enfríe
```

Aquí hay otro [enlace](https://cfss.uchicago.edu/notes/pipes/#:~:text=Pipes are an extremely useful,code and combine multiple operations) que describe la utilidad de Los pipes.

El uso de pipes no es una función **base**. Para usarlos, debe estar instalado y cargado el paquete **magrittr** (esto se hace normalmente cargando el paquete **tidyverse** o **dplyr** que lo incluyen). Puedes [leer más sobre pipes en la documentación de magrittr](https://magrittr.tidyverse.org/).

Ten en cuenta que, al igual que otros comandos de R, Los pipes se pueden utilizar para mostrar sólo el resultado, o para guardar/re-guardar un objeto, dependiendo de si el operador de asignación `<-` está involucrado. Mira ambas cosas a continuación:

```{r, eval=F}
# Crear o sobreescribir objetos, definiéndolo como como recuentos agregados por categoría de edad (sin mostrarlo)
linelist_summary <- linelist %>% 
  count(age_cat)
```

```{r}
# Muestra la tabla de recuentos en la consola, pero no la guarda
linelist %>% 
  count(age_cat)
```


**`%<>%`**  
Se trata de un "pipe de asignación" del paquete **magrittr**, que *lleva un objeto hacia adelante y también redefine el objeto*. Debe ser el primer operador pipe en la cadena. Es una forma abreviada. Los dos comandos siguientes son equivalentes: 

```{r, eval=F}
linelist <- linelist %>%
  filter(age > 50)

linelist %<>% filter(age > 50)
```


<!-- ======================================================= -->
### Definir objetos intermedios {.unnumbered}

Este enfoque para cambiar objetos/dataframes puede ser mejor si:

* Necesitas manipular múltiples objetos
* Hay pasos intermedios que son significativos y merecen nombres de objetos separados

**Riesgos:**

* Crear nuevos objetos para cada paso significa crear muchos objetos. Si usas el equivocado, ¡puedes no darte cuenta!
* Nombrar todos los objetos puede ser confuso
* Los errores pueden no ser fácilmente detectables

O bien nombrar cada objeto intermedio, o sobrescribir el original, o combinar todas las funciones juntas. Todo ello conlleva sus propios riesgos.

A continuación se muestra el mismo ejemplo de "pastel" falso que el anterior, pero utilizando este estilo:


```{r piping_example_redefine, eval=F}
# Un ejemplo falso de cómo hornear un pastel definiendo objetos intermedios
batter_1 <- left_join(flour, eggs)
batter_2 <- left_join(batter_1, oil)
batter_3 <- left_join(batter_2, water)

batter_4 <- mix_together(object = batter_3, utensil = spoon, minutes = 2)

cake <- bake(batter_4, degrees = 350, system = "fahrenheit", minutes = 35)

cake <- let_cool(cake)
```

Combinar todas las funciones juntas - esto es difícil de leer:

```{r eval=F}
# un ejemplo de combinación/anidación de múltiples funciones - difícil de leer
cake <- let_cool(bake(mix_together(batter_3, utensil = spoon, minutes = 2), degrees = 350, system = "fahrenheit", minutes = 35))
```


<!-- ======================================================= -->
## Operadores y funciones clave {#operators}

Esta sección detalla los operadores en R, como por ejemplo

* Operadores de definición
* Operadores relacionales (menor que, igual a ..)
* Operadores lógicos (and, or...)
* Tratamiento de los valores faltantes
* Operadores y funciones matemáticas (+/-, >, sum(), median(), ...)
* El operador %in%


<!-- ======================================================= -->
### Operadores de asignación {.unnumbered} 

**`<-`**

El operador de asignación básico en R es `<-`. Como en `nombre_objeto <- valor`.
Este operador de asignación también se puede escribir como `=`. Aconsejamos de forma general el uso de `<-` .
También aconsejamos rodear tales operadores con espacios, para mejorar la legibilidad.

**`<<-`**

Si se [escriben funciones](#writing-functions-1), o se utiliza R de forma interactiva con scripts de origen, entonces puede ser necesario utilizar este operador de asignación `<<-` (de R **base**). Este operador se utiliza para definir un objeto en un entorno R superior "padre". Mira esta [referencia online](https://stat.ethz.ch/R-manual/R-devel/library/base/html/assignOps.html).

**`%<>%`**

Se trata de un "pipe de asignación" del paquete **magrittr**, que canaliza un objeto hacia adelante y *también redefine el objeto*. Debe ser el primer operador de pipe en la cadena. Es una forma abreviada, como se muestra a continuación en dos ejemplos equivalentes:

```{r, eval=F}
linelist <- linelist %>% 
  mutate(age_months = age_years * 12)
```  

Lo anterior equivale a lo siguiente:

```{r, eval=F}
linelist %<>% mutate(age_months = age_years * 12)
```


**`%<+%`**

Se utiliza para añadir datos a los árboles filogenéticos con el paquete **ggtree**. Consulta la página sobre [árboles filogenéticos](#phylogenetic-trees-1) o este [libro de recursos](https://yulab-smu.top/treedata-book/) en línea.



<!-- ======================================================= -->
### Operadores relacionales y lógicos {.unnumbered} 

Los operadores relacionales comparan valores y se utilizan a menudo al definir nuevas variables y subconjuntos de datos. Estos son los operadores relacionales más comunes en R:

Significado             |Operador     |Ejemplo       |Ejemplo de resultado
------------------------|-------------|--------------|---------------------------
Igual a                 |`==`         |`"A" == "a"`  |`FALSE` (R distingue entre mayúsculas y minúsculas) *Ten en cuenta que == (doble igual) es diferente de = (simple igual), que actúa como el operador de asignación `<-`*
No es igual a           |`!=`         |`2 != 0`      |`TRUE`
Mayor que               |`>`          |`4 > 2`       |`TRUE`
Menor que               |`<`          |`4 < 2`       |`FALSE`
Mayor que o igual a     |`>=`         |`6 >= 4`      |`TRUE`
Menor que o igual a     |`<=`         |`6 <= 4`      |`FALSE`
Falta el valor          |`is.na()`    |`is.na(7)`    |`FALSE` (véase la página sobre [Valores faltantes](#missing-data))
No falta el valor       |`!is.na()`   |`!is.na(7)`   |`TRUE`



Los operadores lógicos o booleanos, como AND y OR, suelen utilizarse para conectar operadores relacionales y crear criterios más complicados. Las sentencias complejas pueden requerir paréntesis () para la agrupación y el orden de aplicación.

  ----------------- -----------------------------------------------------------------------------
Significado |Operador
------------|------------------------
AND         |`&`
OR          |`|` (barra vertical)
Paréntesis  |`( )` Se utiliza para agrupar criterios y aclarar el orden de las operaciones
  ----------------- -----------------------------------------------------------------------------

Por ejemplo, a continuación, tenemos un listado con dos variables que queremos utilizar para crear nuestra definición de caso, `hep_e_rdt`, un resultado de la prueba y `other_cases_in_hh`, que nos dirá si hay otros casos en el hogar. El comando siguiente utiliza la función `case_when()` para crear la nueva variable `case_def` tal que:


```{r eval=FALSE}
linelist_cleaned <- linelist %>%
  mutate(case_def = case_when(
    is.na(rdt_result) & is.na(other_case_in_home)            ~ NA_character_,
    rdt_result == "Positive"                                 ~ "Confirmed",
    rdt_result != "Positive" & other_cases_in_home == "Yes"  ~ "Probable",
    TRUE                                                     ~ "Suspected"
  ))
```


Criterios del ejemplo anterior          |Valor resultante en la nueva variable "case_def"
----------------------------------------|-------------------------------------------------
Si falta el valor de las variables `rdt_result` y `other_cases_in_home` | NA (valor faltante)
Si el valor de `rdt_result` es "Positivo" | "Confirmado"
Si el valor de `rdt_result` NO es "Positivo" Y el valor de `other_cases_in_home es` "Si" | "Probable"
Si no se cumple alguno de los criterios anteriores | "Sospechoso"

*Ten en cuenta que R distingue entre mayúsculas y minúsculas, por lo que "Positivo" es diferente de "positivo"...*

 
<!-- ======================================================= -->
### Valores faltantes {.unnumbered}

En R, los valores faltantes (missing value) se representan con el valor especial `NA` (una palabra "reservada" para ello) (letras mayúsculas N y A - sin comillas). Si importas datos que registran los valores faltantes de otra manera (por ejemplo, 99, "Nulo", o .), es posible que quieras re-codificar esos valores a NA. En la página de [importación y exportación](#import-and-export) se explica cómo hacerlo.

**Para comprobar si un valor es `NA`, utiliza la función especial `is.na()`**, que devuelve `TRUE` o `FALSE`.


```{r basics_operators_missing}
rdt_result <- c("Positive", "Suspected", "Positive", NA)   # dos casos positivos, uno sospechoso y uno desconocido
is.na(rdt_result)  # Comprueba si el valor de rdt_result es NA
```

Puedes leer más sobre los valores faltantes, infinitos, `NULL` e imposibles en la página sobre [Valores faltantes](#missing-data)]. Aprende a convertir los valores faltantes al importar datos en la página sobre [Importación y exportación](#import-and-export).


<!-- ======================================================= -->
### Matemáticos y estadísticos {.unnumbered}  

Todos los operadores y funciones de esta página están disponibles automáticamente en R **base**.



#### Operadores matemáticos {.unnumbered} 

Se utilizan a menudo para realizar sumas, divisiones, crear nuevas columnas, etc. A continuación se muestran los operadores matemáticos más comunes en R. No es importante poner espacios alrededor de los operadores.


Propósito           |Ejemplo en R
--------------------|-------------
suman               | 2 + 3
resta               | 2 - 3
multiplicación      | 2 * 3
división            | 30 / 5
exponente           | 2^3
orden de operaciones| ( )


#### Funciones matemáticas {.unnumbered}

Propósito          |Función**
-------------------|-------------
redondeo           | round(x, digits = n)  
redondeo           | janitor::round_half_up(x, digits = n)
redondeo arriba    | ceiling(x)
redondeo abajo     | floor(x)
valor absoluto     | abs(x)
raiz cuadrada      | sqrt(x)
exponente          | exponent(x)
logaritmo natural  | log(x)
log base 10        | log10(x)
log base 2         | log2(x)

Nota: para `round()` los `dígits =` especifican el número de decimales. Utiliza `signif()` para redondear a un número de cifras significativas.


#### Notación científica {.unnumbered}

La probabilidad de que se utilice la notación científica depende del valor de la opción `scipen`.

De la documentación de `?options`: scipen es una penalización que se aplica cuando se decide obtener valores numéricos en notación fija o exponencial. Los valores positivos se inclinan hacia la notación fija y los negativos hacia la notación científica: se preferirá la notación fija a menos que tenga más dígitos que 'scipen'.

Si se establece en un número bajo (por ejemplo, 0) estará "activada" siempre. Para "desactivar" la notación científica en tu sesión de R, configúrela con un número muy alto, por ejemplo:


```{r, eval=F}
# turn off scientific notation
options(scipen=999)
```



#### Redondeo {.unnumbered}


<span style="color: red;">**_PELIGRO:_** `round()` utiliza el "redondeo del banquero" que redondea hacia arriba desde un 0,5 sólo si el número superior es par. Utiliza `round_half_up()` de **janitor** para redondear consistentemente las mitades hacia arriba al número entero más cercano. [Aquí tienes esta explicación](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes)</span>

```{r}
# utilizar la función de redondeo adecuada para tu trabajo
round(c(2.5, 3.5))

janitor::round_half_up(c(2.5, 3.5))
```


#### Funciones estadísticas {.unnumbered} 

<span style="color: orange;">**_PRECAUCIÓN:_** Las funciones que aparecen a continuación incluyen por defecto los valores faltantes en los cálculos. Los valores faltantes darán como resultado una salida de `NA`, a menos que se especifique el argumento `na.rm = TRUE`. Esto se puede escribir de forma abreviada como `na.rm = T`.</span>

Objetivo                  |Función**
--------------------------|---------------------
media (promedio)          | mean(x, na.rm=T)
mediana                   | median(x, na.rm=T)
desviación estándar       | sd(x, na.rm=T)
cuantiles*                | quantile(x, probs)
suma                      | sum(x, na.rm=T)
valor mínimo              | min(x, na.rm=T)
valor máximo              | max(x, na.rm=T)
rango de valores numéricos| range(x, na.rm=T)
resumen**                 | summary(x)

Notas:

* `*quantile()`: `x` es el vector numérico a examinar, y `probs = ` es un vector numérico con probabilidades entre 0 y 1.0, por ejemplo `c(0.5, 0.8, 0.85)`

* `**summary()`: da un resumen estadístico sobre un vector numérico incluyendo la media, mediana y percentiles más comunes

<span style="color: red;">**_PELIGRO:_** Si proporciona un vector de números a una de las funciones anteriores, asegúrese de envolver los números dentro de `c()`.</span>

```{r}
# Si se suministran números sin procesar a una función, envuélvalos en c()
mean(1, 6, 12, 10, 5, 0)    # !!! INCORRECTO !!!  

mean(c(1, 6, 12, 10, 5, 0)) # CORRECTO
```



#### Otras funciones útiles {.unnumbered}  


Objetivo                      |Función             |Ejemplo**
------------------------------|--------------------|-----------------------------------------------
crear una secuencia           |  seq(de, a, por)   |`seq(1, 10, 2)`
repetir x, n veces            |  rep(x, nveces)    |`rep(1:3, 2)` or `rep(c("a", "b", "c"), 3)` 
subdividir un vector numérico |  cut(x, n)         |`cut(linelist$age, 5)`
tomar una muestra aleatoria   |  sample(x, tamaño) |`sample(linelist$id, size = 5, replace = TRUE)`

<!-- ======================================================= -->
### `%in%` {.unnumbered}  

Un operador muy útil para comparar valores, y para evaluar rápidamente si un valor está dentro de un vector o dataframe.

```{r}
my_vector <- c("a", "b", "c", "d")
```

```{r}
"a" %in% my_vector
"h" %in% my_vector
```

Para preguntar si un valor **no está** `%in%` en un vector, pon un signo de exclamación (!) **delante** de la declaración lógica:

```{r}
# para negar, pon una exclamación delante
!"a" %in% my_vector
!"h" %in% my_vector
```

`%in%` es muy útil cuando se utiliza la función `case_when()` de **dplyr**. Se puede definir un vector previamente y referenciarlo después. Por ejemplo:

```{r eval=F}
affirmative <- c("1", "Yes", "YES", "yes", "y", "Y", "oui", "Oui", "Si")

linelist <- linelist %>% 
  mutate(child_hospitaled = case_when(
    hospitalized %in% affirmative & age < 18 ~ "Hospitalized Child",
    TRUE                                      ~ "Not"))
```


Nota: Si quieres detectar una cadena parcial, quizás usando `str_detect()` de **stringr**, no aceptará un vector de caracteres como `c("1", "Yes", "yes", "y")`. En su lugar, se le debe dar una *expresión regular* - una cadena condensada con barras OR, como "1|Yes|yes|y". Por ejemplo, `str_detect(hospitalized, "1|Yes|yes|y")`. Consulta la página sobre [Caracteres y cadenas](#characters-and-strings) para obtener más información.

Puedes convertir un vector de caracteres en una expresión regular con este comando:

```{r}
affirmative <- c("1", "Yes", "YES", "yes", "y", "Y", "oui", "Oui", "Si")
affirmative

# condense to 
affirmative_str_search <- paste0(affirmative, collapse = "|")  # option with base R
affirmative_str_search <- str_c(affirmative, collapse = "|")   # option with stringr package

affirmative_str_search
```




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Errores & avisos {#errors-warnings}

En esta sección se explica:

* La diferencia entre errores y avisos (warnings)
* Consejos generales de sintaxis para escribir código R
* Código de asistencia

En las página de [Errores comunes](#common-errors) y de [Obtener ayuda](#getting-help) se pueden encontrar los errores y avisos más comunes, así como consejos para la resolución de problemas.



<!-- ======================================================= -->
### Error vs aviso {.unnumbered}

Cuando se ejecuta un comando, la Consola de R puede mostrarte mensajes de aviso o error en texto rojo.

* Una **aviso** significa que R ha completado su comando, pero ha tenido que dar pasos adicionales o ha producido una salida inusual de la que deberías estar al tanto.

* Un **error** significa que R no pudo completar su comando.

Busca pistas:

* El mensaje de error/advertencia suele incluir un número de línea donde está el problema.

* Si un objeto "es desconocido" o "no se encuentra", quizás lo hayas escrito mal, hayas olvidado llamar a un paquete con library(), o hayas olvidado volver a ejecutar tu script después de hacer cambios.

Si todo lo demás falla, copia el mensaje de error en Google junto con algunos términos clave; lo más probable es que a alguien le haya pasado lo mismo y ¡ya haya resuelto el problema!.



<!-- ======================================================= -->
### Consejos generales de sintaxis {.unnumbered}


Algunas cosas que hay que recordar al escribir comandos en R, para evitar errores y advertencias:

* Cierra siempre los paréntesis - consejo: cuenta el número de paréntesis de apertura "(" y de cierre ")" de cada trozo de código
* Evita los espacios en los nombres de columnas y objetos. Utiliza barras bajas ( _ ) o puntos ( . ) en su lugar
* No olvides separar los argumentos de una función con comas
* R distingue entre mayúsculas y minúsculas, lo que significa que `Variable_A` es *diferente* de `variable_A`


<!-- ======================================================= -->
### Código de asistencia {.unnumbered}


Cualquier script (RMarkdown o de otro tipo) te dará pistas cuando hayas cometido un error. Por ejemplo, si te olvidaste de escribir una coma donde se necesita, o de cerrar un paréntesis, RStudio levantará una bandera en esa línea, a la derecha del script, para avisarte.









```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/basics.Rmd-->


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Transición a R {#transition-to-r}

A continuación, te ofrecemos algunos consejos y recursos que resultan útiles si te estás pasando a R.

R se introdujo a finales de la década de 1990 y desde entonces su alcance ha crecido de forma espectacular. Sus capacidades son tan amplias que las alternativas comerciales han reaccionado a los desarrollos de R para seguir siendo competitivas. ([lee este artículo que compara R, SPSS, SAS, STATA y Python](https://www.inwt-statistics.com/read-blog/comparison-of-r-python-sas-spss-and-stata.html)).

Además, R es mucho más fácil de aprender que hace 10 años. Antes, R tenía fama de ser difícil para los principiantes. Ahora es mucho más fácil, con interfaces de usuario amigables como RStudio, código intuitivo como **tidyverse**, y muchos recursos tutoriales.

<span style="color: darkgreen;">**¡No te dejes intimidar, ven a descubrir el mundo de R!**</span>  

  

```{r, echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "transition_door.png"))
```




## Desde Excel  {#from-excel}

La transición de Excel directamente a R es un objetivo muy alcanzable. Puede parecer desalentador, ¡pero puedes hacerlo!

Es cierto que alguien con grandes conocimientos de Excel puede realizar actividades muy avanzadas sólo con Excel, incluso utilizando herramientas de scripting como VBA. Excel se utiliza en todo el mundo y es una herramienta esencial para la epidemiología. Sin embargo, complementarlo con R puede mejorar y ampliar drásticamente tus flujos de trabajo. 

### Beneficios {.unnumbered}  

Descubrirás que el uso de R ofrece inmensos beneficios en cuanto a ahorro de tiempo, análisis más consistentes y precisos, reproducibilidad, posibilidad de compartir y una corrección de errores más rápida. Como cualquier software nuevo, hay una "curva de aprendizaje" en la que hay que invertir tiempo para familiarizarse. Los dividendos serán significativos y se te abrirá un inmenso abanico de nuevas posibilidades con R.

Excel es un software muy conocido que permite que un principiante pueda realizar análisis y visualizaciones simples con "apuntar y clicar" de manera sencilla. En comparación, puede llevar un par de semanas sentirse cómodo con las funciones y la interfaz de R. Sin embargo, R ha evolucionado en los últimos años para ser mucho más amigable con los principiantes.

Muchos flujos de trabajo de Excel se basan en la memoria y en la repetición, por lo que hay muchas posibilidades de error. Además, generalmente la limpieza de datos, la metodología de análisis y las ecuaciones utilizadas están ocultas a la vista. A un nuevo colega le puede llevar mucho tiempo aprender lo que hace un libro de Excel y cómo resolver problemas que surjan. Con R, todos los pasos se escriben explícitamente en el script y pueden verse, editarse, corregirse y aplicarse fácilmente a otros conjuntos de datos.


**Para comenzar tu transición de Excel a R debes ajustar tu mentalidad en algunos aspectos importantes:**


### Datos ordenados (tidy data) {.unnumbered}  

Debes utilizar datos "ordenados" (tidy), legibles por la máquina en lugar de datos desordenados "legibles por el ser humano". Estos son los tres requisitos principales que los datos "ordenados" deben cumplir, como se explica en este tutorial sobre [datos "ordenados" en R](https://es.r4ds.hadley.nz/datos-ordenados.html):

* Cada variable debe tener su propia columna
* Cada observación debe tener su propia fila
* Cada valor debe tener su propia celda

Para los usuarios de Excel: piensa en el papel que desempeñan las ["tablas" de Excel](https://exceljet.net/excel-tables) para estandarizar los datos y hacer que el formato sea más predecible.

Un ejemplo de datos "ordenados" sería el listado de casos utilizado en este manual: cada variable está contenida en una columna, cada observación (un caso) tiene su propia fila y cada valor está en una sola celda. A continuación, puede ver las primeras 50 filas del listado:

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, message=FALSE, echo=F}
# muestra el listado como una tabla
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

*La principal razón por la que nos encontramos con datos no ordenados es porque muchas hojas de cálculo en Excel están diseñadas para dar prioridad a la lectura fácil por parte de los humanos, no a la lectura fácil por parte de las máquinas/el software.*

Para ayudarte a ver la diferencia, a continuación se presentan algunos ejemplos ficticios de **datos no ordenados**, los cuales dan prioridad a la legibilidad *humana* sobre la legibilidad-*mecánica*:
			
```{r, echo=F, out.width = "100%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Excel_nonTidy_1.png"))
```

*Problemas:* En la hoja de cálculo de arriba, hay *celdas combinadas* que no son fácilmente digeridas por R. No está claro qué fila debe utilizarse para la "cabecera". A la derecha hay un diccionario basado en colores y los valores de las celdas están representados por colores, lo que tampoco es fácilmente interpretado por R (¡ni por los humanos que padecen daltonismo!). Además, se combinan diferentes informaciones en una celda (varias organizaciones asociadas que trabajan en un área, o el estado "TBC" en la misma celda que "Partner D").


```{r, echo=F, out.width = "100%", out.height="100%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Excel_nonTidy_2.png"))
```

Problemas:* En la hoja de cálculo anterior, hay numerosas filas y columnas vacías adicionales dentro de los datos, lo que provocará dolores de cabeza a la hora de limpiarlos con R. Además, las coordenadas GPS están repartidas en dos filas para un centro de tratamiento determinado. Y, una nota adicional, ¡las coordenadas GPS están en dos formatos diferentes!

Los datos "ordenados" pueden no ser tan legibles para el ojo humano, pero facilitan mucho la limpieza y el análisis de los datos. Los datos ordenados pueden almacenarse en varios formatos, por ejemplo, "largos" o "anchos" (véase la página sobre [Pivotar datos](#pivoting-data)), pero se siguen observando los principios anteriores.


### Funciones {.unnumbered}  

Puede que la palabra "función" en R sea nueva, pero el concepto existe también en Excel y se le conoce como *fórmulas*. Las fórmulas en Excel también requieren una sintaxis precisa (por ejemplo, la colocación de puntos y comas y paréntesis). Lo único que hay que hacer es aprender algunas funciones nuevas y cómo funcionan en R.


### Scripts {.unnumbered}  

En lugar de clicar en los botones y arrastrar las celdas, escribirás *cada* paso y procedimiento en un "script" (secuencia de órdenes). Los usuarios de Excel pueden estar familiarizados con las "macros VBA", que también emplean un enfoque de scripting (secuencia de comandos VBA).

*El script de R consiste en instrucciones paso a paso.* Esto permite que cualquier colega pueda leer el script y ver fácilmente los pasos que has dado. Esto también ayuda a depurar errores o cálculos inexactos. Consulta la sección sobre scripts del capítulo [Fundamentos de R](#r-basics)  para ver algunos ejemplos.

Este es un ejemplo de un script en R:

```{r, echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "example_script.png"))
```

### Recursos en la migración Excel a R {.unnumbered}

Aquí hay algunos enlaces a tutoriales que te ayudarán en la transición a R desde Excel:

* [R vs. Excel](https://www.northeastern.edu/graduate/blog/r-vs-excel/)

* [Curso de RStudio en R para usuarios de Excel](https://rstudio-conf-2020.github.io/r-for-excel/)


### Interacción R-Excel {.unnumbered}  

R tiene formas robustas de importar libros de Excel, trabajar con los datos, exportar/guardar archivos de Excel y trabajar con los detalles de las hojas de Excel.

Es cierto que algunos de los formatos más estéticos de Excel pueden perderse en la traducción (por ejemplo, la cursiva, el texto lateral, etc.). Si tu flujo de trabajo requiere que pases documentos de un lado a otro entre R y Excel conservando el formato original de Excel, prueba paquetes como **openxlsx**.






## Desde Stata  {#from-stata}
<!-- ======================================================= -->

**Llegando a R desde Stata**

A muchas personas en el campo de la epidemiología se les enseña primero a usar Stata, y puede parecer desalentador pasar a R. Sin embargo, si eres un usuario habitual de Stata, el salto a R es ciertamente más manejable de lo que podrías pensar. Si bien hay algunas diferencias clave entre Stata y R en la forma en que se pueden crear y modificar los datos, así como en la forma en que se implementan las funciones de análisis - después de aprender estas diferencias clave serás capaz de adaptar tus habilidades.

A continuación, se presentan algunas traducciones clave entre Stata y R, que pueden ser útiles mientras revisas esta guía.

**Notas generales**

**STATA**                    | **R**  
---------------------------- | ---------------------------------------------    
Sólo se puede ver y manipular unos datos a la vez | Puedes ver y manipular varios conjuntos de datos al mismo tiempo, por lo que con frecuencia tendrás que especificar el conjunto de datos dentro del código
Comunidad en línea disponible a través de [https://www.statalist.org/](https://www.statalist.org/) | Comunidad online disponible a través de [RStudio](https://community.rstudio.com/), [StackOverFlow](https://stackoverflow.com/questions/tagged/r) y [R-bloggers](https://www.r-bloggers.com/)
Funcionalidad de apuntar y clicar como una opción | Funcionalidad mínima de apuntar y clicar
Ayuda para los comandos disponibles mediante el `[comando] help` | Ayuda disponible con la `[función]?` o mediante búsqueda en el panel de ayuda
Comentar el código usando *  o /// o /*  TEXTO * /| Comment code using #
Casi todos los comandos son propios de Stata. Las funciones nuevas/escritas por el usuario pueden instalarse como archivos **ado** utilizando el [paquete] **ssc install ** | R se instala con las funciones **base**, pero el uso típico implica la instalación de otros paquetes desde CRAN (véase el capítulo sobre [Fundamentos de R](#r-basics))
El análisis se suele escribir en un archivo **do** | El análisis se escribe en un script de R en el panel de fuentes de RStudio. Los scripts de R markdown son una alternativa.


**Directorio de trabajo**  

**STATA**                        | **R**  
-------------------------------- | ---------------------------------------------
Los directorios de trabajo implican rutas de archivo absolutas (por ejemplo, "C:/nombredeusuario/documentos/proyectos/datos/")| Los directorios de trabajo pueden ser absolutos, o relativos a la carpeta raíz del proyecto utilizando el paquete **here** (ver [Importar y exportar](#import-and-export)) 
Ver el directorio de trabajo actual con **pwd** | Utiliza `getwd()` o `here()` (si utilizas el paquete **here**), con paréntesis vacíos
Establecer el directorio de trabajo con **cd** "ubicación de la carpeta" | Usar `setwd("ubicación de la carpeta")`, o `set_here("ubicación de la carpeta")`, si utilizas el paquete **here**)

**Importación y visualización de datos**  

**STATA**                    | **R**  
-------------------------------- | ---------------------------------------------
Comandos específicos por tipo de archivo | Usar `import()` del paquete **rio** para casi todos los tipos de archivos. Existen funciones específicas como alternativas (véase [Importar y exportar](#import-and-export))
La lectura de los archivos csv se realiza mediante la **importación delimitada** "nombrearchivo.csv" | Usar `import("nombredearchivo.csv")`
La lectura de los archivos xslx se realiza mediante la **importación de excel** "nombre de archivo.xlsx" | Usar `import("nombredearchivo.xlsx")`
Examinar sus datos en una nueva ventana utilizando el comando **browse** | Ver unos datos en el panel de origen de RStudio utilizando `View(datos)`. *Es necesario especificar el nombre de los datos a la función en R porque se pueden mantener varios conjuntos de datos al mismo tiempo. Atención a la "V" mayúscula en esta función*
Obtener una visión general de alto nivel de su set de datos utilizando **summarize**, que proporciona los nombres de las variables y la información básica | Obtener una visión general de los datos mediante `summary(datos)`

**Manipulación básica de datos**  

**STATA**                    | **R**  
-------------------------------- | ---------------------------------------------
Las columnas de los datos suelen denominarse "variables" | Más a menudo se denominan "columnas" o a veces "vectores" o "variables"
No es necesario especificar los datos | En cada uno de los siguientes comandos, es necesario especificar los datos - véase la página sobre [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions) para ver ejemplos
Las nuevas variables se crean con el comando **generate** *varname* =  | Generar nuevas variables utilizando la función mutate(varname = ). Consultar la página sobre [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions) para obtener detalles sobre todas las funciones de **dplyr** que aparecen a continuación.
Las variables se renombran mediante **rename** *nombre_antiguo nombre_nuevo* | Las columnas pueden renombrarse mediante la función rename(nombre_antiguo = nombre_nuevo)
Las variables se eliminan con **drop** *variable* | Las columnas pueden eliminarse mediante la función select() con el nombre de la columna detrás de un signo menos, entre paréntesis
Las variables factoriales se pueden etiquetar utilizando una serie de comandos como **label define** | El etiquetado de los valores puede hacerse convirtiendo la columna en tipo Factor y especificando los niveles. Mira en la página sobre [Factores](#factors). Los nombres de las columnas no suelen estar etiquetados como en Stata.

**Análisis descriptivo**  

**STATA**                    | **R**  
-------------------------------- | ---------------------------------------------
Tabular los recuentos de una variable mediante el **tab** *variable* | Proporcionar los datos y el nombre de la columna al comando table() como table(conjunto_de_datos$nombre_columna). Alternativamente, utilizar count(varname) del paquete **dplyr**, como se explica en [Agrupar datos](#grouping-data)
La tabulación cruzada de dos variables en una tabla de 2x2 se realiza con **tab** *variable1 variable2* | Utilizar `table(datos$nombre_variable1, datos$nombre_variable2 o count(nombre_variable1, nombre_variable2)`


Aunque esta lista ofrece una visión general de los fundamentos de la traducción de los comandos de Stata a R, no es exhaustiva. Hay muchos otros grandes recursos para los usuarios de Stata que podrían ser de interés en tu transición a R:

* https://dss.princeton.edu/training/RStata.pdf  
* https://clanfear.github.io/Stata_R_Equivalency/docs/r_stata_commands.html  
* http://r4stats.com/books/r4stata/  




## Desde SAS  {#from-stata}
<!-- ======================================================= -->

**Pasar de SAS a R**

SAS se utiliza habitualmente en las agencias de salud pública y en los campos de investigación académica. Aunque la transición a un nuevo lenguaje no suele ser un proceso sencillo, entender las diferencias clave entre SAS y R puede ayudarte a empezar a navegar por el nuevo lenguaje utilizando el lenguaje de partida. A continuación se describen las principales traducciones en materia de gestión de datos y análisis descriptivo entre SAS y R.

**Notas generales**

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Comunidad en línea disponible a través del [Servicio de Atención al Cliente de SAS](https://support.sas.com/en/support-home.html)|Comunidad online disponible a través de RStudio, StackOverFlow y R-bloggers
Ayuda para los comandos disponibles mediante `help [comando]`|Ayuda disponible usando mediante `[función]?` o buscando en el panel de ayuda
Comentar el código usando `* TEXTO` ; o `/* TEXTO */`|Comentar el código usando #
Casi todos los comandos están incorporados. Los usuarios pueden escribir nuevas funciones utilizando macros SAS, SAS/IML, SAS Component Language (SCL) y, más recientemente, los procedimientos `Proc Fcmp` y `Proc Proto`|R se instala con las funciones **base**, pero el uso típico implica la instalación de otros paquetes desde CRAN (véase la página sobre [Fundamentos de R](#r-basics))
El análisis suele realizarse escribiendo un programa SAS en la ventana del Editor. |Análisis escrito en un script de R en el panel de fuentes de RStudio. Los scripts de R markdown son una alternativa.

**Directorio de trabajo**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Los directorios de trabajo pueden ser absolutos o relativos a la carpeta raíz del proyecto, definiendo la carpeta raíz con `%let rootdir=/ruta raíz; %include "&rootdir/subfoldername/archivo"`|Los directorios de trabajo pueden ser absolutos, o relativos a la carpeta raíz del proyecto utilizando el paquete **here** (ver [Importar y exportar](#import-and-export))
Ver el directorio de trabajo actual con `%put %sysfunc(getoption(work));`|Utilizar `getwd()` o `here()` (si utilizas el paquete **here**), con paréntesis vacíos
Establecer el directorio de trabajo con `libname "ubicación de la carpeta"`|Utiliza `setwd("ubicación de la carpeta")`, o `set_here("ubicación de la carpeta")` si utilizas el paquete **here**


**Importación y visualización de datos**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Utiliza el procedimiento `Proc Import` o la sentencia `Data Step Infile`.|Utiliza `import()` del paquete **rio** para casi todos los tipos de archivos. Existen funciones específicas como alternativas (véase [Importar y exportar](#import-and-export))
La lectura de los archivos csv se realiza mediante `Proc Import datafile="nombre de archivo.csv" out=nombre de archivo dbms=CSV; run;` O mediante la sentencia [Data Step Infile](http://support.sas.com/techsup/technote/ts673.pdf)|Utiliza import("nombredearchivo.csv")
La lectura de los archivos xslx se realiza utilizando `Proc Import datafile="filename.xlsx" out=work.filename dbms=xlsx; run;` O utilizando la sentencia [Data Step Infile|Use](http://support.sas.com/techsup/technote/ts673.pdf)|Utiliza `import("filename.xlsx")`
Examinar los datos en una nueva ventana abriendo la ventana del Explorador y seleccionar la biblioteca deseada y los datos|Ver unos datos en el panel de RStudio utilizando `View(datos)`. Se necesita especificar el nombre del set de datos a la función en R porque se pueden mantener múltiples conjuntos de datos al mismo tiempo. Atención a la "V" mayúscula en esta función

**Manipulación básica de datos**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Las columnas de los datos suelen denominarse "variables"|Más a menudo se denominan "columnas" o a veces "vectores" o "variables"
No es necesario ningún procedimiento especial para crear una variable. Las nuevas variables se crean simplemente escribiendo el nombre de la nueva variable, seguido de un signo igual, y luego una expresión para el valor|Generar nuevas variables utilizando la función mutate(). Consulta la página sobre [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions) para obtener detalles sobre todas las funciones de **dplyr** que aparecen a continuación.
Las variables se renombran utilizando rename `*nombre_antiguo=nuevo_nombre*`.|Las columnas pueden renombrarse mediante la función `rename(nuevo_nombre = nombre_antiguo)`
Las variables se guardan con `**keep**=nombre de la variable`|Las columnas pueden seleccionarse mediante la función `select()` con el nombre de la columna entre paréntesis
Las variables se eliminan con `**drop**=nombre de la variable`|Las columnas pueden eliminarse mediante la función `select()` con el nombre de la columna detrás de un signo menos, entre paréntesis
Las variables factoriales pueden etiquetarse en el mediante la sentencia `Label`|El etiquetado de los valores puede hacerse convirtiendo la columna en una de tipo Factor y especificando los niveles. Véase la página sobre [Factores](#factors). Los nombres de las columnas no se suelen etiquetar.
Los registros se seleccionan utilizando la sentencia `Where o If`. Las condiciones de selección múltiple se separan con el comando "and".|Los registros se seleccionan mediante la función `filter()` con múltiples condiciones de selección separadas por un operador AND (&) o una coma
Los datos se combinan utilizando la sentencia `Merge`. Los datos que se van a combinar deben ordenarse primero mediante el procedimiento `Proc Sort`.|El paquete **dplyr** ofrece algunas funciones para fusionar conjuntos de datos. Para más detalles, consulta la página de [Unir datos](#joining-data).

**Análisis descriptivo**  

**SAS**                          | **R**  
-------------------------------- | ---------------------------------------------
Obtener una visión general de los datos mediante el procedimiento `Proc Summary`, que proporciona los nombres de las variables y las estadísticas descriptivas|Obtener una visión general de tus datos mediante `summary(datos)` o `skim(datos)` del paquete **skimr**
Tabular los recuentos de una variable utilizando `proc freq data=Dataset; Tables varname; Run;`|Véase la página sobre [tablas descriptivas](#descriptive-tables). Las opciones incluyen `table()` de R **base**, y `tabyl()` del paquete **janitor**, entre otras. Ten en cuenta que tendrás que especificar los datos y el nombre de la columna, ya que R mantiene múltiples conjuntos de datos.
La tabulación cruzada de dos variables en una tabla 2x2 se realiza con `proc freq data=Dataset; Tables rowvar* colvar; Run;`|De nuevo, se puedes utilizar table(), tabyl() u otras opciones como se describe en la página de [tablas descriptivas](#descriptive-tables).

**Algunos recursos útiles:**  

[R for SAS and SPSS Users (2011)](https://www.amazon.com/SAS-SPSS-Users-Statistics-Computing/dp/1461406846/ref=sr_1_1?dchild=1&gclid=EAIaIQobChMIoqLOvf6u7wIVAhLnCh1c9w_DEAMYASAAEgJLIfD_BwE&hvadid=241675955927&hvdev=c&hvlocphy=9032185&hvnetw=g&hvqmt=e&hvrand=16854847287059617468&hvtargid=kwd-44746119007&hydadcr=16374_10302157&keywords=r+for+sas+users&qid=1615698213&sr=8-1)

[SAS and R, Second Edition (2014)](https://www.amazon.com/SAS-Management-Statistical-Analysis-Graphics-dp-1466584491/dp/1466584491/ref=dp_ob_title_bk)



## Interoperabilidad de los datos {#from-stata}
<!-- ======================================================= -->

Consulta la página de [importación y exportación](#import-and-export) para obtener detalles sobre cómo el paquete **rio** puede importar y exportar los archivos .dta de STATA, los archivos .xpt y .sas7bdat de SAS, los archivos .por y .sav de SPSS, y muchos otros.



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/transition_to_R.Rmd-->

# Paquetes recomendados {#suggested-packages-1}

A continuación se muestra lista de paquetes de R de utilidad para la realización de tareas frecuentes en epidemiología. Puedes copiar este código, ejecutarlo, y todos estos paquetes se instalarán desde CRAN y se cargarán para su uso en la sesión actual de R. Si un paquete ya está instalado, únicamente se cargará.

Puedes modificar el código con símbolos `#` para excluir los paquetes que no desees instalar.

A tener en cuenta:

* Instala primero el paquete **pacman** antes de ejecutar el código. Puedes hacerlo con `install.packages("pacman")`. En este manual hacemos hincapié en `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para utilizarlo en la sesión actual de R. También puedes cargar paquetes ya instalados con `library()` de R **base**.

* En el código siguiente, los paquetes que se incluyen al instalar/cargar otro paquete se indican con una sangría y un hash (#). Por ejemplo, **ggplot2** aparece bajo **tidyverse** en forma de comentario.

* Si varios paquetes tienen funciones con el mismo nombre, puede ocurrir *enmascaramiento* cuando la función del paquete cargado más recientemente tiene prioridad. Lee más en la página de [Fundamentos de R](#r-basics). Puedes gestionar estos conflictos con el paquete **conflicted** .

* Consulta la sección de [Fundamentos de R](#r-basics) sobre paquetes para obtener más información sobre **pacman** y el enmascaramiento.

Para ver las versiones de R, RStudio y los paquetes de R utilizados durante la producción de este manual, consulta la página de [notas editoriales y técnicas](#editorial-and-technical-notes).

## Paquetes desde CRAN {#packages-from-cran}

```{r, eval=F}

######################################################
# Listado de paquetes útiles para epidemiología en R #
######################################################

# Este código usa la función p_load() del paquete "pacman", 
# la cual instala el paquete si todavía no está instalado y en caso de no ser necesaria la instalación, procede a cargar el paquete. 


# Asegúrate que el paquete "pacman" está instalado
if (!require("pacman")) install.packages("pacman")


# Paquetes disponibles desde CRAN
#################################
pacman::p_load(
     
     # Aprendiendo R
     ###############
     learnr,   # tutoriales interactivos en tu panel de R Studio
     swirl,    # tutoriales interactivos en tu consola de R
        
     # Manejo de archivos y proyecto 
     ###############################
     here,     # describir ruta de archivo dentro de la carpeta principal del proyecto
     rio,      # importación/exportación de múltiples tipos de datos
     openxlsx, # importación/exportación de libros con múltiples hojas de excel
     
     # Manejo e instalación de paquetes
     ##################################
     pacman,   # instalación/carga de paquetes
     renv,     # manejo de versiones de paquetes para trabajar con grupos colaborativos.
     remotes,  # instalación de paquetes desde github
     
     # Manejo general de datos
     #########################
     tidyverse,    # incluye múltiples paquetes para el tratamiento de datos en formato tidy y presentación de los mismos.
          #dplyr,      # manejo de datos
          #tidyr,      # manejo de datos
          #ggplot2,    # visualización de datos
          #stringr,    # trabajo con cadenas y caracteres
          #forcats,    # trabajo con factores
          #lubridate,  # trabajo con fechas
          #purrr       # iteraciones y trabajo con listas
     linelist,     # limpiar linelists
     naniar,       # trabajo con valores perdidos
     
     # Estadística  
     ############
     janitor,      # tablas y limpieza de datos
     gtsummary,    # hacer tablas descriptivas con valores estadísticos
     rstatix,      # realización rápida de test estadísticos y tablas descriptivas
     broom,        # pasar a formato tidy los resultados de las regresiones
     lmtest,       # realizar test de likelihood-ratio
     easystats,
          # parameters, # alternativa para pasar a formato tidy los resultados de las regresiones 
          # see,        # alternativa para visualizar forest plots 
     
     # Realización de modelos epidémicos
     ###################################
     epicontacts,  # analizar cadenas de transmisión
     EpiNow2,      # estimación de Rt 
     EpiEstim,     # estimación de Rt
     projections,  # proyecciones de incidencia
     incidence2,   # hacer curvas epidémicas y manejar datos de incidencia
     i2extras,     # Funciones extra para el paquete incidence2 
     epitrix,      # funciones útiles para epidemiología
     distcrete,    # Distribuciones discretas de demora o retardo
     
     # plots - general
     #################
     #ggplot2,         # incluido en tidyverse
     cowplot,          # combinar plots  
     # patchwork,      # alternativa para combinar plots    
     RColorBrewer,     # escalas de color
     ggnewscale,       # para añadir capas de color adicionales
     
     # plots - tipos específicos
     ########################
     DiagrammeR,       # diagramas empleando lenguaje DOT
     incidence2,       # curvas epidémicas
     gghighlight,      # destacar un subgrupo
     ggrepel,          # etiquetas inteligentes (smart labels)
     plotly,           # gráficos interactivos
     gganimate,        # gráficos animados

     
     # gis
     ######
     sf,               # manejo de datos espaciales usando el formato Simple Features
     tmap,             # producción sencilla de mapas, tanto estáticos como interactivos
     OpenStreetMap,    # añadir una base con un mapa de OSM a un mapa en ggplot
     spdep,            # estadística espacial
     
     # reportes rutinarios
     #################
     rmarkdown,        # producción de archivos PDF, Word, Powerpoint y HTML
     reportfactory,    # auto-organización de los trabajos realizados en R Markdown 
     officer,          # powerpoints
     
     # dashboards
     ############
     flexdashboard,    # convierte código de R Markdown en un dashboard
     shiny,            # aplicaciones web interactivas
     
     # tablas for para presentaciones
     #########################
     knitr,            # generación de reportes y tablas HTML con R Markdown 
     flextable,        # tablas HTML tables
     #DT,              # tablas HTML (alternativa)
     #gt,              # tablas HTML (alternativa)
     #huxtable,        # tablas HTML (alternativa) 
     
     # filogenética
     ###############
     ggtree,           # visualización and anotación de árboles
     ape,              # análisis de filogenética y evolución
     treeio            # visualización de archivos filogenéticos
 
)

```

## Paquetes desde Github {#packages-from-github}

A continuación se muestran los comandos para instalar dos paquetes directamente desde los repositorios de Github.

* La versión de desarrollo de **epicontacts** tiene la capacidad de hacer árboles de transmisión con un eje temporal-x

* El paquete **epirhandbook** contiene todos los datos de ejemplo de este manual y puede utilizarse para descargar la versión sin conexión del manual.

```{r, eval=F}
# Paquetes para descargar desde Github (no disponibles en CRAN)
##########################################################

# Version en desarrollo de epicontacts (para cadenas de transmisión con eje temporal en el eje x)
pacman::p_install_gh("reconhub/epicontacts@timeline")

# El paquete de este manual, el cual incluye todos los datos empleados en los ejemplos 
pacman::p_install_gh("appliedepi/epirhandbook")



```
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/packages_suggested.Rmd-->

# Proyectos en R {#r-projects}

Un proyecto de R permite agrupar tu trabajo en una carpeta que contiene todos los archivos vínculados al mismo, facilitando su manejo. Dentro del proyecto, todos los scripts relevantes, los archivos de datos, las figuras/resultados y el historial se almacenan en subcarpetas y, lo que es más importante, el *directorio de trabajo* de dicho proyecto constituye la carpeta raíz del mismo.

## Uso sugerido {#suggested-use}

Una forma común, eficiente y sencilla de utilizar R es combinar 3 elementos. Un proyecto de trabajo concreto se aloja dentro de un proyecto R. Cada uno de los tres elementos anteriores se describe a continuación.

1)  Un **proyecto en R**

-   Un entorno de trabajo autónomo con carpetas para datos, scripts, salidas (outputs), etc

2)  El paquete **here**, el cual se utiliza para indicar las rutas relativas de los archivos

-   Las rutas de los archivos se escriben en relación con la ubicación de la carpeta raíz del proyecto R - véase [Importar y exportar](#import-and-export) para más información

3)  El paquete **rio** para importar/exportar -`import()` y `export()` manejan cualquier tipo de archivo por su extensión (por ejemplo, .csv, .xlsx, .png)

<!-- ======================================================= -->

## Creación de un proyecto R {#creating-an-r-project}

Para crear un proyecto R, selecciona "New proyect" en el menú File (Archivo).

-   Si quieres crear una nueva carpeta para el proyecto, selecciona "New directory" e indica dónde quieres crear la carpeta.
-   Si deseas crear el proyecto dentro de una carpeta existente, cliquea en "Existing Directory" e indica la carpeta.
-   Si quieres clonar un repositorio de Github, selecciona la tercera opción "Version Control" y luego "Git". Consulta la página [Control de versiones y colaboración con Git y Github](#version-control-and-collaboration-with-git-and-github) para más detalles.

```{r out.width = "75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "create_project.png"))
```

El proyecto R que has creado estará en una carpeta que contiene un archivo *.Rproj*. Este archivo es un acceso directo y probablemente la forma más directa de abrir tu proyecto. También puedes abrir un proyecto seleccionando "Open Project" en el menú File. Alternativamente, en el extremo superior derecho de RStudio verás un icono de R projects y un menú desplegable de proyectos disponibles.

Para salir de un proyecto R, abre un nuevo proyecto o cierra el proyecto actual (Archivo - Cerrar proyecto).

### Cambiar de proyecto  {.unnumbered}

Para cambiar entre proyectos, clica en el icono de R projects en el menú desplegable en la parte superior derecha de RStudio. Verás las opciones de Cerrar proyecto, Abrir proyecto y una lista de proyectos recientes.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "Rproject_dropdown.png"))
```

### Ajustes  {.unnumbered}

Generalmente se aconseja que inicies RStudio cada vez con una "pizarra limpia" - es decir, con tu espacio de trabajo **no** arrastrado de la sesión anterior. Esto significará que los objetos y resultados de una sesión no persistirán en la siguiente sesión (deberás volver a crearlos al ejecutar tus scripts). Esto es bueno, porque te obligará a escribir mejores scripts y evitar errores a largo plazo.

Para configurar RStudio para que haga "borrón y cuenta nueva" cada vez que se inicie:

-   Selecciona "Project Options" en el menú Tools (Herramientas).

-   En la pestaña "General", configura RStudio para que **no** restaure .RData en el espacio de trabajo al iniciar, y para que **no** guarde el espacio de trabajo en .RData al salir.

### Organización  {.unnumbered}

Es habitual tener subcarpetas en tu proyecto. Piensa en tener carpetas como "datos", "scripts", "figuras" y "presentaciones". Puedes añadir carpetas de la forma típica en que añadirías una nueva carpeta en tu ordenador. Alternativamente, puedes ver en la página sobre [interacciones con directorios](#directory-interactions) para aprender a crear nuevas carpetas con los comandos de R.

### Control de versiones  {.unnumbered}

Considera utilizar un sistema de control de versiones. Podría ser algo tan simple como tener fechas en los nombres de los scripts (por ejemplo, "transmission_analysis_2020-10-03.R") y una carpeta de "archivado". También es buena idea tener un texto de cabecera agregando al comienzo de cada script una descripción, etiquetas, autores y un registro de cambios.

Un método más complicado implicaría utilizar Github o una plataforma similar para el control de versiones. Consulta la página sobre [Control de versiones y colaboración con Git y Github](#version-control-and-collaboration-with-git-and-github).

Un consejo es que puedes realizas búsquedas en todo un proyecto o carpeta utilizando la herramienta "Buscar en archivos" (menú Edición). Puedes buscar e incluso reemplazar líneas de script en varios archivos.

## Ejemplos {#examples}

A continuación se muestran algunos ejemplos de importación/exportación/guardado utilizando `here()` desde un proyecto R. Lea más sobre el uso del paquete **here** en la página de [importación y exportación](#import-and-export).

*Importación de `linelist_raw.xlsx` desde la carpeta "data" de tu proyecto R*

```{r eval=F}
linelist <- import(here("data", "linelist_raw.xlsx"))
```

*Exportar linelist de objetos de R como "my_linelist.rds" a la carpeta "clean" dentro de la carpeta "data" de tu proyecto R.*

```{r, eval=F}
export(linelist, here("data","clean", "my_linelist.rds"))
```

*Guardando el último gráfico creado como "epicurve_2021-02-15.png" dentro de la carpeta "epicurves" en la carpeta "outputs" de tu proyecto R.*

```{r, eval=F}
ggsave(here("outputs", "epicurves", "epicurve_2021-02-15.png"))
```

<!-- ======================================================= -->

## Recursos {#resources}

Página web de RStudio sobre [uso de proyectos R](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/r_projects.Rmd-->

# Importación y exportación {#import-and-export}


```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Import_Export_1500x500.png"))
```



En esta página describimos las formas de localizar, importar y exportar
archivos:

* Uso del paquete **rio** y las funciones `import()` y `export()` para importar muchos tipos de archivos.

* Uso del paquete **here** para localizar archivos relativos a la raíz de un proyecto R - para evitar complicaciones de las rutas de los archivos que son específicas de un ordenador

* Escenarios específicos de importación, como:
 * Hojas de Excel
 * Encabezados desordenados y filas que se saltan/son omitidas
 * Desde las hojas de Google
 * A partir de datos publicados en sitios web
 * Con las API
 * Importar el archivo *más reciente*
* Introducción manual de datos
* Tipos de archivos específicos de R, como RDS y RData
* Exportar/guardar archivos y gráficos


<!-- ======================================================= -->
## Resumen {#overview}

Cuando importas unos "datos" en R, generalmente estás creando un nuevo objeto *data frame* en tu entorno de R y definiéndolo como un archivo importado (por ejemplo, Excel, CSV, TSV, RDS) que será guardado en tu disco en una determinada ruta/dirección.

Puedes importar/exportar muchos tipos de archivos, incluidos los creados por otros programas estadísticos (SAS, STATA, SPSS). También puedes conectarte a bases de datos relacionales.

R tiene incluso sus propios formatos de datos:

* Un archivo RDS (.rds) almacena un único objeto R, como un dataframe. Son útiles para almacenar datos limpios, ya que mantienen los tipos de columnas de R. Lee más en [esta sección](#import_rds).
* Un archivo RData (.Rdata) puede utilizarse para almacenar múltiples objetos, o incluso un espacio de trabajo R completo. Lee más en [esta sección](#import_rdata).


<!-- ======================================================= -->
## El paquete **rio** {#the-rio-package}

El paquete de R que recomendamos es: **rio**. El nombre "rio" es una abreviatura de "R I/O" (input/output).

Sus funciones `import()` y `export()` pueden manejar muchos tipos de archivos diferentes (por ejemplo, .xlsx, .csv, .rds, .tsv). Cuando se proporciona una ruta de archivo a cualquiera de estas funciones (incluyendo la extensión del archivo como ".csv"), **rio** leerá la extensión y utilizará la herramienta correcta para importar o exportar el archivo.

La alternativa al uso de **rio** es utilizar funciones de muchos otros paquetes, cada uno de los cuales es específico para un tipo de archivo. Por ejemplo, `read.csv()` (R **base**), `read.xlsx()` (paquete **openxlsx**), y `write_csv()` (paquete **readr**), etc. Estas alternativas pueden ser difíciles de recordar, mientras que usar `import()` y `export()` de **rio** es fácil.

Las funciones `import()` y `export()` de **rio** utilizan el paquete y la función adecuados para un archivo determinado, basándose en su extensión. Al final de esta página puedes ver una tabla completa de los paquetes/funciones que utiliza **rio** en segundo plano. También puede utilizarse para importar archivos de STATA, SAS y SPSS, entre otras docenas de tipos de archivos.

La importación/exportación de shapefiles requiere otros paquetes, como se detalla en la página sobre [Conceptos básicos de los SIG](#gis-basics) (Sistemas de Información Geográfica).


## El paquete **here** {#here}

El paquete **here** y su función `here()` facilitan la tarea de decirle a R dónde encontrar y guardar tus archivos - en esencia, construye rutas de archivos.

Utilizado junto con un proyecto R, **here** te permite describir la ubicación de los archivos en tu proyecto R en relación con el *directorio raíz* de los proyectos de R (la carpeta de nivel superior). Esto es útil cuando el proyecto R puede ser compartido o accedido por múltiples personas/ordenadores. Evita las complicaciones debidas a las rutas de archivos únicas en diferentes ordenadores (por ejemplo, `"C:/Users/Laura/Documents..."` al "comenzar" la ruta de archivos en un lugar común a todos los usuarios (la raíz del proyecto R).

Así es como funciona `here()` dentro de un proyecto R:

* Cuando, dentro del proyecto R, se carga por primera vez el paquete **here**, se coloca un pequeño archivo llamado ".here" en la carpeta raíz de tu proyecto R como un "punto de referencia" o "ancla"

* En tus scripts, para referenciar un archivo en las subcarpetas del proyecto R, se utiliza la función `here()` para construir la ruta del archivo *en relación con ese ancla*

* Para construir la ruta de los archivos, escribe los nombres de las carpetas más allá de la raíz, entre comillas, separados por comas, y finalmente termina con el nombre y la extensión del archivo, como se muestra a continuación

* Las rutas de `here()` pueden utilizarse tanto para la importación como para la exportación

Por ejemplo, a continuación, la función import() recibe una ruta de archivo construida con `here()`.

```{r, eval=F}
linelist <- import(here("data", "linelists", "ebola_linelist.xlsx"))
```

El comando `here("data", "linelists", "ebola_linelist.xlsx")` en realidad está proporcionando la ruta completa del archivo que es *única para el ordenador del usuario*:


```
"C:/Users/Laura/Documents/my_R_project/data/linelists/ebola_linelist.xlsx"
```

Lo bueno es que el comando `here()` puede ejecutarse en cualquier ordenador que acceda al proyecto R.


<span style="color: darkgreen;">**_CONSEJO:_** Si no estás seguro de dónde está la raíz ".here", ejecuta la función `here()` con los paréntesis vacíos.</span>  

Lee más sobre el paquete **here** [en este enlace](https://here.r-lib.org/).  



<!-- ======================================================= -->
## Rutas de los archivos {#file-paths}  

Al importar o exportar datos, debes proporcionar una ruta de archivo. Puedes hacerlo de tres maneras:

1.  *Recomendado:* proporcionar una ruta de archivo "relativa" con el paquete **here**
2.  Proporcionar la ruta "completa" / "absoluta" del archivo
3.  Seleccionar manualmente los archivos



### Rutas de archivos "relativas" {.unnumbered}

En R, las rutas de archivo "relativas" consisten en la ruta de archivo *relativa a* la raíz de un proyecto R. Permiten rutas de archivo más simples que pueden funcionar en diferentes ordenadores (por ejemplo, si el proyecto R está en una unidad compartida o se envía por correo electrónico). Como se ha descrito [anteriormente](#here) las rutas de archivo relativas se facilitan mediante el uso del paquete **here**.

A continuación se muestra un ejemplo de una ruta de archivo relativa construida con `here()`. Suponemos que el trabajo está en un proyecto R que contiene una subcarpeta "data" y dentro de ella una subcarpeta "linelists", en la que está el archivo .xlsx de interés.

```{r, eval=F}
linelist <- import(here("data", "linelists", "ebola_linelist.xlsx"))
```



### Rutas "absolutas" {.unnumbered}  

Las rutas absolutas o "completas" de los archivos pueden proporcionarse a funciones como `import()`, pero son "frágiles", ya que son únicas para el ordenador específico del usuario y, por tanto, *no se recomiendan*.

A continuación se muestra un ejemplo de ruta absoluta de archivos, donde en el ordenador de Laura hay una carpeta "analysis", una subcarpeta "data" y dentro de ésta una subcarpeta "linelists", en la que se encuentra el archivo .xlsx de interés.

```{r, eval=F}
linelist <- import("C:/Users/Laura/Documents/analysis/data/linelists/ebola_linelist.xlsx")
```

Hay que tener en cuenta algunas cosas sobre las rutas absolutas de los archivos: 

* **Evita utilizar rutas absolutas de archivos**, ya que el script no funcionará si se ejecuta en un ordenador diferente  
* *Utiliza barras inclinadas* (`/`), como en el ejemplo anterior (nota: esto *NO* es el valor predeterminado para las rutas de archivos de Windows)
* Las rutas de archivos que comienzan con barras dobles (por ejemplo, "//...") probablemente **no serán reconocidas por R** y producirán un error. Considera la posibilidad de trasladar tu trabajo a una unidad "con nombre" o que comience con una letra (por ejemplo, "J:" o "C:"). Consulta la página sobre [interacciones con directorios](#directory-interactions) para obtener más detalles sobre esta cuestión.

Un escenario en el que las rutas absolutas de los archivos pueden ser apropiadas es cuando se quiere importar un archivo desde una unidad compartida que tiene la misma ruta de archivo completa para todos los usuarios.

<span style="color: darkgreen;">**_CONSEJO:_** Para convertir rápidamente las barras inclinadas `\` a `/`, resalta el código de interés, usa Ctrl+f (en Windows), marca la casilla de opción para "En selección", y luego usa la funcionalidad de reemplazo para convertirlos.</span>  



<!-- ======================================================= -->
### Selección manual {.unnumbered}

Puedes importar datos manualmente mediante uno de estos métodos:

1.  En el panel de entorno de RStudio, cliquea en "Import Dataset", y selecciona el tipo de datos
2.  Cliquea en File / Import dataset / (selecciona el tipo de datos)
3.  Para codificar la selección manual, utiliza el comando de `file.choose()` (dejando los paréntesis vacíos) para provocar la aparición de una **ventana emergente** que permita al usuario seleccionar manualmente el archivo de su ordenador. Por ejemplo:

```{r import_choose, eval=F}
# Selección manual de un archivo. Cuando se ejecute este comando, aparecerá una ventana EMERGENTE.

# La ruta del archivo seleccionado será suministrada al comando import().

my_data <- import(file.choose())
```

<span style="color: darkgreen;">**_CONSEJO:_** La **ventana emergente** puede aparecer DETRÁS de la ventana de RStudio.</span>



## Importar datos {#import-data}

Utilizar `import()` es bastante sencillo. Simplemente escribe la ruta del archivo (incluyendo el nombre y la extensión del archivo) entre comillas. Si utilizas here() para construir la ruta del archivo, sigue las instrucciones anteriores. A continuación se muestran algunos ejemplos:

Para importar un archivo csv que se encuentra en tu "directorio de trabajo" o en la carpeta raíz del proyecto R:

```{r, eval=F}
linelist <- import("linelist_cleaned.csv")
```


Para importar la primera hoja de un archivo de Excel que se encuentra en las subcarpetas "data" y "linelists" del proyecto R (la ruta del archivo construida con `here()`):

```{r, eval=F}
linelist <- import(here("data", "linelists", "linelist_cleaned.xlsx"))
```


Para importar un data frame (un archivo .rds) utilizando una ruta de archivo absoluta:

```{r, eval=F}
linelist <- import("C:/Users/Laura/Documents/tuberculosis/data/linelists/linelist_cleaned.rds")
```





### Hojas de Excel específicas {.unnumbered}

Por defecto, si proporcionas un archivo de Excel (.xlsx) a `import()`, se importará la primera hoja del libro. Si deseas importar una **hoja** específica, incluye el nombre de la hoja con el argumento `which = `. Por ejemplo:

```{r eval=F}
my_data <- import("my_excel_file.xlsx", which = "Sheetname")
```

Utilizando el método `here()` para proporcionar una vía relativa a import(), también podés importar una hoja específica añadiendo el argumento which = después del paréntesis de cierre de la función here().

```{r import_sheet_here, eval=F}
# Demostración: importación de una hoja de Excel específica al utilizar rutas relativas con el paquete 'here'
linelist_raw <- import(here("data", "linelist.xlsx"), which = "Sheet1")`  
```

Para *exportar* un dataframe de R a una hoja de Excel específica y que el resto del archivo de Excel permanezca sin cambios, tendrás que importar, editar y exportar con un paquete alternativo destinado a este fin, como **openxlsx**. Vea más información en la página sobre las [interacciones con directorios](#directory-interactions) o [en esta página de github](https://ycphs.github.io/openxlsx/).

Si tu libro de Excel es .xlsb (libro de Excel en formato binario) es posible que no puedas importarlo con **rio**. Considera la posibilidad de volver a guardarlo como .xlsx, o de utilizar un paquete como **readxlsb**, creado para [este fin](https://cran.r-project.org/web/packages/readxlsb/vignettes/read-xlsb-workbook.html).






<!-- ======================================================= -->
### Valores faltantes {#import_missing .unnumbered}  

Es posible que desees designar qué valor(es) de tus datos se debe(n) considerar como faltantes (missing values). Como se explica en la página sobre [Valores faltantes](#missing-data), el valor en R para los valores faltantes es `NA`, pero tal vez los datos que vas a importar utiliza 99, "Missing", o simplemente el espacio de caracteres vacíos "" en su lugar.

Utiliza el argumento `na = ` en `import()` y proporciona el(los) valor(es) entre comillas (incluso si son números). Puedes especificar varios valores incluyéndolos dentro de un vector, utilizando `c()` como se muestra a continuación.

Aquí, el valor "99" en los datos importados se considera valor faltante y se convierte en `NA` en R.

```{r, eval=F}
linelist <- import(here("data", "my_linelist.xlsx"), na = "99")
```

Aquí, cualquiera de los valores "Missing", "" (celda vacía), o " " (un solo espacio) en los datos importados se convierten en `NA` en R. 

```{r, eval=F}
linelist <- import(here("data", "my_linelist.csv"), na = c("Missing", "", " "))
```


<!-- ======================================================= -->
### Saltar filas {.unnumbered} 

Si querés evitar la importación de una fila de datos, puedes hacerlo con el argumento `skip = ` utilizando `import()` de **rio** en un archivo .xlsx o .csv. Debes proporcionar el número de filas que deseas omitir.


```{r, eval=F}
linelist_raw <- import("linelist_raw.xlsx", skip = 1)  # no importa la fila de cabecera
```

Desafortunadamente, `skip = ` sólo acepta un valor entero, *no* un rango (por ejemplo, "2:10" no funciona). Para omitir la importación de filas específicas que no son consecutivas desde el principio, considera la posibilidad de importar varias veces y utilizar `bind_rows()` de **dplyr**. Mira en el ejemplo siguiente cómo se omite sólo la fila 2.



### Gestionar una segunda fila de cabecera {.unnumbered}  

A veces, tus datos pueden tener una *segunda* fila de cabecera, por ejemplo, si se trata de una fila de "diccionario de datos", como se muestra a continuación. Esta situación puede ser problemática porque puede hacer que todas las columnas se importen como de tipo "carácter".

```{r, echo=F}
# OCULTO PARA EL LECTOR
#######################
# Crear la segunda fila de cabecera del "diccionario de datos" e insertarla en la fila 2. Guardar como nuevo dataframe.
linelist_2headers <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) %>%         
        mutate(across(everything(), as.character)) %>% 
        add_row(.before = 1,
                #row_num = "000",
                case_id = "case identification number assigned by MOH",
                generation = "transmission chain generation number",
                date_infection = "estimated date of infection, mm/dd/yyyy",
                date_onset = "date of symptom onset, YYYY-MM-DD",
                date_hospitalisation = "date of initial hospitalization, mm/dd/yyyy",
                date_outcome = "date of outcome status determination",
                outcome = "either 'Death' or 'Recovered' or 'Unknown'",
                gender = "either 'm' or 'f' or 'unknown'",
                hospital = "Name of hospital of first admission",
                lon = "longitude of residence, approx",
                lat = "latitude of residence, approx",
                infector = "case_id of infector",
                source = "context of known transmission event",
                age = "age number",
                age_unit = "age unit, either 'years' or 'months' or 'days'",
                fever = "presence of fever on admission, either 'yes' or 'no'",
                chills = "presence of chills on admission, either 'yes' or 'no'",
                cough = "presence of cough on admission, either 'yes' or 'no'",
                aches = "presence of aches on admission, either 'yes' or 'no'",
                vomit = "presence of vomiting on admission, either 'yes' or 'no'",
                time_admission = "time of hospital admission HH:MM")
```

A continuación se muestra un ejemplo de este tipo de de datos (en el que la primera fila es el diccionario de datos).

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist_2headers, 5), rownames = FALSE, filter="top", options = list(pageLength = 4, scrollX=T), class = 'white-space: nowrap' )
```

#### Eliminar la segunda fila de la cabecera {.unnumbered}  

Para eliminar la segunda fila de la cabecera, tendrás que importar los datos dos veces.

1) Importar los datos para almacenar los nombres correctos de las columnas
2) Importar los datos de nuevo, saltándose las *dos* primeras filas (cabecera y segunda fila)
3)Vincular los nombres correctos en el dataframe reducido

El argumento exacto utilizado para vincular los nombres de las columnas correctas depende del tipo de archivo de datos (.csv, .tsv, .xlsx, etc.). Esto se debe a que **rio** utiliza una función diferente para los distintos tipos de archivos (véase la tabla anterior).

**Para los archivos de Excel:** (`col_names = `)

```{r, eval=F}
# importa por primera vez; almacena los nombres de las columnas
linelist_raw_names <- import("linelist_raw.xlsx") %>% names()  # guarda los nombres de columna

# importa por segunda vez; omite la fila 2, y asigna los nombres de las columnas al argumento col_names =
linelist_raw <- import("linelist_raw.xlsx",
                       skip = 2,
                       col_names = linelist_raw_names
                       ) 
```

**Para archivos CSV:** (`col.names = `)

```{r, eval=F}
# primera importación; almacena los nombres de las columnas
linelist_raw_names <- import("linelist_raw.csv") %>% names() # save true column names

# nota: el argumento para archivos csv es 'col.names = '
linelist_raw <- import("linelist_raw.csv",
                       skip = 2,
                       col.names = linelist_raw_names
                       ) 
```

**Opción alternativa** - cambiar los nombres de las columnas utilizando un comando separado

```{r, eval=F}
# asigna/reescribe cabecesas usando la función 'colnames()' de R base
colnames(linelist_raw) <- linelist_raw_names
```


#### Hacer un diccionario de datos {.unnumbered}  

Bonus! Si tienes una segunda fila que es un diccionario de datos, puedes crear fácilmente un diccionario de datos propio a partir de ella. Este consejo está adaptado de este [post](https://alison.rbind.io/post/2018-02-23-read-multiple-header-rows/).  


```{r}
dict <- linelist_2headers %>%             # linelist con como primera fila
  head(1) %>%                             # mantener sólo los nombres de las columnas y la primera fila del diccionario 
  pivot_longer(cols = everything(),       # pivotar todas las columnas a formato largo
               names_to = "Column",       # asignar nuevos nombres de columnas
               values_to = "Description")
```


```{r message=FALSE, echo=F}
DT::datatable(dict, rownames = FALSE, filter="top", options = list(pageLength = 4, scrollX=T), class = 'white-space: nowrap' )
```



#### Combinar las dos filas de la cabecera {.unnumbered}  

En algunos casos, cuando los datos crudos tienen *dos* filas de cabecera (o, más concretamente, la segunda fila de datos es una cabecera secundaria), es posible que desees "combinarlas" o añadir los valores de la segunda fila de cabecera a la primera fila de cabecera.

El comando siguiente definirá los nombres de las columnas del dataframe como la combinación del primer encabezado (verdadero) con el valor inmediatamente inferior (en la primera fila). 

```{r, eval=F}
names(my_data) <- paste(names(my_data), my_data[1, ], sep = "_")
```



<!-- ======================================================= -->
### Hojas de Google {.unnumbered}

Puedes importar datos de una hoja de cálculo de Google en línea con el paquete **googlesheet4** y autenticando tu acceso al archivo.


```{r, eval=F}
pacman::p_load("googlesheets4")
```

A continuación, se importa y guarda una hoja de Google de demostración. Este comando puede solicitar la autentificación de tu cuenta de Google. Sigue las indicaciones y las ventanas emergentes de tu navegador web para conceder a los paquetes de la API de Tidyverse permisos para editar, crear y eliminar sus hojas de cálculo en Google Drive.

La hoja que aparece a continuación es "visible para cualquiera con el enlace" y puedes intentar importarla.  

```{r, eval=F}
Gsheets_demo <- read_sheet("https://docs.google.com/spreadsheets/d/1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY/edit#gid=0")
```

La hoja también puede importarse utilizando sólo el ID de la hoja, así es una url más corta: 

```{r, eval=F}
Gsheets_demo <- read_sheet("1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY")
```


Otro paquete, **googledrive** ofrece funciones útiles para escribir, editar y eliminar hojas de Google. Por ejemplo, utilizando las funciones `gs4_create()` y `sheet_write()` que se encuentran en este paquete.

Aquí hay otros tutoriales útiles en línea:\
[tutorial básico de importación de hojas de Google](https://arbor-analytics.com/post/getting-your-data-into-r-from-google-sheets/)\
[tutorial más detallado](https://googlesheets4.tidyverse.org/articles/googlesheets4.html)\
[interacción entre googlesheets4 y tidyverse](https://googlesheets4.tidyverse.org/articles/articles/drive-and-sheets.html)




## Múltiples archivos - importar, exportar, dividir, combinar  {#multiple-files---import-export-split-combine}

Consulta la página sobre [Iteración, bucles y listas](#iteration-loops-and-lists) para ver ejemplos de cómo importar y combinar múltiples archivos, o múltiples archivos de Excel. Esa página también tiene ejemplos sobre cómo dividir un dataframe en partes y exportar cada uno por separado, o como hojas específicas en un archivo de Excel.




<!-- ======================================================= -->
## Importar desde Github {#import_github}

Importar datos directamente de Github a R puede ser muy fácil o puede requerir algunos pasos - dependiendo del tipo de archivo. A continuación se presentan algunos enfoques: 

### CSV files {.unnumbered}  

Es fácil importar un archivo .csv directamente desde Github a R con un comando de R.

1) Ve al repositorio de Github, localiza el archivo de interés y clica sobre él
2) Cliquea en el botón "Raw" (entonces verás los datos csv "crudos", como se muestra a continuación)

3) Copia la URL (dirección web)
4) Pega la URL entre comillas dentro del comando de R `import()`

```{r, out.width=c('100%', '100%'), fig.align = "left", echo=F}
knitr::include_graphics(here::here("images", "download_csv_raw.png"))
```

### XLSX files {.unnumbered}  

Es posible que no puedas ver los datos "en crudo" (raw) de algunos archivos (por ejemplo, .xlsx, .rds, .nwk, .shp)

1.  Ve al repositorio de Github, localica el archivo de interés y clica sobre él
2.  Cliquea en el botón "Download", como se muestra a continuación
3.  Guarda el archivo en tu ordenador e impórtalo en R


```{r , out.width=c('100%', '100%'), fig.align = "left", echo=F}
knitr::include_graphics(here::here("images", "download_xlsx.png"))
```

### Shapefiles {.unnumbered} 

Los shapefiles tienen muchos archivos subcomponentes, cada uno con una extensión diferente. Un archivo tendrá la extensión ".shp", pero otros tienen ".dbf", ".prj", etc. Para descargar un shapefile de Github, tendrás que descargar cada uno de los archivos subcomponentes individualmente, y guardarlos en la *misma* carpeta de tu ordenador. En Github, cliquea en cada archivo individualmente y descárgalos clicando en el botón "Download".

Una vez guardado en tu ordenador, puedes importar el archivo shape como se muestra en la página de [Conceptos básicos de los SIG](#gis-basics) utilizando `st_read()` del paquete **sf**. Sólo tienes que proporcionar la ruta del archivo y el nombre del archivo ".shp", siempre que los demás archivos relacionados estén en la misma carpeta de tu ordenador.

A continuación, se puede ver cómo el shapefile "sle_adm3" consta de muchos archivos, cada uno de los cuales debe descargarse de Github.

```{r , out.width=c('100%', '100%'), fig.align = "left", echo=F}
knitr::include_graphics(here::here("images", "download_shp.png"))
```





<!-- ======================================================= -->
## Grabación manual de datos {#manual-data-entry}

### Entrada por filas {.unnumbered}  

Utiliza la función `tribble()` del paquete **tibble** de tidyverse ([referencia online de tibble](https://tibble.tidyverse.org/reference/tribble.html)).

Observa que las cabeceras de las columnas comienzan con una *tilde* (~). Observa también que cada columna debe contener sólo un tipo de datos (carácter, numérico, etc.). Puedes utilizar tabulaciones, espacios y nuevas filas para que la entrada de datos sea más intuitiva y legible. Los espacios no importan entre los valores, pero cada fila está representada por una nueva línea de código. Por ejemplo:

```{r import_manual_row}
# crea el dataset manualmente por filas
manual_entry_rows <- tibble::tribble(
  ~colA, ~colB,
  "a",   1,
  "b",   2,
  "c",   3
  )
```

Y ahora mostramos el nuevo conjunto de datos:

```{r, echo=F}
# display the new dataset
DT::datatable(manual_entry_rows)
```


### Entrada por columnas {.unnumbered}  

Dado que un dataframe consiste en vectores (columnas verticales), el enfoque básico para la creación manual de dataframes en R espera que definas cada columna y luego las unas. Esto puede ser contrario a la intuición en epidemiología, ya que normalmente pensamos en nuestros datos como una observación por filas (como arriba).

```{r import_manual_col}
# define cada vector (columna vertical) por separado, cada uno con su propio nombre
PatientID <- c(235, 452, 778, 111)
Treatment <- c("Yes", "No", "Yes", "Yes")
Death     <- c(1, 0, 1, 0)
```

<span style="color: orange;">**_PRECAUCIÓN:_** Todos los vectores deben tener la misma longitud (el mismo número de valores).</span>

A continuación, los vectores pueden unirse mediante la función `data.frame()`:

```{r}
# combine the columns into a data frame, by referencing the vector names
manual_entry_cols <- data.frame(PatientID, Treatment, Death)
```

Y ahora mostramos el nuevo conjunto de datos:

```{r, echo=F}
# Mostrar el dataset nuevo 
DT::datatable(manual_entry_cols)
```




### Pegar desde el portapapeles {.unnumbered}  

Si copias los datos de otro lugar y los tienes en el portapapeles, puedes probar una de las dos formas siguientes:

Con el paquete **clipr**, puedes utilizar `read_clip_tbl()` para importar como un dataframe, o simplemente `read_clip()` para importar como un vector de caracteres. En ambos casos, deja los paréntesis vacíos.

```{r, eval=F}
linelist <- clipr::read_clip_tbl()  # importar portapapeles actual como data frame
linelist <- clipr::read_clip()      # importar como vector de caracteres
```
También puedes exportar fácilmente al portapapeles de tu sistema con **clipr.** Consulta la sección siguiente sobre Exportación.


Alternativamente, pueder utilizar la función `read.table()` de R **base** con `file = "clipboard"`) para importar como un dataframe:

```{r, eval=F}
df_from_clipboard <- read.table(
  file = "clipboard",  # especifica esto como "portapapeles"
  sep = "t",           # el separador puede ser un tabulador, o una coma, etc.
  header=TRUE)         # si hay una fila de cabecera
```






## Importar el archivo más reciente {#import-most-recent-file}  

A menudo puedes recibir actualizaciones diarias de tus datos. En este caso, querrás escribir un código que importe el archivo más reciente. A continuación presentamos dos maneras de abordar esto:

* Seleccionar el archivo en función de la fecha del nombre del archivo
* Seleccionar el archivo en función de los metadatos del archivo (última modificación)


### Fechas en el nombre del archivo {.unnumbered}  

Este enfoque se basa en tres premisas:

1.  Confías en las fechas en los nombres de los archivos
2.  Las fechas son numéricas y aparecen *generalmente* en el mismo formato (por ejemplo, año, mes y día)
3.  No hay otros números en el nombre del archivo

Te explicaremos paso a paso y te mostraremos todos los pasos combinados al final.

En primer lugar, utiliza `dir()` de R **base** para extraer sólo los nombres de los archivos de la carpeta de interés. Consulta la página sobre [interacciones con directorios](#directory-interactions) para obtener más detalles sobre `dir()`. En este ejemplo, la carpeta de interés es la carpeta "linelists" dentro de la carpeta "example" dentro de "data" dentro del proyecto R.

```{r}
linelist_filenames <- dir(here("data", "example", "linelists")) # obtiene los nombres de ficheros de la carpeta
linelist_filenames                                              # los muestra
```

Una vez que tengas este vector de nombres, puedes extraer las fechas de ellos aplicando `str_extract()` de **stringr** utilizando esta expresión regular. Este comando extrae cualquier número en el nombre del archivo (incluyendo cualquier otro carácter en el medio como guiones o barras). Puedes leer más sobre **stringr** en la página [Caracteres y cadenas](#characters-and-strings).

```{r}
linelist_dates_raw <- stringr::str_extract(linelist_filenames, "[0-9].*[0-9]") # extract numbers and any characters in between
linelist_dates_raw  # print
```

Suponiendo que las fechas estén escritas en general con el mismo formato de fecha (por ejemplo, Año, Mes y Día) y que los años tengan 4 dígitos, puedes utilizar las funciones de conversión de **lubridate** (`ymd()`, `dmy()` o `mdy()`) para convertirlas en fechas. Para estas funciones, no importan los guiones, espacios o barras, sino el orden de los números. Lee más en la página [Trabajando con fechas](#working-with-dates).

```{r}
linelist_dates_clean <- lubridate::ymd(linelist_dates_raw)
linelist_dates_clean
```

La función de R **base** `wich.max()` puede utilizarse para devolver la posición del índice (por ejemplo, 1ª, 2ª, 3ª, ...) del valor máximo de la fecha. El último archivo se identifica correctamente como el sexto archivo - "case_linelist_2020-10-08.xlsx".

```{r}
index_latest_file <- which.max(linelist_dates_clean)
index_latest_file
```

Si condensamos todos estos comandos, el código completo podría ser como el siguiente. Observa que el `.` en la última línea es un marcador de posición para el objeto canalizado en ese punto de la secuencia de pipes. En ese punto el valor es simplemente el número 6. Esto se coloca entre corchetes dobles para extraer el sexto elemento del vector de nombres de archivo producido por `dir()`.

```{r}
# load packages
pacman::p_load(
  tidyverse,         # gestión de datos
  stringr,           # trabajar con cadenas/caracteres
  lubridate,         # trabajar con fechas
  rio,               # importar / exportar
  here,              # rutas relativas 
  fs)                # interacciones de directorio

# extraer el nombre del último archivo
latest_file <- dir(here("data", "example", "linelists")) %>%  # nombres de archivos de la subcarpeta "linelists"    
  str_extract("[0-9].*[0-9]") %>%                  # extraer fechas (números)
  ymd() %>%                                        # convertir los números en fechas (asumiendo el formato año-mes-día)
  which.max() %>%                                 # obtener el índice de la fecha máxima (último archivo)
  dir(here("data", "example", "linelists"))[[.]]              #  devuelve el nombre del archivo del último linelist

latest_file  # mostrar el nombre del último archivo
```

Ahora puedes utilizar este nombre para terminar la ruta relativa del archivo, con `here()`:

```{r, eval=F}
here("data", "example", "linelists", latest_file) 
```

Y ahora puedes importar el último archivo: 

```{r, eval=F}
# import
import(here("data", "example", "linelists", latest_file)) # importar 
```

 



### Utiliza la información del archivo {.unnumbered}  

Si tus archivos no tienen fechas en sus nombres (o no te fías de esas fechas), puedes intentar extraer la última fecha de modificación de los metadatos del archivo. Utiliza las funciones del paquete **fs** para examinar la información de los metadatos de cada archivo, que incluye la fecha y hora de la última modificación y la ruta del archivo.

A continuación, proporcionamos la carpeta de interés a `dir_info()` de **fs**. En este caso, la carpeta de interés está en el proyecto R en la carpeta "data", la subcarpeta "example", y su subcarpeta "linelists". El resultado es un dataframe con una línea por cada archivo y columnas para `modification_time`, `path`, etc. Puedes ver un ejemplo visual de esto en la página sobre [interacciones con directorios](#directory-interactions).

Podemos ordenar este dataframe de archivos por la columna `modification_time`, y luego mantener sólo la fila superior (último archivo) con la función `head()` de R **base**. A continuación, podemos extraer la ruta de este último archivo sólo con la función **dplyr** `pull()` en la columna `path`. Finalmente podemos pasar esta ruta de archivo a import(). El archivo importado se guarda como `latest_file`.

```{r, eval=F}
latest_file <- dir_info(here("data", "example", "linelists")) %>%  # recoger información de todos los archivos en el directorio
  arrange(desc(modification_time)) %>%      # ordenar por tiempo de modificación
  head(1) %>%                               # mantener sólo el archivo superior (más reciente)
  pull(path) %>%                            # extraer sólo la ruta del archivo
  import()                                  # importar el archivo

```



<!-- ======================================================= -->
## APIs {#import_api}

Una "Interfaz de Programación Automatizada" (API) puede utilizarse para solicitar datos directamente de un sitio web. Las API son un conjunto de reglas que permiten que una aplicación de software interactúe con otra. El cliente (tu) envía una "solicitud" y recibe una "respuesta" con contenido. Los paquetes de R **httr** y **jsonlite** pueden facilitar este proceso.

Cada sitio web habilitado para la API tendrá su propia documentación y detalles con los que hay que familiarizarse. Algunos sitios están disponibles públicamente y cualquiera puede acceder a ellos. Otros, como las plataformas con ID de usuario y credenciales, requieren autenticación para acceder a sus datos.

Obviamente es necesario disponer de una conexión a Internet para importar datos a través de la API. Te daremos ejemplos breves de uso de las API para importar datos, y presentaremos enlaces a otros recursos.

*Nota: recuerda que los datos pueden estar* publicados*  en un sitio web sin una API, que puede ser más fácil de recuperar. Por ejemplo, un archivo CSV publicado puede ser accesible simplemente proporcionando la URL del sitio a `import()` como se describe en la sección sobre la [importación desde Github](#import_github).* 


### Petición HTTP {.unnumbered}  

El intercambio de la API se realiza normalmente a través de una solicitud HTTP. HTTP es el Protocolo de Transferencia de Hipertexto, y es el formato subyacente de una solicitud/respuesta entre un cliente y un servidor. La entrada y la salida exactas pueden variar en función del tipo de API, pero el proceso es el mismo: una "Solicitud" (a menudo Solicitud HTTP) del usuario, que suele contener una consulta, seguida de una "Respuesta", que contiene información de estado sobre la solicitud y posiblemente el contenido solicitado.

Estos son algunos de los componentes de una *petición HTTP*:

* La URL completa de la API
* El "Método" (o "Verbo")
* Headers (Encabezados)
* Body (Cuerpo)

El "método" de la petición HTTP es la acción que se quiere realizar. Los dos métodos HTTP más comunes son `GET` y `POST`, pero otros pueden ser `PUT`, `DELETE`, `PATCH`, etc. Cuando se importan datos a R lo más probable es que se utilice `GET`.

Después de la solicitud, tu ordenador recibirá una "respuesta" en un formato similar al que se envió, incluyendo la URL, el estado HTTP (¡status 200 es lo que quieres!), el tipo de archivo, el tamaño y el contenido deseado. A continuación, tendrá que analizar esta respuesta y convertirla en un dataframe viable dentro de su entorno R.


### Paquetes {.unnumbered}  

El paquete **httr** funciona bien para manejar peticiones HTTP en R. Requiere poco conocimiento previo de las APIs de la web y puede ser utilizado por personas menos familiarizadas con la terminología de desarrollo de software. Además, si la respuesta HTTP es .json, puede utilizar **jsonlite** para analizar la respuesta.

```{r, eval=F}
# load packages
pacman::p_load(httr, jsonlite, tidyverse)
```


### Datos de acceso público {.unnumbered}  

A continuación se muestra un ejemplo de solicitud HTTP, tomado de un tutorial de [Trafford Data Lab](https://www.trafforddatalab.io/open_data_companion/#A_quick_introduction_to_APIs). Este sitio tiene varios otros recursos para aprender y ejercicios de API.

Escenario: Queremos importar una lista de establecimientos de comida rápida en la ciudad de Trafford, Reino Unido. Se puede acceder a los datos desde la API de la Food Standards Agency (Agencia de Normas Alimentarias), que proporciona datos de calificación de higiene alimentaria para el Reino Unido.

Estos son los parámetros de nuestra solicitud:

* Verbo HTTP: GET
* URL del punto de la API: http://api.ratings.food.gov.uk/Establishments
* Parámetros seleccionados: name, address, longitude, latitude, businessTypeId, ratingKey, localAuthorityId
* Cabeceras: "x-api-version", 2
* Formato(s) de datos: JSON, XML
* Documentación: http://api.ratings.food.gov.uk/help

El código R sería el siguiente:

```{r, eval=F, warning=F, message=F}
# preparar la petición
path <- "http://api.ratings.food.gov.uk/Establishments"
request <- GET(url = path,
             query = list(
               localAuthorityId = 188,
               BusinessTypeId = 7844,
               pageNumber = 1,
               pageSize = 5000),
             add_headers("x-api-version" = "2"))

# Comprobar si hay error con el servidor ("200" es el correcto!)
request$status_code

# enviar la solicitud, analizar la respuesta y convertirla en un data frame
response <- content(request, as = "text", encoding = "UTF-8") %>%
  fromJSON(flatten = TRUE) %>%
  pluck("establishments") %>%
  as_tibble()
```

Ahora puedes limpiar y utilizar el dataframe `response`, que contiene una fila por establecimiento de comida rápida.


### Se requiere autenticación {.unnumbered}  

Algunas APIs requieren autenticación - para que se demuestre quién eres y poder acceder a datos restringidos. Para importar estos datos, es posible que tengas que utilizar primero un método POST para proporcionar un nombre de usuario, una contraseña o un código. Esto devolverá un token de acceso, que puede ser utilizado para posteriores solicitudes del método GET para obtener los datos deseados.

A continuación se muestra un ejemplo de consulta de datos de *Go.Data*, que es una herramienta de investigación de brotes. *Go.Data* utiliza una API para todas las interacciones entre la interfaz de la web y las aplicaciones de los smartphones utilizadas para la captura de datos. *Go.Data* se utiliza en todo el mundo. Se requiere autenticación, dado que los datos de los brotes son sensibles y sólo debes poder acceder a los datos de *tu* brote.

A continuación se muestra un ejemplo de código R que utiliza **httr** y **jsonlite** para conectarse a la API de *Go.Data* para importar datos sobre el seguimiento de los contactos de tu brote.


```{r, eval=F}
# establecer credenciales para la autorización
url <- "https://godatasampleURL.int/"           # url correcta de Go.Data
username <- "username"                          # usuario Go.Data válido 
password <- "password"                          # contraseña válida 
outbreak_id <- "xxxxxx-xxxx-xxxx-xxxx-xxxxxxx"  # ID de brote de Go.Data

# obtener token de acceso
url_request <- paste0(url,"api/oauth/token?access_token=123") # definir URL base de la solicitud

# preparar la petición
response <- POST(
  url = url_request,  
  body = list(
    username = username,    # utiliza el nombre de usuario/contraseña guardado  arriba para autorizar                               
    password = password),                                       
    encode = "json")

# ejecutar la petición y analizar la respuesta
content <-
  content(response, as = "text") %>%
  fromJSON(flatten = TRUE) %>%          # acoplar el JSON anidado
  glimpse()

# Guardar el token de acceso de la respuesta
access_token <- content$access_token    # guardar el token de acceso para permitir las siguientes llamadas a la API

# importar contactos del brote
# Utilizar el token de acceso 
response_contacts <- GET(
  paste0(url,"api/outbreaks/",outbreak_id,"/contacts"),          # petición GET
  add_headers(
    Authorization = paste("Bearer", access_token, sep = " ")))

json_contacts <- content(response_contacts, as = "text")         # # convertir JSON a texto

contacts <- as_tibble(fromJSON(json_contacts, flatten = TRUE))   # acoplar JSON a tibble
```

<span style="color: orange;">**_PRECAUCIÓN:_** Si estás importando grandes cantidades de datos desde una API que requiere autenticación, es posible que se agote el tiempo de espera. Para evitarlo, recupera el access_token antes de cada solicitud GET de la API y prueba a utilizar filtros o límites en la consulta. </span> 

<span style="color: darkgreen;">**_CONSEJO:_** La función fromJSON() del paquete **jsonlite** no se ajuste completamente la primera vez que se ejecuta, por lo que es probable que todavía tengas elementos de la lista en tu tibble resultante. Tendrás que ajustar aún más ciertas variables, dependiendo de lo jerarquizado que esté tu .json. Para ver más información sobre esto, consulta la documentación del paquete **jsonlite**, como la [función flatten()](https://rdrr.io/cran/jsonlite/man/flatten.html). </span>


Para más detalles, mira la documentación en el [Explorador de LoopBack](https://loopback.io/doc/en/lb4/index.html), la página de [Rastreo de Contactos](#contact-tracing-1) o los consejos de la API en el [repositorio Github de Go.Data](https://worldhealthorganization.github.io/godata/api-docs)

Puedes leer más sobre el paquete *httr* [aquí](https://httr.r-lib.org/articles/quickstart.html)

Esta sección también se inspiró en [este tutorial](https://www.dataquest.io/blog/r-api-tutorial/) y [este otro tutorial](https://medium.com/@traffordDataLab/querying-apis-in-r-39029b73d5f1). 




<!-- ======================================================= -->
## Exportar {#export}  

### Con el paquete **rio** {.unnumbered}

Con **rio**, puedes utilizar la función `export()` de forma muy similar a `import()`. Primero indica el nombre del objeto de R que deseas guardar (por ejemplo, `linelist`) y luego escribe entre comillas la ruta de acceso al archivo donde deseas guardarlo, incluyendo el nombre y la extensión de archivo deseados. Por ejemplo:

Esto guarda el dataframe `linelist` como un archivo de Excel en el directorio de trabajo/carpeta raíz del proyecto R:

```{r, eval=F}
export(linelist, "my_linelist.xlsx") # lo guardará en el directorio de trabajo
```

Se puede guardar el mismo dataframe como un archivo csv cambiando la extensión. Por ejemplo, también lo guardamos en una ruta de archivo construida con `here()`:

```{r, eval=F}
export(linelist, here("data", "clean", "my_linelist.csv"))
```


### Al portapapeles {.unnumbered}

Para exportar un dataframe al "portapapeles" de tu ordenador (para luego pegarlo en otro software como Excel, Google Spreadsheets, etc.) puedes utilizar `write_clip()` del paquete **clipr**.

```{r, eval=F}
# exporta el data frame linelist al portapapeles de tu sistema
clipr::write_clip(linelist)
```




## Archivos RDS {#import_rds}

Además de .csv, .xlsx, etc., también puedes exportar/guardar dataframes de R como archivos .rds. Este es un formato de archivo específico de R, y es muy útil si sabes que vas a trabajar con los datos exportados de nuevo en R.

Los tipos de columnas se conservan, por lo que no hay que volver a hacer la limpieza cuando se importan (con un archivo Excel o incluso CSV esto puede ser un dolor de cabeza). También es un archivo más pequeño, lo que es útil para la exportación e importación si tu conjunto de datos es grande.

Por ejemplo, si trabajas en un equipo de epidemiología y necesitas enviar archivos a un equipo de SIG para la elaboración de mapas, y ellos también utilizan R, ¡sólo tienes que enviarles el archivo .rds! Así se conservan todos los tipos de columnas y ellos tienen menos trabajo que hacer.

```{r, eval=F}
export(linelist, here("data", "clean", "my_linelist.rds"))
```



<!-- ======================================================= -->
## Archivos Rdata y listas de datos {#import_rdata}

Los archivos `.Rdata` pueden almacenar múltiples objetos de R - por ejemplo, múltiples dataframes, resultados de modelos, listas, etc. Esto puede ser muy útil para consolidar o compartir muchos de tus datos para un proyecto determinado.

En el siguiente ejemplo, se almacenan múltiples objetos R dentro del archivo exportado "my_objects.Rdata":


```{r, eval=F}
rio::export(my_list, my_dataframe, my_vector, "my_objects.Rdata")
```

Nota: si estás intentando *importar* una lista, utiliza `import_list()` de **rio** para importarla con la estructura y el contenido originales completos.

```{r, eval=F}
rio::import_list("my_list.Rdata")
```







<!-- ======================================================= -->
## Guardar gráficos {#saving-plots} 

Las instrucciones sobre cómo guardar los gráficos, como los creados por `ggplot()`, se discuten en profundidad en la página de [Conceptos básicos de ggplot](#ggplot-basics).

En resumen, ejecuta `ggsave("my_plot_filepath_and_name.png")` después de obtener tu gráfico. Puedes proporcionar un gráfico guardado con plot = argumento, o sólo especificar la ruta de archivo de destino (con extensión de archivo) para guardar el gráfico mostrado más recientemente. También puedes controlar el ancho `width = `, la altura `height = `, las unidades `units = ` y los puntos por pulgada `dpi = `.

La forma de guardar un gráfico de red, como un árbol de transmisión, se aborda en la página [Cadenas de transmisión](#transmission-chains).


<!-- ======================================================= -->
## Recursos {#resources-1} 

El [manual de importación y exportación de datos de R](https://cran.r-project.org/doc/manuals/r-release/R-data.html)

[Capítulo de R 4 Data Science en español sobre la importación de datos](https://es.r4ds.hadley.nz/importaci%C3%B3n-de-datos.html)

[documentación de ggsave()](https://ggplot2.tidyverse.org/reference/ggsave.html)

A continuación se muestra una tabla, extraída de la [viñeta online](https://cran.r-project.org/web/packages/rio/vignettes/rio.html) de **rio**. Para cada tipo de datos muestra: la extensión de archivo esperada, el paquete que **rio** utiliza para importar o exportar los datos, y si esta funcionalidad está incluida en la versión instalada de **rio**.



Formato                    | Extensión típica  |Paquete importación|Paquete exportación | Instalado por defecto
---------------------------|-------------------|-------------------|--------------------|---------------------
Datos separados por comas | .csv | data.table `fread()` | data.table |	Yes
Datos separados por pipe  |	.psv | data.table `fread()` | data.table | Yes
Datos separados por tabul | .tsv | data.table `fread()` | data.table | Yes
SAS | .sas7bdat | haven | haven | Yes
SPSS | .sav | haven | haven | Yes
Stata | .dta | haven | haven | Yes
SAS | XPORT | .xpt | haven | haven | Yes
SPSS Portable | .por | haven | | Yes
Excel | .xls | readxl | | Yes
Excel | .xlsx | readxl | openxlsx | Yes
Syntaxis R | .R	| base | base | Yes
Objetos R guardados | .RData, .rda | base | base | Yes
Objetos R serializados | .rds | base | base | Yes
Epiinfo | .rec | foreign | | Yes
Minitab | .mtp | foreign | | Yes
Systat | .syd |	foreign | | Yes
“XBASE” | database files | .dbf | foreign | foreign | Yes
Formato de archivo Weka Attribute-Relation | .arff | foreign | foreign | Yes
Formato de intercambio de datos | .dif | utils | | Yes
Datos de Fortran | no recognized extension | utils | | Yes
Formato de ancho fijo | .fwf | utils | utils | Yes
datos separados por comas gzip | .csv.gz | utils | utils | Yes
CSVY (CSV + cabecera de metadatos YAML) | .csvy | csvy | csvy | No
EViews | .wf1 |hexView | | No
Formato de intercambio Feather R/Python | .feather | feather | feather | No
Almacenamiento rápido | .fst | fst |	fst | No
JSON | .json | jsonlite | jsonlite | No
Matlab | .mat | rmatio | rmatio | No
Hoja de cálculo OpenDocument | .ods | readODS | readODS | No
Tablas HTML | .html | xml2 | xml2 | No
Documentos XML | .xml | xml2 | xml2 | No
YAML | .yml | yaml | yaml	| No
Portapapeles por defecto es tsv | |  clipr | clipr | No



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/importing.Rmd-->

# (PART) Gestión de datos {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_data_management.Rmd-->

# Limpieza de datos y funciones básicas {#cleaning-data-and-core-functions} 

```{r, out.height = "10%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "cleaning.png"))

```

Esta página muestra los pasos más utilizados en el proceso de "limpieza" de datos, y también explica el uso de muchas funciones esenciales de gestión de datos en R. 

Para explicarlo, esta página comienza importando datos de un listado de casos crudo, y se avanza paso a paso a través del proceso de limpieza. En el código R, esto se manifiesta como una cadena de "tuberías", que hacen referencia al operador "tuberías" `%>%` que pasa unos datos de una operación a la siguiente. 

### Funciones principales  {.unnumbered}

Este manual hace hincapié en el uso de las funciones de la familia de paquetes de R [**tidyverse**](https://www.tidyverse.org/). Las funciones esenciales que se muestran en esta página se enumeran a continuación. 

Muchas de estas funciones pertenecen al paquete [**dplyr**](https://dplyr.tidyverse.org/), que proporciona funciones "verbales" para resolver los retos de la manipulación de datos (el nombre hace una referencia a unos alicates - [plier](https://www.thefreedictionary.com/plier#:~:text=also ply·er (plī′,holding%2C bending%2C or cutting.)) - de dataframes). **dplyr** forma parte de la familia de paquetes de R **tidyverse** (que también incluye **ggplot2**, **tidyr**, **stringr**, **tibble**, **purrr**, **magrittr** y **forcats**, entre otros). 


Función      | Utilidad                             | Paquete
-------------|--------------------------------------|------------------------------
[%>%](https://magrittr.tidyverse.org/reference/pipe.html) | "canalizar" (pasar) datos de una función a la siguiente | **magrittr**   
[mutate()](https://dplyr.tidyverse.org/reference/mutate.html) | crear, transformar y redefinir columnas | **dplyr**   
[selecct()](https://dplyr.tidyverse.org/reference/select.html) | mantener, eliminar, seleccionar o renombrar columnas | **dplyr**  
[rename()](https://dplyr.tidyverse.org/reference/rename.html) | cambiar el nombre de las columnas | **dplyr**  
clean_names() | estandarizar la sintaxis de los nombres de las columnas | **janitor** 
[as.character()](https://rdrr.io/r/base/character.html), [as.numeric()](https://rdrr.io/r/base/numeric.html), [as.Date()](https://rdrr.io/r/base/as.Date.html), etc. | convertir el tipo de una columna | R **base**
across() | transformar varias columnas a la vez | **dplyr**   
funciones **tidyselect** | utilizar la lógica para seleccionar las columnas | **tidyselect**
[filter()](https://dplyr.tidyverse.org/reference/filter.html) | mantener ciertas filas | **dplyr**  
[distinct()](https://dplyr.tidyverse.org/reference/distinct.html) | de-duplicar filas | **dplyr**  
rowwise() | operaciones por/en cada fila | **dplyr**   
add_row() | añadir filas manualmente | **tiblle**   
[arrange()](https://dplyr.tidyverse.org/reference/arrange.html) | ordenar las filas | **dplyr**   
recode() |recodificar los valores de una columna | **dplyr**   
case_when() | recodificar los valores de una columna con criterios lógicos más complejos | **dplyr**   
replace_na(), na_if(), coalesce() | funciones especiales de recodificación | **tidyr**   
age_categories() y [cut()](https://rdrr.io/r/base/cut.html) | crear grupos categóricos a partir de una columna numérica | **epikit** y R **base**
match_df() | recodificación/limpieza de valores mediante un diccionario de datos | **matchmaker**   
[which()](https://rdrr.io/r/base/which.html) | aplicar los criterios lógicos; devolver los índices | R **base**

Si quieres ver cómo se comparan estas funciones con los comandos de Stata o SAS, consulta la página sobre la [transición a R](#transition-to-r). 

Puedes encontrar una gestión de datos alternativa en el paquete R **data.table** con operadores como `:=` y el uso frecuente de corchetes `[ ]`. Este enfoque y la sintaxis se explican brevemente en la página [Data.Table](#data-table). 

### Nomenclatura {.unnumbered} 

En este manual, generalmente hacemos referencia a "columnas" y "filas" en lugar de "variables" y "observaciones". Como se explica en este manual sobre ["datos ordenados"](https://tidyr.tidyverse.org/articles/tidy-data.html), la mayoría de los conjuntos de datos estadísticos epidemiológicos se componen estructuralmente de filas, columnas y valores. 

Las *variables* contienen los valores que miden el mismo atributo subyacente (como el grupo de edad, el resultado o la fecha de inicio). Las *observaciones* contienen todos los valores medidos en la misma unidad (por ejemplo, una persona, un lugar o una muestra de laboratorio). Por lo tanto, estos aspectos pueden ser más difíciles de definir de forma tangible. 

En los conjuntos de datos "ordenados", cada columna es una variable, cada fila es una observación y cada celda es un único valor. Sin embargo, algunos conjuntos de datos que se encuentran no se ajustan a este molde: unos datos de formato "amplio" pueden tener una variable dividida en varias columnas (véase un ejemplo en la página [Pivotar datos](#pivoting-data)). Del mismo modo, las observaciones pueden estar divididas en varias filas. 

La mayor parte de este manual trata sobre la gestión y la transformación de datos, por lo que las referencias a las estructuras de datos concretas de filas y columnas son más relevantes que las observaciones y las variables más abstractas. Las excepciones se dan sobre todo en las páginas sobre análisis de datos, en las que verás más referencias a las variables y las observaciones. 

<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->

## Limpieza de tuberías {#cleaning-pipeline}

**Esta página recorre los pasos típicos de limpieza, añadiéndolos secuencialmente a una cadena de tuberías de limpieza.** 

En el análisis epidemiológico y el procesamiento de datos, los pasos de limpieza se realizan a menudo de forma secuencial, enlazados entre sí. En R, esto se manifiesta a menudo como una "tubería" de limpieza, en la que *los datos en bruto se pasan o se "canalizan" de un paso de limpieza a otro*. 

Estas cadenas utilizan las funciones de **dplyr** y el operador `%>%` de **magrittr**. Esta tubería comienza con los datos "en bruto" ("linelist_raw.xlsx") y termina con un dataframe de R "limpio" (`linelist`) que se puede utilizar, guardar, exportar, etc. 

En un proceso de limpieza, el orden de los pasos es importante. Los pasos de limpieza pueden incluir: 

* Importación de datos 
* Limpieza o cambio de los nombres de las columnas 
* de-duplicación 
* Creación y transformación de columnas (por ejemplo, recodificación o normalización de valores) 

* Filtrado o añadido de filas 


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->

## Carga de paquetes  {#load-packages}

Este trozo de código muestra la carga de los paquetes necesarios para el análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base.** Consulta la página sobre [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R. 

```{r, message = F}
pacman::p_load(
  rio,        # importing data  
  here,       # relative file pathways  
  janitor,    # data cleaning and tables
  lubridate,  # working with dates
  epikit,     # age_categories() function
  tidyverse   # data management and visualization
)
```


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->

## Importar datos {#import-data-1}

### Importar {.unnumbered} 

Aquí importamos el archivo Excel de la lista de casos "en bruto" utilizando la función `import()` del paquete **rio**. El paquete **rio** maneja con flexibilidad muchos tipos de archivos (por ejemplo, .xlsx, .csv, .tsv, .rds. Consulta la página sobre [importación y exportación](#import-and-export) para obtener más información y consejos sobre situaciones inusuales (por ejemplo, omitir filas, establecer valores que faltan, importar hojas de Google, etc). 

Para continuar, [cliquea para descargar linelist "en crudo"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_raw.xlsx) (como archivo .xlsx). 

Si tu conjunto de datos es grande y tarda mucho en importarse, puede ser útil que el comando de importación esté separado de la cadena de tuberías y que el "crudo" se guarde como un archivo distinto. Esto también permite comparar fácilmente las versiones original y limpia. 

A continuación, importamos el archivo de Excel sin procesar y lo guardamos como el dataframe `linelist_raw`. Suponemos que el archivo se encuentra en tu directorio de trabajo o en la raíz del proyecto R, por lo que no se especifican subcarpetas en la ruta del archivo. 


```{r, echo=F, message=F}
# HIDDEN FROM READER
# actually load the data using here()
linelist_raw <- rio::import(here::here("data", "case_linelists", "linelist_raw.xlsx"))
```

```{r, eval=F}
linelist_raw <- import("linelist_raw.xlsx")
```

Puedes ver las primeras 50 filas del dataframe a continuación. Nota: la función **base** de R `head(n)` te permite ver sólo las primeras n filas en la consola de R. 

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist_raw,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Revisar {.unnumbered}  

Puedes utilizar la función `skim()` del paquete **skimr** para obtener una visión general de todo el dataframe (véase la página sobre [tablas descriptivas](#descriptive-tables) para más información). Las columnas se resumen por clase o tipo, como, por ejemplo, carácter, numérico. Nota: "POSIXct" es un tipo de fecha cruda (ver [Trabajar con fechas](#working-with-dates-1). 

```{r, eval=F}
skimr::skim(linelist_raw)
```

```{r, echo=F}
skimr::skim_without_charts(linelist_raw)
```

<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->

## Nombres de columnas {#column-names}

En R, los *nombres* de las columnas son la "cabecera" o el valor "superior" de una columna. Se utilizan para referirse a las columnas en el código, y sirven como etiqueta por defecto en las figuras. 

Otros programas estadísticos, como SAS y STATA, utilizan *"etiquetas"* que coexisten como versiones impresas más largas de los nombres de columna más cortos. Aunque R ofrece la posibilidad de añadir etiquetas de columna a los datos, no es una práctica que sea muy utilizada. Para hacer que los nombres de las columnas sean "fáciles de imprimir" para las figuras, normalmente se ajusta su visualización dentro de los comandos de gráficas que crean las salidas (por ejemplo, los títulos de los ejes o de las leyendas de una gráfica, o las cabeceras de las columnas en una tabla impresa - véase la [sección de escalas de la página de consejos de ggplot](#ggplot_tips_scales) y las páginas de [Tablas para la presentación](#tables-for-presentation)). Si deseas asignar etiquetas de columna en los datos, lee más online [aquí](https://cran.r-project.org/web/packages/expss/vignettes/labels-support.html) y [aquí](https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html). 

Como los nombres de las columnas de R se utilizan con mucha frecuencia, deben tener una sintaxis "limpia". Sugerimos lo siguiente: 

* Nombres cortos 
* Sin espacios (sustituir por barras bajas _ ) 
* Sin caracteres inusuales (&, #, <, >, ...) 
* Nomenclatura de estilo similar (por ejemplo, todas las columnas de fecha nombradas como **date**_onset, **date**_report, **date**_death...) 

Los nombres de las columnas de `linelist_raw` se muestran a continuación utilizando `names()` de R **base**. Podemos ver que inicialmente 

* Algunos nombres contienen espacios (por ejemplo, `infection date`) 

* Se utilizan diferentes patrones de nomenclatura para las fechas (`date onset` vs. `infection date`) 

* Debe haber habido una *cabecera fusionada* en las dos últimas columnas del .xlsx. Lo sabemos porque el nombre de dos columnas fusionadas ("merged_header") fue asignado por R a la primera columna, y a la segunda columna se le asignó un nombre de marcador de posición "...28" (ya que entonces estaba vacía y es la columna 28). 

```{r}
names(linelist_raw)
```

<span style="color: black;">***NOTA:*** Para hacer referencia a un nombre de columna que incluya espacios, rodea el nombre con tildes, por ejemplo: linelist$`` ` '\x60infection date\x60'` ``. Ten en cuenta que, en tu teclado, la tilde (\`) es diferente de la comilla simple ('). </span>

### Limpieza automática {.unnumbered}  

La función `clean_names()` del paquete **janitor** estandariza los nombres de las columnas y los hace únicos haciendo lo siguiente: 

* Convierte todos los nombres para que estén compuestos sólo por barras bajas, números y letras 
* Los caracteres acentuados se transliteran a ASCII (por ejemplo, la o alemana con diéresis se convierte en "o", la "ñ" española se convierte en "n") 
* Se puede especificar la preferencia de mayúsculas para los nuevos nombres de columna utilizando `case =` argumento ("snake" es el valor por defecto, las alternativas incluyen "sentence", "title", "small_camel"...) 
* Puedes especificar sustituciones de nombres concretos proporcionando un vector `replace =` argumento (por ejemplo, `replace = c(onset = "date_of_onset")`)

* Aquí puedes encontrar una  [viñeta](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#cleaning) en línea sobre dicho paquete. 

A continuación, el proceso de limpieza comienza utilizando `clean_names()` sobre linelist_raw. 

```{r clean_names}
# pipe the raw dataset through the function clean_names(), assign result as "linelist"  
linelist <- linelist_raw %>% 
  janitor::clean_names()

# see the new column names
names(linelist)
```

<span style="color: black;">***NOTA:*** El nombre de la última columna "...28" se ha cambiado por "x28". </span>

### Limpieza manual de nombres {.unnumbered}  

A menudo es necesario renombrar las columnas manualmente, incluso después del paso de estandarización anterior. A continuación, el renombramiento se realiza utilizando la función `rename()` del paquete **dplyr**, como parte de una cadena de tuberías. `rename()` utiliza el estilo `NUEVO = ANTIGUO` - el nombre nuevo de la columna se escribe antes que el antiguo. 

A continuación, se añade un comando de renombramiento a la tubería de limpieza. Se han añadido espacios estratégicamente para alinear el código y facilitar la lectura. 

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome)
```


Ahora puedes ver que los nombres de las columnas han cambiado: 

```{r message=FALSE, echo=F}
names(linelist)
```

#### Renombrar por posición de columna {.unnumbered} 

También puedes renombrar por la posición de la columna, en lugar del nombre de la columna, por ejemplo: 

```{r, eval=F}
rename(newNameForFirstColumn  = 1,
       newNameForSecondColumn = 2)
```

#### Renombrar mediante `select()` y `summarise()` {.unnumbered}

Como método abreviado, también puedes cambiar el nombre de las columnas dentro de las funciones de **dplyr** `select()` y `summarise()`. `select()` se utiliza para mantener sólo ciertas columnas (y se muestra más adelante en esta página). `summarise()` se muestra en las páginas [Agrupar datos](#grouping-data) y [Tablas descriptivas](#descriptive-tables). Estas funciones también utilizan el formato `nombre_nuevo = nombre_antiguo`. He aquí un ejemplo: 

```{r, eval=F}
linelist_raw %>% 
  select(# NEW name             # OLD name
         date_infection       = `infection date`,    # rename and KEEP ONLY these columns
         date_hospitalisation = `hosp date`)
```

### Otros retos {.unnumbered} 

#### Nombres de columnas de Excel vacíos {.unnumbered} 

R no puede tener columnas de conjuntos de datos que no tengan nombres de columnas (cabeceras). Así, si importa unos datos de Excel con datos pero sin cabeceras de columna, R rellenará las cabeceras con nombres como "...1" o "...2". El nombre asignado representa el número de la columna (por ejemplo, si la cuarta columna de los datos no tiene cabecera, R la nombrará "...4"). 

Puedes limpiar estos nombres manualmente haciendo referencia a su número de posición (véase el ejemplo anterior), o a su nombre asignado (`linelist_raw$...1`). 

#### Nombres de columnas y celdas fusionadas de Excel {.unnumbered}

Las celdas combinadas en un archivo de Excel son una ocurrencia común cuando se reciben datos. Como se explica en [Transición a R](#transition-to-r), las celdas combinadas pueden ser agradables para la lectura humana de los datos, pero no son "datos ordenados" y causan muchos problemas para la lectura de los datos por parte de las máquinas. R no puede ajustar las celdas combinadas. 

Recuerda a las personas que introducen los datos que **los datos legibles para el ser humano no son lo mismo que los datos legibles para la máquina**. Esfuérzate en formar a los usuarios sobre los principios de los [**datos ordenados**](https://es.r4ds.hadley.nz/datos-ordenados.html). Si es posible, intenta cambiar los procedimientos para que los datos lleguen en un formato ordenado y sin celdas fusionadas. 

* Cada variable debe tener su propia columna. 
* Cada observación debe tener su propia fila. 
* Cada valor debe tener su propia celda. 

Al utilizar la función `import()` de **rio**, el valor de una celda combinada se asignará a la primera celda y las siguientes estarán vacías. 

Una solución para tratar las celdas combinadas es importar los datos con la función `readWorkbook()` del paquete **openxlsx**. Establece el argumento `fillMergedCells = TRUE`. Esto da el valor en una celda fusionada a todas las celdas dentro del rango de fusión. 

```{r, eval=F}
linelist_raw <- openxlsx::readWorkbook("linelist_raw.xlsx", fillMergedCells = TRUE)
```

<span style="color: red;">***PELIGRO:*** Si los nombres de las columnas se fusionan con `readWorkbook()`, terminarás con nombres de columnas duplicados, que tendrás que arreglar manualmente - ¡R no funciona bien con nombres de columnas duplicados! Puedes renombrarlas haciendo referencia a su posición (por ejemplo, la columna 5), como se explica en la sección de limpieza manual de nombres de columnas. </span>




 

<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->

## Seleccionar o reordenar columnas {#select-or-re-order-columns} 

Utiliza `select()` de **dplyr** para seleccionar las columnas que deseas conservar y para especificar su orden en el dataframe. 

<span style="color: orange;">***ATENCIÓN:*** En los ejemplos siguientes, el dataframe `linelist` se modifica con `select()` y se muestra, pero no se guarda. Esto es a efectos de demostración. Los nombres de las columnas modificadas se imprimen pasando el dataframe a `names()`.</span>

**Aquí están TODOS los nombres de las columnas en linelist en este punto de la cadena de limpieza:** 

```{r}
names(linelist)
```

### Mantener las columnas {.unnumbered}  

**Selecciona sólo las columnas que desees conservar** 

Escribe sus nombres en el comando `select()`, sin comillas. Aparecerán en el dataframe en el orden que indiques. Ten en cuenta que si incluyes una columna que no existe, R devolverá un error (véase el uso de `any_of()` más adelante para evitar un error de este tipo). 

```{r}
# linelist dataset is piped through select() command, and names() prints just the column names
linelist %>% 
  select(case_id, date_onset, date_hospitalisation, fever) %>% 
  names()  # display the column names
```



### Funciones de ayuda "tidyselect" {#clean_tidyselect .unnumbered}

Estas funciones de ayuda existen para facilitar la especificación de las columnas a conservar, descartar o transformar. Provienen del paquete **tidyselect**, que se incluye en **tidyverse** y se basa en la forma en que se seleccionan las columnas en las funciones de **dplyr**. 

Por ejemplo, si deseas reordenar las columnas, `everything()` es una función útil para indicar "todas las demás columnas no mencionadas". El comando siguiente mueve las columnas `date_onset` y `date_hospitalisation` al principio (izquierda) de los datos, pero mantiene todas las demás columnas después. Fíjate en que `everything()` se escribe con paréntesis vacíos: 

```{r}
# move date_onset and date_hospitalisation to beginning
linelist %>% 
  select(date_onset, date_hospitalisation, everything()) %>% 
  names()
```

Aquí hay otras funciones de ayuda "tidyselect" que también funcionan *dentro de* las funciones de **dplyr** como `select()`, `across()` y `summarise()`: 

* `everything()` - todas las demás columnas no mencionadas 
* `last_col()` - la última columna 
* `where()` - aplica una función a todas las columnas y selecciona las que son TRUE 
* `contains()` - columnas que contienen una cadena de caracteres
  * ejemplo: `select(contains("time"))`
* `starts_with()` - coincide con un prefijo especificado  
  * ejemplo: `select(starts_with("date_"))`
* `ends_with()` - coincide con un sufijo especificado  
  * ejemplo: `select(ends_with("_post))` 
* `matches()` - para aplicar una expresión regular (regex)  
  * ejemplo: `select(matches("[pt]al"))` 
* `num_range()` - un rango numérico como x01, x02, x03 
* `any_of()` - coincide con la columna SI existe pero no devuelve ningún error si no se encuentra
  * ejemplo: `select(any_of(date_onset, date_death,     cardiac_arrest)) `

Además, utiliza operadores normales como `c()` para listar varias columnas, `:` para columnas consecutivas, `!` para opuestas, `&` para "Y" y `|` para "O". 

Utiliza `where()` para especificar criterios lógicos para las columnas. Si escribes una función dentro de `where()`, no incluyas los paréntesis vacíos de la función. El comando siguiente selecciona las columnas de tipo Numeric. 

```{r}
# select columns that are class Numeric
linelist %>% 
  select(where(is.numeric)) %>% 
  names()
```

Utiliza `contains()` para seleccionar sólo las columnas en las que el nombre de la columna contiene una cadena de caracteres especificada. `ends_with()` y `starts_with()` proporcionan más matices. 

```{r}
# select columns containing certain characters
linelist %>% 
  select(contains("date")) %>% 
  names()
```

La función `matches()` funciona de forma similar a `contains()`, pero puede escribirse en una expresión regular (mira la página sobre [Caracteres y cadenas](#characters-and-strings)), como varias cadenas separadas por barras "O" dentro de los paréntesis: 

```{r}
# searched for multiple character matches
linelist %>% 
  select(matches("onset|hosp|fev")) %>%   # note the OR symbol "|"
  names()
```

<span style="color: orange;">***ATENCIÓN:*** Si has escrito un nombre de columna y no existen datos para ella, puede devolver un error y detener tu código. Considera el uso de `any_of()` para citar columnas que pueden o no existir, especialmente útil en selecciones negativas (eliminar). </span>

Sólo existe una de estas columnas, pero no se produce ningún error y el código continúa sin detener su cadena de limpieza. 

```{r}
linelist %>% 
  select(any_of(c("date_onset", "village_origin", "village_detection", "village_residence", "village_travel"))) %>% 
  names()
```


### Eliminar columnas {.unnumbered} 

**Indica qué columnas se van a eliminar** colocando el símbolo "-" delante del nombre de la columna (por ejemplo, `select(-outcome)`), o un vector de nombres de columnas (como se indica a continuación). Todas las demás columnas se mantendrán. 
```{r}
linelist %>% 
  select(-c(date_onset, fever:vomit)) %>% # remove date_onset and all columns from fever to vomit
  names()
```

También puedes eliminar una columna utilizando la sintaxis de R **base**, definiéndola como `NULL`. Por ejemplo: 
```{r, eval=F}
linelist$date_onset <- NULL   # deletes column with base R syntax 
```



### Independiente {.unnumbered}

`select()` también puede utilizarse como un comando independiente (no en una cadena de tuberías). En este caso, el primer argumento es el dataframe original sobre el que se va a operar. 

```{r}
# Create a new linelist with id and age-related columns
linelist_age <- select(linelist, case_id, contains("age"))

# display the column names
names(linelist_age)
```



#### Añadir a la cadena de tuberías {.unnumbered}  

En `linelist_raw`, hay algunas columnas que no necesitamos: `row_num`, `merged_header` y `x28`. Las eliminamos con un comando `select()` en la cadena de tuberías de limpieza: 

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    #####################################################

    # remove column
    select(-c(row_num, merged_header, x28))
```




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## De-duplicación {#deduplication}

Consulta la página sobre [de-duplicación](#de-duplication) para ver la cantidad de opciones sobre cómo eliminar las duplicidades (de-duplicar). Aquí sólo se presenta un ejemplo muy sencillo de de-duplicación de filas. 

El paquete **dplyr** ofrece la función `distinct()`. Esta función examina cada fila y reduce el dataframe con sólo filas únicas. Es decir, elimina las filas que están 100% duplicadas. 

Al evaluar las filas duplicadas, tiene en cuenta un rango de columnas - por defecto considera todas las columnas. Como se muestra en la página de de-duplicación, puedes ajustar este rango de columnas para que la singularidad de las filas sólo se evalúe con respecto a determinadas columnas. 

En este sencillo ejemplo, simplemente añadimos el comando vacío `distinct()` a la cadena de tuberías. Esto garantiza que no haya filas que estén 100% duplicadas de otras filas (evaluadas en todas las columnas). 

Comenzamos con ` nrow(linelist)` filas en `linelist`. 

```{r}
linelist <- linelist %>% 
  distinct()
```

Después de la de-duplicación hay `nrow(linelist)` filas. Las filas eliminadas habrían sido 100% duplicados de otras filas. 

A continuación, se añade el comando `distinct()` a la cadena de tuberías de limpieza: 

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    #####################################################
    
    # de-duplicate
    distinct()
```





<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Creación y transformación de columnas  {#column-creation-and-transformation}

**Recomendamos utilizar la función  `mutate()` de dplyr para añadir una nueva columna, o para modificar una existente.** 

A continuación se muestra un ejemplo de creación de una nueva columna con `mutate()`. La sintaxis es: `mutate(nombre_nueva_columna = valor o transformación) `

En Stata, esto es similar al comando `generate`, pero también se puede utilizar `mutate()` de R para modificar una columna existente. 


### Nuevas columnas  {.unnumbered}

El comando más básico de `mutate()` para crear una nueva columna podría tener este aspecto. Crea una nueva columna `new_col` donde el valor en cada fila es 10. 

```{r, eval=F}
linelist <- linelist %>% 
  mutate(new_col = 10)
```

También puedes referenciar valores en otras columnas, para realizar cálculos. A continuación, se crea una nueva columna `bmi` para mantener el Índice de Masa Corporal (BMI) de cada caso - calculado mediante la fórmula `BMI = kg/m^2`, utilizando la columnas `ht_cm` y `wt_kg`. 

```{r}
linelist <- linelist %>% 
  mutate(bmi = wt_kg / (ht_cm/100)^2)
```

Si creas varias columnas nuevas, separa cada una con una coma y una nueva línea. A continuación se muestran ejemplos de nuevas columnas, incluidas las que consisten en valores de otras columnas combinadas mediante `str_glue()` del paquete **stringr** (véase la página sobre [Caracteres y cadenas](#characters-and-strings).

```{r}
new_col_demo <- linelist %>%                       
  mutate(
    new_var_dup    = case_id,             # new column = duplicate/copy another existing column
    new_var_static = 7,                   # new column = all values the same
    new_var_static = new_var_static + 5,  # you can overwrite a column, and it can be a calculation using other variables
    new_var_paste  = stringr::str_glue("{hospital} on ({date_hospitalisation})") # new column = pasting together values from other columns
    ) %>% 
  select(case_id, hospital, date_hospitalisation, contains("new"))        # show only new columns, for demonstration purposes
```


Revisa las columnas nuevas. A efectos de demostración, sólo se muestran las columnas nuevas y las utilizadas para crearlas:  


```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(new_col_demo,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: darkgreen;">***CONSEJO:*** Una variación de `mutate()` es la función `transmute()`. Esta función añade una nueva columna al igual que `mutate()`, pero también elimina todas las demás columnas que no se mencionan dentro de sus paréntesis. </span>


```{r, eval=F}
# HIDDEN FROM READER
# removes new demo columns created above
# linelist <- linelist %>% 
#   select(-contains("new_var"))
```



### Convertir el tipo de columna  {.unnumbered}

Las columnas que contienen valores que son fechas, números o valores lógicos (TRUE/FALSE) sólo se comportarán como se espera si están correctamente clasificadas. Hay una diferencia entre "2" de tipo carácter y 2 de tipo numérico! 

Hay formas de establecer el tipo de la columna durante los comandos de importación, pero esto suele ser engorroso. Consulta la sección sobre los tipos de objeto en [Fundamentos de R](#r-basics) para saber más sobre la conversión de los tipos de objetos y columnas. 

En primer lugar, vamos a realizar algunas comprobaciones en las columnas importantes para ver si son del tipo correcto. También vimos esto al principio cuando ejecutamos `skim()`. 

Actualmente, el tipo de la columna `age` es un carácter. Para realizar análisis cuantitativos, ¡necesitamos que estos números sean reconocidos como numéricos!. 

```{r}
class(linelist$age)
```

El tipo de la columna `date_onset` ¡también es un carácter! Para realizar los análisis, ¡estas fechas deben ser reconocidas como fechas! 
 
```{r}
class(linelist$date_onset)
```


Para resolver esto, utiliza la capacidad de `mutate()` para redefinir una columna mediante una transformación. Definimos la columna como ella misma, pero convertida a un tipo diferente. He aquí un ejemplo básico, convirtiendo o asegurando que la columna `age` sea de tipo Numeric: 

```{r, eval=F}
linelist <- linelist %>% 
  mutate(age = as.numeric(age))
```

De forma similar, puedes utilizar `as.character()` y `as.logical()`. Para convertir al tipo Factor, puedes utilizar `factor()` de R **base** o `as_factor()` de **forcats**. Lee más sobre esto en la página de [Factores](#factors). 

Hay que tener cuidado al convertir al tipo Fecha. En la página [Trabajar con fechas](#working-with-dates-1) se explican varios métodos. Normalmente, los valores de fecha en el fichero crudo deben estar todos en el mismo formato para que la conversión funcione correctamente (por ejemplo, "MM/DD/AAAA", o "DD MM AAAA"). Después de convertir al tipo Fecha, comprueba tus datos para confirmar que cada valor se ha convertido correctamente. 




### Datos agrupados {.unnumbered}  

Si tu dataframe ya está *agrupado* (véase la página sobre [Agrupar datos](#grouping-data)), `mutate()` puede comportarse de forma diferente que si el dataframe no está agrupado. Cualquier función de resumen, como `mean()`, `median()`, `max()`, etc. calculará con datos agrupados, no con filas de registros individualizados.    

```{r, eval=F}
# age normalized to mean of ALL rows
linelist %>% 
  mutate(age_norm = age / mean(age, na.rm=T))

# age normalized to mean of hospital group
linelist %>% 
  group_by(hospital) %>% 
  mutate(age_norm = age / mean(age, na.rm=T))
```

Lee más sobre el uso de `mutate()` sobre dataframes agrupados en esta [documentación mutate de tidyverse](https://dplyr.tidyverse.org/reference/mutate.html). 



### Transformar múltiples columnas  {#clean_across .unnumbered}

A menudo, para escribir un código conciso, se desea aplicar la misma transformación a varias columnas a la vez. Se puede aplicar una transformación a varias columnas a la vez utilizando la función `across()` del paquete **dplyr** (también contenido en el paquete **tidyverse**). `across()` se puede utilizar con cualquier función **de dplyr**, pero se suele utilizar dentro de `select()`, `mutate()`, `filter()` o `summarise()`. Mira cómo se aplica a `summarise()` en la página sobre [Tablas descriptivas](#descriptive-tables). 

Especificar los argumentos de las columnas `.cols = ` y la(s) función(es) a aplicar a `.fns = `. Cualquier argumento adicional a la función `.fns ` puede incluirse después de una coma, todavía dentro de `across()`.  

#### Selección de columnas con `across()`  {.unnumbered}  

Especificar las columnas de `.cols = `. Puedes nombrarlas individualmente, o utilizar funciones de ayuda "tidyselect". Especifica la función en `.fns = `. Ten en cuenta que, utilizando el modo de función mostrado a continuación, la función se escribe *sin* sus paréntesis (). 

Aquí la transformación `as.character()` se aplica a columnas específicas nombradas dentro de `across()`.

```{r, eval=F}
linelist <- linelist %>% 
  mutate(across(.cols = c(temp, ht_cm, wt_kg), .fns = as.character))
```

Las funciones de ayuda "tidyselect" están disponibles para ayudarle a especificar las columnas. Se detallan más arriba en la sección sobre Selección y reordenación de columnas, e incluyen: `everything()`, `last_col()`, `where()`, `starts_with()`, `ends_with()`, `contains()`, `matches()`, `num_range()` y `any_of()`. 

Este es un ejemplo de cómo se pueden cambiar **todas las columnas** al tipo carácter: 

```{r, eval=F}
#to change all columns to character class
linelist <- linelist %>% 
  mutate(across(.cols = everything(), .fns = as.character))
```

Convertir en caracteres todas las columnas cuyo nombre contenga la cadena "date" (fíjate en la colocación de comas y paréntesis): 

```{r, eval=F}
#to change all columns to character class
linelist <- linelist %>% 
  mutate(across(.cols = contains("date"), .fns = as.character))
```

A continuación, un ejemplo de mutación de las columnas que actualmente son de tipo POSIXct (un tipo datetime cruda que muestra etiquetas) - en otras palabras, donde la función `is.POSIXct()` evalúa a `TRUE`. Entonces queremos convertirlas con la función `as.Date()` en columnas de tipo Date normal. 

```{r, eval=F}
linelist <- linelist %>% 
  mutate(across(.cols = where(is.POSIXct), .fns = as.Date))
```

* Ten en cuenta que dentro de `across()` también utilizamos la función `where()` como `is.POSIXct` está evaluando a TRUE o FALSE. 

* Ten en cuenta que `is.POSIXct()` es del paquete **lubridate**. Otras funciones "is" similares como `is.character()`, `is.numeric()`, e `is.logical()` son de R **base** 

#### funciones `across()`  {.unnumbered}

Puedes leer la documentación de ayuda con detalles sobre cómo proporcionar funciones a `across()` escribiendo `?across`: hay varias formas de especificar la(s) función(es) a realizar en una columna e incluso puedes definir tus propias funciones: 

* Puedes escribir el nombre de la función sola (por ejemplo, `mean` o `as.character`) 
* Puedes escribir la función en **estilo purrr** (por ejemplo, `~ mean(.x, na.rm = TRUE)`) (mira [esta página](#iteration-loops-and-lists)) 
* Puedes especificar varias funciones escribiendo una lista (por ejemplo,  `list(mean = mean, n_miss = ~ sum(is.na(.x))`).  * Si proporcionas varias funciones, se devolverán varias columnas     transformadas por cada columna de entrada, con nombres únicos con formato `col_fn`. Puedes ajustar cómo se nombran las columnas nuevas con el argumento `.names = ` utilizando la sintaxis **glue** (mira la página sobre [Caracteres y cadenas](#characters-and-strings)) donde `{.col}` y `{.fn}` son la abreviatura de la columna de entrada y la función. 

Aquí hay algunos recursos en línea sobre el uso de `across()`: [pensamientos/razones del creador Hadley Wickham](https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-colwise/) 




### `coalesce()` {.unnumbered}  

Esta función de **dplyr** encuentra el primer valor no missing en cada posición. Rellena los valores que faltan con el primer valor disponible en el orden que especifiques. 

Aquí hay un ejemplo *fuera del contexto de un dataframe*: Supongamos que tienes dos vectores, uno que contiene el pueblo de detección del paciente y otro que contiene el pueblo de residencia del paciente. Puedes utilizar coalesce para elegir el primer valor no ausente de cada índice: 

```{r}
village_detection <- c("a", "b", NA,  NA)
village_residence <- c("a", "c", "a", "d")

village <- coalesce(village_detection, village_residence)
village    # print
```

Esto funciona de la misma manera si se proporcionan columnas del dataframe: para cada fila, la función asignará el nuevo valor de la columna con el primer valor que no falte en las columnas proporcionadas (en el orden indicado). 

```{r, eval=F}
linelist <- linelist %>% 
  mutate(village = coalesce(village_detection, village_residence))
```

Este es un ejemplo de operación "por filas". Para cálculos más complicados por filas, consulta la sección siguiente sobre cálculos por filas. 



### Matemáticas acumulativas {.unnumbered}

Si deseas que una columna refleje acumulados la sum/mean/min/max, etc., tal y como se ha evaluado en las filas de un dataframe hasta ese punto, utiliza las siguientes funciones: 

`cumsum()` devuelve la suma acumulada, como se muestra a continuación: 

```{r}
sum(c(2,4,15,10))     # returns only one number
cumsum(c(2,4,15,10))  # returns the cumulative sum at each step
```

Esto se puede utilizar en un dataframe al crear una nueva columna. Por ejemplo, para calcular el número acumulado de casos por día en un brote, considere un código como este:  

```{r, warning=F, message=F}
cumulative_case_counts <- linelist %>%  # begin with case linelist
  count(date_onset) %>%                 # count of rows per day, as column 'n'   
  mutate(cumulative_cases = cumsum(n))  # new column, of the cumulative sum at each row
```

A continuación se muestran las 10 primeras filas: 

```{r}
head(cumulative_case_counts, 10)
```

Consulta la página sobre [curvas epidémicas](#epidemic-curves) para saber cómo representar la incidencia acumulada con epicurve. 

Véase también:  
`cumsum()`, `cummean()`, `cummin()`, `cummax()`, `cumany()`, `cumall()`  





### Utilizando R **base**  {.unnumbered}  

Para definir una nueva columna (o redefinir una columna) utilizando R **base**, escribe el nombre del dataframe, conectado con `$`, a la *nueva* columna (o la columna a modificar). Utiliza el operador de asignación `<-` para definir el nuevo valor o valores. Recuerda que al usar R **base** debes especificar siempre el nombre del dataframe antes del nombre de la columna (por ejemplo, `dataframe$column`). Este es un ejemplo de creación de la columna `bmi` usando R **base**: 

```{r, eval=F}
linelist$bmi = linelist$wt_kg / (linelist$ht_cm / 100) ^ 2)
```



### Añadir a la cadena de tuberías  {.unnumbered}  

**A continuación, se añade una nueva columna a la cadena de tuberías y se convierten algunos tipos.**  

```{r }
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 
  
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    ###################################################
    # add new column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>% 
  
    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) 
```





## Recodificar valores  {#re-code-values}

A continuación, se presentan algunos escenarios en los que es necesario recodificar (cambiar) los valores: 

* para editar un valor específico (por ejemplo, una fecha con un año o formato incorrecto) 
* para conciliar valores que no se escriben igual 
* para crear una nueva columna de valores categóricos 
* para crear una nueva columna de categorías numéricas (por ejemplo, categorías de edad)  



### Valores específicos  {.unnumbered}  

Para cambiar los valores manualmente puedes utilizar la función `recode()` dentro de la función `mutate()`. 

Imagínate que hay una fecha sin sentido en los datos (por ejemplo, "2014-14-15"): podrías corregir la fecha manualmente en los datos originales, o bien, podrías escribir el cambio en la serie de comandos de limpieza a través de `mutate()` y `recode()`. Esto último es más transparente y reproducible para cualquier otra persona que quiera entender o repetir su análisis. 

```{r, eval=F}
# fix incorrect values                   # old value       # new value
linelist <- linelist %>% 
  mutate(date_onset = recode(date_onset, "2014-14-15" = "2014-04-15"))
```

La línea `mutate()` anterior puede leerse como: "mutar la columna `date_onset` para que sea igual a la columna `date_onset` recodificada de forma que el VALOR ANTIGUO se cambie por el NUEVO VALOR". Ten en cuenta que este patrón (VIEJO = NUEVO) para `recode()` es el opuesto a la mayoría de los patrones de R (nuevo = viejo). La comunidad de desarrollo de R está trabajando en la revisión de esto. 

**Aquí hay otro ejemplo de recodificación de múltiples valores dentro de una columna.** 

En `linelist` hay que limpiar los valores de la columna "hospital". Hay varias grafías diferentes y muchos valores que faltan. 

```{r}
table(linelist$hospital, useNA = "always")  # print table of all unique values, including missing  
```

El comando `recode()` de abajo redefine la columna "hospital" como la columna actual "hospital", pero con los cambios especificados en la recodificación. ¡No olvides las comas después de cada uno! 

```{r}
linelist <- linelist %>% 
  mutate(hospital = recode(hospital,
                     # for reference: OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      ))
```


Ahora vemos que se han corregido y consolidado las grafías de la columna `hospital`: 

```{r}
table(linelist$hospital, useNA = "always")
```

<span style="color: darkgreen;">***CONSEJO:*** El número de espacios antes y después de un signo de igualdad no importa. Haz que tu código sea más fácil de leer alineando el signo = para todas o la mayoría de las filas. Además, considera la posibilidad de añadir una fila de comentarios con hash (#) para aclarar a los futuros lectores qué lado es VIEJO y qué lado es NUEVO.  </span>  

<span style="color: darkgreen;">***CONSEJO:*** A veces existe un valor con caracteres *en blanco* en unos datos (no reconocido como valor Missing - `NA` de R. Puedes hacer referencia a este valor con dos comillas sin espacio intermedio (""). </span>  




### Por lógica  {.unnumbered}

A continuación, demostramos cómo recodificar los valores de una columna utilizando lógica y condiciones: 

* Uso de `replace()`, `ifelse()` e `if_else()` para una lógica simple 
* Uso de `case_when()` para una lógica más compleja 
  



### Lógica simple  {.unnumbered}  


#### sustituir con `replace()` {.unnumbered}  

Para recodificar con criterios lógicos simples, puedes utilizar `replace()` dentro de `mutate()`. `replace()` es una función de R **base**. Utiliza una condición lógica para especificar las filas a cambiar. La sintaxis general es: 

`mutate(col_to_change = replace(col_a_cambiar, criterio para filas, nuevo valor))`.  

Una situación frecuente es utilizar `replace()` para **cambiar sólo un valor en una fila, utilizando un identificador de fila único**. A continuación, el género se cambia a "Mujer" en la fila donde la columna `case_id` es "2195".   

```{r, eval=F}
# Example: change gender of one specific observation to "Female" 
linelist <- linelist %>% 
  mutate(gender = replace(gender, case_id == "2195", "Female"))
```

Abajo se puede ver un ejemplo equivalente utilizando la sintaxis de R **base** y los paréntesis de indexación `[ ]`. Se lee como "Cambia el valor de la columna `gender` del dataframe `linelist` a 'Female'" (para las filas en las que la columna `case_id` de `linelist` tiene el valor '2195').   

```{r, eval=F}
linelist$gender[linelist$case_id == "2195"] <- "Female"
```




#### `ifelse()` e `if_else()` {.unnumbered}  

Otra herramienta para la lógica simple es `ifelse()` y su compañero `if_else()`. Sin embargo, en la mayoría de los casos para la recodificación es más claro utilizar `case_when()` (detallado a continuación). Estos comandos "if else" son versiones simplificadas de una sentencia de programación `if` y `else`. La sintaxis general es:  
`ifelse(condición, valor a devolver si la condición evalúa como TRUE, valor a devolver si la condición evalúa como FALSE)` 

A continuación, se define la columna `source_known`. Su valor en una fila determinada se establece como "known" si *no falta* el valor de la fila en la columna `source`. *Si falta* el valor en `source`, el valor de `source_known` se establece como "unknown". 

```{r, eval=F}
linelist <- linelist %>% 
  mutate(source_known = ifelse(!is.na(source), "known", "unknown"))
```

`if_else()` es una versión especial de **dplyr** que maneja fechas. Ten en cuenta que, si el valor "verdadero" es una fecha, el valor "falso" también debe calificar una fecha, de ahí que se utilice el valor especial `NA_real_` en lugar de simplemente `NA`. 

```{r, eval=F}
# Create a date of death column, which is NA if patient has not died.
linelist <- linelist %>% 
  mutate(date_death = if_else(outcome == "Death", date_outcome, NA_real_))
```

**Evita encadenar muchos comandos ifelse... ¡utilza** case_when**() en su lugar!** `case_when()` es mucho más fácil de leer y cometerás menos errores.   

```{r, fig.align = "center", out.width = "100%", echo=F}
knitr::include_graphics(here::here("images", "ifelse bad.png"))
```

Fuera del contexto de un dataframe, si deseas que un objeto utilizado en su código cambie su valor, considere el uso de `switch()` de R **base**. 



### Lógica compleja  {#clean_case_when .unnumbered}  

Utiliza `case_when()` de **dplyr** si estás recodificando en muchos grupos nuevos, o si necesita utilizar sentencias lógicas complejas para recodificar valores. Esta función evalúa si cada fila del dataframe cumple los criterios especificados y asigna el nuevo valor correcto. 

Los comandos `case_when()` consisten en sentencias que tienen un lado derecho (RHS) y un lado izquierdo (LHS) separados por una "tilde" `~` (cola de chancho). Los criterios lógicos están en el lado izquierdo y los valores de conformidad están en el lado derecho de cada sentencia. Las declaraciones están separadas por comas. 

Por ejemplo, aquí utilizamos las columnas `age` y `age_unit` para crear una columna `age_years`: 


```{r}
linelist <- linelist %>% 
  mutate(age_years = case_when(
            age_unit == "years"  ~ age,       # if age is given in years
            age_unit == "months" ~ age/12,    # if age is given in months
            is.na(age_unit)      ~ age))      # if age unit is missing, assume years
                                              # any other circumstance, assign NA (missing)
```


A medida que se evalúa cada fila de los datos, los criterios se aplican/evalúan en el orden en que se escriben las sentencias `case_when()`, de arriba a abajo. Si el criterio superior se evalúa como `TRUE` para una fila determinada, se asigna el valor RHS, y los criterios restantes ni siquiera se prueban para esa fila. Por lo tanto, es mejor escribir los criterios más específicos primero y los más generales al final. A una fila de datos que no cumpla ninguno de los criterios del RHS se le asignará `NA`.   

En esta línea, en su declaración final, coloca `TRUE` en el lado izquierdo, lo que capturará cualquier fila que no cumpla ninguno de los criterios anteriores. Al lado derecho de esta declaración se le podría asignar un valor como "¡comprobado!" o faltante.  

A continuación se muestra otro ejemplo de `case_when()` utilizado para crear una nueva columna con la clasificación del paciente, según una definición de caso para los casos confirmados y sospechosos:  

```{r, eval=F}
linelist <- linelist %>% 
     mutate(case_status = case_when(
          
          # if patient had lab test and it is positive,
          # then they are marked as a confirmed case 
          ct_blood < 20                   ~ "Confirmed",
          
          # given that a patient does not have a positive lab result,
          # if patient has a "source" (epidemiological link) AND has fever, 
          # then they are marked as a suspect case
          !is.na(source) & fever == "yes" ~ "Suspect",
          
          # any other patient not addressed above 
          # is marked for follow up
          TRUE                            ~ "To investigate"))
```


<span style="color: red;">***PELIGRO:* Los valores del lado derecho deben ser todos del mismo tipo**: numéricos, de caracteres, de fecha, lógicos, etc. Para asignar faltantes (`NA`), puede ser necesario utilizar variaciones especiales de `NA` como `NA_character_`, `NA_real_` (para numérico o POSIX), y `as.Date(NA)`. Lee más en [Trabajar con fechas](#working-with-dates-1). </span>  




### Valores faltantes {.unnumbered} 

A continuación, se presentan funciones especiales para el tratamiento de los valores faltantes en el contexto de la limpieza de datos. 

Consulta la página sobre [Valores faltantes](#missing-data) para obtener consejos más detallados sobre la identificación y el tratamiento de los valores faltantes. Por ejemplo, la función `is.na()` que comprueba lógicamente la ausencia de datos. 


**`replace_na()`**  

Para cambiar los valores faltantes (`NA`) por un valor específico, como "Missing", utiliza la función de **dplyr** `replace_na()` dentro de `mutate()`. Ten en cuenta que se utiliza de la misma manera que recodificar anteriormente - el nombre de la variable debe repetirse dentro de `replace_na()`.  

```{r}
linelist <- linelist %>% 
  mutate(hospital = replace_na(hospital, "Missing"))
```


**fct_explicit_na()**  

Esta es una función del paquete **forcats**. El paquete **forcats** maneja columnas del tipo Factor. Los factores son la forma en que R maneja valores *ordenados* como `c("First", "Second", "Third")` o para establecer el orden en que los valores (por ejemplo, hospitales) aparecen en las tablas y gráficos. Vea la página sobre [Factores](#factors). 

Si tus datos son del tipo Factor y tratas de convertir `NA` en "Missing" utilizando `replace_na()`, obtendrás este error: `invalid factor level, NA generated` (nivel de factor no válido, NA generado). Has intentado añadir "Missing" como valor, cuando no estaba definido como un posible nivel del factor, y ha sido rechazado. 

La forma más fácil de resolver esto es utilizar la función `fct_explicit_na()` de  **forcats** que convierte una columna en factor de tipo, y convierte los valores `NA` en el carácter "(Missing)".  

```{r, eval=F}
linelist %>% 
  mutate(hospital = fct_explicit_na(hospital))
```

Una alternativa más lenta sería añadir el nivel del factor utilizando `fct_expand()` y luego convertir los valores que faltan.   

**`na_if()`**  

Para convertir un *valor específico en* `NA`, utiliza `na_if()` de **dplyr**. El comando siguiente realiza la operación opuesta a `replace_na()`. En el siguiente ejemplo, cualquier valor de "Missing" en la columna `hospital` se convierte en `NA`. 

```{r}
linelist <- linelist %>% 
  mutate(hospital = na_if(hospital, "Missing"))
```

Nota: `na_if()` **no puede utilizarse para criterios lógicos** (por ejemplo, "todos los valores > 99") - utiliza `replace()` o `case_when()` para ello:  

```{r, eval=F}
# Convert temperatures above 40 to NA 
linelist <- linelist %>% 
  mutate(temp = replace(temp, temp > 40, NA))

# Convert onset dates earlier than 1 Jan 2000 to missing
linelist <- linelist %>% 
  mutate(date_onset = replace(date_onset, date_onset > as.Date("2000-01-01"), NA))
```




### Diccionario de limpieza  {.unnumbered}

Utiliza el paquete R **matchmaker** y su función `match_df()` para limpiar un dataframe con un *diccionario de limpieza*.  

1.  Crear un diccionario de limpieza con 3 columnas:  
     * Una columna "desde" (el valor incorrecto)  
     * Una columna "para" (el valor correcto)  
     * Una columna que especifica la columna a la que se aplicarán los cambios (o ".global" para aplicarlo a todas las columnas) 

Nota: Las entradas del diccionario .global serán anuladas por las entradas del diccionario específico de la columna. 

```{r, fig.align = "center", out.width = "100%", echo=F}
knitr::include_graphics(here::here("images", "cleaning_dict.png"))
```


2.  Importa el archivo del diccionario a R. Este ejemplo puede descargarse a través de las instrucciones de la página [Descargar manual y datos](#download-handbook-and-data).   

```{r, echo=F}
cleaning_dict <- rio::import(here("data", "case_linelists", "cleaning_dict.csv"))
```

```{r, eval=F}
cleaning_dict <- import("cleaning_dict.csv")
```

3.  Pasa linelist crudas a `match_df()`, especificando en `dictionary = ` el dataframe del diccionario de limpieza. El argumento `from = ` debe ser el nombre de la columna del diccionario que contiene los valores "originales", el argumento `by = ` debe ser la columna del diccionario que contiene los correspondientes valores "nuevos", y la tercera columna enumera la columna en la que se realizará el cambio. Utilice `.global` en la columna `by = ` para aplicar un cambio en todas las columnas. Una cuarta columna del diccionario `order` se puede utilizar para especificar el orden del factor de los nuevos valores.  

```{r}
linelist <- linelist %>%     # provide or pipe your dataset
     matchmaker::match_df(
          dictionary = cleaning_dict,  # name of your dictionary
          from = "from",               # column with values to be replaced (default is col 1)
          to = "to",                   # column with final values (default is col 2)
          by = "col"                   # column with column names (default is col 3)
  )
```

Ahora desplázate a la derecha para ver cómo han cambiado los valores - en particular el `gender` (de minúsculas a mayúsculas), y todas las columnas de síntomas se han transformado de sí/no a 1/0. 

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist,50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Ten en cuenta que los nombres de las columnas en el diccionario de limpieza deben corresponder a los nombres *en este punto* de tu script de limpieza. Consulta esta [referencia en línea para el paquete linelist](https://cran.r-project.org/web/packages/matchmaker/vignettes/intro.html) para obtener más detalles. 






#### Añadir a la cadena de tuberías {.unnumbered}  

**A continuación, se añaden algunas columnas y transformaciones de columna nuevas a la cadena de tuberías.** 

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 
  
    # add column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # add column: delay to hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
   # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
   ###################################################

    # clean values of hospital column
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # create age_years column (from age and age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age))
```






<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Categorías numéricas  {#num_cats}

Aquí describimos algunos enfoques especiales para crear categorías a partir de columnas numéricas. Algunos ejemplos comunes son las categorías de edad, los grupos de valores de laboratorio, etc. Aquí discutiremos: 

* `age_categories()`, del paquete **epikit**  
* `cut()`, de R **base**
* `case_when()`  
* ruptura de cuantiles con `quantile()` y `ntile()` 


### Revisión de la distribución {.unnumbered}

Para este ejemplo crearemos una columna `age_cat` utilizando la columna `age_years.` 

```{r}
#check the class of the linelist variable age
class(linelist$age_years)
```

En primer lugar, examina la distribución de tus datos, para hacer los puntos de corte apropiados. Consulta la página sobre [Conceptos básicos de ggplot](#ggplot-basics).  

```{r, out.height='50%'}
# examine the distribution
hist(linelist$age_years)
```

```{r}
summary(linelist$age_years, na.rm=T)
```

<span style="color: orange;">***ATENCIÓN:*** A veces, las variables numéricas se importarán como tipo "carácter". Esto ocurre si hay caracteres no numéricos en algunos de los valores, por ejemplo, una entrada de "2 meses" para la edad, o (dependiendo de la configuración de su configuración local de R) si se utiliza una coma en el lugar de los decimales (por ejemplo, "4,5" para significar cuatro años y medio). </span>


<!-- ======================================================= -->
### `age_categories()` {.unnumbered}

Con el paquete **epikit**, puedes utilizar la función `age_categories()` para categorizar y etiquetar fácilmente las columnas numéricas (nota: esta función puede aplicarse también a las variables numéricas no relacionadas con la edad). Además, la columna de salida es automáticamente un factor ordenado. 

Aquí están las entradas requeridas: 

* Un vector numérico (columna) 
* El argumento + breakers = ` - proporciona un vector numérico de puntos de ruptura para los nuevos grupos 

Primero, el ejemplo más sencillo:   

```{r}
# Simple example
################
pacman::p_load(epikit)                    # load package

linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(             # create new column
      age_years,                            # numeric column to make groups from
      breakers = c(0, 5, 10, 15, 20,        # break points
                   30, 40, 50, 60, 70)))

# show table
table(linelist$age_cat, useNA = "always")
```

Los valores de ruptura que especificas son por defecto los límites inferiores - es decir, están incluidos en el grupo "superior" / los grupos están "abiertos" en la parte inferior/izquierda. Como se muestra a continuación, puedes añadir 1 a cada valor de ruptura para conseguir grupos que estén abiertos por la parte superior/derecha. 
 
```{r}
# Include upper ends for the same categories
############################################
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      breakers = c(0, 6, 11, 16, 21, 31, 41, 51, 61, 71)))

# show table
table(linelist$age_cat, useNA = "always")
```


Puedes ajustar cómo se muestran las etiquetas con el `separator = `. El valor predeterminado es "-" 

Puedes ajustar cómo se manejan los números superiores, con el argumento `ceiling = `. Para establecer un corte superior establezca `ceiling = TRUE`. En este uso, el valor de ruptura más alto proporcionado es un "techo" y no se crea una categoría "XX+". Cualquier valor por encima del valor de corte más alto (o hasta el límite  `upper = `, si está definido) se categoriza como `NA`. A continuación, se muestra un ejemplo con `ceiling = TRUE`, de modo que no hay categoría de XX+ y los valores por encima de 70 (el valor de ruptura más alto) se asignan como `NA`. 

```{r}
# With ceiling set to TRUE
##########################
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      breakers = c(0, 5, 10, 15, 20, 30, 40, 50, 60, 70),
      ceiling = TRUE)) # 70 is ceiling, all above become NA

# show table
table(linelist$age_cat, useNA = "always")
```

Alternativamente, en lugar de los `breakers = `, puedes proporcionar todos los `lower = `, `upper = `, and `by = `: 

* `lower = ` El número más bajo que se quiere considerar - por defecto es 0 
* `upper = ` El número más alto que quiere que se considere   
* `by = `    El número de años entre los grupos 

```{r}
linelist <- linelist %>% 
  mutate(
    age_cat = age_categories(
      age_years, 
      lower = 0,
      upper = 100,
      by = 10))

# show table
table(linelist$age_cat, useNA = "always")
```


Consulta la página de ayuda de la función para más detalles (escribe `?age_categories` en la consola de R). 


<!-- ======================================================= -->
### `cut()` {.unnumbered}

`cut()` es una alternativa a `age_categories()` de R **base**, pero creo que verás por qué `age_categories()` se desarrolló para simplificar este proceso. Algunas diferencias notables de `age_categories()` son: 

* No es necesario instalar/cargar otro paquete 
* Puedes especificar si los grupos están abiertos/cerrados a la derecha/izquierda 
* Debes proporcionar etiquetas precisas 
* Si quieres que el 0 se incluya en el grupo más bajo debes especificarlo 

La sintaxis básica dentro de `cut()` es proporcionar primero la columna numérica que se va a cortar (`age_years`), y luego el argumento *breaks*, que es un vector numérico `c()` de puntos de ruptura. Utilizando `cut()`, la columna resultante es un factor ordenado. 

Por defecto, la categorización se produce de manera que el lado derecho/superior es "abierto" e inclusivo (y el lado izquierdo/inferior es "cerrado" o exclusivo). Este es el comportamiento opuesto al de la función `age_categories()`. Las etiquetas por defecto utilizan la notación "(A, B]", lo que significa que A no está incluido pero B sí. **Invierte este comportamiento proporcionando el argumento right = TRUE**. 

Así, por defecto, ¡los valores "0" se excluyen del grupo más bajo, y se categorizan como `NA`! Los valores "0" podrían ser codificados para los bebés como edad 0, así que ¡ten cuidado! Para cambiar esto, añade el argumento `include.lowest = TRUE`  para que cualquier valor "0" se incluya en el grupo más bajo. La etiqueta generada automáticamente para la categoría más baja será entonces "[A],B]". Ten en cuenta que si incluye el argumento `include.lowest = TRUE` **y** `right = TRUE`, la inclusión extrema se aplicará ahora al valor del punto de ruptura y a la categoría *más altos*, no a los más bajos. 

Puedes proporcionar un vector de etiquetas personalizadas utilizando el argumento `labels = `. Como se escriben manualmente, ¡ten mucho cuidado de que sean precisas! Comprueba el trabajo utilizando una tabulación cruzada, como se describe a continuación. 

A continuación se muestra un ejemplo de `cut()` aplicado a `age_years` para crear la nueva variable `age_cat`: 

```{r}
# Create new variable, by cutting the numeric age variable
# lower break is excluded but upper break is included in each category
linelist <- linelist %>% 
  mutate(
    age_cat = cut(
      age_years,
      breaks = c(0, 5, 10, 15, 20,
                 30, 50, 70, 100),
      include.lowest = TRUE         # include 0 in lowest group
      ))

# tabulate the number of observations per group
table(linelist$age_cat, useNA = "always")
```


**¡Comprueba tu trabajo!** Verifica que cada valor de edad fue asignado a la categoría correcta cruzando las columnas numéricas y de categoría. Examina la asignación de los valores límite (por ejemplo, 15, si las categorías vecinas son 10-15 y 16-20). 

```{r}
# Cross tabulation of the numeric and category columns. 
table("Numeric Values" = linelist$age_years,   # names specified in table for clarity.
      "Categories"     = linelist$age_cat,
      useNA = "always")                        # don't forget to examine NA values
```





**Re-etiquetado de los valores `NA`** 

Puedes asignar a los valores `NA` una etiqueta como "Missing". Como la nueva columna es del tipo Factor (valores restringidos), no puedes simplemente mutarla con `replace_na()`, ya que este valor será rechazado. En su lugar, utilice `fct_explicit_na()` de **forcats** como se explica en la página de [Factores](#factors).   

```{r}
linelist <- linelist %>% 
  
  # cut() creates age_cat, automatically of class Factor      
  mutate(age_cat = cut(
    age_years,
    breaks = c(0, 5, 10, 15, 20, 30, 50, 70, 100),          
    right = FALSE,
    include.lowest = TRUE,        
    labels = c("0-4", "5-9", "10-14", "15-19", "20-29", "30-49", "50-69", "70-100")),
         
    # make missing values explicit
    age_cat = fct_explicit_na(
      age_cat,
      na_level = "Missing age")  # you can specify the label
  )    

# table to view counts
table(linelist$age_cat, useNA = "always")
```

**Realiza rápidamente pausas y etiquetas** 

Para una forma rápida de hacer rupturas y etiquetar vectores, utiliza algo como lo siguiente. Consulta la página de [fundamentos de R](#r-basics) para obtener referencias sobre `seq()` y `rep()`. 

```{r, eval=F}
# Make break points from 0 to 90 by 5
age_seq = seq(from = 0, to = 90, by = 5)
age_seq

# Make labels for the above categories, assuming default cut() settings
age_labels = paste0(age_seq + 1, "-", age_seq + 5)
age_labels

# check that both vectors are the same length
length(age_seq) == length(age_labels)
```


Lee más sobre `cut()` en la página de ayuda escribiendo `?cut` en la consola de R.  




### Roturas cuartílicas  {.unnumbered}  

En el entendimiento común, los "cuartiles" o "percentiles" suelen referirse a un valor por debajo del cual cae una proporción de valores. Por ejemplo, el percentil 95 de las edades en `linelist` sería la edad por debajo de la cual cae el 95% de las edades. 

Sin embargo, en el lenguaje común, "cuartiles" y "deciles" también pueden referirse a los *grupos de datos* divididos por igual en 4 o 10 grupos (Ten en cuenta que habrá un punto de ruptura más que un grupo). 

Para obtener los puntos de ruptura de los cuartiles, se puede utilizar `quantile()` del paquete **stats** de R **base.** Se proporciona un vector numérico (por ejemplo, una columna en unos datos) y un vector de valores de probabilidad numérica que van de 0 a 1,0. Los puntos de ruptura se devuelven como un vector numérico. Explore los detalles de las metodologías estadísticas escribiendo `?quantile.` 

* Si su vector numérico de entrada tiene valores faltantes, es mejor establecer `na.rm = TRUE` 
* Establecer `names = FALSE` para obtener un vector numérico sin nombre 

```{r}
quantile(linelist$age_years,               # specify numeric vector to work on
  probs = c(0, .25, .50, .75, .90, .95),   # specify the percentiles you want
  na.rm = TRUE)                            # ignore missing values 
```

Puedes utilizar los resultados de `quantile()` como puntos de ruptura en `age_categories()` o `cut()`. A continuación creamos una nueva columna `deciles` utilizando `cut()` donde los puntos de ruptura se definen utilizando `quantiles()` en `age_years`. A continuación, mostramos los resultados utilizando `tabyl()` de **janitor** para que puedas ver los porcentajes (véase la página de [tablas descriptivas](#descriptive-tables)). Observa cómo no son exactamente el 10% en cada grupo. 

```{r}
linelist %>%                                # begin with linelist
  mutate(deciles = cut(age_years,           # create new column decile as cut() on column age_years
    breaks = quantile(                      # define cut breaks using quantile()
      age_years,                               # operate on age_years
      probs = seq(0, 1, by = 0.1),             # 0.0 to 1.0 by 0.1
      na.rm = TRUE),                           # ignore missing values
    include.lowest = TRUE)) %>%             # for cut() include age 0
  janitor::tabyl(deciles)                   # pipe to table to display
```

### Grupos de tamaño uniforme  {.unnumbered}  

Otra herramienta para hacer grupos numéricos es la función `ntile()` de **dplyr**, que intenta dividir los datos en n *grupos de tamaño uniforme -* pero ten en cuenta que, a diferencia de *quantile(),* el mismo valor podría aparecer en más de un grupo*.* Proporcione el vector numérico y luego el número de grupos. Los valores de la nueva columna creada son sólo "números" de grupo (por ejemplo, del 1 al 10), no el rango de valores en sí mismo como cuando se utiliza `cut()`. 

```{r}
# make groups with ntile()
ntile_data <- linelist %>% 
  mutate(even_groups = ntile(age_years, 10))

# make table of counts and proportions by group
ntile_table <- ntile_data %>% 
  janitor::tabyl(even_groups)
  
# attach min/max values to demonstrate ranges
ntile_ranges <- ntile_data %>% 
  group_by(even_groups) %>% 
  summarise(
    min = min(age_years, na.rm=T),
    max = max(age_years, na.rm=T)
  )

# combine and print - note that values are present in multiple groups
left_join(ntile_table, ntile_ranges, by = "even_groups")
```


<!-- ======================================================= -->
### `case_when()` { .unnumbered}

Es posible utilizar la función `case_when()` de **dplyr** para crear categorías a partir de una columna numérica, pero es más fácil utilizar `age_categories()` de **epikit** o `cut()` porque éstas crearán un factor ordenado automáticamente. 

Si utilizas `case_when()`, por favor, revise el uso adecuado como se ha descrito anteriormente en la sección Re-codificar valores de esta página. También Ten en cuenta que todos los valores del lado derecho deben ser del mismo tipo. Por lo tanto, si quiere `NA` en el lado derecho debes escribir "Missing" o utilizar el valor especial `NA_character_`. 


### Añadir a la cadena de tuberías {.unnumbered}  

A continuación, se añade el código para crear dos columnas categóricas de edad a la cadena de tuberías de limpieza: 

```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 

    # add column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # add column: delay to hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
    # clean values of hospital column
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # create age_years column (from age and age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age)) %>% 
  
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    ###################################################   
    mutate(
          # age categories: custom
          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),
        
          # age categories: 0 to 85 by 5s
          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5)))
```








<!-- ======================================================= -->
## Añadir filas {#add-rows}

### Uno a uno  {.unnumbered}  

Añadir filas una a una manualmente es tedioso pero puede hacerse con `add_row()` de **dplyr**. Recuerda que cada columna debe contener valores de un solo tipo (ya sea carácter, numérico, lógico, etc.). Así que añadir una fila requiere matizar para mantener esto. 

```{r, eval=F}
linelist <- linelist %>% 
  add_row(row_num = 666,
          case_id = "abc",
          generation = 4,
          `infection date` = as.Date("2020-10-10"),
          .before = 2)
```

Utiliza `.before` y `.after`. para especificar la ubicación de la fila que deseas añadir. `.before = 3` pondrá la nueva fila antes de la tercera fila actual. El comportamiento por defecto es añadir la fila al final. Las columnas no especificadas se dejarán vacías (`NA`). 

El nuevo *número de fila* puede parecer extraño ("...23") pero los números de fila de las filas preexistentes *han* cambiado. Por lo tanto, si utiliza el comando dos veces, examine/pruebe la inserción cuidadosamente. 

Si uno de los tipos que proporcionas está desactivado, verás un error como este: 

```
Error: Can't combine ..1$infection date <date> and ..2$infection date <character>.
```

(al insertar una fila con un valor de fecha, recuerde envolver la fecha en la función `as.Date()` como `as.Date("2020-10-10")`). 


### Unir filas  {.unnumbered}  

Para combinar conjuntos de datos uniendo las filas de un dataframe al final de otro dataframe, puedes utilizar `bind_rows()` de **dplyr**. Esto se explica con más detalle en la página [Unir datos](#joining-data).




<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Filtrar filas  {#filter-rows}


Un paso típico de limpieza después de haber limpiado las columnas y recodificado los valores es *filtrar* el dataframe para filas específicas usando el verbo **dplyr** `filter()`. 

Dentro de `filter()`, especifique la lógica que debe ser `TRUE` para que se mantenga una fila en los datos. A continuación, mostramos cómo filtrar filas basándose en condiciones lógicas simples y complejas.  



<!-- ======================================================= -->
### Filtro simple  {.unnumbered} 

Este sencillo ejemplo redefine el dataframe `linelist` en sí mismo, habiendo filtrado las filas para que cumplan una condición lógica. **Sólo se conservan las filas en las que la declaración lógica dentro de los paréntesis se evalúa como TRUE.** 

En este ejemplo, la sentencia lógica es `gender == "f"`, que pregunta si el valor de la columna `gender` es igual a "f" (distingue entre mayúsculas y minúsculas). 

Antes de aplicar el filtro, el número de filas de `linelist` es ` nrow(linelist)`. 

```{r, eval=F}
linelist <- linelist %>% 
  filter(gender == "f")   # keep only rows where gender is equal to "f"
```

Después de aplicar el filtro, el número de filas de `linelist` is ` linelist %>% filter(gender == "f") %>% nrow()`. 


### Filtrar los valores faltantes  {.unnumbered}  

Es bastante común querer filtrar las filas que tienen valores faltantes. Resiste la tentación de escribir `filter(!is.na(column) & !is.na(column))` y utiliza en su lugar la función de tidyr que está hecha a medida para este propósito: drop_na(). Si se ejecuta con paréntesis vacíos, elimina las filas con *cualquier* valor que falte. Como alternativa, puedes proporcionar los nombres de las columnas específicas que deben evaluarse para comprobar si faltan, o utilizar las funciones de ayuda "tidyselect" descritas [anteriormente](#clean_tidyselect).  

```{r, eval=F}
linelist %>% 
  drop_na(case_id, age_years)  # drop rows with missing values for case_id or age_years
```

Consulta la página sobre [Valores faltantes](#missing-data) para conocer muchas técnicas para analizar y gestionar los datos ausentes. 




### Filtrar por número de fila  {.unnumbered}  

En un dataframe o tibble, cada fila suele tener un "número de fila" que (cuando se ve en R Viewer) aparece a la izquierda de la primera columna. No es en sí misma una columna real en los datos, pero puede utilizarse en una sentencia `filter()`. 

Para filtrar en base al "número de fila", puedes utilizar la función `row_number()` de **dplyr** con paréntesis abiertos como parte de una sentencia lógica de filtrado. A menudo se utiliza el operador `%in%` y un rango de números como parte de esa sentencia lógica, como se muestra a continuación. Para ver las *primeras* N filas, también puedes utilizar la función especial `head()` de **dplyr**.  

```{r, eval=F}
# View first 100 rows
linelist %>% head(100)     # or use tail() to see the n last rows

# Show row 5 only
linelist %>% filter(row_number() == 5)

# View rows 2 through 20, and three specific columns
linelist %>% filter(row_number() %in% 2:20) %>% select(date_onset, outcome, age)
```

También puedes convertir los números de fila en una verdadera columna pasando su dataframe a la función `rownames_to_column()` de **tibble** (no escribas nada en los paréntesis). 


<!-- ======================================================= -->
### Filtro complejo  {.unnumbered} 

Se pueden construir sentencias lógicas más complejas utilizando paréntesis `( )`, OR `|`, negación `!` , `%in%`, y operadores AND `&`. Un ejemplo es el siguiente: 

Nota: Puedes utilizar el operador `!` delante de un criterio lógico para negarlo. Por ejemplo, `!is.na(column)` se evalúa como verdadero si el valor de la columna *no* falta. Del mismo modo, `!column %in% c("a", "b", "c")` es verdadero si el valor de la columna *no está* en el vector.  


#### Examinar los datos  {.unnumbered}  

A continuación, se muestra un sencillo comando de una línea para crear un histograma de las fechas de inicio. Vea que un segundo brote más pequeño de 2012-2013 también está incluido en este conjunto de datos sin procesar. **Para nuestros análisis, queremos eliminar las entradas de este brote anterior.**  

```{r, out.width = "50%"}
hist(linelist$date_onset, breaks = 50)
```


#### ¿Cómo manejan los filtros los valores numéricos y de fecha que faltan? {.unnumbered}  
 
¿Podemos simplemente filtrar por `date_onset` a las filas posteriores a junio de 2013? **Precaución. La aplicación del código `filter(date_onset > as.Date("2013-06-01"))` eliminaría todas las filas de la epidemia posterior con una fecha de inicio ausente!** 

<span style="color: red;">***PELIGRO:*** Filtrar a mayor que (>) o menor que (<) una fecha o número puede eliminar cualquier fila con valores faltantes (`NA`). Esto se debe a que `NA` es tratado como infinitamente grande y pequeño. </span>

*(Consulta la página sobre el [trabajando con fechas](#working-with-dates) para obtener más información sobre el trabajo con fechas y el paquete **lubridate**)* 

#### Diseñar el filtro {.unnumbered}  

Examina una tabulación cruzada para asegurarte de que excluimos sólo las filas correctas:  


```{r}
table(Hospital  = linelist$hospital,                     # hospital name
      YearOnset = lubridate::year(linelist$date_onset),  # year of date_onset
      useNA     = "always")                              # show missing values
```

¿Qué otros criterios podemos filtrar para eliminar el primer brote (en 2012 y 2013) de los datos? Vemos que: 

* La primera epidemia en 2012 y 2013 ocurrió en el Hospital A, el Hospital B, y que también hubo 10 casos en el Hospital del Puerto. 
* Los hospitales A y B *no* tuvieron casos en la segunda epidemia, pero Port Hospital sí. 

Queremos excluir: 

* Las filas con inicio en 2012 y 2013 en cualquiera de los hospitales A, B o Port `nrow(linelist %>% filter(hospital %in% c("Hospital A", "Hospital B") | date_onset < as.Date("2013-06-01")))` :  
  * Excluir `nrow(linelist %>% filter(date_onset < as.Date("2013-06-01")))` filas con inicio en 2012 y 2013   * Excluir `nrow(linelist %>% filter(hospital %in% c('Hospital A', 'Hospital B') & is.na(date_onset)))` filas de los hospitales A y B con fechas de inicio ausentes  
  * **No** excluir `nrow(linelist %>% filter(!hospital %in% c('Hospital A', 'Hospital B') & is.na(date_onset)))` otras filas con fechas de inicio ausentes. 

Comenzamos con un listado de `nrow(linelist)`. Aquí está nuestro filtro: 

```{r}
linelist <- linelist %>% 
  # keep rows where onset is after 1 June 2013 OR where onset is missing and it was a hospital OTHER than Hospital A or B
  filter(date_onset > as.Date("2013-06-01") | (is.na(date_onset) & !hospital %in% c("Hospital A", "Hospital B")))

nrow(linelist)
```

Cuando volvemos a hacer la tabulación cruzada, vemos que los hospitales A y B se eliminan por completo, y los 10 casos del Port Hospital de 2012 y 2013 se eliminan, y todos los demás valores son los mismos, tal y como queríamos. 
 
```{r}
table(Hospital  = linelist$hospital,                     # hospital name
      YearOnset = lubridate::year(linelist$date_onset),  # year of date_onset
      useNA     = "always")                              # show missing values
```

Se pueden incluir varias sentencias dentro de un comando de filtrado (separadas por comas), o siempre se puede canalizar a un comando `filter()` separado para mayor claridad. 

*Nota: algunos lectores pueden notar que sería más fácil filtrar sólo por date_hospitalisation porque es 100% completo sin valores faltantes. Esto es cierto. Pero date_onset se utiliza para demostrar un filtro complejo.* 




### Independiente  {.unnumbered}  

El filtrado también puede realizarse como un comando independiente (no como parte de una cadena de tuberías). Como otros verbos de **dplyr**, en este caso el primer argumento debe ser el propio conjunto de datos.   

```{r, eval=F}
# dataframe <- filter(dataframe, condition(s) for rows to keep)

linelist <- filter(linelist, !is.na(case_id))
```

También puedes utilizar R **base** para hacer un subconjunto utilizando corchetes que reflejen las [filas, columnas] que deseas conservar.  

```{r, eval=F}
# dataframe <- dataframe[row conditions, column conditions] (blank means keep all)

linelist <- linelist[!is.na(case_id), ]
```





### Revisar rápidamente los registros  {.unnumbered} 

A menudo se quiere revisar rápidamente unos pocos registros, para sólo unas pocas columnas. La función View() de R **base** imprimirá un dataframe para su visualización en su RStudio. 

Mira el listado en RStudio: 

```{r, eval=F}
View(linelist)
```

Aquí hay dos ejemplos de visualización de celdas específicas (filas específicas y columnas específicas): 

**Con las funciones `filter()` y `select()` de dplyr :**

Dentro de `View()`, canaliza los datos a `filter()` para mantener ciertas filas, y luego a `select()` para mantener ciertas columnas. Por ejemplo, para revisar las fechas de inicio y hospitalización de 3 casos específicos: 

```{r, eval=F}
View(linelist %>%
       filter(case_id %in% c("11f8ea", "76b97a", "47a5f5")) %>%
       select(date_onset, date_hospitalisation))
```


Puedes lograr lo mismo con la sintaxis de R **base**, utilizando los corchetes `[ ]` para el subconjunto que deseas ver. 

```{r, eval=F}
View(linelist[linelist$case_id %in% c("11f8ea", "76b97a", "47a5f5"), c("date_onset", "date_hospitalisation")])
```





#### Añadir a la cadena de pipes  {.unnumbered}  


```{r}
# CLEANING 'PIPE' CHAIN (starts with raw data and pipes it through cleaning steps)
##################################################################################

# begin cleaning pipe chain
###########################
linelist <- linelist_raw %>%
    
    # standardize column name syntax
    janitor::clean_names() %>% 
    
    # manually re-name columns
           # NEW name             # OLD name
    rename(date_infection       = infection_date,
           date_hospitalisation = hosp_date,
           date_outcome         = date_of_outcome) %>% 
    
    # remove column
    select(-c(row_num, merged_header, x28)) %>% 
  
    # de-duplicate
    distinct() %>% 

    # add column
    mutate(bmi = wt_kg / (ht_cm/100)^2) %>%     

    # convert class of columns
    mutate(across(contains("date"), as.Date), 
           generation = as.numeric(generation),
           age        = as.numeric(age)) %>% 
    
    # add column: delay to hospitalisation
    mutate(days_onset_hosp = as.numeric(date_hospitalisation - date_onset)) %>% 
    
    # clean values of hospital column
    mutate(hospital = recode(hospital,
                      # OLD = NEW
                      "Mitylira Hopital"  = "Military Hospital",
                      "Mitylira Hospital" = "Military Hospital",
                      "Military Hopital"  = "Military Hospital",
                      "Port Hopital"      = "Port Hospital",
                      "Central Hopital"   = "Central Hospital",
                      "other"             = "Other",
                      "St. Marks Maternity Hopital (SMMH)" = "St. Mark's Maternity Hospital (SMMH)"
                      )) %>% 
    
    mutate(hospital = replace_na(hospital, "Missing")) %>% 

    # create age_years column (from age and age_unit)
    mutate(age_years = case_when(
          age_unit == "years" ~ age,
          age_unit == "months" ~ age/12,
          is.na(age_unit) ~ age)) %>% 
  
    mutate(
          # age categories: custom
          age_cat = epikit::age_categories(age_years, breakers = c(0, 5, 10, 15, 20, 30, 50, 70)),
        
          # age categories: 0 to 85 by 5s
          age_cat5 = epikit::age_categories(age_years, breakers = seq(0, 85, 5))) %>% 
    
    # ABOVE ARE UPSTREAM CLEANING STEPS ALREADY DISCUSSED
    ###################################################
    filter(
          # keep only rows where case_id is not missing
          !is.na(case_id),  
          
          # also filter to keep only the second outbreak
          date_onset > as.Date("2013-06-01") | (is.na(date_onset) & !hospital %in% c("Hospital A", "Hospital B")))
```







<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
## Cálculos por filas   {#row-wise-calculations}

Si deseas realizar un cálculo dentro de una fila, puedes utilizar `rowwise()` de **dplyr**. Consulta esta viñeta en línea sobre los cálculos [por filas](https://cran.r-project.org/web/packages/dplyr/vignettes/rowwise.html). Por ejemplo, este código aplica `rowwise()` y luego crea una nueva columna que suma el número de las columnas de síntomas especificadas que tienen valor "yes", para cada fila Las columnas se especifican dentro de `sum()` por su nombre dentro de un vector `c()`. `rowwise()` es esencialmente un tipo especial de `group_by()`, por lo que es mejor utilizar `ungroup()` cuando hayas terminado (página sobre [Agrupar datos](#grouping-data)). 

```{r,}
linelist %>%
  rowwise() %>%
  mutate(num_symptoms = sum(c(fever, chills, cough, aches, vomit) == "yes")) %>% 
  ungroup() %>% 
  select(fever, chills, cough, aches, vomit, num_symptoms) # for display
```

  
Al especificar la columna a evaluar, puedes utilizar las funciones de ayuda "tidyselect" descritas en la sección `select()` de esta página. Sólo tiene que hacer un ajuste (porque no las está utilizando dentro de una función **de dplyr** como `select()` o `summarise()`). 

Especifica los criterios de la columna dentro de la función `c_across()` de **dplyr**. Esto se debe a que c_across ([documentación](https://dplyr.tidyverse.org/reference/c_across.html)) está diseñada para trabajar con `rowwise()` específicamente. Por ejemplo, el siguiente código: 

* Utiliza `rowwise()` para que la siguiente operación (`sum()`) se aplique dentro de cada fila (no sumando columnas enteras) 
* Crea una nueva columna `num_NA_dates`, definida para cada fila como el número de columnas (con nombre que contiene "date") para las que `is.na()` se evaluó como TRUE (son valores faltantes). 
* `ungroup()` para eliminar los efectos de `rowwise()` en los pasos siguientes  

```{r,}
linelist %>%
  rowwise() %>%
  mutate(num_NA_dates = sum(is.na(c_across(contains("date"))))) %>% 
  ungroup() %>% 
  select(num_NA_dates, contains("date")) # for display
```

También podrías proporcionar otras funciones, como `max()` para obtener la fecha más reciente o más reciente de cada fila:  

```{r}
linelist %>%
  rowwise() %>%
  mutate(latest_date = max(c_across(contains("date")), na.rm=T)) %>% 
  ungroup() %>% 
  select(latest_date, contains("date"))  # for display
```


## Ordenar y clasificar {#arrange-and-sort}

Utiliza la función arrange() de **dplyr** para ordenar las filas por los valores de las columnas. 

Lista las columnas en el orden en que deben ser ordenadas. Especifica `.by_group = TRUE` si deseas que la ordenación se realice primero por cualquier *agrupación* aplicada a los datos (véase la página sobre [Agrupar datos](#grouping-data)). 

Por defecto, la columna se ordenará en orden "ascendente" (que se aplica a las columnas numéricas y también a las de caracteres). Puedes ordenar una variable en orden "descendente" envolviéndola con `desc()`. 

La ordenación de los datos con `arrange()` es particularmente útil cuando se hacen [Tablas para presentaciones](#tables-for-presentation), utilizando `slice()` para tomar las filas "superiores" por grupo, o estableciendo el orden de los niveles de los factores por orden de aparición. 

Por ejemplo, para ordenar las filas de nuestro `linelist` por `hospital` y luego por `date_onset` (fecha de inicio) en orden descendente, utilizaríamos:  

```{r, eval=F}
linelist %>% 
   arrange(hospital, desc(date_onset))
```


```{r, echo=F}
# HIDDEN
#
# convert one remaining old outbreak row to missing for ease
linelist <- linelist %>% 
  mutate(
    date_hospitalisation = case_when(
      date_hospitalisation < as.Date("2013-01-01") ~ as.Date(NA),
      TRUE                                         ~ date_hospitalisation),
    date_outcome = case_when(
      date_outcome < as.Date("2013-01-01") ~ as.Date(NA),
      TRUE                                 ~ date_outcome)
    )

#min(linelist$date_hospitalisation, na.rm=T)
#min(linelist$date_outcome, na.rm=T)
```



```{r echo=F}
# REARRANGE COLUMNS FOR EXPORT
linelist <- linelist %>% 
  select(case_id:gender, age, age_unit, age_years, age_cat, age_cat5, everything())
```

```{r echo=F}
# EXPORT CLEANED LINELIST FILE TO "DATA" FOLDER
#jfmont rio::export(linelist, here::here("data", "case_linelists", "linelist_cleaned.xlsx"))
rio::export(linelist, here::here("data", "case_linelists", "linelist_cleaned.rds"))
```
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cleaning.Rmd-->


# Trabajando con Fechas {#working-with-dates}


```{r, out.width=c('50%'), fig.align='center', echo=F, message=F}
knitr::include_graphics(here::here("images", "Dates_500x500.png"))
```

Trabajar con fechas en R requiere más atención que trabajar con otros tipos de objetos. A continuación, ofrecemos algunas herramientas y ejemplos para hacer este proceso menos doloroso. Por suerte, las fechas pueden manejarse fácilmente con la práctica y con un conjunto de paquetes útiles como  **lubridate**.  

Al importar los datos en bruto, R suele interpretar las fechas como objetos de carácter, lo que significa que no pueden utilizarse para operaciones generales con fechas, como la creación de series temporales y el cálculo de intervalos de tiempo. Para hacer las cosas más difíciles, hay muchas maneras de formatear una fecha y debes ayudar a R a saber qué parte de una fecha representa qué (mes, día, hora, etc.).

Las fechas en R son su propio tipo de objeto - el tipo Date. Hay que tener en cuenta que también hay un tipo que almacena objetos con fecha y hora. Los objetos fecha-hora se denominan formalmente tipos `POSIXt`, `POSIXct`, o  `POSIXlt` (la diferencia no es importante). Estos objetos se denominan informalmente tipos *datetime*.

* Es importante hacer que R reconozca cuando una columna contiene fechas.
* Las fechas son un tipo de objeto y pueden ser difíciles de trabajar.
* Aquí presentamos varias formas de convertir columnas de fecha al tipo Date.


<!-- ======================================================= -->
## Preparación {#preparation}

### Cargar paquetes {.unnumbered}  


Este trozo de código muestra la carga de paquetes necesaria para esta página. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con   `library()` de R **base**. Consulta la página sobre los [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r dates_packages, warning=F, message=F}
# Ccomprueba si el paquete está instalado, lo instala si es necesario y lo carga para la sesión actual.


pacman::p_load(
  lubridate,  # general package for handling and converting dates  
  linelist,   # has function to "guess" messy dates
  aweek,      # another option for converting dates to weeks, and weeks to dates
  zoo,        # additional date/time functions
  tidyverse,  # data management and visualization  
  rio)        # data import/export
```

### Importar datos {.unnumbered}  

Importamos los datos de casos de una epidemia de ébola simulada. Si deseas descargar los datos para seguirlos paso a paso, consulta las instrucciones en la página de [descarga de manuales y datos](#download-handbook-and-data). Asumimos que el archivo está en el directorio de trabajo, por lo que no se especifican subcarpetas en esta ruta de archivo.
```{r,  echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist <- import("linelist_cleaned.xlsx")

```



<!-- ======================================================= -->
## Fecha actual {#current-date}

Puedes obtener la fecha actual del "sistema" o la fecha-hora del sistema de tu ordenador haciendo lo siguiente con R **base**.

```{r}
# get the system date - this is a DATE class
Sys.Date()

# get the system time - this is a DATETIME class
Sys.time()
```


Con el paquete **lubridate** también se pueden devolver con `today()` y `now()`, respectivamente. `date()` devuelve la fecha y la hora actuales con los nombres del día de la semana y del mes.
  
  

<!-- ======================================================= -->
## Convertir en fecha {#convert-to-date}

Después de importar unos datos a R, los valores de las columnas de fecha pueden tener el aspecto de "1989/12/30", "05/06/2014" o "13 Ene 2020". En estos casos, es probable que R siga tratando estos valores como valores de carácter. Hay que *decirle* a R que estos valores son fechas... y cuál es el formato de la fecha (qué parte es Día, cuál es Mes, cuál es Año, etc).

Una vez dicho esto, R convierte estos valores al tipo Date. En segundo plano, R almacenará las fechas como números (el número de días desde su fecha "origen" 1 Ene 1970). No interactuarás con el número de la fecha a menudo, pero esto permite a R tratar las fechas como variables continuas y permitir operaciones especiales como el cálculo de la distancia entre las fechas.

Por defecto, los valores del tipo Date en R se muestran como AAAA-MM-DD. Más adelante en esta sección discutiremos cómo cambiar la visualización de los valores de fecha.

A continuación presentamos dos enfoques para convertir una columna de valores de carácter al tipo Date.

<span style="color: darkgreen;">**_CONSEJO:_**: Puedes comprobar el tipo actual de una columna con la función `class()`de R **base**, como `class(linelist$date_onset)`.</span>  

  

### R **base** {.unnumbered}  
`as.Date()` es la función estándar de R **base** para convertir un objeto o una columna en el tipo Date (nótese la "D" en mayúscula).

El uso de `as.Date()` requiere que:

*	Se especifique el formato **existente** de la fecha de carácter en bruto o la fecha de origen si se suministran las fechas como números (véase la sección sobre las fechas de Excel)
*	Si se utiliza en una columna de caracteres, todos los valores de fecha deben tener el mismo formato exacto (si no es el caso, pruebe con `parse_date()` del paquete **parsedate**)

En **primer** lugar, comprueba el tipo de la columna con `class()` de R **base** . Si no estás seguro o estás confundido sobre el tipo de datos (por ejemplo, ve "POSIXct", etc.) puede ser más fácil convertir primero la columna al tipo Character con `as.character()`, y luego convertirla al tipo Date.

En **segundo** lugar, dentro de la función `as.Date()`, utiliza el argumento `format =` para indicar a R el formato *actual* de los componentes de la fecha con caracteres - qué caracteres se refieren al mes, al día y al año, y cómo están separados. Si sus valores ya están en uno de los formatos de fecha estándar de R ("AAAA-MM-DD" o "AAAA/MM/DD") el argumento `format =` no es necesario.

Para usar `format =`, escribe una cadena de caracteres (entre comillas) que represente el formato *actual* de la fecha utilizando las abreviaturas especiales "strptime" que aparecen a continuación. Por ejemplo, si las fechas de caracteres están actualmente en el formato "DD/MM/AAAA", como "24/04/1968", entonces usarías `format = "%d/%m/%Y"` para convertir los valores en fechas. **Es necesario poner el formato entre comillas. ¡Y no olvides las barras o guiones!**.

```{r eval=F}
# Convert to class date
linelist <- linelist %>% 
  mutate(date_onset = as.Date(date_of_onset, format = "%d/%m/%Y"))
```

La mayoría de las abreviaturas de strptime se enumeran a continuación. Puedes ver la lista completa ejecutando `?strptime`.

%d = Número del día del mes (5, 17, 28, etc.)
%j = Número del día del año (día juliano 001-366)
%a = Día de la semana abreviado (lunes, martes, miércoles, etc.)
%A = Día de la semana completo (lunes, martes, etc.) %w = Número del día de la semana (0-6, el domingo es 0)
%u = Número del día de la semana (1-7, el lunes es 1)
%W = Número de la semana (00-53, el lunes es el comienzo de la semana)
%U = Número de la semana (01-53, el domingo es el comienzo de la semana)
%m = Número del mes (p. ej. 01, 02, 03, 04)
%b = Mes abreviado (enero, febrero, etc.)
%B = Mes completo (enero, febrero, etc.)
%y = Año de 2 dígitos (p. ej. 89)
%Y = Año de 4 dígitos (p. ej. 1989)
%h = Horas (reloj de 24 horas)
%m = Minutos
%s = Segundos 
%z = Desplazamiento respecto a GMT
%Z = Huso horario (carácter)

<span style="color: darkgreen;">**_CONSEJO:_** El argumento `format =` de `as.Date()` *no* le dice a R el formato que quiere que tengan las fechas, sino cómo identificar las partes de la fecha tal y como son *antes* de ejecutar el comando.</span>


<span style="color: darkgreen;">**_CONSEJO:_** Asegúrate que en el argumento `format =`  se utiliza el mismo *separador de partes de fechas* (por ejemplo, /, -, o espacio) que está en tus fechas.</span>

Una vez que los valores están en el tipo Fecha, R los mostrará por defecto en el formato estándar, que es AAAA-MM-DD.



### **lubridate** {.unnumbered}  

La conversión de objetos de carácter a fechas puede facilitarse utilizando el paquete **lubridate**. Se trata de un paquete **tidyverse** diseñado para hacer que el trabajo con fechas y horas sea más sencillo y consistente que en R **base**. Por estas razones, el paquete **lubridate** se considera a menudo el estándar de oro para las fechas y la hora, y se recomienda siempre que se trabaje con ellas.

El paquete **lubridate** proporciona varias funciones de ayuda diferentes diseñadas para convertir objetos de caracteres en fechas de una manera intuitiva y más indulgente que especificando el formato en `as.Date()`. Estas funciones son específicas para el formato de fecha aproximado, pero permiten una variedad de separadores, y sinónimos para las fechas (por ejemplo, 01 vs Jan vs Enero) - se denominan según las abreviaturas de los formatos de fecha.


```{r, }
# install/load lubridate 
pacman::p_load(lubridate)
```

La flexibilidad de la función `ymd()` convierte de forma flexible los valores de fecha suministrados como **año, luego mes y luego día**.

```{r}
# read date in year-month-day format
ymd("2020-10-11")
ymd("20201011")
```

La función `mdy()` convierte de forma flexible los valores de fecha suministrados como **mes, luego día y luego año**.  

```{r}
# read date in month-day-year format
mdy("10/11/2020")
mdy("Oct 11 20")
```

La función `dmy()` convierte de forma flexible los valores de fecha suministrados como **día, luego mes y luego año.**

```{r}
# read date in day-month-year format
dmy("11 10 2020")
dmy("11 October 2020")
```

<!-- The `as.character()` and `as.Date()` commands can optionally be combined as:   -->

<!-- ```{r eval=F} -->
<!-- linelist_cleaned$date_of_onset <- as.Date(as.character(linelist_cleaned$date_of_onset), format = "%d/%m/%Y") -->
<!-- ``` -->

Si se utilizan pipes, la conversión de una columna de caracteres a fechas con **lubridate** podría tener este aspecto:

```{r, eval=F}
linelist <- linelist %>%
  mutate(date_onset = lubridate::dmy(date_onset))
```

Una vez completado, puedes ejecutar `class()` para verificar el tipo de la columna

```{r, eval=F}
# Check the class of the column
class(linelist$date_onset)  
```


Una vez que los valores están en el tipo Fecha, R los mostrará por defecto en el formato estándar, que es AAAA-MM-DD.

Ten en cuenta que las funciones anteriores funcionan mejor con años de 4 dígitos. Los años de 2 dígitos pueden producir resultados inesperados, ya que **lubridate** intenta adivinar el siglo.

Para convertir un año de 2 dígitos en un año de 4 dígitos (todos en el mismo siglo) puedes convertirlo a tipo carácter y luego combinar los dígitos existentes con un prefijo usando `str_glue()` del paquete **stringr**. Ver [Caracteres y cadenas](#characters-and-strings). A continuación, convierte a fecha.

```{r}
two_digit_years <- c("15", "15", "16", "17")
str_glue("20{two_digit_years}")
```



### Combinar columnas {.unnumbered}  

Puedes utilizar las funciones de **lubridate** `make_date()` y `make_datetime()` para combinar varias columnas numéricas en una columna de fecha. Por ejemplo, si tiene columnas numéricas `onset_day`, `onset_month` y `onset_year` en el dataframe `linelist`:


```{r, eval=F}
linelist <- linelist %>% 
  mutate(onset_date = make_date(year = onset_year, month = onset_month, day = onset_day))
```




<!-- ======================================================= -->
## Fechas en Excel {#excel-dates}

En el fondo, la mayoría de los programas informáticos almacenan las fechas como números. R almacena las fechas desde un origen del 1 de enero de 1970. Así, si ejecutas `as.numeric(as.Date("1970-01-01"))` obtendrás `0`.

Microsoft Excel almacena las fechas con un origen dependiendo del sistema operativo, del 30 de diciembre de 1899 (Windows) o del 1 de enero de 1904 (Mac). Consulta esta [guía de Microsoft](https://docs.microsoft.com/en-us/office/troubleshoot/excel/1900-and-1904-date-system) para obtener más información.

Las fechas de Excel suelen importarse a R como estos valores numéricos en lugar de como caracteres. Si los datos que has importado de Excel muestran las fechas como números o caracteres como "41369"... utiliza `as.Date()` (o la función `as_date()` de **lubridate**) para convertirlas, pero **en lugar de suministrar un "formato" como el anterior, suministra la fecha de origen de Excel** al argumento `origin =` .

Esto no funcionará si la fecha de Excel se almacena en R como de tipo carácter, ¡así que asegúrate de que el número es de tipo numérico!.

<span style="color: black;">**_NOTA:_** Debes proporcionar la fecha de origen en el formato de fecha por defecto de R ("AAAA-MM-DD").</span>


```{r, eval = FALSE}
# An example of providing the Excel 'origin date' when converting Excel number dates
data_cleaned <- data %>% 
  mutate(date_onset = as.numeric(date_onset)) %>%   # ensure class is numeric
  mutate(date_onset = as.Date(date_onset, origin = "1899-12-30")) # convert to date using Excel origin
```



<!-- ======================================================= -->
## Fechas desordenadas {#messy-dates}

La función `parse_date()` del paquete **parsedate** intenta leer una columna de fecha "desordenada" que contiene fechas en muchos formatos diferentes y convertir las fechas a un formato estándar. [Puedes leer más en línea sobre `guess_dates()` ](https://www.repidemicsconsortium.org/linelist/reference/guess_dates.html).  

Por ejemplo `parse_date()` vería un vector de las siguientes fechas de caracteres "03 Ene 2018", "07/03/1982", y "08/20/85" y las convertiría al tipo Date como `2018-01-03`, `1982-03-07`, y `1985-08-20`.

```{r, }
parsedate::parse_date(c("03 Jany 2018",
                        "07/03/1982",
                        "08/20/85"))
```


```{r eval = FALSE}
# An example using guess_dates on the column dater_onset
linelist <- linelist %>%                 # the dataset is called linelist
  mutate(
    date_onset = parsedate::parse_date(date_onset))  # the parse_date() from package "parsedate"
```




<!-- ======================================================= -->
## Trabajar con el tipo fecha-hora {#working-with-date-time-class}

Como se mencionó anteriormente, R también soporta un tipo `datetime` - una columna que contiene información de fecha **y** hora. Al igual que con el tipo Date, a menudo es necesario convertirlas de objetos `character` a objetos `datetime`.

### Convertir fechas con horas {.unnumbered}  

Un objeto `datetime` estándar se formatea con la fecha en primer lugar, seguida de un componente de tiempo - por ejemplo, *01 Ene 2020, 16:30*. Al igual que con las fechas, hay muchas maneras de formatearlas, y hay numerosos niveles de precisión (horas, minutos, segundos) que se pueden suministrar.

Por suerte, también existen funciones de ayuda **de lubridate** para ayudar a convertir estas cadenas en objetos `datetime`. Estas funciones son extensiones de las funciones de ayuda a la fecha, con `_h` (sólo se suministran las horas), `_hm` (se suministran las horas y los minutos), o `_hms` (se suministran las horas, los minutos y los segundos) añadidas al final (por ejemplo, `dmy_hms()`). Se pueden utilizar como se indica:

Convertir `datetime` con sólo horas a objeto `datetime`

```{r}
ymd_h("2020-01-01 16hrs")
ymd_h("2020-01-01 4PM")
```

Convertir `datetime` con horas y minutos a objeto `datetime`

```{r}
dmy_hm("01 January 2020 16:20")
```

Convertir `datetime` con horas, minutos y segundos a objeto `datetime`

```{r}
mdy_hms("01 January 2020, 16:20:40")
```

Puedes indicar la zona horaria, pero se ignora. Consulta la sección más adelante en esta página sobre las zonas horarias.

```{r}
mdy_hms("01 January 2020, 16:20:40 PST")

```

Cuando se trabaja con un dataframe, las columnas de fecha y hora pueden combinarse para crear una columna de fecha y hora utilizando `str_glue()`del paquete **stringr** y una función apropiada de **lubridate**. Consulta la página sobre [Caracteres y cadenas](#characters-and-strings) para obtener detalles sobre **stringr**.

En este ejemplo, el dataframe `linelist` tiene una columna con formato "horas:minutos". Para convertirla en una fecha, hay que seguir algunos pasos:

1.  Crea una columna de tiempo de admisión "limpia" con los valores faltantes rellenados con la mediana de la columna. Hacemos esto porque **lubridate** no opera con valores faltantes. Combínala con la columna `date_hospitalisation` y utiliza la función `ymd_hm()` para convertirla.

```{r, eval = FALSE}
# packages
pacman::p_load(tidyverse, lubridate, stringr)

# time_admission is a column in hours:minutes
linelist <- linelist %>%
  
  # when time of admission is not given, assign the median admission time
  mutate(
    time_admission_clean = ifelse(
      is.na(time_admission),         # if time is missing
      median(time_admission),        # assign the median
      time_admission                 # if not missing keep as is
  ) %>%
  
    # use str_glue() to combine date and time columns to create one character column
    # and then use ymd_hm() to convert it to datetime
  mutate(
    date_time_of_admission = str_glue("{date_hospitalisation} {time_admission_clean}") %>% 
      ymd_hm()
  )

```

### Convertir sólo horas {.unnumbered}

Si tus datos contienen sólo un carácter de tiempo (horas y minutos), puedes convertirlos y manipularlos como tiempos utilizando `strptime()` desde R **base**. Por ejemplo, para obtener la diferencia entre dos de estos horas:

```{r}
# raw character times
time1 <- "13:45" 
time2 <- "15:20"

# Times converted to a datetime class
time1_clean <- strptime(time1, format = "%H:%M")
time2_clean <- strptime(time2, format = "%H:%M")

# Difference is of class "difftime" by default, here converted to numeric hours 
as.numeric(time2_clean - time1_clean)   # difference in hours

```

Sin embargo, ten en cuenta que si no se proporciona un valor de fecha, se asume que la fecha es hoy. Para combinar una cadena de fecha y una cadena de hora, observa cómo se usa **stringr** en la sección anterior. Puedes leer más sobre `strptime()` [aquí](https://rdrr.io/r/base/strptime.html).

Para convertir números de un solo dígito a dos dígitos (por ejemplo, para "rellenar" las horas o los minutos con ceros a la izquierda para conseguir 2 dígitos), consulta la [sección "Longitud de relleno" de la página Caracteres y cadenas](#str_pad).


### Extraer fracciones de hora {.unnumbered}  

Puedes extraer elementos de una hora con `hour()`, `minute()`, o `second()` de **lubridate**.

He aquí un ejemplo de extracción de la hora y posterior clasificación como parte del día. Comenzamos con la columna `time_admission`, que es de tipo Carácter en formato "HH:MM". En primer lugar, se utiliza `strptime()` como se ha descrito anteriormente para convertir los caracteres en `tipo datetime`. A continuación, se extrae la hora con `hour()`, devolviendo un número del 0 al 24. Por último, se crea una columna `time_period` utilizando la lógica con `case_when()` para clasificar las filas en Mañana/Tarde/Anochecer/Noche en función de su hora de entrada.

```{r}
linelist <- linelist %>%
  mutate(hour_admit = hour(strptime(time_admission, format = "%H:%M"))) %>%
  mutate(time_period = case_when(
    hour_admit > 06 & hour_admit < 12 ~ "Morning",
    hour_admit >= 12 & hour_admit < 17 ~ "Afternoon",
    hour_admit >= 17 & hour_admit < 21 ~ "Evening",
    hour_admit >=21 | hour_admit <= 6 ~ "Night"))
```

Para saber más sobre `case_when()`, consulta la página sobre [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions).

<!-- ======================================================= -->
## Trabajar con fechas {#working-with-dates-1}

`lubridate` también puede utilizarse para otras funciones, como la **extracción de aspectos de una fecha/hora**, **realización de cálculos aritméticos de fechas** o **cálculo de intervalos de fechas**

Aquí definimos una fecha que se utilizará para los ejemplos:

```{r, }
# create object of class Date
example_date <- ymd("2020-03-01")
```

### Extraer los componentes de la fecha {.unnumbered}  

Puedes extraer aspectos comunes como el mes, el día, el día de la semana:

```{r}
month(example_date)  # month number
day(example_date)    # day (number) of the month
wday(example_date)   # day number of the week (1-7)
```

También puede extraer componentes de tiempo de un objeto o columna  `datetime`. Esto puede ser útil si quieres ver la distribución de los tiempos de admisión.

```{r, eval=F}
example_datetime <- ymd_hm("2020-03-01 14:45")

hour(example_datetime)     # extract hour
minute(example_datetime)   # extract minute
second(example_datetime)   # extract second
```

Hay varias opciones para recuperar las semanas. Consulta la sección sobre semanas epidemiológicas más abajo.

Ten en cuenta que si deseas *mostrar* una fecha de una forma determinada (por ejemplo, "enero de 2020" o "jueves 20 de marzo" o "semana 20 de 1977") puedes hacerlo de forma más flexible, tal y como se describe en la sección sobre Visualización de fechas.


### Fecha matemática {.unnumbered}  

Puedes añadir ciertos números de días o semanas utilizando su respectiva función de **lubridate**.

```{r}
# add 3 days to this date
example_date + days(3)
  
# add 7 weeks and subtract two days from this date
example_date + weeks(7) - days(2)
```

### Intervalos de fechas {.unnumbered}  

La diferencia entre las fechas se puede calcular mediante:

1.  Asegúrate que ambas fechas son del mismo tipo
2.  Utiliza la resta para devolver la diferencia "difftime" entre las dos fechas
3.  Si es necesario, convierte el resultado en tipo numéricoa para realizar los cálculos matemáticos posteriores

A continuación se calcula y muestra el intervalo entre dos fechas. Se pueden encontrar intervalos utilizando el símbolo de resta "menos" en los valores que son de tipo Fecha. Ten en cuenta, sin embargo, que el tipo del valor devuelto es "difftime", como se muestra a continuación, y debe ser convertido a numérico.

```{r}
# find the interval between this date and Feb 20 2020 
output <- example_date - ymd("2020-02-20")
output    # print
class(output)
```

Para realizar operaciones posteriores sobre un "difftime", conviértelo en numérico con `as.numeric()`.

Todo esto puede unirse para trabajar con datos, por ejemplo:

```{r, eval = F}
pacman::p_load(lubridate, tidyverse)   # load packages

linelist <- linelist %>%
  
  # convert date of onset from character to date objects by specifying dmy format
  mutate(date_onset = dmy(date_onset),
         date_hospitalisation = dmy(date_hospitalisation)) %>%
  
  # filter out all cases without onset in march
  filter(month(date_onset) == 3) %>%
    
  # find the difference in days between onset and hospitalisation
  mutate(days_onset_to_hosp = date_hospitalisation - date_of_onset)
```



En un contexto de dataframe, si falta alguna de las fechas anteriores, la operación fallará para esa fila. El resultado será un NA en lugar de un valor numérico. Cuando utilices esta columna para los cálculos, asegúrate de establecer el argumento `na.rm` = en TRUE. Por ejemplo:

```{r, eval = FALSE}
# calculate the median number of days to hospitalisation for all cases where data are available
median(linelist_delay$days_onset_to_hosp, na.rm = T)
```


<!-- ======================================================= -->
## Visualización de fechas {#date-display}

Una vez que las fechas son del tipo correcto, a menudo se desea mostrarlas de forma diferente, por ejemplo para que se muestren como "lunes 05 de enero" en lugar de "2018-01-05". También puedes querer ajustar la visualización para agrupar las filas por los elementos de fecha mostrados, por ejemplo, para agrupar por mes-año.

### `format()` {.unnumbered}  

Ajusta la visualización de la fecha con la función `format()` de R **base**. Esta función acepta una cadena de caracteres (entre comillas) que especifica el formato de salida *deseado* en las abreviaturas strptime "%" (la misma sintaxis que se utiliza en `as.Date()`). A continuación se muestran las abreviaturas más comunes.

Nota: el uso de `format()` convertirá los valores al tipo Character, por lo que generalmente se utiliza hacia el final de un análisis o sólo para fines de visualización. Puedes ver la lista completa ejecutando `?strptime`.

%d = Número del día del mes (5, 17, 28, etc.)
%j = Número del día del año (día juliano 001-366)
%a = Día de la semana abreviado (lunes, martes, miércoles, etc.)
%A = Día de la semana completo (lunes, martes, etc.)
%w = Número del día de la semana (0-6, el domingo es 0)
%u = Número del día de la semana (1-7, el lunes es 1)
%W = Número de la semana (00-53, el lunes es el comienzo de la semana)
%U = Número de la semana (01-53, el domingo es el comienzo de la semana)
%m = Número del mes (p. ej. 01, 02, 03, 04)
%b = Mes abreviado (enero, febrero, etc.)
%B = Mes completo (enero, febrero, etc.)
%y = Año de 2 dígitos (p. ej. 89)
%Y = Año de 4 dígitos (p. ej. 1989)
%h = Horas (reloj de 24 horas)
%m = Minutos
%s = Segundos
%z = Desplazamiento respecto a GMT
%Z = Huso horario (carácter)

Un ejemplo de formato de la fecha de hoy:

```{r}
# today's date, with formatting
format(Sys.Date(), format = "%d %B %Y")

# easy way to get full date and time (default formatting)
date()

# formatted combined date, time, and time zone using str_glue() function
str_glue("{format(Sys.Date(), format = '%A, %B %d %Y, %z  %Z, ')}{format(Sys.time(), format = '%H:%M:%S')}")

# Using format to display weeks
format(Sys.Date(), "%Y Week %W")
```

Ten en cuenta que si utilizas `str_glue()`, dentro de las comillas dobles " sólo debes utilizar comillas simples (como arriba).


### Mes-Año {.unnumbered}  

Para convertir una columna de fecha al formato mes-año, te sugerimos que utilice la función `as.yearmon()` del paquete **zoo**. Esto convierte la fecha al tipo "yearmon" y mantiene el orden correcto. Por el contrario, usar `format(columna, "%Y %B")` convertirá al tipo Carácter y ordenará los valores alfabéticamente (incorrectamente).

A continuación, se crea una nueva columna `yearmonth` a partir de la columna `date_onset`, utilizando la función as.yearmon()`. La ordenación por defecto (correcta) de los valores resultantes se muestra en la tabla.

```{r}
# create new column 
test_zoo <- linelist %>% 
     mutate(yearmonth = zoo::as.yearmon(date_onset))

# print table
table(test_zoo$yearmon)
```

Por el contrario, se puede ver cómo sólo utilizando `format()` se consigue el formato de visualización deseado, pero no el orden correcto.

```{r}
# create new column
test_format <- linelist %>% 
     mutate(yearmonth = format(date_onset, "%b %Y"))

# print table
table(test_format$yearmon)
```

Nota: si estás trabajando con `ggplot()` y quieres ajustar sólo cómo se *muestran* las fechas, puede ser suficiente proporcionar un formato strptime al argumento `date_labels = ` en `scale_x_date()` - puedes utilizar `"%b %Y"` o `"%Y %b"`. Consulta la página de [consejos de ggplot](#ggplot-tips).

**zoo** también ofrece la función `as.yearqtr()`, y puedes usar `scale_x_yearmon()` cuando uses `ggplot()`.



<!-- ======================================================= -->
## Semanas epidemiológicas {#dates_epi_wks}

### **lubridate** {.unnumbered}  

Consulta la página sobre [Agrupar datos](#grouping-data) para ver ejemplos más extensos de agrupación de datos por fecha. A continuación describimos brevemente la agrupación de datos por semanas.

Generalmente recomendamos utilizar la función `floor_date()` de **lubridate**, con el argumento `unit = "week"`. Esto redondea la fecha hacia abajo al "inicio" de la semana, como se define por el argumento `week_start =`. El inicio de la semana por defecto es el 1 (para los lunes), pero se puede especificar cualquier día de la semana como inicio (por ejemplo, el 7 para los domingos). `floor_date()` es versátil y se puede utilizar para redondear hacia abajo a otras unidades de tiempo estableciendo `unit = ` "second", "minute", "hour", "day", "month", o "year".  

El valor devuelto es la fecha de inicio de la semana, en tipo Date. El tipo `Date` es útil a la hora de representar los datos, ya que serán fácilmente reconocidos y ordenados correctamente por `ggplot()`.

Si sólo tienes interés en ajustar las fechas para que *se muestren* por semanas en un gráfico, consulta la sección de esta página sobre Visualización de fechas. Por ejemplo, al representar una epicurva puedes formatear la visualización de la fecha proporcionando la nomenclatura strptime "%" deseada. Por ejemplo, utiliza "%Y-%W" o "%Y-%U" para devolver el año y el número de semana (dado el comienzo de la semana del lunes o del domingo, respectivamente).

### Recuentos semanales {.unnumbered}  

Consulta la página sobre [Agrupar datos](#grouping-data) para obtener una explicación detallada de la agrupación de datos con `count()`, `group_by()`, and `summarise()`. A continuación se muestra un breve ejemplo. 

1.  Crear una nueva columna "semana" con `mutate()`, utilizando `floor_date()` con `unit = "week"` 

2.  Obtener el recuento de filas (casos) por semana con `count()`; filtra los casos a los que les falte la fecha

3.  Termina con `complete()` de **tidyr** para asegurarte que *todas* las semanas aparecen en los datos - incluso las que no tienen filas/casos. Por defecto, los valores de recuento para cualquier fila "nueva" son NA, pero puedes hacerlos 0 con el argumento `fill =`, que espera una lista con nombre (abajo, `n` es el nombre de la columna de recuentos).

```{r}
# Make aggregated dataset of weekly case counts
weekly_counts <- linelist %>% 
  drop_na(date_onset) %>%             # remove cases missing onset date
  mutate(weekly_cases = floor_date(   # make new column, week of onset
    date_onset,
    unit = "week")) %>%            
  count(weekly_cases) %>%           # group data by week and count rows per group (creates column 'n')
  tidyr::complete(                  # ensure all weeks are present, even those with no cases reported
    weekly_cases = seq.Date(          # re-define the "weekly_cases" column as a complete sequence,
      from = min(weekly_cases),       # from the minimum date
      to = max(weekly_cases),         # to the maxiumum date
      by = "week"),                   # by weeks
    fill = list(n = 0))             # fill-in NAs in the n counts column with 0
```

Aquí están las primeras filas del dataframe resultante:

```{r message=FALSE, echo=F}
DT::datatable(head(weekly_counts, 20), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Alternativas a Epiweek {.unnumbered}  

Ten en cuenta que **lubridate** también tiene las funciones  `week()`, `epiweek()`, e `isoweek()`, cada una de las cuales tiene fechas de inicio ligeramente diferentes y otros matices. Sin embargo, en términos generales, `floor_date()` debería ser todo lo que necesitas. Puedes leer más detalles de estas funciones introduciendo ?week en la consola o leyendo la documentación [aquí](https://www.rdocumentation.org/packages/lubridate/versions/1.7.4/topics/week).

Puedes usar del paquete **aweek** para establecer semanas epidemiológicas. Puedes leer más sobre él [en el sitio web de RECON](https://www.repidemicsconsortium.org/aweek/). Tiene las funciones `date2week()` y `week2date()` en las que se puede establecer el día de inicio de la semana con `week_start = "Monday"`. Este paquete es el más fácil si se desea obtener resultados del tipo "week" (por ejemplo, "2020-W12"). Otra ventaja de **aweek** es que cuando `date2week()` se aplica a una columna de fecha, la columna devuelta (formato de semana) es automáticamente del tipo Factor e incluye niveles para todas las semanas en el lapso de tiempo (esto evita el paso extra de `complete()` descrito anteriormente). Sin embargo, **aweek** no tiene la funcionalidad de redondear fechas a otras unidades de tiempo como meses, años, etc.


Otra alternativa para las series temporales que también funciona bien para mostrar un formato de "semana" ("2020 W12") es `yearweek()` del paquete **tsibble**, como se demuestra en la página sobre [series temporales y detección de brotes](#time-series-and-outbreak-detection). 


<!-- ======================================================= -->
## Conversión de fechas/zonas horarias {#converting-datestime-zones}

Cuando los datos están presentes en diferentes husos horarios, a menudo puede ser importante normalizar estos datos en un huso horario unificado. Esto puede suponer un reto adicional, ya que el componente de zona horaria de los datos debe codificarse manualmente en la mayoría de los casos.

En R, cada objeto *datetime* tiene un componente de zona horaria. Por defecto, todos los objetos `datetime` llevarán la zona horaria local para el ordenador que se está utilizando - esto es generalmente específico para una *ubicación* en lugar de una zona horaria, ya que las zonas horarias a menudo cambian en los lugares debido al horario de verano. No es posible compensar con precisión las zonas horarias sin un componente de tiempo de una fecha, ya que el evento que representa una columna de fecha no puede ser atribuido a un tiempo específico, y por lo tanto los cambios de tiempo medidos en horas no pueden ser razonablemente contabilizados.

Para tratar las zonas horarias, hay una serie de funciones de ayuda en lubridate que pueden utilizarse para cambiar la zona horaria de un objeto `datetime` de la zona horaria local a una zona horaria diferente. Las zonas horarias se establecen atribuyendo una zona horaria válida de la base de datos tz al objeto `datetime`. [Aquí](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones) se puede encontrar una lista de éstas - si la ubicación que se está utilizando en los datos no está en esta lista, las grandes ciudades cercanas en la zona horaria están disponibles y sirven para el mismo propósito.



```{r}
# assign the current time to a column
time_now <- Sys.time()
time_now

# use with_tz() to assign a new timezone to the column, while CHANGING the clock time
time_london_real <- with_tz(time_now, "Europe/London")

# use force_tz() to assign a new timezone to the column, while KEEPING the clock time
time_london_local <- force_tz(time_now, "Europe/London")


# note that as long as the computer that was used to run this code is NOT set to London time,
# there will be a difference in the times 
# (the number of hours difference from the computers time zone to london)
time_london_real - time_london_local

```

Esto puede parecer muy abstracto, y a menudo no es necesario si el usuario no está trabajando a través de zonas horarias.





<!-- ======================================================= -->
## Cálculos de retardo y de avance {#lagging-and-leading-calculations}

`lead()` y `lag()` son funciones del paquete **dplyr** que ayudan a encontrar los valores anteriores (retardados) o posteriores (principales) en un vector, normalmente un vector numérico o de fechas. Esto es útil cuando se hacen cálculos de cambio/diferencia entre unidades de tiempo.`


```{r, echo=F}
counts <- import(here("data", "example", "district_weekly_count_data.xlsx")) %>% 
  filter(District == "Nibari") %>% 
  mutate(Date = as.Date(Date),
         week_start = lubridate::floor_date(Date, "week")) %>%
  group_by(week_start) %>% 
  summarize(cases_wk = sum(Cases, na.rm=T)) %>% 
  complete(week_start = seq.Date(min(week_start), max(week_start), by = "week"), fill = list(cases_wk = 0))
```

Supongamos que se quiere calcular la diferencia de casos entre una semana actual y la anterior. Los datos se proporcionan inicialmente en recuentos semanales, como se muestra a continuación.

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

**Al utilizar lag() o lead(), el orden de las filas en el dataframe es muy importante. - presta atención a si tus fechas/números son ascendentes o descendentes**

En primer lugar, crea una nueva columna que contenga el valor de la semana anterior (retardada).

* Controla el número de unidades hacia atrás/adelante con n = (debe ser un entero no negativo)

* Utiliza `default =` para definir el valor colocado en las filas no existentes (por ejemplo, la primera fila para la que no hay un valor retardado). Por defecto es `NA`.

* Utiliza `order_by = TRUE` si tus filas no están ordenadas por su columna de referencia


```{r}
counts <- counts %>% 
  mutate(cases_prev_wk = lag(cases_wk, n = 1))
```

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

A continuación, crea una nueva columna que sea la diferencia entre las dos columnas de los casos:

```{r}
counts <- counts %>% 
  mutate(cases_prev_wk = lag(cases_wk, n = 1),
         case_diff = cases_wk - cases_prev_wk)
```

```{r message=FALSE, echo=F}
DT::datatable(counts, rownames = FALSE,  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Puedes leer más sobre `lead()` y `lag()` en [esta documentación aquí](https://dplyr.tidyverse.org/reference/lead-lag.html) o introduciendo `?lag` en tu consola. 


<!-- ======================================================= -->
## Recursos  {#resources-2}

Página de lubridate** [tidyverse](https://lubridate.tidyverse.org/)

Página de **lubridate** RStudio [cheatsheet](https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf)

R for Data Science en español sobre [fechas y horas]https://es.r4ds.hadley.nz/fechas-y-horas.html

[Tutorial en línea](https://www.statmethods.net/input/dates.html) 

[Formatos de fecha](https://www.r-bloggers.com/2013/08/date-formats-in-r/)]



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/dates.Rmd-->

# Caracteres y cadenas  {#characters-and-strings} 

```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Characters_Strings_1500x500.png"))
```

Esta página muestra el uso del paquete **stringr** para evaluar y manejar valores de caracteres, también denominados "cadenas" ("strings").

1.  Combinar, ordenar, dividir, organizar - `str_c()`, `str_glue()`, `str_order()`, `str_split()`\

2.  Limpiar y normalizar

    -   Ajustar la longitud - `str_pad()`, `str_trunc()`, `str_wrap()`\
    -   Cambiar mayúsculas y minúsculas - `str_to_upper()`, `str_to_title()`, `str_to_lower()`, `str_to_sentence()`\

3.  Evaluar y extraer por posición - `str_length()`, `str_sub()`, `word()`\

4.  Patrones

    -   Detectar y localizar - `str_detect()`, `str_subset()`, `str_match()`, `str_extract()`\
    -   Modificar y reemplazar - `str_sub()`, `str_replace_all()`\

5.  Expresiones regulares ("regex")

Para facilitar la visualización, la mayoría de los ejemplos se muestran actuando sobre un vector de caracteres definido brevemente, aunque pueden adaptarse fácilmente a una columna dentro de un dataframe.

Esta [viñeta de stringr](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html) proporcionó gran parte de la inspiración para esta página.

<!-- ======================================================= -->

## Preparación {#preparation-1}

### Cargar paquetes {.unnumbered}

Instala o carga el paquete **stringr** y otros paquetes de **tidyverse**.

```{r}
# install/load packages
pacman::p_load(
  stringr,    # many functions for handling strings
  tidyverse,  # for optional data manipulation
  tools)      # alternative for converting to title case

```

### Importar datos {.unnumbered}
Importar datos 

En esta página haremos referencia de vez en cuando a la lista limpia de casos de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica para descargar linelist "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa los datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - consulta la página de [importación y exportación](#import-and-export) para más detalles). 

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas del listado. 

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<!-- ======================================================= -->

## Unir, dividir y ordenar {#unite-split-and-arrange}

Esta sección abarca: 

* Uso de `str_c()`, `str_glue()`, y `unite()` para combinar cadenas 

* Uso de `str_order()` para ordenar las cadenas 

* Uso de `str_split()` y `separate()` para dividir cadenas 

<!-- ======================================================= -->

### Combinar cadenas {.unnumbered}

Para combinar o concatenar varias cadenas en una sola, sugerimos utilizar `str_c` de **stringr**. Si tienes valores de caracteres distintos para combinar, simplemente proporciónalos como argumentos únicos, separados por comas. 

```{r}
str_c("String1", "String2", "String3")
```

El argumento `sep = ` inserta un valor de carácter entre cada uno de los argumentos proporcionados (por ejemplo, insertando una coma, un espacio o una nueva línea `"\n"`) 

```{r}
str_c("String1", "String2", "String3", sep = ", ")
```

El argumento `collapse = ` es relevante si estás introduciendo múltiples *vectores* como argumentos a `str_c()`. Se utiliza para separar los elementos de lo que sería un vector de salida, de forma que el vector de salida sólo tenga un elemento de carácter largo. 

El ejemplo siguiente muestra la combinación de dos vectores en uno (nombres y apellidos). Otro ejemplo similar podría ser el de las jurisdicciones y su número de casos. En este ejemplo: 

* El valor `sep = ` aparece entre cada nombre y apellido 

* El valor de `collapse = ` aparece entre cada persona 

```{r}
first_names <- c("abdul", "fahruk", "janice") 
last_names  <- c("hussein", "akinleye", "okeke")

# sep displays between the respective input strings, while collapse displays between the elements produced
str_c(first_names, last_names, sep = " ", collapse = ";  ")
```

Nota: Dependiendo del contexto de visualización deseado, al imprimir una cadena combinada de este tipo con nuevas líneas, puede ser necesario envolver toda la frase en `cat()` para que las nuevas líneas se impriman correctamente: 

```{r}
# For newlines to print correctly, the phrase may need to be wrapped in cat()
cat(str_c(first_names, last_names, sep = " ", collapse = ";\n"))
```

<!-- ======================================================= -->

### Cadenas dinámicas {.unnumbered}

Utiliza `str_glue()` para insertar código R dinámico en una cadena. Se trata de una función muy útil para crear títulos o pies de gráfico dinámicos, como se muestra a continuación. 

* Todo el contenido va entre comillas dobles `str_glue("")` 

* Cualquier código dinámico o referencias a valores predefinidos se colocan entre llaves `{}` dentro de las comillas dobles. Puede haber muchas llaves en el mismo comando `str_glue()`. 

* Para usar las comillas de caracteres '' dentro de la función, utiliza comillas *simples* dentro de las comillas dobles que las rodean (por ejemplo, al proporcionar el formato de la fecha - véase el ejemplo siguiente) 

* Consejo: Puedes utilizar `\n` para forzar una nueva línea 

* Consejo: Utiliza `format()` para ajustar la visualización de la fecha, y utiliza `Sys.Date()` para mostrar la fecha actual 

Un ejemplo sencillo, de un título de gráfico dinámico: 

```{r}
str_glue("Data include {nrow(linelist)} cases and are current to {format(Sys.Date(), '%d %b %Y')}.")
```

Un formato alternativo es utilizar marcadores de posición dentro de los paréntesis y definir el código en argumentos separados al final de la función `str_glue()`, como se indica a continuación. Esto puede mejorar la legibilidad del código si el texto es largo. 

```{r}
str_glue("Linelist as of {current_date}.\nLast case hospitalized on {last_hospital}.\n{n_missing_onset} cases are missing date of onset and not shown",
         current_date = format(Sys.Date(), '%d %b %Y'),
         last_hospital = format(as.Date(max(linelist$date_hospitalisation, na.rm=T)), '%d %b %Y'),
         n_missing_onset = nrow(linelist %>% filter(is.na(date_onset)))
         )

```

**Extraer de un dataframe** 

A veces, es útil extraer datos de un dataframe y pegarlos secuencialmente. A continuación se muestra un ejemplo de dataframe. Lo utilizaremos para hacer una declaración resumida sobre las jurisdicciones y los recuentos de casos nuevos y totales. 

```{r}
# make case data frame
case_table <- data.frame(
  zone        = c("Zone 1", "Zone 2", "Zone 3", "Zone 4", "Zone 5"),
  new_cases   = c(3, 0, 7, 0, 15),
  total_cases = c(40, 4, 25, 10, 103)
  )
```

```{r, echo=F}
DT::datatable(case_table, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Utiliza `str_glue_data()`, que está hecho especialmente para obtener datos de las filas del dataframe: 

```{r}
case_table %>% 
  str_glue_data("{zone}: {new_cases} ({total_cases} total cases)")
```

**Combinar cadenas a través de las filas** 

Si estás intentando "enrollar" valores en una columna del dataframe, por ejemplo, combinar valores de varias filas en una sola fila pegándolos con un separador, consulta la sección ["combinación de valores"](#str_rollup) de la página de **De-duplicación**. 

**Dataframe a una línea** 

Puedes hacer que la declaración aparezca en una línea utilizando `str_c()` (especificando el dataframe y los nombres de las columnas), y proporcionando los argumentos `sep =` y `collapse =`. 

```{r}
str_c(case_table$zone, case_table$new_cases, sep = " = ", collapse = ";  ")
```

Podría añadir el texto "Nuevos casos:" fijado al principio de la frase envolviéndolo con un `str_c()` separado (si "Nuevos casos:" estuviera dentro del `str_c()` original aparecería varias veces). 

```{r}
str_c("New Cases: ", str_c(case_table$zone, case_table$new_cases, sep = " = ", collapse = ";  "))
```

### Unir columnas  {#str_unite .unnumbered}

Dentro de un dataframe, la unión de valores de caracteres de varias columnas puede lograrse con `unite()` de **tidyr**. Esto es lo contrario de `separate()`. 

Indica el nombre de la nueva columna (que contendrá los valores unidos) y, a continuación, indica los nombres de las columnas que deseas unir. 

* Por defecto, el separador utilizado en la columna unida es el guión bajo `_`, pero puede cambiarse con el argumento `sep =`. 

* `remove =` elimina las columnas del dataframe que queremos unir (TRUE por defecto) 

* `na.rm =` elimina los valores perdidos (NA) al realizar la unión (FALSE por defecto) 

A continuación, definimos un mini-dataframe con el que hacer una demostración:

```{r, message = F, warning=F}
df <- data.frame(
  case_ID = c(1:6),
  symptoms  = c("jaundice, fever, chills",     # patient 1
                "chills, aches, pains",        # patient 2 
                "fever",                       # patient 3
                "vomiting, diarrhoea",         # patient 4
                "bleeding from gums, fever",   # patient 5
                "rapid pulse, headache"),      # patient 6
  outcome = c("Recover", "Death", "Death", "Recover", "Recover", "Recover"))
```

```{r}
df_split <- separate(df, symptoms, into = c("sym_1", "sym_2", "sym_3"), extra = "merge")
```

Este es el dataframe de ejemplo: 

```{r, echo=F}
DT::datatable(df_split, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

A continuación, unimos las tres columnas de síntomas: 

```{r}
df_split %>% 
  unite(
    col = "all_symptoms",         # name of the new united column
    c("sym_1", "sym_2", "sym_3"), # columns to unite
    sep = ", ",                   # separator to use in united column
    remove = TRUE,                # if TRUE, removes input cols from the data frame
    na.rm = TRUE                  # if TRUE, missing values are removed before uniting
  )
```

<!-- ======================================================= -->

### Dividir  {.unnumbered}

Para dividir una cadena basada en un patrón, utiliza `str_split()`. Esta función evalúa las cadenas y devuelve una `lista` de vectores de caracteres formada por los valores recién divididos. 

El ejemplo que sigue evalúa una cadena y la divide en tres. Por defecto, devuelve un objeto de tipo `list` con un elemento (un vector de caracteres) por cada cadena proporcionada inicialmente. Si `simplify = TRUE` devuelve una matriz de caracteres. 

En este ejemplo, se proporciona una cadena y la función devuelve una lista con un elemento: un vector de caracteres con tres valores. 

```{r}
str_split(string = "jaundice, fever, chills",
          pattern = ",")
```

Si se guarda la salida, puedes acceder al enésimo valor dividido con la sintaxis de corchetes. Para acceder a un valor específico puedes utilizar una sintaxis como esta: `the_returned_object[[1]][2]`, que accedería al segundo valor de la primera cadena evaluada ("fever"). Consulta la página de [fundamentos de R](#r-basics) para obtener más detalles sobre el acceso a los elementos. 

```{r}
pt1_symptoms <- str_split("jaundice, fever, chills", ",")

pt1_symptoms[[1]][2]  # extracts 2nd value from 1st (and only) element of the list
```

Si se proporcionan varias cadenas mediante `str_split()`, habrá más de un elemento en la lista devuelta.

```{r}
symptoms <- c("jaundice, fever, chills",     # patient 1
              "chills, aches, pains",        # patient 2 
              "fever",                       # patient 3
              "vomiting, diarrhoea",         # patient 4
              "bleeding from gums, fever",   # patient 5
              "rapid pulse, headache")       # patient 6

str_split(symptoms, ",")                     # split each patient's symptoms
```

Para devolver una "matriz de caracteres" en su lugar, que puede ser útil si se crean columnas de dataframes, establece el argumento `simplify = TRUE` como se muestra a continuación: 

```{r}
str_split(symptoms, ",", simplify = TRUE)
```

También puedes ajustar el número de divisiones a crear con el argumento `n =`. Por ejemplo, esto restringe el número de divisiones a 2. De este modo, cualquier otra coma permanece dentro del segundo valor. 

```{r}
str_split(symptoms, ",", simplify = TRUE, n = 2)
```

*Nota - los mismos resultados se pueden conseguir con `str_split_fixed()`, en la que no se da el argumento `simplify`, sino que se debe designar el número de columnas (`n`).* 

```{r, eval=F}
str_split_fixed(symptoms, ",", n = 2)
```

### Dividir columnas   {.unnumbered}

Si estás intentando dividir una columna de un dataframe, es mejor utilizar la función `separate()` de **dplyr**. Se utiliza para dividir una columna de caracteres en otras columnas. 

Digamos que tenemos un dataframe simple `df` (definido y unido en la [sección de unir columnas](#str_unite)) que contiene una columna `case_ID`, una columna de caracteres con muchos síntomas y una columna de resultados. Nuestro objetivo es separar la columna de `symptoms` en varias columnas, cada una de las cuales contiene un síntoma.

```{r, echo=F}
DT::datatable(df, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Asumiendo que los datos son enlazados con pipe en `separate()`, primero proporciona la columna a separar. A continuación, proporcione en = como un vector `c()` que contiene los nombres de las *nuevas* columnas, como se muestra a continuación. 

* `sep =` el separador, puede ser un carácter, o un número (interpretado como la posición del carácter a dividir) 

* `remove =` FALSE por defecto, elimina la columna de entrada 

* `convert =` FALSE por defecto, hará que las cadenas "NA "s se conviertan en `NA` 

* `extra = ` controla lo que sucede si hay más valores creados por la separación que nuevas columnas nombradas. 

* `extra = "warn"` significa que verá una advertencia, pero quitará los valores en exceso (**el valor por defecto**) 

* `extra = "drop"` significa que los valores sobrantes se eliminarán sin previo aviso 

* **`extra = "merge"` sólo dividirá hasta el número de nuevas columnas listadas en `into` - *esta configuración preservará todos tus datos*** 

A continuación se muestra un ejemplo con `extra = "merge"` - no se pierde ningún dato. Se definen dos nuevas columnas pero cualquier tercer síntoma se deja en la segunda columna nueva: 

```{r}
# third symptoms combined into second new column
df %>% 
  separate(symptoms, into = c("sym_1", "sym_2"), sep=",", extra = "merge")
```

Cuando se utiliza el `extra = "drop"` por defecto a continuación, se da una advertencia pero se pierden los terceros síntomas: 

```{r}
# third symptoms are lost
df %>% 
  separate(symptoms, into = c("sym_1", "sym_2"), sep=",")
```

<span style="color: orange;">***PRECAUCIÓN:*** Si no proporcionas suficientes valores `into` en las nuevas columnas, tus datos pueden quedar truncados. </span>

<!-- ======================================================= -->

### Ordenar alfabéticamente  {.unnumbered}

Se pueden ordenar varias cadenas por orden alfabético. `str_order()` devuelve el orden, mientras que `str_sort()` devuelve las cadenas en ese orden. 

```{r}
# strings
health_zones <- c("Alba", "Takota", "Delta")

# return the alphabetical order
str_order(health_zones)

# return the strings in alphabetical order
str_sort(health_zones)
```

Para utilizar un alfabeto diferente, añade el argumento `locale =`. Mira lista completa de locales introduciendo `stringi::stri_locale_list()` en la consola de R. 

<!-- ======================================================= -->

### funciones R base {.unnumbered}

Es común ver las funciones de R **base** `paste()` y `paste0()`, que concatenan vectores después de convertir todas las partes en caracteres. Actúan de forma similar a `str_c()` pero la sintaxis es posiblemente más complicada - en los paréntesis cada parte está separada por una coma. Las partes son o bien texto de carácter (entre comillas) o bien objetos de código predefinidos (sin comillas). Por ejemplo: 

```{r}
n_beds <- 10
n_masks <- 20

paste0("Regional hospital needs ", n_beds, " beds and ", n_masks, " masks.")
```

Se pueden especificar los argumentos `sep =` y `collapse =`. `paste()` es simplemente `paste0()` con un `sep =` " " por defecto (un espacio). 

## Limpiar y normalizar  {#clean-and-standardise}

<!-- ======================================================= -->

### Cambiar mayúsculas  {.unnumbered}

A menudo hay que alterar las mayúsculas y minúsculas de un valor de cadena, por ejemplo los nombres de las jurisdicciones. Utiliza `str_to_upper()`, `str_to_lower()`, y `str_to_title()`, de **stringr**, como se muestra a continuación: 

```{r}
str_to_upper("California")

str_to_lower("California")
```

Usando R **base**, lo anterior también se puede lograr con `toupper()`, `tolower()`. 

**Capitalización del título**

Se puede transformar la cadena para que cada palabra esté en mayúsculas con `str_to_title()`: 

```{r}
str_to_title("go to the US state of california ")
```

Utiliza `toTitleCase()` del paquete de **tools** para lograr una capitalización más matizada (palabras como "a", "el" y "de" no se escriben en mayúsculas). 

```{r}
tools::toTitleCase("This is the US state of california")
```

También puedes utilizar `str_to_sentence()`, que sólo pone en mayúsculas la primera letra de la cadena.

```{r}
str_to_sentence("the patient must be transported")
```

### Longitud de la cadena {#str_pad .unnumbered}

Utiliza `str_pad()` para añadir caracteres a una cadena, con una longitud mínima. Por defecto se añaden espacios, pero también puedes rellenar con otros caracteres utilizando el argumento `pad =`. 

```{r}
# ICD codes of differing length
ICD_codes <- c("R10.13",
               "R10.819",
               "R17")

# ICD codes padded to 7 characters on the right side
str_pad(ICD_codes, 7, "right")

# Pad with periods instead of spaces
str_pad(ICD_codes, 7, "right", pad = ".")
```

Por ejemplo, para rellenar números con ceros a la izquierda (como en el caso de las horas o los minutos), puedes rellenar el número hasta una longitud mínima de 2 con pad = "0".

```{r}
# Add leading zeros to two digits (e.g. for times minutes/hours)
str_pad("4", 2, pad = "0") 

# example using a numeric column named "hours"
# hours <- str_pad(hours, 2, pad = "0")
```

### Truncar {.unnumbered}

`str_trunc()` establece una longitud máxima para cada cadena. Si una cadena supera esta longitud, se trunca (acorta) y se incluye una elipsis (...) para indicar que la cadena era antes más larga. Ten en cuenta que la elipsis *se* cuenta en la longitud. Los caracteres de la elipsis pueden cambiarse con el argumento `ellipsis =`. El argumento opcional `side = `especifica dónde aparecerá la elipsis dentro de la cadena truncada ("left", "right", o "center"). 

```{r}
original <- "Symptom onset on 4/3/2020 with vomiting"
str_trunc(original, 10, "center")
```

### Normalizar la longitud  {.unnumbered}

Utiliza `str_trunc()` para establecer una longitud máxima y, a continuación, utiliza `str_pad()` para ampliar las cadenas muy cortas hasta esa longitud truncada. En el ejemplo siguiente, se establece 6 como longitud máxima (se trunca un valor), y luego se rellena un valor muy corto para alcanzar la longitud de 6. 

```{r}
# ICD codes of differing length
ICD_codes   <- c("R10.13",
                 "R10.819",
                 "R17")

# truncate to maximum length of 6
ICD_codes_2 <- str_trunc(ICD_codes, 6)
ICD_codes_2

# expand to minimum length of 6
ICD_codes_3 <- str_pad(ICD_codes_2, 6, "right")
ICD_codes_3
```

### Eliminar los espacios en blanco iniciales y finales  {.unnumbered}

Utiliza `str_trim()` para eliminar los espacios, las nuevas líneas (`\n`) o los tabuladores (`\t`) de los lados de una cadena de entrada. Añade `"right"` `"left"`, o `"both"` al comando para especificar qué lado recortar (por ejemplo, `str_trim(x, "right")`. 
```{r}
# ID numbers with excess spaces on right
IDs <- c("provA_1852  ", # two excess spaces
         "provA_2345",   # zero excess spaces
         "provA_9460 ")  # one excess space

# IDs trimmed to remove excess spaces on right side only
str_trim(IDs)
```

### Eliminar los espacios en blanco repetidos en una cadena  {.unnumbered}

Utiliza `str_squish()` para eliminar los espacios repetidos que aparecen *dentro* de una cadena. Por ejemplo, para convertir espacios dobles en espacios simples. También elimina espacios, nuevas líneas o tabulaciones en el exterior de la cadena como `str_trim()`. 

```{r}
# original contains excess spaces within string
str_squish("  Pt requires   IV saline\n") 
```

Escribe `?str_trim`, `?str_pad` en tu consola de R para ver más detalles. 

### convertir en párrafos {.unnumbered}

Utiliza `str_wrap()` para convertir un texto largo no estructurado en un párrafo estructurado con una longitud de línea fija. Proporciona la longitud de caracteres ideal para cada línea, y aplica un algoritmo para insertar nuevas líneas (\n) dentro del párrafo, como se ve en el ejemplo siguiente. 

```{r}
pt_course <- "Inicio de los síntomas 1/4/2020 vómitos escalofríos fiebre. La paciente consultó a un curandero tradicional en su pueblo natal el 2/4/2020. El 5/4/2020 los síntomas empeoraron y fue ingresada en la clínica Lumta. Se tomó una muestra y fue trasladada al hospital regional el 6/4/2020. La paciente murió en el hospital regional el 7/4/2020"

str_wrap(pt_course, 40)
```

La función **base** `cat()` puede envolver el comando anterior para imprimir la salida, mostrando las nuevas líneas añadidas. 

```{r}
cat(str_wrap(pt_course, 40))
```

<!-- ======================================================= -->

## Manipular por posición {#handle-by-position}

### Extraer por posición de carácter {.unnumbered}

Utiliza `str_sub()` para devolver sólo una parte de una cadena. La función toma tres argumentos principales: 

1.  el(los) vector(es) de caracteres 

2.  posición de inicio 

3.  posición final 

Algunas notas sobre los números de posición: 

* Si un número de posición es positivo, la posición se cuenta a partir del extremo izquierdo de la cadena. 

* Si un número de posición es negativo, se cuenta a partir del extremo derecho de la cadena. 

* Los números de posición son inclusivos. 

* Las posiciones que se extienden más allá de la cadena serán truncadas (eliminadas). 

A continuación se muestran algunos ejemplos aplicados a la cadena "pneumonia": 

```{r}
# start and end third from left (3rd letter from left)
str_sub("pneumonia", 3, 3)

# 0 is not present
str_sub("pneumonia", 0, 0)

# 6th from left, to the 1st from right
str_sub("pneumonia", 6, -1)

# 5th from right, to the 2nd from right
str_sub("pneumonia", -5, -2)

# 4th from left to a position outside the string
str_sub("pneumonia", 4, 15)
```

### Extraer por posición de palabra {.unnumbered}

Para extraer la enésima 'palabra', utiliza `word()`, también de **stringr**. Proporciona la(s) cadena(s), luego la primera y la última posición de la palabra a extraer. 

Por defecto, se asume que el separador entre 'palabras' es un espacio, a menos que se indica lo contrario con `sep =` (por ejemplo, `sep = "_"` cuando las palabras están separadas por barra baja. 

```{r}
# strings to evaluate
chief_complaints <- c("I just got out of the hospital 2 days ago, but still can barely breathe.",
                      "My stomach hurts",
                      "Severe ear pain")

# extract 1st to 3rd words of each string
word(chief_complaints, start = 1, end = 3, sep = " ")
```

### Sustituir por posición de carácter  {.unnumbered}

`str_sub()` emparejado con el operador de asignación (`<-`) puede utilizarse para modificar una parte de una cadena:

```{r}
word <- "pneumonia"

# convert the third and fourth characters to X 
str_sub(word, 3, 4) <- "XX"

# print
word
```

Un ejemplo aplicado a varias cadenas (por ejemplo, una columna). Obsérvese la ampliación de la longitud de "HIV". 

```{r}
words <- c("pneumonia", "tubercolosis", "HIV")

# convert the third and fourth characters to X 
str_sub(words, 3, 4) <- "XX"

words
```

### Evaluar la longitud  {.unnumbered}

```{r}
str_length("abc")
```

Como alternativa, utiliza `nchar()` de R **base** 

<!-- ======================================================= -->

## Patrones  {#patterns}

Muchas funciones de **stringr** trabajan para detectar, localizar, extraer, hacer coincidir, reemplazar y dividir basándose en un *patrón* especificado. 

<!-- ======================================================= -->

### Detectar un patrón {.unnumbered}

Utiliza `str_detect()` como se indica a continuación para detectar la presencia/ausencia de un patrón dentro de una cadena. Primero proporciona la cadena o vector en la que a buscar (`string =`), y luego el patrón a buscar (`pattern =`). Ten en cuenta que, por defecto, la búsqueda *distingue entre mayúsculas y minúsculas*. 

```{r}
str_detect(string = "primary school teacher", pattern = "teach")
```

Se puede incluir el argumento `negate =` y ponerlo a `TRUE` si se quiere saber si el patrón NO está presente. 

```{r}
str_detect(string = "primary school teacher", pattern = "teach", negate = TRUE)
```

Para ignorar las mayúsculas y minúsculas, envuelve el patrón dentro de `regex()`, y *dentro* de `regex()` añade el argumento `ignore_case = TRUE` (o `T` como abreviatura). 

```{r}
str_detect(string = "Teacher", pattern = regex("teach", ignore_case = T))
```

Cuando `str_detect()` se aplica a un vector de caracteres o a una columna de un dataframe, devolverá TRUE o FALSE para cada uno de los valores.

```{r}
# a vector/column of occupations 
occupations <- c("field laborer",
                 "university professor",
                 "primary school teacher & tutor",
                 "tutor",
                 "nurse at regional hospital",
                 "lineworker at Amberdeen Fish Factory",
                 "physican",
                 "cardiologist",
                 "office worker",
                 "food service")

# Detect presence of pattern "teach" in each string - output is vector of TRUE/FALSE
str_detect(occupations, "teach")
```

Si necesitas contar los `TRUE`, simplemente `sum()` la salida. Esto cuenta el número de `TRUE`. 

```{r}
sum(str_detect(occupations, "teach"))
```

Para buscar con varios términos, inclúyelos separados por barras (`|`) dentro del argumento `pattern =`, como se muestra a continuación: 

```{r}
sum(str_detect(string = occupations, pattern = "teach|professor|tutor"))
```

Si necesitas construir una larga lista de términos de búsqueda, puedes combinarlos usando `str_c()` y `sep = |`, luego definir que esto es un objeto de caracteres, y luego referenciar el vector más adelante de manera más sucinta. El ejemplo siguiente incluye posibles términos de búsqueda de ocupación para proveedores médicos de primera línea. 

```{r}
# search terms
occupation_med_frontline <- str_c("medical", "medicine", "hcw", "healthcare", "home care", "home health",
                                "surgeon", "doctor", "doc", "physician", "surgery", "peds", "pediatrician",
                               "intensivist", "cardiologist", "coroner", "nurse", "nursing", "rn", "lpn",
                               "cna", "pa", "physician assistant", "mental health",
                               "emergency department technician", "resp therapist", "respiratory",
                                "phlebotomist", "pharmacy", "pharmacist", "hospital", "snf", "rehabilitation",
                               "rehab", "activity", "elderly", "subacute", "sub acute",
                                "clinic", "post acute", "therapist", "extended care",
                                "dental", "dential", "dentist", sep = "|")

occupation_med_frontline
```

Este comando devuelve el número de ocupaciones que contienen alguno de los términos de búsqueda para proveedores médicos de primera línea (`occupation_med_frontline`): 

```{r}
sum(str_detect(string = occupations, pattern = occupation_med_frontline))
```

**Funciones de búsqueda de cadenas en R base** 

`grepl()` de R **base**  funciona de forma similar a `str_detect()`, en el sentido de que busca coincidencias con un patrón y devuelve un vector lógico. La sintaxis básica es `grepl(patrón,  cadenas_de_búsqueda, ignore.case = FALSE, ...)`. Una ventaja es que el argumento ignore.case es más fácil de escribir (no hay necesidad de involucrar la función `regex()`). 

Asimismo, las funciones `sub()` y `gsub()`de R **base** actúan de forma similar a `str_replace()`. Su sintaxis básica es: `gsub(patrón, reemplazo, cadenas_de_búsqueda, ignore.case = FALSE)`. `sub()` reemplazará la primera instancia del patrón, mientras que `gsub()` reemplazará todas las instancias del patrón. 

#### Convertir comas en puntos  {.unnumbered}

He aquí un ejemplo de uso de `gsub()` para convertir comas en puntos en un vector de números. Esto podría ser útil si tus datos proceden de otras partes del mundo que no sean Estados Unidos o Gran Bretaña. 

`gsub()` internamente actúa primero sobre  `lengths` convirtiendo cualquier punto en sin espacio "". El carácter de punto"." tiene que ser "escapado" con dos barras inclinadas para significar realmente un punto, porque "." en regex significa "cualquier carácter". A continuación, el resultado (con sólo comas) se pasa a la función externa `gsub()` en la que las comas se sustituyen por puntos. 

```{r, eval=F}
lengths <- c("2.454,56", "1,2", "6.096,5")

as.numeric(gsub(pattern = ",",                # find commas     
                replacement = ".",            # replace with periods
                x = gsub("\\.", "", lengths)  # vector with other periods removed (periods escaped)
                )
           )                                  # convert outcome to numeric
```

### Sustituir todo  {.unnumbered}

Utiliza `str_replace_all()` como herramienta de "búsqueda y sustitución". Primero, proporcione las cadenas a evaluar a `string =`, luego el patrón a reemplazar a `pattern =`, y luego el valor de reemplazo a `replacement =`. El ejemplo siguiente reemplaza todas las instancias de "dead" con "deceased". Ten en cuenta que esto distingue entre mayúsculas y minúsculas. 

```{r}
outcome <- c("Karl: dead",
            "Samantha: dead",
            "Marco: not dead")

str_replace_all(string = outcome, pattern = "dead", replacement = "deceased")
```

Notas: 

* Para sustituir un patrón por `NA`, utiliza `str_replace_na()`. 

* La función `str_replace()` reemplaza sólo la primera instancia del patrón dentro de cada cadena evaluada. 

<!-- ======================================================= -->

### Detectar con lógica {.unnumbered}

**Dentro de case_when()** 

`str_detect()` se utiliza a menudo dentro de `case_when()` (de **dplyr**). Digamos que ocupaciones es una columna en linelist. La función `mutate()` de abajo crea una nueva columna llamada `is_educator` utilizando la lógica condicional a través de `case_when()`. Mira la página sobre limpieza de datos para aprender más sobre `case_when()`. 

```{r, eval=F}
df <- df %>% 
  mutate(is_educator = case_when(
    # term search within occupation, not case sensitive
    str_detect(occupations,
               regex("teach|prof|tutor|university",
                     ignore_case = TRUE))              ~ "Educator",
    # all others
    TRUE                                               ~ "Not an educator"))
```

Como recordatorio, puede ser importante añadir criterios de exclusión a la lógica condicional (`negate = F`): 

```{r, eval=F}
df <- df %>% 
  # value in new column is_educator is based on conditional logic
  mutate(is_educator = case_when(
    
    # occupation column must meet 2 criteria to be assigned "Educator":
    # it must have a search term AND NOT any exclusion term
    
    # Must have a search term
    str_detect(occupations,
               regex("teach|prof|tutor|university", ignore_case = T)) &              
    
    # AND must NOT have an exclusion term
    str_detect(occupations,
               regex("admin", ignore_case = T),
               negate = TRUE                        ~ "Educator"
    
    # All rows not meeting above criteria
    TRUE                                            ~ "Not an educator"))
```

<!-- ======================================================= -->

### Localizar la posición de un patrón {.unnumbered}

Para localizar la *primera* posición de un patrón, utiliza `str_locate()`. Esta función da como resultado una posición inicial y una final. 

```{r}
str_locate("I wish", "sh")
```

Al igual que otras funciones `str`, existe una versión "\_all" (`str_locate_all()`) que devolverá las posiciones de *todas* las instancias del patrón dentro de cada cadena. La salida es una `lista`. 

```{r}
phrases <- c("I wish", "I hope", "he hopes", "He hopes")

str_locate(phrases, "h" )     # position of *first* instance of the pattern
str_locate_all(phrases, "h" ) # position of *every* instance of the pattern
```

<!-- ======================================================= -->

### Extraer una coincidencia  {.unnumbered}

`str_extract_all()` devuelve los patrones coincidentes en sí mismos, lo que resulta muy útil cuando se han ofrecido varios patrones mediante condiciones "OR". Por ejemplo, buscando en el vector de cadenas de ocupaciones (véase la pestaña anterior) *cualquiera* "enseñ", "profesor" o "tutor". 

`str_extract_all()` devuelve una `lista` que contiene *todas las coincidencias* de cada cadena evaluada. Mira a continuación cómo la ocupación 3 tiene dos coincidencias de patrón dentro de ella. 

```{r}
str_extract_all(occupations, "teach|prof|tutor")
```

`str_extract()` extrae *sólo la primera coincidencia* en cada cadena evaluada, produciendo un vector de caracteres con un elemento por cada cadena evaluada. Devuelve NA cuando no hay coincidencias. Los `NA`s pueden ser eliminados envolviendo el vector devuelto con `na.exclude()`. Observa cómo la segunda de las coincidencias de la ocupación 3 no se muestra.

```{r}
str_extract(occupations, "teach|prof|tutor")
```

<!-- ======================================================= -->

### Subconjunto y recuento  {.unnumbered}

Las funciones alineadas incluyen `str_subset()` y `str_count()`. 

`str_subset()` devuelve los valores reales que contienen el patrón: 

```{r}
str_subset(occupations, "teach|prof|tutor")
```

`str_count()` devuelve un vector de números: el **número de veces** que aparece un término de búsqueda en cada valor evaluado. 

```{r}
str_count(occupations, regex("teach|prof|tutor", ignore_case = TRUE))
```

<!-- ======================================================= -->

### Grupos Regex  {.unnumbered}

EN CONSTRUCCIÓN 

<!-- ======================================================= -->

## Caracteres especiales {#special-characters}

**Barra invertida `\` como código de escape** 

La barra invertida `\` se utiliza para "escapar" del significado del siguiente carácter. De este modo, se puede utilizar una barra invertida para que una comilla aparezca *dentro* de otras comillas (`\"`) - la comilla del medio no "romperá" las comillas circundantes. 

Nota - por lo tanto, si quieres *mostrar una barra* invertida, debes escapar su significado con *otra* barra invertida. Así que debes escribir dos barras invertidas `\\` para mostrar una. 

**Caracteres especiales** 

| Carácter especial                                                                 | Representa                                   |
|-----------------------------------------------------------------------------------|----------------------------------------------|
| `"\\"`                                                                            | barra invertida                              |
| `"\n"`                                                                            | una nueva línea                              |
| `"\""`                                                                            | comillas dobles *dentro de* comillas dobles  |
| `'\''`                                                                            | comillas simples *dentro de* comillas simples|
| `"\`"`| acento grave`"\r"`| retorno carro `"\t"`| tab`"\v"`| tab vertical `"\b"\` | retroceso                                    |

Ejecuta `?"'"` en la consola de R para mostrar una lista completa de estos caracteres especiales (aparecerá en el panel de ayuda de RStudio).

<!-- ======================================================= -->

## Expresiones regulares (regex)  {#regular-expressions-regex}

<!-- ======================================================= -->

## Regex y caracteres especiales {#regex-and-special-characters}

Las expresiones regulares, o "regex", son un lenguaje conciso para describir patrones en las cadenas. Si no está familiarizado con él, una expresión regular puede parecer un lenguaje extraño. Aquí tratamos de desmitificar un poco este lenguaje. 

*Gran parte de esta sección está adaptada de [este tutorial](https://towardsdatascience.com/a-gentle-introduction-to-regular-expressions-with-r-df5e897ca432) y de [esta hoja de trucos](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)*. Aquí adaptamos selectivamente sabiendo que este manual podría ser visto por personas sin acceso a internet para ver los otros tutoriales. 

Una expresión regular se aplica a menudo para extraer patrones específicos de texto "no estructurado", por ejemplo, notas médicas, quejas principales, historial del paciente u otras columnas de texto libre en un dataframe. 

Hay cuatro herramientas básicas que se pueden utilizar para crear una expresión regular básica: 

1.  Juegos de caracteres 

2.  Metacaracteres 

3.  Cuantificadores 

4.  Grupos 

**Juegos de caracteres** 

Los conjuntos de caracteres, son una forma de expresar las opciones de la lista para una coincidencia de caracteres, entre paréntesis. Así, cualquier coincidencia se activará si cualquiera de los caracteres dentro de los paréntesis se encuentra en la cadena. Por ejemplo, para buscar vocales se podría utilizar este conjunto de caracteres "[aeiou]". Otros conjuntos de caracteres comunes son: 

| Caracteres    | Coinciden con                    |
|---------------|----------------------------------|
| `"[A-Z]"`     | una  letra mayúscula             |
| `"[a-z]"`     | una  letra minúscula             |
| `"[0-9]"`     | un dígito                        |
| `[:alnum:]`   | un carácter alfanumérico         |
| `[:digit:]`   | un dígito numérico               |
| `[:alpha:]`   | una letra (mayúscula o minúscula)|
| `[:upper:]`   | una  letra mayúscula             |
| `[:lower:]`   | una  letra minúscula             |

Los conjuntos de caracteres pueden combinarse dentro de un paréntesis (¡sin espacios!), como `"[A-Za-z]"` (cualquier letra mayúscula o minúscula), u otro ejemplo `"[t-z0-5]"` (de la t a la z en minúscula o del número 0 al 5).

**Meta caracteres**

Los metacaracteres son la abreviatura de los juegos de caracteres. A continuación se enumeran algunos de los más importantes:

| Meta carácter  | Coincide con                                         |
|----------------|------------------------------------------------------|
| `"\\s"`        | un solo espacio                                      |
| `"\\w"`        | cualquier carácter alfanumérico (A-Z, a-z, o 0-9)    |
| `"\\d"`        | cualquier dígito numérico (0-9)                      |

**Cuantificadores** 

Normalmente no se desea buscar una coincidencia en un solo carácter. Los cuantificadores le permiten designar la longitud de las letras/números para permitir la coincidencia. 

Los cuantificadores son números escritos entre corchetes `{ }` *después* del carácter que cuantifican, por ejemplo, 

* `"A{2}"` devolverá instancias de **dos** letras A mayúsculas. 

* `"A{2,4}"` devolverá instancias de **entre dos y cuatro** letras A mayúsculas *(¡no ponga espacios!)*. 

* `"A{2,}"` devolverá instancias de **dos o más** letras A mayúsculas. 

* `"A+"` devolverá instancias de **una o más** letras A mayúsculas (grupo extendido hasta que se encuentre un carácter diferente). 

* Preceder con un asterisco `*` para devolver **cero o más** coincidencias (útil si no está seguro de que el patrón está presente) 

Utilizando el símbolo `+` como cuantificador, la coincidencia se producirá hasta que se encuentre un carácter diferente. Por ejemplo, esta expresión devolverá todas las *palabras* (caracteres alfa: `"[A-Za-z]+"` 

```{r}
# test string for quantifiers
test <- "A-AA-AAA-AAAA"
```

Cuando se utiliza un cuantificador de {2}, sólo se devuelven los pares de A consecutivos. Se identifican dos pares dentro de `AAAA`. 

```{r}
str_extract_all(test, "A{2}")
```

Cuando se utiliza un cuantificador de {2,4}, se devuelven grupos de A consecutivos de dos a cuatro.

```{r}
str_extract_all(test, "A{2,4}")
```

Con el cuantificador `+`, se devuelven grupos de **uno o más**: 

```{r}
str_extract_all(test, "A+")
```

**Posición relativa** 

Expresan los requisitos de lo que precede o sigue a un patrón. Por ejemplo, para extraer frases, "dos números que van seguidos de un punto" (`""`). (?\<=\\.)\\s(?=[A-Z]) 

```{r}
str_extract_all(test, "")
```

| Definición posición | Coincide con                             |
|---------------------|------------------------------------------|
| `"(?<=b)a"`         | "a" que **está precedida** con una "b"   |
| `"(?<!b)a"`         | "a" que **NO está precedida** con una "b"|
| `"a(?=b)"`          | "a" que **se sigue** de una "b"          |
| `"a(?!b)"`          | "a" que **NO se sigue** de una "b"       |

**Grupos** 

La captura de grupos en su expresión regular es una forma de tener una salida más organizada al momento de la extracción. 

**Ejemplos de Regex** 

A continuación se presenta un texto libre para los ejemplos. Intentaremos extraer información útil del mismo utilizando un término de búsqueda de expresión regular. 

```{r}
pt_note <- "El paciente llegó a la sala de urgencias del Broward Hospital a las 18:00 horas del 6/12/2005. Se presentó con dolor abdominal irradiado desde el cuadrante LR. La piel estaba pálida, fría y húmeda. Su temperatura era de 99,8 grados Farinheit. El pulso era de 100 lpm y filiforme. La frecuencia respiratoria era de 29 por minuto"
```

Esta expresión coincide con todas las palabras (cualquier carácter hasta llegar a un no carácter como un espacio): 

```{r}
str_extract_all(pt_note, "[A-Za-z]+")
```

La expresión `"[0-9]{1,2}"` coincide con números consecutivos de 1 o 2 dígitos. También podría escribirse `"\\d{1,2}"`, o `"[:digit:]{1,2}"`.

```{r}
str_extract_all(pt_note, "[0-9]{1,2}")
```

<!-- This expression will extract all sentences (assuming first letter is capitalized, and the sentence ends with a period). The pattern reads in English as: "A capital letter followed by some lowercase letters, a space, some letters, a space,     -->

<!-- ```{r} -->

<!-- str_extract_all(pt_note, "[A-Z][a-z]+\\s\\w+\\s\\d{1,2}\\s\\w+\\s*\\w*") -->

<!-- ``` -->

Puedes ver una lista útil de expresiones regex y consejos en la página 2 de [esta hoja de trucos](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf)

Mira también este [tutorial](https://towardsdatascience.com/a-gentle-introduction-to-regular-expressions-with-r-df5e897ca432).

<!-- ======================================================= -->

## Recursos  {#resources-3}

Puedes encontrar una hoja de referencia para las funciones de **stringr** [aquí](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf) 

Puedes encontrar una viñeta sobre **stringr** [aquí](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html) 
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/characters_strings.Rmd-->

# Factores {#factors} 


```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Factors_1500x500.png"))
```

En R, los *factores* son un tipo de datos que permiten categorías ordenadas con un conjunto fijo de valores. 

Normalmente, se convierte una columna de tipo numérico o de caracteres en un factor si se desea establecer un orden intrínseco a los valores ("*niveles"*) para que puedan mostrarse de forma no alfabética en gráficos y tablas. Otro uso común de los factores es normalizar las leyendas de los gráficos para que no fluctúen si ciertos valores están temporalmente faltantes de datos. 

En esta página se muestra el uso de las funciones del paquete **forcats** (nombre abreviado de "**For** **cat**egorical variables") y algunas funciones  de R **base**. También se aborda el uso de **lubridate** y **aweek** para casos de factores especiales relacionados con semanas epidemiológicas. 

Puedes encontrar una lista completa de las funciones de **forcats** en línea [aquí](https://forcats.tidyverse.org/reference/index.html). A continuación mostramos algunas de las más comunes. 


<!-- ======================================================= -->
## Preparación  {#preparation-2}

### Cargar paquetes {.unnumbered}  

Este trozo de código muestra la carga de los paquetes necesarios para el análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puede cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre  [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R. 

```{r}
pacman::p_load(
  rio,           # import/export
  here,          # filepaths
  lubridate,     # working with dates
  forcats,       # factors
  aweek,         # create epiweeks with automatic factor levels
  janitor,       # tables
  tidyverse      # data mgmt and viz
  )
```



### Importar datos {.unnumbered}  

Importamos los datos de casos de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica para descargar linelist "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa sus datos con la función `import()` del paquete **rio** (acepta muchos tipos de archivos como .xlsx, .rds, .csv - Mira la página de [importación y exportación](#import-and-export) para más detalles). 

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```


```{r, eval=F}
# import your dataset
linelist <- import("linelist_cleaned.rds")
```


### Nueva variable categórica  {#fct_newcat .unnumbered}  

Para mostrarlo en esta página utilizaremos un escenario común - la creación de una nueva variable categórica. 

Ten en cuenta que si conviertes una columna numérica en una de tipo factor, no podrás calcular estadísticas numéricas sobre ella. 

#### Crear columna {.unnumbered}  

Utilizamos la columna existente `days_onset_hosp` (días desde el inicio de los síntomas hasta el ingreso en el hospital) y creamos una nueva columna `delay_cat` clasificando cada fila en una de varias categorías. Lo hacemos con la función **dplyr** `case_when()`, que aplica secuencialmente criterios lógicos (lado derecho) a cada fila y devuelve el valor correspondiente del lado izquierdo para la nueva columna `delay_cat`. Puedes leer más sobre `case_when()` en [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions). 

```{r}
linelist <- linelist %>% 
  mutate(delay_cat = case_when(
    # criteria                                   # new value if TRUE
    days_onset_hosp < 2                        ~ "<2 days",
    days_onset_hosp >= 2 & days_onset_hosp < 5 ~ "2-5 days",
    days_onset_hosp >= 5                       ~ ">5 days",
    is.na(days_onset_hosp)                     ~ NA_character_,
    TRUE                                       ~ "Check me"))  
```


#### Orden de valores por defecto  {.unnumbered}  

Tal y como se creó con `case_when()`, la nueva columna `delay_cat` es una columna categórica de tipo Character - aún *no* es un factor. Así, en una tabla de frecuencias, vemos que los valores únicos aparecen en un orden alfanumérico por defecto - un orden que no tiene mucho sentido intuitivo:   

```{r}
table(linelist$delay_cat, useNA = "always")
```

Del mismo modo, si hacemos un gráfico de barras, los valores también aparecen en este orden en el eje x (ver la página de [conceptos básicos de ggplot](#ggplot-basics) para más información sobre **ggplot2** - el paquete de visualización más común en R). 

```{r, warning=F, message=F}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = delay_cat))
```



## Convertir en factor  {#convert-to-factor}

Para convertir una columna numérica o de caracteres en una de *tipo factor*, puedes utilizar cualquier función del paquete **forcats** (muchas se detallan [a continuación](#fct_adjust)). Las convertirán en otra de tipo factor y luego también realizarán o permitirán cierto ordenamiento de los niveles - por ejemplo usando `fct_relevel()` permite especificar manualmente el orden de los niveles. La función `as_factor()` simplemente convierte el tipo sin ninguna otra capacidad. 

La función `factor()` de R **base** convierte una columna en factor y permite especificar manualmente el orden de los niveles, como un vector de caracteres a su argumento `levels = `. 

A continuación utilizamos `mutate()` y `fct_relevel()` para convertir la columna delay_cat de tipo carácter a tipo factor. La columna `delay_cat` se crea en la sección de [preparación](#fct_newcat) anterior. 

```{r}
linelist <- linelist %>%
  mutate(delay_cat = fct_relevel(delay_cat))
```

*Los "valores" únicos de esta columna se consideran ahora "niveles" del factor.* Los niveles tienen un *orden*, que puede imprimirse con la función de `levels()`, o alternativamente verse en una tabla de recuento mediante `table()` de  R **base** o `tabyl()` de **janitor**. Por defecto, el orden de los niveles será alfanumérico, como antes. Ten en cuenta que `NA` no es un nivel de factor.  

```{r}
levels(linelist$delay_cat)
```

La función `fct_relevel()` tiene la utilidad adicional de permitir especificar manualmente el orden de los niveles. Simplemente escribe los valores de nivel en orden, entre comillas, separados por comas, como se muestra a continuación. Ten en cuenta que la ortografía debe coincidir exactamente con los valores. Si deseas crear niveles que no existen en los datos, utiliza [fct_expand() en su lugar](#fct_add)). 

```{r}
linelist <- linelist %>%
  mutate(delay_cat = fct_relevel(delay_cat, "<2 days", "2-5 days", ">5 days"))
```

Ahora podemos ver que los niveles están ordenados, como se especificó en el comando anterior, en un orden sensato. 

```{r}
levels(linelist$delay_cat)
```

Ahora el orden de la gráfica también tiene un sentido más intuitivo.   

```{r, warning=F, message=F}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = delay_cat))
```


## Añadir o quitar niveles {#add-or-drop-levels}

### Añadir  {#fct_add .unnumbered}

Si necesitas añadir niveles a un factor, puedes hacerlo con `fct_expand()`. Basta con escribir el nombre de la columna seguido de los nuevos niveles (separados por comas). Al tabular los valores, podemos ver los nuevos niveles y los recuentos de cero. Puedes utilizar `table()` de R **base**, o `tabyl()` de **janitor**:   

```{r}
linelist %>% 
  mutate(delay_cat = fct_expand(delay_cat, "Not admitted to hospital", "Transfer to other jurisdiction")) %>% 
  tabyl(delay_cat)   # print table
```


Nota: existe una función especial de **forcats** para añadir fácilmente valores faltantes (`NA`) como nivel. Véase la sección sobre [valores faltantes](#fct_missing) más adelante.  


### Quitar {.unnumbered}  

Si utilizas `fct_drop()`, los niveles "no utilizados" con recuento cero se eliminarán del conjunto de niveles. Los niveles que hemos añadido anteriormente ("No admitido en un hospital") existen como nivel, pero ninguna fila tiene realmente esos valores. Por tanto, se eliminarán aplicando `fct_drop()` a nuestra columna de factores: 

```{r}
linelist %>% 
  mutate(delay_cat = fct_drop(delay_cat)) %>% 
  tabyl(delay_cat)
```



## Ajustar el orden de los niveles  {#fct_adjust} 

El paquete **forcats** ofrece funciones útiles para ajustar fácilmente el orden de los niveles de un factor (después de haber definido una columna como de tipo factor): 

Estas funciones pueden aplicarse a una columna de factores en dos contextos: 

1.  A la columna del dataframe, como es habitual, para que la transformación esté disponible para cualquier uso posterior de los datos 
2.  *Dentro de un gráfico*, para que el cambio se aplique sólo dentro del gráfico 


### Manualmente  {.unnumbered} 

Esta función se utiliza para ordenar manualmente los niveles de los factores. Si se utiliza en una columna no factorial, la columna se convertirá primero en de tipo factor. 

Dentro del paréntesis, indica primero el nombre de la columna del factor y, a continuación, escribe 

* Todos los niveles en el orden deseado (como un vector de caracteres `c()`), o 
* Un nivel y se corrige la colocación utilizando el argumento `after = `

He aquí un ejemplo de redefinición de la columna `delay_cat` (que ya es de tipo Factor) y especificando todo el orden de niveles deseado.   

```{r}
# re-define level order
linelist <- linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, c("<2 days", "2-5 days", ">5 days")))
```

Si sólo quieres mover un nivel, puedes especificarlo sólo en `fct_relevel()` y dar un número al argumento `after = `para indicar en qué lugar del orden debe estar. Por ejemplo, el comando siguiente desplaza "<2 días" a la segunda posición: 

```{r, eval=F}
# re-define level order
linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, "<2 days", after = 1)) %>% 
  tabyl(delay_cat)
```




### Dentro de un gráfico  {.unnumbered}  

Los comandos **forcats** pueden utilizarse para establecer el orden de los niveles en el dataframe, o sólo dentro de un gráfico. Al utilizar el comando para "envolver" el nombre de la columna *dentro* del comando `ggplot()`, puedes invertir/nivelar/etc. la transformación sólo se aplicará dentro de ese gráfico. 

A continuación, se crean dos gráficos con `ggplot()` (véase la página de [conceptos básicos de ggplot](#ggplot-basics)). En el primero, la columna delay_cat se asigna al eje x del gráfico, con su orden de nivel por defecto como en linelist de datos. En el segundo ejemplo se envuelve dentro de `fct_relevel()` y se cambia el orden en el gráfico. 

```{r, echo =F}
linelist <- linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, c("2-5 days", "<2 days", ">5 days")))

```



```{r, warning=F, message=F, out.width = c('50%', '50%'), fig.show='hold'}
# Alpha-numeric default order - no adjustment within ggplot
ggplot(data = linelist)+
    geom_bar(mapping = aes(x = delay_cat))

# Factor level order adjusted within ggplot
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = fct_relevel(delay_cat, c("<2 days", "2-5 days", ">5 days"))))
```

Ten en cuenta que el título del eje x por defecto es ahora bastante complicado - puedes sobrescribir este título con el argumento de **ggplot2** `labs()`. 


### Invertir  {.unnumbered}  

Es bastante común que se quiera invertir el orden de los niveles. Basta con envolver el factor con `fct_rev()`. 

Ten en cuenta que si deseas revertir *sólo* una leyenda del gráfico pero no los niveles reales del factor, puedes hacerlo con `guides()` (ver [consejos de ggplot](#ggplot-tips)). 


### Por frecuencia  {.unnumbered}  

Para ordenar por la frecuencia con que el valor aparece en los datos, utiliza `fct_infreq()`. Cualquier valor que falte (`NA`) se incluirá automáticamente al final, a menos que se convierta en un nivel explícito (véase [esta sección](#fct_missing)). Puedes invertir el orden envolviendo más con `fct_rev()`. 

Esta función puede utilizarse dentro de `ggplot()`, como se muestra a continuación. 

```{r, out.width = c('50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# ordered by frequency
ggplot(data = linelist, aes(x = fct_infreq(delay_cat)))+
  geom_bar()+
  labs(x = "Delay onset to admission (days)",
       title = "Ordered by frequency")

# reversed frequency
ggplot(data = linelist, aes(x = fct_rev(fct_infreq(delay_cat))))+
  geom_bar()+
  labs(x = "Delay onset to admission (days)",
       title = "Reverse of order by frequency")
```


### Por apariencia {.unnumbered}  

Utiliza `fct_inorder()` para establecer el orden de los niveles para que coincida con el orden de aparición en los datos, empezando por la primera fila. Esto puede ser útil si primero organizas cuidadosamente  `arrange()` los datos en el dataframe, y luego utiliza esto para establecer el orden de los factores. 



### Por estadística resumida de otra columna {.unnumbered}  

Puedes utilizar `fct_reorder()` para ordenar los niveles de una columna *por una estadística de resumen de otra columna*. Visualmente, esto puede dar lugar a gráficos agradables en los que las barras/puntos ascienden o descienden de forma constante a través del gráfico. 

En los ejemplos siguientes, el eje x es `delay_cat`, y el eje y es la columna numérica `ct_blood` (valor de umbral de ciclo). Los gráficos de caja muestran la distribución del valor CT por grupo `delay_cat`. Queremos ordenar los gráficos de caja en orden ascendente por mediana del grupo CT. 

En el primer ejemplo de abajo, se utiliza el orden por defecto de los niveles alfa-numéricos. Se puede ver que las alturas de los gráficos de caja están mezcladas y no en ningún orden particular. En el segundo ejemplo, la columna `delay_cat` (asignada al eje x) se ha envuelto en `fct_reorder()`, la columna `ct_blood` se da como segundo argumento, y la "mediana" se da como tercer argumento (también podría usar "max", "mean", "min", etc). Por lo tanto, el orden de los niveles de `delay_cat` reflejará ahora los valores ascendentes de la mediana del CT de cada grupo de `delay_cat`. Esto se refleja en el segundo gráfico: los gráficos de caja se han reordenado de forma ascendente. Observa cómo `NA` (missing) aparecerá al final, a menos que se convierta en un nivel explícito.

```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# boxplots ordered by original factor levels
ggplot(data = linelist)+
  geom_boxplot(
    aes(x = delay_cat,
        y = ct_blood, 
        fill = delay_cat))+
  labs(x = "Delay onset to admission (days)",
       title = "Ordered by original alpha-numeric levels")+
  theme_classic()+
  theme(legend.position = "none")


# boxplots ordered by median CT value
ggplot(data = linelist)+
  geom_boxplot(
    aes(x = fct_reorder(delay_cat, ct_blood, "median"),
        y = ct_blood,
        fill = delay_cat))+
  labs(x = "Delay onset to admission (days)",
       title = "Ordered by median CT value in group")+
  theme_classic()+
  theme(legend.position = "none")
```

Observa que en este ejemplo no se requieren pasos previos a la llamada a `ggplot()` - la agrupación y los cálculos se realizan internamente en el comando ggplot.   


### Por valor "final"  {.unnumbered}  

Utiliza `fct_reorder2()` para los gráficos de líneas agrupadas. Ordena los niveles (y, por tanto, la *leyenda*) para que se alineen con la ordenación vertical de las líneas en el "final" del gráfico. Técnicamente hablando, "ordena por los valores-y asociados a los valores-x más grandes". 

Por ejemplo, si tienes líneas que muestran los recuentos de casos por hospital a lo largo del tiempo, puedes aplicar `fct_reorder2()` al argumento `color = `dentro de `aes()`, de forma que el orden vertical de los hospitales que aparecen en la leyenda se alinee con el orden de las líneas en el extremo terminal del gráfico. Lee más en la [documentación en línea](https://forcats.tidyverse.org/reference/fct_reorder.html). 

```{r, warning=F, message=F}
epidemic_data <- linelist %>%         # begin with the linelist   
    filter(date_onset < as.Date("2014-09-21")) %>%    # cut-off date, for visual clarity
    count(                                            # get case counts per week and by hospital
      epiweek = lubridate::floor_date(date_onset, "week"),  
      hospital                                            
    ) 
  
ggplot(data = epidemic_data)+                       # start plot
  geom_line(                                        # make lines
    aes(
      x = epiweek,                                  # x-axis epiweek
      y = n,                                        # height is number of cases per week
      color = fct_reorder2(hospital, epiweek, n)))+ # data grouped and colored by hospital, with factor order by height at end of plot
  labs(title = "Factor levels (and legend display) by line height at end of plot",
       color = "Hospital")                          # change legend title
```




## Valores faltantes  {#fct_missing}  

Si hay valores `NA` en lu columna de factores, puede convertirlos fácilmente a un nivel con nombre como "Missing" con `fct_explicit_na()`. Los valores `NA` se convierten por defecto en "(Missing)" al final del orden de los niveles. Puedes ajustar el nombre del nivel con el argumento `na_level =`. 

A continuación, esta operación se realiza en la columna `delay_cat` y se imprime una tabla con `tabyl()` con `NA` convertido en "Missing delay". 

```{r}
linelist %>% 
  mutate(delay_cat = fct_explicit_na(delay_cat, na_level = "Missing delay")) %>% 
  tabyl(delay_cat)
```





## Combinar niveles {#combine-levels}


### Manualmente  {.unnumbered}  

Puedes ajustar las visualizaciones de los niveles manualmente con `fct_recode()`. Es como la función `recode()` de **dplyr**  (véase [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions)), pero permite la creación de nuevos niveles de factores. Si utilizas la función simple `recode()` en un factor, los nuevos valores recodificados serán rechazados a menos que ya hayan sido establecidos como niveles permitidos. 

Esta herramienta también puede utilizarse para "combinar" niveles, asignando a varios niveles el mismo valor recodificado. Sólo hay que tener cuidado de no perder información. Considere la posibilidad de realizar estos pasos de combinación en una nueva columna (sin sobreescribir la columna existente). 

`fct_recode()` tiene una sintaxis diferente a la de `recode()`. `recode()` utiliza `OLD = NEW`, mientras que `fct_recode()` utiliza `NEW = OLD`. 

Los niveles actuales de `delay_cat` son:  
```{r, echo=F}
linelist <- linelist %>% 
  mutate(delay_cat = fct_relevel(delay_cat, "<2 days", after = 0))
```


```{r}
levels(linelist$delay_cat)
```

Los nuevos niveles se crean utilizando la sintaxis `fct_recode(column, "new" = "old", "new" = "old", "new" = "old")` y se imprimen: 

```{r}
linelist %>% 
  mutate(delay_cat = fct_recode(
    delay_cat,
    "Less than 2 days" = "<2 days",
    "2 to 5 days"      = "2-5 days",
    "More than 5 days" = ">5 days")) %>% 
  tabyl(delay_cat)
```

Aquí se combinan manualmente con `fct_recode()`. Obsérvese que no se produce ningún error en la creación de un nuevo nivel "Menos de 5 días". 


```{r, warning=F, message=F}
linelist %>% 
  mutate(delay_cat = fct_recode(
    delay_cat,
    "Less than 5 days" = "<2 days",
    "Less than 5 days" = "2-5 days",
    "More than 5 days" = ">5 days")) %>% 
  tabyl(delay_cat)
```



### Reducir a "Otros"  {.unnumbered}  

Puedes utilizar `fct_other()` para asignar manualmente niveles de factor a un nivel "Otro". A continuación, todos los niveles de la columna `hospital`, aparte de "Port Hospital" y "Central Hospital", se combinan en "Otros". Puedes proporcionar el vector `keep = `, o `drop = ` para mantener o eliminarlo. Puedes cambiar la visualización del nivel "Otro" con `other_level =`. 

```{r}
linelist %>%    
  mutate(hospital = fct_other(                      # adjust levels
    hospital,
    keep = c("Port Hospital", "Central Hospital"),  # keep these separate
    other_level = "Other Hospital")) %>%            # All others as "Other Hospital"
  tabyl(hospital)                                   # print table

```




### Reducir por frecuencia  {.unnumbered}

Puedes combinar los niveles del factor menos frecuente automáticamente utilizando `fct_lump()`. 

Para "agrupar" muchos niveles de baja frecuencia en un grupo "Otros", puedes hacer una de las siguientes cosas: 

* Establecer con `n =` el número de grupos que deseas conservar. Los n niveles más frecuentes se mantendrán, y todos los demás se combinarán en "Otros". 

* Fijar con `prop =` la proporción de frecuencia del umbral para los niveles por encima de los cuales deseas mantener. Todos los demás valores se combinarán en "Otros". 

Puedes cambiar la visualización del nivel "Otros" con `other_level =`. A continuación, todos los hospitales excepto los dos más frecuentes se combinan en "Other hospitals". 

```{r, warning=F, message=F}
linelist %>%    
  mutate(hospital = fct_lump(                      # adjust levels
    hospital,
    n = 2,                                          # keep top 2 levels
    other_level = "Other Hospital")) %>%            # all others as "Other Hospital"
  tabyl(hospital)                                   # print table

```




## Mostrar todos los niveles {#show-all-levels}

Una de las ventajas del uso de factores es la estandarización del aspecto de las leyendas de los gráficos y de las tablas, independientemente de los valores que estén realmente presentes en unos datos. 

Si estás preparando muchas figuras (por ejemplo, para varias jurisdicciones), querrás que las leyendas y las tablas aparezcan de forma idéntica incluso con distintos niveles de cumplimentación o de composición de los datos. 

### En los gráficos  {.unnumbered}  

En una figura `ggplot()`, basta con añadir el argumento `drop = FALSE` en la función `scale_xxxx()` correspondiente. Se mostrarán todos los niveles de los factores, independientemente de si están presentes en los datos. Si sus niveles de columna de factores se muestran con `fill =`, entonces en `scale_fill_discrete()` incluye `drop = FALSE,` como se muestra a continuación. Si sus niveles se muestran con `x =` (al eje-x) `color =` o `size =`, deberás establecer esto con `scale_color_discrete()` o `scale_size_discrete()` según corresponda. 

Este ejemplo es un gráfico de barras apiladas de la categoría de edad, por hospital. Añadiendo `scale_fill_discrete(drop = FALSE)` se garantiza que todos los grupos de edad aparezcan en la leyenda, aunque no estén presentes en los datos. 

```{r}
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = hospital, fill = age_cat)) +
  scale_fill_discrete(drop = FALSE)+                        # show all age groups in the legend, even those not present
  labs(
    title = "All age groups will appear in legend, even if not present in data")
```

### En tablas {.unnumbered}  

Tanto `table()` de R **base** como `tabyl()` de **janitor** mostrarán todos los niveles de los factores (incluso los no utilizados). 

Si utilizas `count()` o `summarise()` de **dplyr** para hacer una tabla, añade el argumento `.drop = FALSE` para incluir los recuentos de todos los niveles del factor, incluso los no utilizados. 

Puedes leer más en la página de [tablas descriptivas](#descriptive-tables), o en la [documentación de scale_discrete](https://ggplot2.tidyverse.org/reference/scale_discrete.html), o en la [documentación de `count()`](https://dplyr.tidyverse.org/reference/count.html). Puedes ver otro ejemplo en la página de [rastreo de  contactos](#contact-tracing-1). 


## Epiweeks  

Por favor, consulta la extensa discusión sobre cómo crear semanas epidemiológicas en la página de [Agrupar datos](#grouping-data).\ Consulta también la página [Trabajar con fechas](#working-with-dates-1) para obtener consejos sobre cómo crear y dar formato a las semanas epidemiológicas.


### Epiweeks en un gráfico  {.unnumbered}  

Si tu objetivo es crear epiweeks para mostrarlos en un gráfico, puedes hacerlo simplemente con `floor_date()` de **lubridate**, como se explica en la página de [Agrupar datos](#grouping-data). Los valores devueltos serán del tipo Date con el formato YYYY-MM-DD. Si utilizas esta columna en un gráfico, las fechas se ordenarán correctamente de forma natural, y no tendrá que preocuparse de los niveles o de la conversión al tipo Factor. Mira el histograma `ggplot()` de las fechas de inicio más abajo. 

En este enfoque, se puede ajustar la *visualización* de las fechas en un eje con `scale_x_date()`. Consulta la página sobre [curvas epidémicas](#epidemic-curves) para obtener más información. Puedes especificar un formato de visualización "strptime" al argumento `date_labels =` de `scale_x_date()`. Estos formatos utilizan marcadores de posición "%" y se tratan en la página [Trabajar con fechas](#working-with-dates-1). Utiliza "%Y" para representar un año de 4 dígitos, y "%W" o "%U" para representar el número de la semana (semana del lunes o del domingo respectivamente).   

```{r, warning=F, message=F}
linelist %>% 
  mutate(epiweek_date = floor_date(date_onset, "week")) %>%  # create week column
  ggplot()+                                                  # begin ggplot
  geom_histogram(mapping = aes(x = epiweek_date))+           # histogram of date of onset
  scale_x_date(date_labels = "%Y-W%W")                       # adjust disply of dates to be YYYY-WWw
```


### Epiweeks en los datos  {.unnumbered}  

Sin embargo, si tu propósito al factorizar *no es* hacer gráficos, puedes enfocar esto de dos maneras: 

1.  *Para un control preciso de la visualización*, convierte la columna de la semana-epi **lubrificada** (AAAA-MM-DD) al formato de visualización deseado (AAAA-WWw) *dentro del propio dataframe,* y luego conviértala en tipo Factor. 

En primer lugar, utiliza `format()`  para convertir la visualización de la fecha de YYYY-MM-DD a YYYY-Www (consulta la página [Trabajar con fechas](#working-with-dates-1)). En este proceso el tipo será convertida a carácter. A continuación, convierta de carácter a tipo Factor con `factor()`.   


```{r}
linelist <- linelist %>% 
  mutate(epiweek_date = floor_date(date_onset, "week"),       # create epiweeks (YYYY-MM-DD)
         epiweek_formatted = format(epiweek_date, "%Y-W%W"),  # Convert to display (YYYY-WWw)
         epiweek_formatted = factor(epiweek_formatted))       # Convert to factor

# Display levels
levels(linelist$epiweek_formatted)
```

<span style="color: red;">***PELIGRO:*** Si colocas las semanas por delante de los años ("Www-YYY") ("%W-%Y"), la ordenación por defecto del nivel alfanumérico será incorrecta (por ejemplo, 01-2015 estará antes que 35-2014). Podría ser necesario ajustar manualmente el orden, lo que sería un proceso largo y doloroso. </span>  

2.  *Para una visualización rápida por defecto*, utiliza el paquete **aweek** y su función `date2week()`. Puedes establecer el día de comienzo con `week_start = `, y si estableces `factor = TRUE` entonces la columna de salida es un factor ordenado. Como ventaja, el factor incluye niveles para *todas las* semanas posibles en el lapso - incluso si no hay casos esa semana. 

```{r, eval=F}
df <- linelist %>% 
  mutate(epiweek = date2week(date_onset, week_start = "Monday", factor = TRUE))

levels(df$epiweek)
```

Consulta la página [Trabajar con fechas](#working-with-dates-1) para obtener más información sobre **aweek**. También ofrece la función inversa `week2date()`.   



<!-- ======================================================= -->
## Recursos {#resources-4} 

Página de R for Data Science en español sobre [factores](https://es.r4ds.hadley.nz/factores.html)
[viñeta del paquete aweek](https://cran.r-project.org/web/packages/aweek/vignettes/introduction.html) 
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/factors.Rmd-->


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Pivotar datos {#pivoting-data} 

```{r, warning=F, message=F, out.height = c('50%'), fig.align="center", fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "pivoting", "Pivoting_500x500.png"))

#knitr::include_graphics(here::here("images", "pivoting", "pivot_longer_new.png"))
#knitr::include_graphics(here::here("images", "pivoting", "pivot_bar.png"))
#knitr::include_graphics(here::here("images", "pivoting", "pivot_wider_new.png"))
```


En la gestión de datos, se puede entender que el *pivoteo se refiere* a uno de los dos procesos: 

1.  La creación de *tablas dinámicas*, que son tablas de estadísticas que resumen los datos de una tabla más extensa 

2.  La conversión de una tabla de formato **largo** a formato **ancho**, o viceversa. 

**En esta página, nos centraremos en la última definición.** La primera es un paso crucial en el análisis de datos, y se trata en las páginas [Agrupar datos](#grouping-data) y [Tablas descriptivas](#descriptive-tables). 

En esta página se tratan los formatos de los datos. Es útil conocer la idea de "datos ordenados", en la que cada variable tiene su propia columna, cada observación tiene su propia fila y cada valor tiene su propia celda. Se puede encontrar más información sobre este tema [en este capítulo en línea de R for Data Science](https://es.r4ds.hadley.nz/datos-ordenados.html). 


## Preparación   {#preparation-3}

### Cargar paquetes {.unnumbered}  

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puede cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R. 

```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  tidyverse)    # data management + ggplot2 graphics
```



### Importar datos {.unnumbered}


### Recuento de casos de malaria {-}  

En esta página, utilizaremos unos datos ficticios de casos diarios de malaria, por centro y grupo de edad. Si quieres seguirlo, [clica aquí para descargarlo (como archivo .rds)](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/malaria_facility_count_data.rds). Importa los datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - mira la página de [importación y exportación](#import-and-export) para más detalles). 
  

```{r, echo=F}
count_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  as_tibble()
```

```{r, eval=F}
# Import data
count_data <- import("malaria_facility_count_data.rds")
```

A continuación se muestran las primeras 50 filas. 

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(count_data, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Listado de casos de Linelist  {-}  

En la parte posterior de esta página, también utilizaremos los datos de casos de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica aqui para descargar linelist "limpio" ](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa tus datos con la función `import()` del paquete **rio** (acepta muchos tipos de archivos como .xlsx, .rds, .csv - mira la página de [importación y exportación](#import-and-export) para más detalles). 

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```


```{r, eval=F}
# import your dataset
linelist <- import("linelist_cleaned.xlsx")
```







<!-- ======================================================= -->
## De ancho a largo  {#wide-to-long}

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "pivoting", "pivot_longer_new.png"))
```


<!-- ======================================================= -->
### "Formato ancho" {.unnumbered}

Los datos suelen introducirse y almacenarse en un formato "amplio", en el que las características o respuestas de un sujeto se almacenan en una sola fila. Aunque esto puede ser útil para la presentación, no es ideal para algunos tipos de análisis. 

Tomemos como ejemplo el set de datos `count_data` importado en la sección "Preparación". Puedes ver que cada fila representa un "centro-día". Los recuentos de casos reales (las columnas más a la derecha) se almacenan en un formato "ancho", de modo que la información de cada grupo de edad en un día determinado del centro se almacena en una sola fila. 

```{r, echo=F}
DT::datatable(count_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
```

Cada observación de este conjunto de datos se refiere a los recuentos de paludismo en una de las 65 instalaciones en una fecha determinada, que va desde `count_data$data_date %\>% min()` hasta `count_data$data_date %\>% max()`. Estas instalaciones están situadas en una `Province` (Norte) y cuatro `District` (Spring, Bolo, Dingo y Barnard). Los datos proporcionan los recuentos globales de malaria, así como los recuentos específicos por edad en cada uno de los tres grupos de edad: \<4 años, 5-14 años y 15 años o más. 

Los datos "anchos" como éste no se ajustan a las normas de "datos ordenados", porque los encabezados de las columnas no representan realmente "variables", sino que representan *valores* de una hipotética variable "grupo de edad". 

Este formato puede ser útil para presentar la información en una tabla, o para introducir datos (por ejemplo, en Excel) a partir de formularios de informes de casos. Sin embargo, en la etapa de análisis, estos datos normalmente deben ser transformados a un formato "largo" más alineado con los estándares de "datos ordenados". El paquete **ggplot2**, en particular, funciona mejor cuando los datos están en un formato "largo". 

La visualización de los recuentos *totales* de malaria a lo largo del tiempo no plantea ninguna dificultad con los datos en su formato actual: 

```{r, warning=F, message=F}
ggplot(count_data) +
  geom_col(aes(x = data_date, y = malaria_tot), width = 1)
```

Sin embargo, ¿qué pasaría si quisiéramos mostrar las contribuciones relativas de cada grupo de edad a este recuento total? En este caso, necesitamos asegurarnos de que la variable de interés (grupo de edad), aparezca en el conjunto de datos en una sola columna que pueda pasarse a {ggplot2} el argumento `aes()` de  "mapping aesthetics". 




<!-- ======================================================= -->
### `pivot_longer()` {.unnumbered}

La función `pivot_longer()` de **tidyr**  hace que los datos sean "largos". **tidyr** forma parte de los paquetes **tidyverse** . 

Acepta un rango de columnas para transformar (especificado a `cols = `). Por lo tanto, puede operar sólo en una parte de unos datos. Esto es útil para los datos de la malaria, ya que sólo queremos pivotar las columnas de recuento de casos. 

En este proceso, terminará con dos "nuevas" columnas - una con las categorías (los antiguos nombres de las columnas), y otra con los valores correspondientes (por ejemplo, recuento de casos). Puedes aceptar los nombres por defecto para estas nuevas columnas, o puede especificar otros con `names_to = ` y `values_to = ` respectivamente. 

Veamos `pivot_longer()` en acción... 



### Pivoteo estándar {.unnumbered}  

Queremos utilizar la función `pivot_longer()` de **tidyr** para convertir los datos "anchos" en un formato "largo". Concretamente, para convertir las cuatro columnas numéricas con datos sobre los recuentos de malaria en dos nuevas columnas: una que contenga los *grupos de edad* y otra que contenga los *valores* correspondientes. 


```{r, eval=F}
df_long <- count_data %>% 
  pivot_longer(
    cols = c(`malaria_rdt_0-4`, `malaria_rdt_5-14`, `malaria_rdt_15`, `malaria_tot`)
  )

df_long
```

Observa que el dataframe recién creado (`df_long`) tiene más filas (12.152 frente a 3.038); se ha hecho *más largo*. De hecho, es precisamente cuatro veces más largo, porque cada fila de los datos originales representa ahora cuatro filas en df_long, una para cada una de las observaciones de recuento de malaria (<4 años, 5-14 años, 15 años+ y total). 

Además de ser más largo, el nuevo conjunto de datos tiene menos columnas (8 frente a 10), ya que los datos que antes se almacenaban en cuatro columnas (las que empiezan por el prefijo `malaria_`) se almacenan ahora en dos. 

Dado que los nombres de estas cuatro columnas comienzan con el prefijo `malaria_`, podríamos haber hecho uso de la práctica función "tidyselect" `starts_with()` para conseguir el mismo resultado (véase la página [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions) para conocer más sobre estas funciones de ayuda). 

```{r}
# provide column with a tidyselect helper function
count_data %>% 
  pivot_longer(
    cols = starts_with("malaria_")
  )
```

o por posición: 

```{r, eval=F}
# provide columns by position
count_data %>% 
  pivot_longer(
    cols = 6:9
  )
```

o por rango de nombres: 

```{r, eval=F}
# provide range of consecutive columns
count_data %>% 
  pivot_longer(
    cols = `malaria_rdt_0-4`:malaria_tot
  )
```


Estas dos nuevas columnas reciben los nombres por defecto de `name` y `value`, pero podemos cambiar estos valores por defecto para proporcionar nombres más significativos, que pueden ayudar a recordar lo que se almacena dentro, utilizando los argumentos `names_to` y `values_to`. Utilicemos los nombres `age_group` y `counts`: 

```{r}
df_long <- 
  count_data %>% 
  pivot_longer(
    cols = starts_with("malaria_"),
    names_to = "age_group",
    values_to = "counts"
  )

df_long
```

Ahora podemos pasar este nuevo conjunto de datos a `{ggplot2}`, y asignar la nueva columna `count` al eje-y y la nueva columna `age_group` al argumento `fill = ` (el color interno de la columna). Esto mostrará los recuentos de malaria en un gráfico de barras apilado, por grupo de edad: 

```{r, warning=F, message=F}
ggplot(data = df_long) +
  geom_col(
    mapping = aes(x = data_date, y = counts, fill = age_group),
    width = 1
  )
```

Examina esta nueva gráfica y compárala con la que hemos creado antes: *¿qué ha fallado?* 

Nos hemos encontrado con un problema común al manejar los datos de vigilancia: hemos incluido también los recuentos totales de la columna `malaria_tot`, por lo que la magnitud de cada barra en el gráfico es el doble de lo que debería ser. 

Podemos manejar esto de varias maneras. Podríamos simplemente filtrar estos totales en los datos antes de pasarlo a `ggplot()`: 

```{r, warning=F, message=F}
df_long %>% 
  filter(age_group != "malaria_tot") %>% 
  ggplot() +
  geom_col(
    aes(x = data_date, y = counts, fill = age_group),
    width = 1
  )
```

Como alternativa, podríamos haber excluido esta variable al ejecutar `pivot_longer()`, manteniéndola así en set de datos como una variable independiente. Observa cómo se "expanden" sus valores para llenar las nuevas filas. 

```{r, warning=F, message=F}
count_data %>% 
  pivot_longer(
    cols = `malaria_rdt_0-4`:malaria_rdt_15,   # does not include the totals column
    names_to = "age_group",
    values_to = "counts"
  )
```





### Pivotear datos de múltiples tipos  {.unnumbered}

El ejemplo anterior funciona bien en situaciones en las que todas las columnas que se quieren "pivotar más" son del mismo tipo (carácter, numérico, lógico...). 

Sin embargo, habrá muchos casos en los que, en el trabajo de campo, se trabaje con datos preparados por personas no especializadas y que sigan su propia lógica no estándar - como señaló Hadley Wickham (haciendo referencia a Tolstoi) en su [artículo seminal](https://vita.had.co.nz/papers/tidy-data.pdf) sobre los principios de **Tidy Data**: "Como las familias, los conjuntos de datos ordenados son todos iguales, pero cada conjunto de datos desordenado es desordenado a su manera". 

Un problema particularmente común que encontrarás será la necesidad de pivotar columnas que contienen diferentes tipos de datos. Este pivote resultará en el almacenamiento de estos diferentes tipos de datos en una sola columna, lo cual no es una buena situación. Se pueden seguir varios enfoques para separar el desorden que esto crea, pero hay un paso importante que puedes seguir usando `pivot_longer()` para evitar crear tal situación tu mismo. 

Tomemos una situación en la que ha habido una serie de observaciones en diferentes pasos de tiempo para cada uno de los tres elementos A, B y C. Ejemplos de estos elementos podrían ser individuos (por ejemplo, contactos de un caso de ébola que se rastrean cada día durante 21 días) o puestos de salud de aldeas remotas que se supervisan una vez al año para garantizar que siguen funcionando. Utilicemos el ejemplo del rastreo de contactos. Imaginemos que los datos se almacenan de la siguiente manera: 


```{r, message=FALSE, echo=F}

df <- 
  tibble::tribble(
     ~id,   ~obs1_date, ~obs1_status,   ~obs2_date, ~obs2_status,   ~obs3_date, ~obs3_status,
     "A", "2021-04-23",    "Healthy", "2021-04-24",    "Healthy", "2021-04-25",     "Unwell",
     "B", "2021-04-23",    "Healthy", "2021-04-24",    "Healthy", "2021-04-25",    "Healthy",
     "C", "2021-04-23",    "Missing", "2021-04-24",    "Healthy", "2021-04-25",    "Healthy"
     ) 

DT::datatable(df, rownames = FALSE)

```

Como puede verse, los datos son un poco complicados. Cada fila almacena información sobre un elemento, pero con la serie temporal cada vez más alejada hacia la derecha a medida que avanza el tiempo. Además, los tipos de columnas alternan entre valores de fecha y caracteres. 

Un ejemplo particularmente malo que encontró este autor fue el de los datos de vigilancia del cólera, en el que se añadieron 8 nuevas columnas de observaciones *cada día en el* transcurso de **4 años**. El simple hecho de abrir el archivo de Excel en el que se almacenaban estos datos me llevó más de 10 minutos en mi ordenador portátil. 

Para trabajar con estos datos, necesitamos transformar el dataframe a formato largo, pero manteniendo la separación entre una columna `date` y una columna de `character` (estado), para cada observación de cada elemento. Si no lo hacemos, podríamos terminar con una mezcla de tipos de variables en una sola columna (un gran "no-no" cuando se trata de gestión de datos y de datos ordenados): 

```{r}
df %>% 
  pivot_longer(
    cols = -id,
    names_to = c("observation")
  )

```

Arriba, nuestro pivote ha fusionado *fechas* y *caracteres* en una sola columna de `value`. R reaccionará convirtiendo toda la columna en tipo carácter, y se pierde la utilidad de las fechas. 

Para evitar esta situación, podemos aprovechar la estructura sintáctica de los nombres de las columnas originales. Hay una estructura de nombres común, con el número de observación, un guión bajo, y luego "estado" o "fecha". Podemos aprovechar esta sintaxis para mantener estos dos tipos de datos en columnas separadas después del pivote. 

Para ello: 

* Proporcionar un vector de caracteres al argumento names_to =, siendo el segundo elemento (`".value"`). Este término especial indica que las columnas pivotadas se dividirán basándose en un carácter de su nombre... 
* También se debe proporcionar el carácter de "división" al argumento `names_sep = `. En este caso, es el guión bajo "_". 

Así, la denominación y división de las nuevas columnas se basa en el guión bajo de los nombres de las variables existentes. 

```{r}

df_long <- 
  df %>% 
  pivot_longer(
    cols = -id,
    names_to = c("observation", ".value"),
    names_sep = "_"
  )

df_long

```

**Toques finales**: 

Ten en cuenta que la columna de fecha es actualmente de tipo *carácter* - podemos convertirla fácilmente en tipo fecha utilizando las funciones `mutate()` y `as_date()` descritas en la página [Trabajar con fechas](#working-with-dates-1). 

También podemos convertir la columna de `observation` a un formato `numeric` eliminando el prefijo "obs" y convirtiendo a numérico. Podemos hacer esto con `str_remove_all()` del paquete **stringr** (véase la página [Caracteres y cadenas](#characters-and-strings)).   

```{r}

df_long <- 
  df_long %>% 
  mutate(
    date = date %>% lubridate::as_date(),
    observation = 
      observation %>% 
      str_remove_all("obs") %>% 
      as.numeric()
  )

df_long

```

Y ahora, podemos empezar a trabajar con los datos en este formato, por ejemplo, trazando un mosaico de calor descriptivo: 

```{r}
ggplot(data = df_long, mapping = aes(x = date, y = id, fill = status)) +
  geom_tile(colour = "black") +
  scale_fill_manual(
    values = 
      c("Healthy" = "lightgreen", 
        "Unwell" = "red", 
        "Missing" = "orange")
  )

```





<!-- ======================================================= -->
## De largo a ancho  {#long-to-wide}

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "pivoting", "pivot_wider_new.png"))
```

En algunos casos, es posible que queramos convertir unos datos a un formato ancho. Para ello, podemos utilizar la función `pivot_wider()`. 

Un caso de uso típico es cuando queremos transformar los resultados de un análisis en un formato que sea más digerible para el lector (como una [tabla para su presentación](#tables-for-presentation)). Por lo general, se trata de transformar unos datos en el que la información de un sujeto está repartida en varias filas en un formato en el que esa información se almacena en una sola fila. 

### Datos {.unnumbered}

Para esta sección de la página, utilizaremos la lista de casos (véase la sección [Preparación](#preparation-3)), que contiene una fila por caso. 

Aquí están las primeras 50 filas: 

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Supongamos que queremos conocer los recuentos de individuos en los diferentes grupos de edad, por género: 

```{r}
df_wide <- 
  linelist %>% 
  count(age_cat, gender)

df_wide
```

Esto nos da un largo conjunto de datos que es genial para producir visualizaciones en **ggplot2**, pero no es ideal para la presentación en una tabla: 

```{r}
ggplot(df_wide) +
  geom_col(aes(x = age_cat, y = n, fill = gender))
```

### Pivote ancho {.unnumbered}  
 
Por lo tanto, podemos utilizar `pivot_wider()` para transformar los datos en un formato mejor para incluirlos como tablas en nuestros informes. 

El argumento `names_from` especifica la columna *from* que genera la columna nueva *names*, mientras que el argumento `values_from` especifica la columna *from* de la que tomar los *values* para rellenar las celdas. El argumento `id_cols = ` es opcional, pero se puede proporcionar un vector de nombres de columnas que no deben ser pivotadas, y que por tanto identificarán cada fila. 

```{r}
table_wide <- 
  df_wide %>% 
  pivot_wider(
    id_cols = age_cat,
    names_from = gender,
    values_from = n
  )

table_wide
```

Esta tabla es mucho más fácil de leer y, por tanto, mejor para incluirla en nuestros informes. Se puede convertir en una tabla bonita con varios paquetes, como **flextable** y **knitr**. Este proceso se elabora en la página [Tablas para presentaciones](#tables-for-presentation).  

```{r}
table_wide %>% 
  janitor::adorn_totals(c("row", "col")) %>% # adds row and column totals
  knitr::kable() %>% 
  kableExtra::row_spec(row = 10, bold = TRUE) %>% 
  kableExtra::column_spec(column = 5, bold = TRUE) 
```

---


<!-- ======================================================= -->
## Rellenar {#fill}

En algunas situaciones después de `pivotar`, y más comúnmente después de unir con `bind`, nos quedan huecos en algunas celdas que nos gustaría rellenar. 

<!-- ======================================================= -->
### Datos {.unnumbered}

Por ejemplo, toma dos conjuntos de datos, cada uno con observaciones para el número de medición, el nombre del centro y el recuento de casos en ese momento. Sin embargo, el segundo conjunto de datos también tiene la variable `Year`.

```{r}
df1 <- 
  tibble::tribble(
       ~Measurement, ~Facility, ~Cases,
                  1,  "Hosp 1",     66,
                  2,  "Hosp 1",     26,
                  3,  "Hosp 1",      8,
                  1,  "Hosp 2",     71,
                  2,  "Hosp 2",     62,
                  3,  "Hosp 2",     70,
                  1,  "Hosp 3",     47,
                  2,  "Hosp 3",     70,
                  3,  "Hosp 3",     38,
       )

df1 

df2 <- 
  tibble::tribble(
    ~Year, ~Measurement, ~Facility, ~Cases,
     2000,            1,  "Hosp 4",     82,
     2001,            2,  "Hosp 4",     87,
     2002,            3,  "Hosp 4",     46
  )

df2
```


Cuando realizamos un `bind_rows()` para unir los dos conjuntos de datos, la variable `Year` se rellena con `NA` para aquellas filas en las que no había información previa (es decir, el primer conjunto de datos): 


```{r}
df_combined <- 
  bind_rows(df1, df2) %>% 
  arrange(Measurement, Facility)

df_combined

```

<!-- ======================================================= -->
### `fill()` {.unnumbered}

En este caso, `Year` es una variable útil para incluir, especialmente si queremos explorar las tendencias a lo largo del tiempo. Por lo tanto, utilizamos `fill()` para *rellenar* esas celdas vacías, especificando la columna a rellenar y la dirección (en este caso **hacia arriba**): 

```{r}
df_combined %>% 
  fill(Year, .direction = "up")
```

Alternativamente, podemos reordenar los datos para que tengamos que rellenar en sentido descendente:

```{r}
df_combined <- 
  df_combined %>% 
  arrange(Measurement, desc(Facility))

df_combined

df_combined <- 
  df_combined %>% 
  fill(Year, .direction = "down")

df_combined
```

Ahora tenemos unos datos útiles para representarlos gráficamente: 

```{r}
ggplot(df_combined) +
  aes(Year, Cases, fill = Facility) +
  geom_col()
```

Pero es menos útil para presentarlo en una tabla, así que practiquemos la conversión de este largo y desordenado dataframe en un dataframe ancho y ordenado: 

```{r}
df_combined %>% 
  pivot_wider(
    id_cols = c(Measurement, Facility),
    names_from = "Year",
    values_from = "Cases"
  ) %>% 
  arrange(Facility) %>% 
  janitor::adorn_totals(c("row", "col")) %>% 
  knitr::kable() %>% 
  kableExtra::row_spec(row = 5, bold = TRUE) %>% 
  kableExtra::column_spec(column = 5, bold = TRUE) 
```

N.B. En este caso, tuvimos que especificar que sólo se incluyeran las tres variables `Facility`, `Year`, y `Cases`, ya que la variable adicional `Measurement` interferiría en la creación de la tabla: 

```{r}
df_combined %>% 
  pivot_wider(
    names_from = "Year",
    values_from = "Cases"
  ) %>% 
  knitr::kable()
```

## Recursos {#resources-5}

Aquí hay un [tutorial](https://datacarpentry.org/r-socialsci/03-dplyr-tidyr/index.html) útil 

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/pivoting.Rmd-->


# Agrupar datos {#grouping-data}  


```{r, out.width=c('100%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "Grouping_1500x500.png"))
```

Esta página cubre cómo agrupar y agregar datos para el análisis descriptivo. Hace uso de la familia de paquetes **tidyverse** para funciones comunes y fáciles de usar. 

La agrupación de datos es un componente esencial de la gestión y el análisis de datos. Los datos agrupados se resumen estadísticamente y pueden representarse gráficamente por grupos. Las funciones del paquete **dplyr** (parte del **tidyverse**) facilitan la agrupación y las operaciones posteriores. 

En esta página se tratarán los siguientes temas: 

* Agrupar datos con la función `group_by()` 
* Des-agrupar datos 
* `summarise()` datos agrupados con estadísticas 
* La diferencia entre `count()` y `tally()` 
* `arrange()` aplicada a datos agrupados 
* `filter()` aplicada a datos agrupados 
* `mutate()` aplicada a datos agrupados 
* `select()` aplicada a datos agrupados 
* El comando `aggregate()` de R **base** como alternativa 




<!-- ======================================================= -->
## Preparación {#preparation-4}
     
### Cargar paquetes {.unnumbered}  

Este trozo de código (chunk) muestra la carga de los paquetes necesarios para el análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.   


```{r}
pacman::p_load(
  rio,       # para importar datos
  here,      # para identificar las carpetas donde se encuentran
  tidyverse, # para limpiar, manipular y dibujar los datos (incluye dplyr)
  janitor)   # para añadir totales en las filas y columnas
```




### Importar datos {.unnumbered}
 
Importamos los datos de casos de una epidemia de ébola simulada. Si quieres seguirlo, [clica para descargar linelist "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Los datos se importan mediante la función `import()` del paquete **rio**. Consulta la página sobre [importación y exportación](#import-and-export) para conocer las distintas formas de importar datos. 

```{r, echo=F}
linelist <- rio::import(here("data", "case_linelists", "linelist_cleaned.rds"))
```

#```{r, eval=F}
#linelist <- import("linelist_cleaned.rds")
#```


Las primeras 50 filas de `linelist`: 

```{r message=FALSE, echo=F}
DT::datatable(head(linelist,50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## Agrupar {#grouping}

La función `group_by()` de **dplyr** agrupa las filas por los valores únicos de la columna que se le especifica. Si se especifican varias columnas, las filas se agrupan por las combinaciones únicas de valores entre las columnas. Cada valor único (o combinación de valores) constituye un grupo. Los cambios posteriores en los datos o los cálculos pueden realizarse en el contexto de cada grupo. 

Por ejemplo, el siguiente comando toma `linelist` y agrupa las filas por valores únicos en la columna `outcome`, guardando la salida como un nuevo dataframe `ll_by_outcome`. La(s) columna(s) de agrupación se colocan dentro de los paréntesis de la función `group_by()`.

```{r}
ll_by_outcome <- linelist %>% 
  group_by(outcome)
```

**Ten en cuenta que no hay ningún cambio perceptible en los datos** después de ejecutar `group_by()`, *hasta que* se aplique otro verbo de **dplyr** como `mutate()`, `summarise()`, o `arrange()` en el dataframe "agrupado". 

Sin embargo, puedes "ver" las agrupaciones imprimiendo el dataframe. Al imprimir un dataframe agrupado, verás que se ha transformado en un [objeto de clase `tibble`](https://tibble.tidyverse.org/) que, al imprimirse, muestra qué agrupaciones se han aplicado y cuántos grupos están -escritos justo encima de la fila de cabecera.  

```{r}
# print para ver los grupos que están activos
ll_by_outcome
```


### Grupos únicos {.unnumbered}  

**Los grupos creados reflejan cada combinación única de valores en las columnas de agrupación.** 

Para ver los grupos *y el número de filas en cada grupo*, pasa los datos agrupados a `tally()`. Para ver sólo los grupos únicos sin recuento puedes pasárselos a `group_keys()`. 

Mira a continuación que hay **tres** valores únicos en el resultado de la columna `outcome`: "Death", "Recover", y `NA`. Fíjate que hubo ` nrow(linelist %\>% filter(outcome == "Death"))` muertes,  `nrow(linelist %\>% filter(outcome == "Recover"))` recuperaciones, y  `nrow(linelist %\>% filter(is.na(outcome))` sin resultado registrado. 

```{r}
linelist %>% 
  group_by(outcome) %>% 
  tally()
```


Se puede agrupar por más de una columna. A continuación, el dataframe se agrupa por `outcome` y `gender`, y luego se cuenta. Observa cómo cada combinación única de `outcome` y `gender` se registra como su propio grupo, incluyendo los valores faltantes para cualquier columna.    

```{r}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally()
```

### Columnas nuevas {.unnumbered} 

También puedes crear una nueva columna de agrupación *dentro de* la sentencia `group_by()`. Esto equivale a llamar a `mutate()` antes de `group_by()`. Para una tabulación rápida este estilo puede ser útil, pero para una mayor claridad en el código mejor crear esta columna en su propio paso `mutate()` y luego canalizarla a `group_by()`. 

```{r}
# group dat based on a binary column created *within* the group_by() command
linelist %>% 
  group_by(
    age_class = ifelse(age >= 18, "adult", "child")) %>% 
  tally(sort = T)
```

### Añadir/descartar columnas de agrupación {.unnumbered}  

Por defecto, si ejecutas `group_by()` sobre datos que ya están agrupados, se eliminarán los grupos antiguos y se aplicarán los nuevos. Si deseas añadir nuevos grupos a los existentes, incluye el argumento `.add = TRUE`. 

```{r, eval=F}
# Grouped by outcome
by_outcome <- linelist %>% 
  group_by(outcome)

# Add grouping by gender in addition
by_outcome_gender <- by_outcome %>% 
  group_by(gender, .add = TRUE)
```


** Mantener todos los grupos** 

Si se agrupa en una columna de tipo factor, puede haber niveles del factor que no estén presentes en los datos. Si agrupas en esta columna, por defecto esos niveles no presentes se descartan y no se incluyen como grupos. Para cambiar esto de manera que todos los niveles aparezcan como grupos (incluso si no están presentes en los datos), escribe `.drop = FALSE` en su comando `group_by()`. 


## Des-agrupar  {#un-group}

Los datos que han sido agrupados permanecerán agrupados hasta que sean específicamente desagrupados mediante `ungroup()`. Si se olvida desagrupar, puede dar lugar a cálculos incorrectos. A continuación se muestra un ejemplo de eliminación de todas las agrupaciones: 

```{r, eval=F}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally() %>% 
  ungroup()
```

También puedes eliminar la agrupación sólo para columnas específicas, colocando el nombre de la columna dentro de `ungroup()`.

```{r, eval=F}
linelist %>% 
  group_by(outcome, gender) %>% 
  tally() %>% 
  ungroup(gender) # remove the grouping by gender, leave grouping by outcome
```


<span style="color: black;">***NOTA:*** El verbo `count()` desagrupa automáticamente los datos después del recuento. </span>



## Resumir {#group_summarise} 

Véase la sección **dplyr** de la página [Tablas descriptivas](#descriptive-tables) para una descripción detallada de cómo producir tablas de resumen con `summarise()`. Aquí abordamos brevemente cómo cambia su comportamiento cuando se aplica a datos agrupados. 

La función **dplyr** `summarise()` (o `summarize()`) toma un dataframe y lo convierte en un *nuevo* dataframe de resumen, con columnas que contienen los estadísticos de resumen que definas. En un dataframe sin agrupar, las estadísticas de resumen se calcularán a partir de todas las filas. La aplicación de `summarise()` a los datos agrupados produce esas estadísticas de resumen *para cada grupo*. 

La sintaxis de `summarise()` es tal que se proporciona el nombre de la(s) **nueva(s)** columna(s) de resumen, un signo de igualdad y, a continuación, una función estadística para aplicar a los datos, como se muestra a continuación. Por ejemplo, `min()`, `max()`, `median()`, o `sd()`. Dentro de la función estadística, indica la columna con la que se va a operar y cualquier argumento relevante (por ejemplo, `na.rm = TRUE`). Puedes utilizar `sum()` para contar el número de filas que cumplen un criterio lógico (con doble igual ==). 

A continuación se muestra un ejemplo de `summarise()` aplicado *sin datos agrupados*. Las estadísticas devueltas se producen a partir del set de datos completo. 

```{r}
# summary statistics on ungrouped linelist
linelist %>% 
  summarise(
    n_cases  = n(),
    mean_age = mean(age_years, na.rm=T),
    max_age  = max(age_years, na.rm=T),
    min_age  = min(age_years, na.rm=T),
    n_males  = sum(gender == "m", na.rm=T))
```

Por el contrario, a continuación se muestra la misma sentencia `summarise()` aplicada a los datos agrupados. Las estadísticas se calculan para cada grupo de `outcome`. Observa cómo se trasladan las columnas de agrupación al nuevo dataframe.    

```{r}
# summary statistics on grouped linelist
linelist %>% 
  group_by(outcome) %>% 
  summarise(
    n_cases  = n(),
    mean_age = mean(age_years, na.rm=T),
    max_age  = max(age_years, na.rm=T),
    min_age  = min(age_years, na.rm=T),
    n_males    = sum(gender == "m", na.rm=T))
```

<span style="color: darkgreen;">***SUGERENCIA:*** La función summarise funciona tanto con la ortografía del Reino Unido como con la de EE.UU. - `summarise()` y `summarize()` llaman a la misma función. </span>




## Counts y tallies  {#counts-and-tallies}

`count()` y `tally()` proporcionan una funcionalidad similar pero son diferentes. Lee más sobre la distinción entre `tally()` y `count()` [aquí](https://dplyr.tidyverse.org/reference/tally.html) 

### `tally()` {.unnumbered}  

`tally()` es la abreviatura de `summarise(n = n())`, y *no* agrupa los datos. Por lo tanto, para lograr recuentos agrupados debe seguir un comando `group_by()`. Puedes añadir `sort = TRUE` para ver primero los grupos más grandes.   

```{r}
linelist %>% 
  tally()
```


```{r}
linelist %>% 
  group_by(outcome) %>% 
  tally(sort = TRUE)
```


### `count()`  {.unnumbered}  

En cambio, `count()` hace lo siguiente: 

1.  aplica `group_by()` a la(s) columna(s) especificada(s) 
2.  aplica `summarise()` y devuelve la columna `n` con el número de filas por grupo 
3.  aplica `ungroup()` 

```{r}
linelist %>% 
  count(outcome)
```

Al igual que con `group_by()` puedes crear una nueva columna dentro del comando `count()`:

```{r}
linelist %>% 
  count(age_class = ifelse(age >= 18, "adult", "child"), sort = T)
```


Puedes llamar varias veces a `count()`, con la funcionalidad "combinada". Por ejemplo, para resumir el número de hospitales presentes para cada género, ejecuta lo siguiente. Ten en cuenta que el nombre de la columna final se ha cambiado de "n" por defecto para mayor claridad (con `name  = `). 

```{r}
linelist %>% 
  # produce counts by unique outcome-gender groups
  count(gender, hospital) %>% 
  # gather rows by gender (3) and count number of hospitals per gender (6)
  count(gender, name = "hospitals per gender" ) 
```


### Añadir recuentos {.unnumbered}  

A diferencia de `count()` y `summarise()`, puedes utilizar `add_count()` para *añadir* una nueva columna `n` con los recuentos de filas por grupo *conservando todas las demás columnas del dataframe*. 

Esto significa que el número de recuentos de un grupo, en la nueva columna n, se imprimirá en cada fila del grupo. Para fines de demostración, añadimos esta columna y luego reordenamos las columnas para facilitar la visualización. Consulta la sección siguiente sobre [filtrar por tamaño del grupo](#group_filter_grp_size) para ver otro ejemplo. 


```{r}
linelist %>% 
  as_tibble() %>%                   # convert to tibble for nicer printing 
  add_count(hospital) %>%           # add column n with counts by hospital
  select(hospital, n, everything()) # re-arrange for demo purposes
```



### Añadir totales  {.unnumbered} 

Para añadir fácilmente filas o columnas del *total de la suma* después de utilizar `tally()` o `count()`, consulta la sección de **janitor** de la página [Tablas descriptivas](#tbl_janitor). Este paquete ofrece funciones como `adorn_totals()` y `adorn_percentages()` para añadir totales y convertirlos para mostrar porcentajes. A continuación se muestra un breve ejemplo: 

```{r}
linelist %>%                                  # case linelist
  tabyl(age_cat, gender) %>%                  # cross-tabulate counts of two columns
  adorn_totals(where = "row") %>%             # add a total row
  adorn_percentages(denominator = "col") %>%  # convert to proportions with column denominator
  adorn_pct_formatting() %>%                  # convert proportions to percents
  adorn_ns(position = "front") %>%            # display as: "count (percent)"
  adorn_title(                                # adjust titles
    row_name = "Age Category",
    col_name = "Gender")
```


Para añadir filas de totales más complejas que incluyan estadísticas de resumen distintas de las *sumas*, consulta [esta sección de la página Tablas descriptivas](#tbl_dplyr_totals). 



## Agrupar por fechas  {#grouping-by-date}

Al agrupar datos por fecha, debes tener (o crear) una columna para la unidad de fecha de interés - por ejemplo "día", "epiweek", "mes", etc. Puedes crear esta columna utilizando `floor_date()` de **lubridate**, como se explica en la [sección Semanas epidemiológicas](#dates_epi_wks) de la página **Trabajar con fechas**. Una vez que tengas esta columna, puedes utilizar `count()` de **dplyr** para agrupar las filas por esos valores de fecha únicos y lograr recuentos agregados. 

Un paso adicional común para las situaciones de fechas, es "rellenar" cualquier fecha en la que no haya  datos. Utiliza `complete()` de **tidyr** para que la serie de fechas agregadas esté *completa*, incluyendo *todas las unidades de fecha posibles* dentro del rango. Sin este paso, una semana sin casos reportados podría no aparecer en tus datos. 

Dentro de `complete()` *redefine* la columna de fecha como una *secuencia* de fechas `seq.Date()` desde el mínimo hasta el máximo - así las fechas se expanden. Por defecto, los valores del recuento de casos en cualquier nueva fila "expandida" serán `NA`. Puedes establecerlos a 0 utilizando el argumento `fill =` de `complete()`, que espera una lista con nombre (si la columna de recuentos se llama `n`, escribe `fill = list(n = 0)`. Consulta `?complete` para obtener más detalles y la página [Trabajar con fechas](#dates_epi_wks) para ver un ejemplo. 



### Casos por día  {.unnumbered}  

Aquí hay un ejemplo de agrupación de casos en días *sin* usar `complete()`. Obsérvese que las primeras filas omiten las fechas sin casos. 

```{r}
daily_counts <- linelist %>% 
  drop_na(date_onset) %>%        # remove that were missing date_onset
  count(date_onset)              # count number of rows per unique date
```

```{r message=FALSE, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

A continuación añadimos el comando `complete()` para asegurarnos de que todos los días del rango están representados. 

```{r, eval=F}
daily_counts <- linelist %>% 
  drop_na(date_onset) %>%                 # remove case missing date_onset
  count(date_onset) %>%                   # count number of rows per unique date
  complete(                               # ensure all days appear even if no cases
    date_onset = seq.Date(                # re-define date colume as daily sequence of dates
      from = min(date_onset, na.rm=T), 
      to = max(date_onset, na.rm=T),
      by = "day"),
    fill = list(n = 0))                   # set new filled-in rows to display 0 in column n (not NA as default) 
```

```{r message=FALSE, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Casos por semana  {.unnumbered}  

Se puede aplicar el mismo principio para las semanas. Primero crea una nueva columna que sea la semana del caso utilizando `floor_date()` con `unit = "week"`. A continuación, utiliza `count()` como en el caso anterior para obtener los recuentos de casos semanales. Termina con `complete()` para asegurarte de que todas las semanas están representadas, incluso si no contienen casos. 

```{r}
# Make dataset of weekly case counts
weekly_counts <- linelist %>% 
  drop_na(date_onset) %>%                 # remove cases missing date_onset
  mutate(week = lubridate::floor_date(date_onset, unit = "week")) %>%  # new column of week of onset
  count(week) %>%                         # group data by week and count rows per group
  complete(                               # ensure all days appear even if no cases
    week = seq.Date(                      # re-define date colume as daily sequence of dates
      from = min(week, na.rm=T), 
      to = max(week, na.rm=T),
      by = "week"),
    fill = list(n = 0))                   # set new filled-in rows to display 0 in column n (not NA as default) 
```

Aquí están las primeras 50 filas del dataframe resultante: 

```{r message=FALSE, echo=F}
DT::datatable(weekly_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Casos por mes  {.unnumbered}

Para agregar casos en meses, vuelve a utilizar `floor_date()` del paquete **lubridate**, pero con el argumento `unit = "months"`. Esto redondea cada fecha hacia abajo al día 1 de su mes. La salida será el tipo Date. Ten en cuenta que en el paso `complete()` también utilizamos `by = "months"`. 


```{r}
# Make dataset of monthly case counts
monthly_counts <- linelist %>% 
  drop_na(date_onset) %>% 
  mutate(month = lubridate::floor_date(date_onset, unit = "months")) %>%  # new column, 1st of month of onset
  count(month) %>%                          # count cases by month
  complete(
    month = seq.Date(
      min(month, na.rm=T),     # include all months with no cases reported
      max(month, na.rm=T),
      by="month"),
    fill = list(n = 0))
```

```{r message=FALSE, echo=F}
DT::datatable(monthly_counts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Recuentos diarios en semanas {.unnumbered}

Para agregar los recuentos diarios en recuentos semanales, utiliza `floor_date()` igual queo arriba. Sin embargo, utiliza `group_by()` y `summarize()` en lugar de `count()` porque necesita `sum()` los recuentos de casos diarios en lugar de limitarse a contar el número de filas por semana. 


#### Daily counts into months {.unnumbered}

Para agregar los recuentos diarios en recuentos por meses, utiliza `floor_date()` con  `unit = "month"` como en el caso anterior. Sin embargo, utiliza `group_by()` y `summarize()` en lugar de `count()` porque necesita`sum()`los recuentos de casos diarios en lugar de limitarse a contar el número de filas por mes. 




## Ordenar los datos agrupados  {#arranging-grouped-data}

El verbo `arrange()` de **dplyr** para ordenar las filas de un dataframe se comporta igual cuando los datos están agrupados, *a menos que se* establezca el argumento `.by_group =TRUE`. En este caso, las filas se ordenan primero por las columnas de agrupación y luego por cualquier otra columna que se especifique en `arrange()`. 



## Filtrar sobre datos agrupados  {#filter-on-grouped-data}

### `filter()` {.unnumbered}

Cuando se aplica junto con funciones que evalúan el dataframe (como `max()`, `min()`, `mean()`), estas funciones se aplicarán ahora a los grupos. Por ejemplo, si deseas filtrar y mantener las filas en las que los pacientes están por encima de la edad media, esto se aplicará ahora por grupo, filtrando para mantener las filas por encima de la edad media del *grupo*. 



### Clasificar filas por grupo{.unnumbered} 


La función `slice()` de **dplyr**, que [filtra las filas según su posición](https://dplyr.tidyverse.org/reference/slice.html) en los datos, también puede aplicarse por grupo. Recuerda que debes tener en cuenta la ordenación de los datos dentro de cada grupo para obtener la "rebanada" deseada. 

Por ejemplo, para recuperar sólo los últimos 5 ingresos de cada hospital: 

1.  Agrupar linelist por columna  `hospital`  
2.  Ordenar los registros por `date_hospitalisation` de más reciente a la más antigua *dentro de cada grupo de hospitales* 
3.  Clasificar para recuperar las 5 primeras filas de cada hospital 

```{r,}
linelist %>%
  group_by(hospital) %>%
  arrange(hospital, date_hospitalisation) %>%
  slice_head(n = 5) %>% 
  arrange(hospital) %>%                            # for display
  select(case_id, hospital, date_hospitalisation)  # for display
```

`slice_head()` - selecciona n filas de la parte superior
`slice_tail()` - selecciona n filas del final
`slice_sample()` - selecciona aleatoriamente n filas
`slice_min()` - selecciona n filas con los valores más altos en `order_by = `columna, usa `with_ties = TRUE` para mantener los empates `slice_max()` - selecciona n filas con los valores más bajos en `order_by = `columna, utiliza `with_ties = TRUE` para mantener los empates 

Consulta la página de [De-duplicación](#de-duplication) para ver más ejemplos y detalles sobre `slice()`. 




### Filtro por tamaño de grupo {#group_filter_grp_size .unnumbered} 

La función `add_count()` añade una columna `n` a los datos originales dando el número de filas en el grupo de esa fila. 

A continuación, `add_count()` se aplica a la columna `hospital`, por lo que los valores de la nueva columna `n` reflejan el número de filas del grupo de hospitales de esa fila. Observe cómo se repiten los valores de la columna `n`. En el ejemplo siguiente, el nombre de la columna n podría cambiarse utilizando name = dentro de add_count(). Para fines de demostración reordenamos las columnas con `select()`. 


```{r}
linelist %>% 
  as_tibble() %>% 
  add_count(hospital) %>%          # add "number of rows admitted to same hospital as this row" 
  select(hospital, n, everything())
```

De este modo, resulta fácil filtrar los casos que fueron hospitalizados en un hospital "pequeño", por ejemplo, un hospital que admitió a menos de 500 pacientes: 

```{r, eval=F}
linelist %>% 
  add_count(hospital) %>% 
  filter(n < 500)
```





## Mutate con datos agrupados   {#mutate-on-grouped-data}

Para conservar todas las columnas y filas (no resumir) y *añadir una nueva columna que contenga estadísticas de grupo*, utiliza `mutate()` después de `group_by()` en lugar de `summarise()`. 

Esto es útil si se desea obtener estadísticas de grupo en los datos originales *con todas las demás columnas presentes*, por ejemplo, para los cálculos que comparan una fila con su grupo. 

Por ejemplo, este código calcula la diferencia entre la demora en el ingreso de una fila y la demora media de su hospital. Los pasos son: 

1.  Agrupar los datos por hospital 
2.  Utiliza la columna `days_onset_hosp` (retraso hasta la hospitalización) para crear una nueva columna que contenga el retraso medio en el hospital de *esa fila* 
3.  Calcular la diferencia entre las dos columnas 

Seleccionamos (`select()`) sólo ciertas columnas para mostrarlas, con fines de demostración. 

```{r}
linelist %>% 
  # group data by hospital (no change to linelist yet)
  group_by(hospital) %>% 
  
  # new columns
  mutate(
    # mean days to admission per hospital (rounded to 1 decimal)
    group_delay_admit = round(mean(days_onset_hosp, na.rm=T), 1),
    
    # difference between row's delay and mean delay at their hospital (rounded to 1 decimal)
    diff_to_group     = round(days_onset_hosp - group_delay_admit, 1)) %>%
  
  # select certain rows only - for demonstration/viewing purposes
  select(case_id, hospital, days_onset_hosp, group_delay_admit, diff_to_group)
```



## Seleccionar sobre datos agrupados   {#select-on-grouped-data}

El verbo `select()` funciona con datos agrupados, pero las columnas de agrupación siempre se incluyen (aunque no se mencionen en `select()`). Si no deseas estas columnas de agrupación, utiliza primero `ungroup()`.  










<!-- ======================================================= -->
## Recursos {#resources-5}

A continuación, algunos recursos útiles para obtener más información: 

Puedes realizar cualquier función de resumen sobre datos agrupados; consulta la [hoja de trucos de transformación de datos de RStudio](https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf) 

La página de Data Carpentry sobre [**dplyr**](https://datacarpentry.org/R-genomics/04-dplyr.html) 

Las páginas de referencia de **tidyverse** sobre [group_by()](https://dplyr.tidyverse.org/reference/group_by.html) y [agrupación](https://dplyr.tidyverse.org/articles/grouping.html) 

Esta página sobre [Manipulación de datos](https://itsalocke.com/files/DataManipulationinR.pdf) 

[Resumir con condiciones en dplyr](https://stackoverflow.com/questions/23528862/summarize-with-conditions-in-dplyr) 
 






```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/grouping.Rmd-->


# Unir datos {#joining-data}  

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "left-join.gif"))
```

*Arriba: un ejemplo animado de una unión por la izquierda ([fuente de la imagen](https://github.com/gadenbuie/tidyexplain/tree/master/images))* 

Esta página describe diferentes formas de unir datos ("join", "match", "link", "bind") y combinar dataframes.


<!-- ======================================================= -->
## Preparación  {#preparation-5}

### Cargar paquetes  {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puede cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R. 

```{r}
pacman::p_load(
  rio,            # import and export
  here,           # locate files 
  tidyverse,      # data management and visualisation
  RecordLinkage,  # probabilistic matches
  fastLink        # probabilistic matches
)
```



### Importar datos  {.unnumbered}

Para empezar, importamos la lista de casos limpiada de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica aquí para descargar el listado "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa los datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - mira la página de [importación y exportación](#import-and-export) para más detalles). 

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas del listado. 

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




<!-- ======================================================= -->
### Datos de los ejemplos {.unnumbered}

En la sección de unión que sigue, utilizaremos los siguientes  datos: 

1.  Una versión "en miniatura" de casos de `linelist`, que contiene sólo las columnas `case_id`, `date_onset`, y `hospital`, y sólo las 10 primeras filas 
2.  Un dataframe separado llamado `hosp_info`, que contiene más detalles sobre cada hospital 

En la sección sobre el emparejamiento probabilístico, utilizaremos dos pequeños conjuntos de datos diferentes. El código para crear esos conjuntos de datos se da en esa sección.  




#### "Miniatura" de casos de linelist {#joins_llmini .unnumbered}  

A continuación se muestra la lista de casos en miniatura, que contiene sólo 10 filas y sólo las columnas `case_id`, `date_onset`, y `hospital`.

```{r}
linelist_mini <- linelist %>%                 # start with original linelist
  select(case_id, date_onset, hospital) %>%   # select columns
  head(10)                                    # only take the first 10 rows
```

```{r message=FALSE, echo=F}
DT::datatable(linelist_mini, rownames = FALSE, options = list(pageLength = nrow(10)))
```




#### dataframe de información hospitalaria  {#joins_hosp_info .unnumbered}  

A continuación se muestra el código para crear un dataframe separado con información adicional sobre siete hospitales (la población de captación y el nivel de atención disponible). Obsérvese que el nombre "Hospital Militar" pertenece a dos hospitales diferentes: uno de nivel primario que atiende a 10000 residentes y otro de nivel secundario que atiende a 50280 residentes.  

```{r}
# Make the hospital information data frame
hosp_info = data.frame(
  hosp_name     = c("central hospital", "military", "military", "port", "St. Mark's", "ignace", "sisters"),
  catchment_pop = c(1950280, 40500, 10000, 50280, 12000, 5000, 4200),
  level         = c("Tertiary", "Secondary", "Primary", "Secondary", "Secondary", "Primary", "Primary")
)
```

Aquí está este dataframe:  

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(hosp_info, rownames = FALSE, options = list(pageLength = nrow(hosp_info)))
```





<!-- ======================================================= -->
### Pre-limpieza  {.unnumbered}

Las uniones tradicionales (no probabilísticas) distinguen entre mayúsculas y minúsculas y requieren coincidencias de caracteres exactas entre los valores de los dos dataframes. Para mostrar algunos de los pasos de limpieza que puedes necesitar antes de iniciar una unión, ahora limpiaremos y alinearemos los datos `linelist_mini` y `hosp_info`. 

**Identificar las diferencias** 

Necesitamos que los valores de la columna `hosp_name` en el dataframe `hosp_info` coincidan con los valores de la columna `hospital` en el dataframe `linelist_mini`. 

Aquí están los valores del dataframe `linelist_mini`, impresos con la función de R **base** `unique()`: 

```{r}
unique(linelist_mini$hospital)
```

y aquí están los valores del dataframe `hosp_info`: 

```{r}
unique(hosp_info$hosp_name)
```

Puedes ver que, aunque algunos de los hospitales existen en ambos dataframes, hay muchas diferencias en la ortografía. 



**Alinear los valores** 

Comenzamos limpiando los valores del dataframe `hosp_info`. Como se explica en la página [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions), podemos recodificar los valores con criterios lógicos utilizando la función `case_when()` de **dplyr**. Para los cuatro hospitales que existen en ambos dataframes, cambiamos los valores para alinearlos con los valores de `linelist_mini`. Para los demás hospitales dejamos los valores como están (`TRUE ~ hosp_name`).  

<span style="color: orange;">***PRECAUCIÓN:*** Normalmente, al limpiar se debe crear una nueva columna (por ejemplo, `hosp_name_clean`), pero para facilitar la demostración mostramos la modificación de la antigua columna </span>

```{r}
hosp_info <- hosp_info %>% 
  mutate(
    hosp_name = case_when(
      # criteria                         # new value
      hosp_name == "military"          ~ "Military Hospital",
      hosp_name == "port"              ~ "Port Hospital",
      hosp_name == "St. Mark's"        ~ "St. Mark's Maternity Hospital (SMMH)",
      hosp_name == "central hospital"  ~ "Central Hospital",
      TRUE                             ~ hosp_name
      )
    )
```

Los nombres de los hospitales que aparecen en ambos dataframes están alineados. Hay dos hospitales en `hosp_info` que no están presentes en `linelist_mini` - nos ocuparemos de ellos más adelante, en la unión. 

```{r}
unique(hosp_info$hosp_name)
```

Antes de una unión, a menudo es más fácil convertir en una columna todas a minúsculas o todas a mayúsculas. Si necesitas convertir todos los valores de una columna a MAYÚSCULAS o minúsculas, utiliza `mutate()` y envuelva la columna con una de estas funciones de **stringr**, como se muestra en la página sobre [Caracteres y cadenas](#characters-and-strings).  

`str_to_upper()`  
`str_to_upper()`  
`str_to_title()`  




<!-- ======================================================= -->
## Uniones en **dplyr**  {#dplyr-joins}

El paquete **dplyr** ofrece varias funciones de unión. **dplyr** está incluido en el paquete **tidyverse**. Estas funciones de unión se describen a continuación, con casos de uso sencillos. 

Muchas gracias a [https://github.com/gadenbuie](https://github.com/gadenbuie/tidyexplain/tree/master/images) por los gifs informativos.  




<!-- ======================================================= -->
### Sintaxis general  {.unnumbered}

Los comandos de unión pueden ejecutarse como comandos independientes para unir dos dataframes en un nuevo objeto, o pueden utilizarse dentro de una cadena de pipes (`%>%`) para fusionar un dataframe en otro mientras se limpia o se modifica de alguna manera. 

En el siguiente ejemplo, la función `left_join()` se utiliza como un comando independiente para crear un nuevo dataframe `joined_data`. Las entradas son los dataframes 1 y 2 (`df1` y `df2`). El primer dataframe es el dataframe de referencia, y el segundo se une *a* él. 

El tercer argumento `by = ` es donde se especifican las columnas de cada dataframe que se utilizarán para alinear las filas de los dos dataframes. Si los nombres de estas columnas son diferentes, proporciónelos dentro de un vector `c()` como se muestra a continuación, donde las filas se emparejan sobre la base de valores comunes entre la columna `ID` en `df1` y la columna `identifier` en `df2`. 

```{r, eval=F}
# Join based on common values between column "ID" (first data frame) and column "identifier" (second data frame)
joined_data <- left_join(df1, df2, by = c("ID" = "identifier"))
```

Si las columnas `by` de ambos dataframes tienen exactamente el mismo nombre, puedes proporcionar sólo este nombre, entre comillas. 

```{r, eval=F}
# Joint based on common values in column "ID" in both data frames
joined_data <- left_join(df1, df2, by = "ID")
```

Si estás uniendo los dataframes basándote en valores comunes en varios campos, enumera estos campos dentro del vector `c()`. Este ejemplo une filas si los valores de tres columnas de cada conjunto de datos se alinean exactamente.  

```{r, eval=F}
# join based on same first name, last name, and age
joined_data <- left_join(df1, df2, by = c("name" = "firstname", "surname" = "lastname", "Age" = "age"))
```


Los comandos de unión también pueden ejecutarse dentro de una cadena de pipes. Esto modificará el dataframe que se está canalizando. 

En el ejemplo siguiente, `df1` se pasa por los pipes, `df2` se une a él y, por tanto, `df `se modifica y se redefine.   

```{r eval=F}
df1 <- df1 %>%
  filter(date_onset < as.Date("2020-03-05")) %>% # miscellaneous cleaning 
  left_join(df2, by = c("ID" = "identifier"))    # join df2 to df1
```


<span style="color: orange;">***ATENCIÓN:*** ¡Las uniones son específicas para cada caso! Por lo tanto, es útil convertir todos los valores a minúsculas o mayúsculas antes de la unión. Consulta la página sobre caracteres/cadenas. </span>





<!-- ======================================================= -->
### Uniones izquierda y derecha {.unnumbered}  

**Una unión a la izquierda o a la derecha se utiliza habitualmente para añadir información a un dataframe**: la nueva información se añade sólo a las filas que ya existían en el dataframe de referencia. Estas uniones son comunes en el trabajo epidemiológico, ya que se utilizan para añadir información de unos datos a otro. 

Al utilizar estas uniones, el orden de escritura de los dataframes en el comando es importante*. 

* En una *unión a la izquierda*, el *primer* dataframe escrito es el de base 
* En una *unión a la derecha*, el *segundo* dataframe escrito es el de base 

**Se conservan todas las filas del dataframe de referencia.** La información del otro dataframe (secundario) se une al dataframe de referencia *sólo si hay una coincidencia a través de la(s) columna(s) del identificador*. Además: 

* Las filas del dataframe secundario que no coinciden se eliminan. 
* Si hay muchas filas de la línea *de* base que coinciden con una fila del dataframe secundarios (muchos a uno), la información secundaria se añade a *cada fila de la línea de base que coincide*. 
* Si una fila del de base coincide con varias filas del dataframe secundario (uno a varios), se dan todas las combinaciones, lo que significa que *se pueden añadir nuevas filas al dataframe devuelto.* 

Ejemplos animados de uniones a la izquierda y a la derecha ([fuente de la imagen](https://github.com/gadenbuie/tidyexplain/tree/master/images)) 

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "left-join.gif"))
knitr::include_graphics(here::here("images", "right-join.gif"))
```

**Ejemplo** 

A continuación se muestra el resultado de un `left_join()` de `hosp_info` (dataframe secundario, [ver aquí](#joins_hosp_info)) *en* `linelist_mini` (dataframe de referencia, [ver aquí](#joins_llmini)). `linelist_mini` original tiene filas `nrow(linelist_mini)`. Se muestra `linelist_mini` modificada. Observa lo siguiente: 

* Se han añadido dos nuevas columnas, `catchment_pop` y `level` en la parte izquierda de `linelist_mini` 
* Se mantienen todas las filas originales del dataframe de referencia `linelist_mini` 
* Cualquier fila original de `linelist_mini` para "Hospital Militar" está duplicada porque coincide con *dos* filas en el dataframe secundario, por lo que se devuelven ambas combinaciones 
* La columna del identificador de la unión del set de datos secundario (`hosp_name`) ha desaparecido porque es redundante con la columna del identificador primario (`hospital`) 
* Cuando una fila de referencia no coincide con ninguna fila secundaria (por ejemplo, cuando el `hospital` is "Other" or "Missing"), `NA` (en blanco) rellena las columnas del dataframe secundario 
* Se eliminaron las filas del dataframe secundario que no coincidían con el dataframe de referencia (hospitales "sisters" e "ignace") 


```{r, eval=F}
linelist_mini %>% 
  left_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  left_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```





#### "¿Debo usar una unión a la derecha o a la izquierda?"  {.unnumbered}  

Para responder a la pregunta anterior, hay que tener claro "¿qué dataframe debe conservar todas sus filas?" - Utiliza éste como base. Una unión *a la izquierda* conserva todas las filas del primer dataframe escrito en el comando, mientras que una *unión a la derecha* conserva todas las filas del segundo dataframe. 

Los dos comandos de abajo consiguen el mismo resultado - 10 filas de `hosp_info` unidas *en* base a `linelist_mini`, pero utilizan diferentes uniones. El resultado es que el orden de las columnas variará en función de si `hosp_info` llega por la derecha (en la unión izquierda) o llega por la izquierda (en la unión derecha). El orden de las filas también puede cambiar en consecuencia. Pero ambas consecuencias pueden ser tratadas posteriormente, utilizando `select()` para reordenar las columnas o `arrange()` para ordenar las filas. 

```{r, eval=F}
# The two commands below achieve the same data, but with differently ordered rows and columns
left_join(linelist_mini, hosp_info, by = c("hospital" = "hosp_name"))
right_join(hosp_info, linelist_mini, by = c("hosp_name" = "hospital"))
```

Este es el resultado de `hosp_info` en `linelist_mini` a través de una unión a la izquierda (nuevas columnas entrando por la derecha) 

```{r message=FALSE, echo=F}
left_join(linelist_mini, hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```

Este es el resultado de `hosp_info` en `linelist_mini` a través de una unión a la derecha (nuevas columnas entrando desde la izquierda)  

```{r message=FALSE, echo=F}
right_join(hosp_info, linelist_mini, by = c("hosp_name" = "hospital")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 11))
```

Considera también si tu caso de uso está dentro de una cadena de pipes (`%>%`). Si los datos del pipe son la base, es probable que utilices una unión izquierda para añadir datos a ella. 


<!-- ======================================================= -->
### Unión completa {.unnumbered} 


**Una unión completa (Full join) es la más *inclusiva* de las uniones**: devuelve todas las filas de ambos dataframes. 

Si hay filas presentes en una y no en la otra (donde no se encontró ninguna coincidencia), el dataframe las incluirá y se hará más largo. Los valores faltantes `NA` se utilizan para rellenar los huecos creados. A medida que se une, observa el número de columnas y filas con cuidado para solucionar el problema de las coincidencias de mayúsculas y minúsculas y de los caracteres exactos. 

El dataframe de "base" es el que se escribe primero en el comando. El ajuste de esto no afectará a los registros devueltos por la unión, pero puede afectar al orden de las columnas resultantes, al orden de las filas y a las columnas de los identificadores que se conservan. 

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "full-join.gif"))
```

Ejemplo animado de una unión completa ([fuente de la imagen](https://github.com/gadenbuie/tidyexplain/tree/master/images)) 

**Ejemplo**  

`A continuación se muestra la salida de` un `full_join()` de `hosp_info` (originalmente ` nrow(hosp_info)`, [view here](#joins_hosp_info))  *into* `linelist_mini` (originalmente ` nrow(linelist_mini)`, [view here](#joins_llmini)). Nota lo siguiente:  

* Se mantienen todas las filas de la base (`linelist_mini`)
* Se conservan las filas de los datos secundarios que no coinciden con la de base ("ignace" y "sisters"), con los valores de las columnas correspondientes de la de base `case_id` y `onset` rellenados con los valores que faltan  
* Del mismo modo, se conservan las filas de los datos de referencia que no coinciden con el secundario ("Otros" y "Falta"), y las columnas secundarias `catchment_pop` y `level` se rellenan con los valores que faltan 
* En el caso de coincidencias de uno a muchos o de muchos a uno (por ejemplo, filas para "Hospital Militar"), se devuelven todas las combinaciones posibles (alargando el conjunto de datos final)  
* Sólo se mantiene la columna del identificador de la línea de base (`hospital`)   


```{r, eval=F}
linelist_mini %>% 
  full_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  full_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 15))
```


<!-- ======================================================= -->
### Unión interna {.unnumbered} 

**Una unión interna es la más *restrictiva* de las uniones**: sólo devuelve las filas que coinciden en ambos dataframes.
Esto significa que el número de filas en el dataframe de referencia puede *reducirse*. El ajuste de qué dataframe es el de "base" (escrito en primer lugar en la función) no afectará a las filas que se devuelven, pero sí al orden de las columnas, al orden de las filas y a las columnas de los identificadores que se conservan. 


```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "inner-join.gif"))
```

Ejemplo animado de una unión interna ([fuente de la imagen](https://github.com/gadenbuie/tidyexplain/tree/master/images)) 


**Ejemplo** 

A continuación se muestra la salida de un `inner_join()` de `linelist_mini` (base) con `hosp_info` (secundario). Observa lo siguiente: 

* Se eliminan las filas del de base que no coinciden con los datos secundarios (filas en las que el hospital es "Missing" u "Other") * Asimismo, se eliminan las filas del dataframe secundario que no tenían ninguna coincidencia en la de base (filas en las que `hosp_name` es "sisters" o "ignace") 
* Sólo se conserva la columna del identificador del de base (`hospital`)   


```{r, eval=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name"))
```


```{r message=FALSE, echo=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 12))
```






<!-- ======================================================= -->
### Semi-unión  {.unnumbered} 

Una semi-unión join es una "unión filtrada" que utiliza otro conjunto de datos *no para añadir filas o columnas, sino para realizar un filtrado*. 

Un **semi-join mantiene todas las observaciones en el dataframe de referencia que tienen una coincidencia con el dataframe secundario** (pero no añade nuevas columnas ni duplica ninguna fila para las coincidencias múltiples). Lee más sobre estas uniones de "filtrado" [aquí](https://towardsdatascience.com/level-up-with-semi-joins-in-r-a068426096e0). 

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "semi-join.gif"))
```

Ejemplo animado de una semiunión ([fuente de la imagen](https://github.com/gadenbuie/tidyexplain/tree/master/images)) 

Como ejemplo, el siguiente código devuelve las filas del dataframe `hosp_info` que tienen coincidencias en `linelist_mini` basadas en el nombre del hospital. 

```{r}
hosp_info %>% 
  semi_join(linelist_mini, by = c("hosp_name" = "hospital"))
```



<!-- ======================================================= -->
### Anti unión {.unnumbered} 

**La anti unión es otra "unión filtrada" que devuelve las filas del dataframe de referencia que *no* tienen una coincidencia en el dataframe secundario.** 

Lee más sobre el filtrado de las uniones [aquí](https://towardsdatascience.com/level-up-with-semi-joins-in-r-a068426096e0). 

Los anti-join son útiles para la identificación de registros que no están presentes en otro dataframe, la solución de problemas de ortografía en un join (revisión de registros que *deberían haber* coincidido) y el examen de registros que fueron excluidos después de otro join. 

**Al igual que con `right_join()` y `left_join()`, el dataframe de *base* (que aparece primero) es importante**. Las filas devueltas son sólo las del dataframe de referencia. Observa en el siguiente gif que la fila del dataframe secundario (fila púrpura 4) no se devuelve a pesar de que no coincide con la línea de base. 

```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "anti-join.gif"))
```

Ejemplo animado de una anti-unión ([fuente de la imagen](https://github.com/gadenbuie/tidyexplain/tree/master/images)) 


#### Ejemplo de `anti_join()` sencillo {.unnumbered}  

Para un ejemplo sencillo, encontremos los hospitales de `hosp_info` que no tienen ningún caso en `linelist_mini`. Enumeramos primero `hosp_info`, como dataframe de referencia. Se devuelven los hospitales que no están presentes en `linelist_mini`. 

```{r, eval=F}
hosp_info %>% 
  anti_join(linelist_mini, by = c("hosp_name" = "hospital"))
```

```{r message=FALSE, echo=F}
hosp_info %>% 
  anti_join(linelist_mini, by = c("hosp_name" = "hospital")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 12))
```


#### Ejemplo de `anti_join()` complejo {.unnumbered}  

Para otro ejemplo, digamos que ejecutamos un `inner_join()` entre `linelist_mini` y `hosp_info`. Esto devuelve sólo un subconjunto de los registros originales de `linelist_mini`, ya que algunos no están presentes en `hosp_info`. 

```{r, eval=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  inner_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 8))
```

Para revisar los registros de `linelist_mini` que fueron excluidos durante el inner join, podemos ejecutar un anti-join con la misma configuración (linelist_mini como base).  

```{r, eval = F}
linelist_mini %>% 
  anti_join(hosp_info, by = c("hospital" = "hosp_name"))
```

```{r message=FALSE, echo=F}
linelist_mini %>% 
  anti_join(hosp_info, by = c("hospital" = "hosp_name")) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 8))
```


Para ver los registros de `hosp_info` que se excluyeron en la unión interna, también podríamos ejecutar una anti unión con `hosp_info` como dataframe de referencia.   



<!-- ======================================================= -->
## Emparejamiento probabilístico {#probabalistic-matching}

Si no dispones de un identificador único común a todos los conjuntos de datos para unirlos, considera  la posibilidad de utilizar un algoritmo de coincidencia probabilística. Este algoritmo buscaría coincidencias entre los registros basándose en la similitud (por ejemplo, la distancia de cadena de Jaro-Winkler o la distancia numérica). A continuación se muestra un ejemplo sencillo utilizando el paquete **fastLink** . 

**Cargar paquetes** 

```{r}
pacman::p_load(
  tidyverse,      # data manipulation and visualization
  fastLink        # record matching
  )
```


A continuación se presentan dos pequeños conjuntos de datos de ejemplo que utilizaremos para demostrar la correspondencia probabilística (`cases` y `test_results`): 

Aquí está el código utilizado para hacer estos conjuntos de datos: 


```{r}
# make datasets

cases <- tribble(
  ~gender, ~first,      ~middle,     ~last,        ~yr,   ~mon, ~day, ~district,
  "M",     "Amir",      NA,          "Khan",       1989,  11,   22,   "River",
  "M",     "Anthony",   "B.",        "Smith",      1970, 09, 19,      "River", 
  "F",     "Marialisa", "Contreras", "Rodrigues",  1972, 04, 15,      "River",
  "F",     "Elizabeth", "Casteel",   "Chase",      1954, 03, 03,      "City",
  "M",     "Jose",      "Sanchez",   "Lopez",      1996, 01, 06,      "City",
  "F",     "Cassidy",   "Jones",      "Davis",     1980, 07, 20,      "City",
  "M",     "Michael",   "Murphy",     "O'Calaghan",1969, 04, 12,      "Rural", 
  "M",     "Oliver",    "Laurent",    "De Bordow" , 1971, 02, 04,     "River",
  "F",      "Blessing",  NA,          "Adebayo",   1955,  02, 14,     "Rural"
)

results <- tribble(
  ~gender,  ~first,     ~middle,     ~last,          ~yr, ~mon, ~day, ~district, ~result,
  "M",      "Amir",     NA,          "Khan",         1989, 11,   22,  "River", "positive",
  "M",      "Tony",   "B",         "Smith",          1970, 09,   19,  "River", "positive",
  "F",      "Maria",    "Contreras", "Rodriguez",    1972, 04,   15,  "Cty",   "negative",
  "F",      "Betty",    "Castel",   "Chase",        1954,  03,   30,  "City",  "positive",
  "F",      "Andrea",   NA,          "Kumaraswamy",  2001, 01,   05,  "Rural", "positive",      
  "F",      "Caroline", NA,          "Wang",         1988, 12,   11,  "Rural", "negative",
  "F",      "Trang",    NA,          "Nguyen",       1981, 06,   10,  "Rural", "positive",
  "M",      "Olivier" , "Laurent",   "De Bordeaux",  NA,   NA,   NA,  "River", "positive",
  "M",      "Mike",     "Murphy",    "O'Callaghan",  1969, 04,   12,  "Rural", "negative",
  "F",      "Cassidy",  "Jones",     "Davis",        1980, 07,   02,  "City",  "positive",
  "M",      "Mohammad", NA,          "Ali",          1942, 01,   17,  "City",  "negative",
  NA,       "Jose",     "Sanchez",   "Lopez",        1995, 01,   06,  "City",  "negative",
  "M",      "Abubakar", NA,          "Abullahi",     1960, 01,   01,  "River", "positive",
  "F",      "Maria",    "Salinas",   "Contreras",    1955, 03,   03,  "River", "positive"
  )

```


**El dataset `cases` tiene 9 registros** de pacientes que están a la espera de los resultados de las pruebas. 

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(cases, rownames = FALSE, options = list(pageLength = nrow(cases), scrollX=T), class = 'white-space: nowrap')
```



**El set de datos `test_results`** tiene 14 registros y contiene la columna resultado, que queremos añadir a los registros en `cases` basado en la coincidencia probabilística de registros.  

```{r message=FALSE, echo=F}
# display the hospital data as a table
DT::datatable(results, rownames = FALSE, options = list(pageLength = nrow(results), scrollX=T), class = 'white-space: nowrap')
```

### Correspondencia probabilística  {.unnumbered}  

La función `fastLink()` del paquete **fastLink** puede utilizarse para aplicar un algoritmo de coincidencia. Esta es la información básica. Puedes leer más detalles escribiendo `?fastLink` en tu consola. 

* Define los dos dataframes para la comparación con los argumentos `dfA = `y `dfB =` 
* En `varnames =` indica todos los nombres de columnas que se utilizarán para la comparación. Todos ellos deben existir tanto en `dfA` como en `dfB`. 
* En `stringdist.match = ` escribe columnas de las que están en `varnames` para ser evaluadas en la cadena "distance". 
* En `numeric.match = ` dar columnas de las que están en `varnames` para ser evaluadas en la distancia numérica. 
* Los valores faltantes se ignoran 
* Por defecto, cada fila de cualquiera de los dos dataframes coincide como máximo con una fila del otro dataframe. Si deseas ver todas las coincidencias evaluadas, establece `dedupe.matches = FALSE`. La deduplicación se realiza mediante la solución de asignación lineal de Winkler. 

*Sugerencia: divide una columna de fecha en tres columnas numéricas separadas utilizando `day()`, `month()`, and `year()` del paquete **lubridate*** 

El umbral por defecto para las coincidencias es de 0,94 (`threshold.match = `), pero puedes ajustarlo más alto o más bajo. Si defines el umbral, ten en cuenta que los umbrales más altos podrían producir más falsos negativos (filas que no coinciden y que en realidad deberían coincidir) y, del mismo modo, un umbral más bajo podría producir más falsos positivos. 

A continuación, los datos se emparejan según la distancia de las cadenas en las columnas de nombre y distrito, y según la distancia numérica para el año, el mes y el día de nacimiento. Se establece un umbral de coincidencia del 95% de probabilidad. 


```{r, message=F, warning=F}
fl_output <- fastLink::fastLink(
  dfA = cases,
  dfB = results,
  varnames = c("gender", "first", "middle", "last", "yr", "mon", "day", "district"),
  stringdist.match = c("first", "middle", "last", "district"),
  numeric.match = c("yr", "mon", "day"),
  threshold.match = 0.95)
```

**Revisar los coincidentes** 

Definimos el objeto devuelto por `fastLink()` como `fl_output`. Es de tipo `list`, y en realidad contiene varios dataframes dentro de él, detallando los resultados de la coincidencia. Uno de estos dataframes es `matches`, que contiene las coincidencias más probables entre `cases` y `results`. Puedes acceder a este dataframe "coincidencias" con `fl_output$matches`. A continuación, se guarda como `my_matches` para facilitar el acceso posterior. 

Cuando se imprime `my_matches`, se ven dos vectores de columnas: los pares de números de fila/índices (también llamados "rownames") en  `cases` ("inds.a") y en `results` ("inds.b") que representan las mejores coincidencias. Si falta un número de fila de un dataframe, entonces no se ha encontrado ninguna coincidencia en el otro dataframe con el umbral de coincidencia especificado. 

```{r}
# print matches
my_matches <- fl_output$matches
my_matches
```

Cosas a tener en cuenta: 

* Las coincidencias se produjeron a pesar de las ligeras diferencias en la ortografía del nombre y las fechas de nacimiento: 
  * "Tony B. Smith" coincide con "Anthony B Smith" 
  * "María Rodríguez" coincide con "Marialisa Rodrigues" 
  * "Betty Chase" coincide con "Elizabeth Chase" 
  * "Olivier Laurent De Bordeaux" coincide con "Oliver Laurent De Bordow" (se ignora la fecha de nacimiento que falta) 
* Una fila de `cases` (para "Blessing Adebayo", fila 9) no tuvo una buena coincidencia en `results`, por lo que no está presente en `my_matches`. 



**Unión en base a las coincidencias probabilísticas** 

Para utilizar estas coincidencias para unir los resultados a los casos, una estrategia es: 

1.  Utilizar `left_join()` para unir  `my_matches` a `cases` (haciendo coincidir rownames en `cases` con "inds.a" en `my_matches`) 
2.  A continuación, utiliza otro `left_join()` para unir `results` a `cases` (haciendo coincidir los "inds.b" recién adquiridos en `cases` con los rownames en `results`) 

Antes de las uniones, debemos limpiar los tres dataframes: 

* Tanto `dfA` como `dfB` deben tener sus números de fila ("rowname") convertidos en una columna propia. 
* Las dos columnas de `my_matches` se convierten en tipo carácter, por lo que pueden unirse a las filas 

```{r}
# Clean data prior to joining
#############################

# convert cases rownames to a column 
cases_clean <- cases %>% rownames_to_column()

# convert test_results rownames to a column
results_clean <- results %>% rownames_to_column()  

# convert all columns in matches dataset to character, so they can be joined to the rownames
matches_clean <- my_matches %>%
  mutate(across(everything(), as.character))



# Join matches to dfA, then add dfB
###################################
# column "inds.b" is added to dfA
complete <- left_join(cases_clean, matches_clean, by = c("rowname" = "inds.a"))

# column(s) from dfB are added 
complete <- left_join(complete, results_clean, by = c("inds.b" = "rowname"))
```

Como se realiza utilizando el código anterior, el dataframe resultante `complete` contendrá *todas* las columnas tanto de  `cases` como de `results`. A muchas de ellas se les añadirán los sufijos ".x" e ".y", ya que de lo contrario los nombres de las columnas estarían duplicados. 

```{r message=FALSE, echo=F}
DT::datatable(complete, rownames = FALSE, options = list(pageLength = nrow(complete), scrollX=T), class = 'white-space: nowrap')
```

Alternativamente, para conseguir sólo los 9 registros "originales" en los casos con la(s) nueva(s) columna(s) de `results`, usa `select()` en `results` antes de las uniones, de forma que sólo contenga los nombres y las columnas que deseas añadir a `cases` (por ej. la columna `result`). 

```{r}
cases_clean <- cases %>% rownames_to_column()

results_clean <- results %>%
  rownames_to_column() %>% 
  select(rowname, result)    # select only certain columns 

matches_clean <- my_matches %>%
  mutate(across(everything(), as.character))

# joins
complete <- left_join(cases_clean, matches_clean, by = c("rowname" = "inds.a"))
complete <- left_join(complete, results_clean, by = c("inds.b" = "rowname"))
```


```{r message=FALSE, echo=F}
DT::datatable(complete, rownames = FALSE, options = list(pageLength = nrow(complete), scrollX=T), class = 'white-space: nowrap')
```


Si deseas subconjuntar cualquiera de los dos conjuntos de datos sólo con las filas que coincidan, puedes utilizar los siguientes códigos: 

```{r}
cases_matched <- cases[my_matches$inds.a,]  # Rows in cases that matched to a row in results
results_matched <- results[my_matches$inds.b,]  # Rows in results that matched to a row in cases
```

O, para ver sólo las filas que **no** coinciden: 

```{r}
cases_not_matched <- cases[!rownames(cases) %in% my_matches$inds.a,]  # Rows in cases that did NOT match to a row in results
results_not_matched <- results[!rownames(results) %in% my_matches$inds.b,]  # Rows in results that did NOT match to a row in cases
```


### De-duplicación probabilística  {.unnumbered}  

La coincidencia probabilística también puede utilizarse para de-duplicar unos datos. Consulta la página sobre [de-duplicación](#de-duplication) para conocer otros métodos de de-duplicación. 

Aquí comenzamos con el conjunto de datos `cases`, pero ahora lo llamamos `cases_dup`, ya que tiene 2 filas adicionales que podrían ser duplicados de filas anteriores: Ver "Tony" con "Anthony", y "Marialisa Rodrigues" con "Maria Rodriguez". 

```{r, echo=F}
## Add duplicates
#cases_dup <- rbind(cases, cases[sample(1:nrow(cases), 3, replace = FALSE),])

cases_dup <- tribble(
  ~gender, ~first,      ~middle,     ~last,        ~yr,   ~mon, ~day, ~district,
  "M",     "Amir",      NA,          "Khan",       1989,  11,   22,   "River",
  "M",     "Anthony",   "B.",        "Smith",      1970, 09, 19,      "River", 
  "F",     "Marialisa", "Contreras", "Rodrigues",  1972, 04, 15,      "River",
  "F",     "Elizabeth", "Casteel",   "Chase",      1954, 03, 03,      "City",
  "M",     "Jose",      "Sanchez",   "Lopez",      1996, 01, 06,      "City",
  "F",     "Cassidy",   "Jones",      "Davis",     1980, 07, 20,      "City",
  "M",     "Michael",   "Murphy",     "O'Calaghan",1969, 04, 12,      "Rural", 
  "M",     "Oliver",    "Laurent",    "De Bordow" , 1971, 02, 04,     "River",
  "F",      "Blessing",  NA,          "Adebayo",   1955,  02, 14,     "Rural",
  "M",     "Tony",   "B.",        "Smith",         1970, 09, 19,      "River", 
  "F",     "Maria",  "Contreras", "Rodriguez",     1972, 04, 15,      "River",
)

```

```{r message=FALSE, echo=F}
DT::datatable(cases_dup, rownames = FALSE, options = list(pageLength = nrow(cases_dup)))
```


Ejecuta `fastLink()` como antes, pero compara el dataframe `cases_dup` consigo mismo. Cuando los dos dataframes proporcionados son idénticos, la función asume que se quiere de-duplicar. Observa que no especificamos `stringdist.match =` o `numeric.match =` como hicimos anteriormente.   

```{r, message = F, warning = F}
## Run fastLink on the same dataset
dedupe_output <- fastLink(
  dfA = cases_dup,
  dfB = cases_dup,
  varnames = c("gender", "first", "middle", "last", "yr", "mon", "day", "district")
)
```

Ahora, puedes revisar los duplicados potenciales con `getMatches()`. Proporciona el dataframe como `dfA =` y `dfB =`, y proporciona la salida de la función `fastLink()` como `fl.out =`. `fl.out` debe ser del tipo `fastLink.dedupe`, o en otras palabras, el resultado de `fastLink()`. 


```{r}
## Run getMatches()
cases_dedupe <- getMatches(
  dfA = cases_dup,
  dfB = cases_dup,
  fl.out = dedupe_output)
```

Véase la columna de la derecha, que indica los IDs duplicados: las dos últimas filas se identifican como probables duplicados de las filas 2 y 3.  

```{r message=FALSE, echo=F}
DT::datatable(cases_dedupe, rownames = FALSE, options = list(pageLength = nrow(cases_dedupe)))
```

Para devolver los números de fila de las filas que probablemente sean duplicadas, puede contar el número de filas por valor único en la columna `dedupe.ids`, y luego filtrar para mantener sólo aquellas con más de una fila. En este caso, esto deja las filas 2 y 3. 

```{r}
cases_dedupe %>% 
  count(dedupe.ids) %>% 
  filter(n > 1)
```

Para inspeccionar las filas completas de los probables duplicados, pon el número de fila en este comando:

```{r}
# displays row 2 and all likely duplicates of it
cases_dedupe[cases_dedupe$dedupe.ids == 2,]   
```



## Enlazamiento y alineación  {#binding-and-aligning}

Otro método para combinar dos dataframes es "unirlos". También se puede pensar en esto como "anexar" o "añadir" filas o columnas. 

En esta sección también se discutirá cómo "alinear" el orden de las filas de un dataframe con el orden de otro dataframe. Este tema se discute más adelante en la sección sobre Vinculación de columnas.



### Enlazar filas  {.unnumbered}

Para unir las filas de un dataframe con el fondo de otro dataframe, utiliza `bind_rows()` de **dplyr**. Es muy inclusivo, por lo que cualquier columna presente en cualquiera de los dataframes se incluirá en la salida. Algunas notas: 

* A diferencia de la versión de R **base** de R `row.bind()`, `bind_rows()` de **dplyr** no requiere que el orden de las columnas sea el mismo en ambos dataframes. Siempre que los nombres de las columnas se escriban de forma idéntica, las alineará correctamente. 
* Puedes especificar opcionalmente el argumento `.id =`. Proporcionar un nombre de columna de caracteres. Esto producirá una nueva columna que sirve para identificar de qué dataframe procede originalmente cada fila. 
* Puedes utilizar `bind_rows()` en una lista de dataframes de estructura similar para combinarlos en un dataframe. Mira un ejemplo en la página [Iteración, bucles y listas](#iteration-loops-and-lists) que implica la importación de múltiples listas de líneas con **purrr**. 

Un ejemplo común de vinculación de filas es vincular una fila "total" a una tabla descriptiva hecha con la función `summarise()` de **dplyr**. A continuación, creamos una tabla de recuentos de casos y valores medianos de TC por hospital con una fila de totales. 

La función `summarise()` se utiliza en los datos agrupados por hospital para devolver un dataframe resumido por hospital. Pero la función `summarise()` no produce automáticamente una fila de "totales", así que la creamos resumiendo los datos *de nuevo*, pero con los datos no agrupados por hospital. Esto produce un segundo dataframe de una sola fila. A continuación, podemos unir estos dataframes para obtener la tabla final. 

Mira otros ejemplos trabajados como éste en las páginas de [Tablas descriptivas](#descriptive-tables) y [Tablas para presentaciones](#tables-for-presentation). 


```{r}
# Create core table
###################
hosp_summary <- linelist %>% 
  group_by(hospital) %>%                        # Group data by hospital
  summarise(                                    # Create new summary columns of indicators of interest
    cases = n(),                                  # Number of rows per hospital-outcome group     
    ct_value_med = median(ct_blood, na.rm=T))     # median CT value per group
```

Este es el dataframe de `hosp_summary`:  

```{r message=FALSE, echo=F}
DT::datatable(hosp_summary, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Crea un dataframe con las estadísticas "totales" (*no agrupadas por hospital*). Esto devolverá una sola fila.

```{r}
# create totals
###############
totals <- linelist %>% 
  summarise(
    cases = n(),                               # Number of rows for whole dataset     
    ct_value_med = median(ct_blood, na.rm=T))  # Median CT for whole dataset
```

Y a continuación está el dataframe `totals`. Observa que sólo hay dos columnas. Estas columnas también están en `hosp_summary`, pero hay una columna en `hosp_summary` que no está en `totals` (`hospital`). 

```{r message=FALSE, echo=F}
DT::datatable(totals, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Ahora podemos unir las filas con `bind_rows()`. 

```{r}
# Bind data frames together
combined <- bind_rows(hosp_summary, totals)
```

Ahora podemos ver el resultado. Observa cómo en la última fila se rellena un valor `NA` vacío para la columna hospital que no estaba en hosp_summary. Como se explica en la página de [Tablas para presentaciones](#tables-for-presentation), podrías "rellenar" esta celda con "Total" utilizando replace_na().   

```{r message=FALSE, echo=F}
DT::datatable(combined, rownames = FALSE, options = list(pageLength = nrow(10)))
```


### Enlazar columnas  {.unnumbered}

Existe una función similar de **dplyr** `bind_cols()` que se puede utilizar para combinar dos dataframes de forma lateral. Ten en cuenta que las filas se emparejan entre sí *por posición* (no como una *unión* anterior) - por ejemplo, la fila 12 en cada dataframe se alineará. 

Como ejemplo, unimos varias tablas de resumen. Para ello, también mostramos cómo reordenar el orden de las filas de un dataframe para que coincida con el orden de otro dataframe, con `match()`. 

Aquí definimos  `case_info` como un dataframe resumido de los casos del listado, por hospital, con el número de casos y el número de muertes.


```{r}
# Case information
case_info <- linelist %>% 
  group_by(hospital) %>% 
  summarise(
    cases = n(),
    deaths = sum(outcome == "Death", na.rm=T)
  )
```

```{r message=FALSE, echo=F}
DT::datatable(case_info, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Y digamos que aquí hay un dataframe diferente `contact_fu` que contiene información sobre el porcentaje de contactos expuestos investigados y "seguidos", de nuevo por hospital. 

```{r}
contact_fu <- data.frame(
  hospital = c("St. Mark's Maternity Hospital (SMMH)", "Military Hospital", "Missing", "Central Hospital", "Port Hospital", "Other"),
  investigated = c("80%", "82%", NA, "78%", "64%", "55%"),
  per_fu = c("60%", "25%", NA, "20%", "75%", "80%")
)
```

```{r message=FALSE, echo=F}
DT::datatable(contact_fu, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Observa que los hospitales son los mismos, pero están en diferente orden en cada dataframe. La solución más sencilla sería utilizar un `left_join()` en la columna `hospitals`, pero también podría utilizar `bind_cols()` con un paso adicional. 

#### Utiliza `match()` para alinear la ordenación  {.unnumbered}  

Debido a que los órdenes de las filas son diferentes, un simple comando `bind_cols()` daría lugar a un desajuste de los datos. Para solucionarlo podemos utilizar `match()` de R *base* para alinear las filas de un dataframe en el mismo orden que en otro. Asumimos para este enfoque que no hay valores duplicados en ninguno de los dos dataframes. 

Cuando utilizamos `match()`, la sintaxis es `match(TARGET ORDER VECTOR, DATA FRAME COLUMN TO CHANGE)`, donde el primer argumento es el orden deseado (ya sea un vector independiente, o en este caso una columna en un dataframe), y el segundo argumento es la columna del dataframe que se reordenará. La salida de `match()` es un vector de números que representa el ordenamiento correcto de las posiciones. Puedes obtener más información con `?match`. 

```{r}
match(case_info$hospital, contact_fu$hospital)
```

Puedes utilizar este vector numérico para reordenar el dataframe - colócalo dentro de los subcorchetes `[ ]` *antes de la coma*. Lee más sobre la **sintaxis** del subconjunto de corchetes en la página de [Fundamentos de R](#r-basics). El comando de abajo crea un nuevo dataframe, definido como el anterior en el que las filas están ordenadas en el vector numérico de arriba.

```{r}
contact_fu_aligned <- contact_fu[match(case_info$hospital, contact_fu$hospital),]
```


```{r message=FALSE, echo=F}
DT::datatable(contact_fu_aligned, rownames = FALSE, options = list(pageLength = nrow(10)))
```

Ahora podemos unir las columnas del dataframe, con el orden correcto de las filas. Ten en cuenta que algunas columnas están duplicadas y será necesario limpiarlas con `rename()`. Lee más sobre `bind_rows()` [aquí](https://dplyr.tidyverse.org/reference/bind.html). 

```{r}
bind_cols(case_info, contact_fu)
```

Una alternativa en R **base** a `bind_cols` es `cbind()`, que realiza la misma operación.   




<!-- ======================================================= -->
## Recursos {#resources-7}
 

Las [páginas de tidyverse sobre join](https://dplyr.tidyverse.org/reference/index.html) 

La [página de R for Data Science sobre datos relacionales](https://r4ds.had.co.nz/relational-data.html) 

La [página de tidyverse en dplyr](https://dplyr.tidyverse.org/reference/bind.html) en la encuadernación 

Una viñeta sobre [fastLink](https://github.com/kosukeimai/fastLink) en la página de Github del paquete 

Publicación que describe la metodología de [fastLink](https://imai.fas.harvard.edu/research/files/linkage.pdf) 

Publicación que describe el [paquete RecordLinkage](https://journal.r-project.org/archive/2010/RJ-2010-017/RJ-2010-017.pdf) 





```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/joining_matching.Rmd-->


# De-duplicación {#de-duplication}  

```{r, out.width=c("50%"), echo=F}
knitr::include_graphics(here::here("images", "deduplication.png"))
```

Esta página cubre las siguientes técnicas de De-duplicación: 

1.  Identificación y eliminación de filas duplicadas 
2.  "Recortar" filas para mantener sólo determinadas filas (por ejemplo, mínimas o máximas) de cada grupo de filas 
3.  "Reunir" o combinar valores de varias filas en una sola fila 


<!-- ======================================================= -->
## Preparación  {#preparation-6}

### Cargar paquetes {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para el análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base** Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R. 

```{r}
pacman::p_load(
  tidyverse,   # deduplication, grouping, and slicing functions
  janitor,     # function for reviewing duplicates
  stringr)      # for string searches, can be used in "rolling-up" values
```

### Importar datos {.unnumbered}

Para la demostración, utilizaremos unos datos de ejemplo que se crea con el código R que aparece a continuación. 

Los datos son registros de encuentros telefónicos COVID-19, incluyendo encuentros con contactos y con casos. Las columnas incluyen `recordID` (generado por ordenador), `personID`, `name`, `date` del encuentro, `time` del encuentro, `purpose` del encuentro (para entrevistar como caso o como contacto), y `symptoms_ever` (si la persona en ese encuentro declaró haber tenido síntomas *alguna vez*). 

Este es el código para crear el set de datos `obs`: 

```{r}
obs <- data.frame(
  recordID  = c(1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18),
  personID  = c(1,1,2,2,3,2,4,5,6,7,2,1,3,3,4,5,5,7,8),
  name      = c("adam", "adam", "amrish", "amrish", "mariah", "amrish", "nikhil", "brian", "smita", "raquel", "amrish",
                "adam", "mariah", "mariah", "nikhil", "brian", "brian", "raquel", "natalie"),
  date      = c("1/1/2020", "1/1/2020", "2/1/2020", "2/1/2020", "5/1/2020", "5/1/2020", "5/1/2020", "5/1/2020", "5/1/2020","5/1/2020", "2/1/2020",
                "5/1/2020", "6/1/2020", "6/1/2020", "6/1/2020", "6/1/2020", "7/1/2020", "7/1/2020", "7/1/2020"),
  time      = c("09:00", "09:00", "14:20", "14:20", "12:00", "16:10", "13:01", "15:20", "14:20", "12:30", "10:24",
                "09:40", "07:25", "08:32", "15:36", "15:31", "07:59", "11:13", "17:12"),
  encounter = c(1,1,1,1,1,3,1,1,1,1,2,
                2,2,3,2,2,3,2,1),
  purpose   = c("contact", "contact", "contact", "contact", "case", "case", "contact", "contact", "contact", "contact", "contact",
                "case", "contact", "contact", "contact", "contact", "case", "contact", "case"),
  symptoms_ever = c(NA, NA, "No", "No", "No", "Yes", "Yes", "No", "Yes", NA, "Yes",
                    "No", "No", "No", "Yes", "Yes", "No","No", "No")) %>% 
  mutate(date = as.Date(date, format = "%d/%m/%Y"))
```


#### Este es el dataframe {#dedup_data .unnumbered}  

Utiliza los cuadros de filtro de la parte superior para revisar los encuentros de cada persona.   

```{r message=FALSE, echo=F}
DT::datatable(obs, rownames = FALSE, filter = "top", options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```


Hay que tener en cuenta algunas cosas al revisar los datos: 

* Los dos primeros registros están 100% duplicados, incluido el `recordID` de registro duplicado (¡debe ser un fallo informático!) 
* Las dos segundas filas están duplicadas, en todas las columnas *excepto en `recordID`* 
* Varias personas tuvieron múltiples encuentros telefónicos, en diversas fechas y horas, y como contactos y/o casos 
* En cada encuentro se preguntaba a la persona si había tenido **alguna vez** síntomas, y parte de esta información falta. 

Y aquí hay un resumen rápido de las personas y los propósitos de sus encuentros, usando `tabyl()` de **janitor**:  

```{r}
obs %>% 
  tabyl(name, purpose)
```
<!-- ======================================================= -->
## De-duplicación {#deduplication-1}

Esta sección describe cómo revisar y eliminar filas duplicadas en un dataframe. También muestra cómo manejar los elementos duplicados en un vector. 


<!-- ======================================================= -->
### Examinar las filas duplicadas  {.unnumbered}  

Para revisar rápidamente las filas que tienen duplicados, puedes utilizar `get_dupes()` del paquete **janitor**. Por *defecto*, se revisan todas las columnas cuando se evalúan los duplicados - las filas devueltas por la función están 100% duplicadas considerando los valores de *todas* las columnas. 

En el dataframe `obs`, las dos primeras filas están *100% duplicadas* - tienen el mismo valor en cada columna (incluyendo la columna recordID, que se *supone* que es única - debe ser algún fallo informático). El dataframe devuelto incluye automáticamente una nueva columna `dupe_count` en el lado derecho, que muestra el número de filas con esa combinación de valores duplicados. 

```{r, eval=F}
# 100% duplicates across all columns
obs %>% 
  janitor::get_dupes()
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes() %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```

Ver los [datos originales](#dedup_data) 

Sin embargo, si decidimos ignorar `recordID`, las filas 3 y 4 también están duplicadas entre sí. Es decir, tienen los mismos valores en todas las columnas *excepto* en recordID. Puedes especificar las columnas que se van a ignorar en la función mediante el símbolo `-` menos.  

```{r, eval=F}
# Duplicates when column recordID is not considered
obs %>% 
  janitor::get_dupes(-recordID)         # if multiple columns, wrap them in c()
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes(-recordID) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = nrow(obs), scrollX=T), class = 'white-space: nowrap' )
```

También puedes especificar positivamente las columnas a considerar. A continuación, sólo se devuelven las filas que tienen los mismos valores en las columnas `name` y `purpose`. Observa cómo "amrish" tiene ahora `dupe_count` igual a 3 para reflejar sus tres encuentros de "contacto". 

*Desplázate a la izquierda para ver más filas** 

```{r, eval=F}
# duplicates based on name and purpose columns ONLY
obs %>% 
  janitor::get_dupes(name, purpose)
```

```{r message=FALSE, echo=F}
obs %>% 
  janitor::get_dupes(name, purpose) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 7, scrollX=T), class = 'white-space: nowrap' )
```

Ver los [datos originales](#dedup_data). 

Para más detalles, consulta `?get_dupes` o esta [referencia en línea](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes)  






<!-- ======================================================= -->
### Mantener sólo filas únicas  {.unnumbered}

Para mantener sólo las filas únicas de un dataframe, utiliza `distinct()` de **dplyr** (como se muestra en la página [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions)). Las filas duplicadas se eliminan de forma que sólo se conserva la primera de dichas filas. Por defecto, "primero" significa el `rownumber` más alto (orden de filas de arriba a abajo). Sólo se mantienen las filas únicas. 

En el ejemplo siguiente, ejecutamos `distinct()` de forma que la columna `recordID` se excluye de la consideración - así **se eliminan dos filas duplicadas**. La primera fila (para "adam") estaba 100% duplicada y ha sido eliminada. También la fila 3 (para "amrish") estaba duplicada en todas las columnas *excepto* en `recordID` (que no se tiene en cuenta), por lo que también se ha eliminado. El set de datos `obs` tiene ahora `nrow(obs)-2`  filas, no `nrow(obs)`). 

*Desplázate a la izquierda para ver el dataframe completo* 


```{r, eval=F}
# added to a chain of pipes (e.g. data cleaning)
obs %>% 
  distinct(across(-recordID), # reduces data frame to only unique rows (keeps first one of any duplicates)
           .keep_all = TRUE) 

# if outside pipes, include the data as first argument 
# distinct(obs)
```

```{r message=FALSE, echo=F}
obs %>% 
  distinct(across(-recordID), # reduces data frame to only unique rows (keeps first one of any duplicates)
           .keep_all = TRUE) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 6, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: orange;">***PRECAUCIÓN:*** Si se utiliza `distinct()` en datos agrupados, la función se aplicará a cada grupo.</span>


**De-duplicar en base a columnas específicas** 

También puedes especificar las columnas que serán la base de la De-duplicación. De esta manera, la De-duplicación sólo se aplica a las filas que están duplicadas dentro de las columnas especificadas. A menos que establece `.keep_all = TRUE`, todas las columnas no mencionadas se eliminarán. 

En el ejemplo siguiente, la De-duplicación sólo se aplica a las filas que tienen valores idénticos para las columnas `name` y `purpose`. Por lo tanto, "brian" sólo tiene 2 filas en lugar de 3: su *primer* encuentro como "contacto" y su único encuentro como "caso". Para ajustar que se mantenga el *último encuentro de brian* de cada propósito, Mira el apartado Cortar dentro de los grupos. 

*Desplázate a la izquierda para ver el dataframe completo* 

```{r, eval=F}
# added to a chain of pipes (e.g. data cleaning)
obs %>% 
  distinct(name, purpose, .keep_all = TRUE) %>%  # keep rows unique by name and purpose, retain all columns
  arrange(name)                                  # arrange for easier viewing
```

```{r message=FALSE, echo=F}
obs %>% 
  distinct(name, purpose, .keep_all = TRUE) %>%  # keep rows unique by name and purpose, retain all columns
  arrange(name) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 6, scrollX=T), class = 'white-space: nowrap' )
```

Ver los [datos originales](#dedup_data). 

<!-- ======================================================= -->
### De-duplicar elementos en un vector  {.unnumbered}  

La función  `duplicated()` de R **base** evaluará un vector (columna) y devolverá un vector lógico de la misma longitud (TRUE/FALSE). La primera vez que aparezca un valor, devolverá FALSE (no es un duplicado), y las siguientes veces que aparezca ese valor devolverá TRUE. Nótese que `NA` se trata igual que cualquier otro valor. 

```{r}
x <- c(1, 1, 2, NA, NA, 4, 5, 4, 4, 1, 2)
duplicated(x)
```

Para devolver sólo los elementos duplicados, se pueden utilizar paréntesis para subconjuntar el vector original:  

```{r}
x[duplicated(x)]
```

Para devolver sólo los elementos únicos, utiliza `unique()` de R **base**. Para eliminar los `NA` de la salida, anida `na.omit()` dentro de `unique()`.  

```{r}
unique(x)           # alternatively, use x[!duplicated(x)]
unique(na.omit(x))  # remove NAs 
```


<!-- ======================================================= -->
### Utilizando R **base**  {.unnumbered}

**Para devolver las filas duplicadas** 

En R **base**, también se puede ver qué filas están 100% duplicadas en un dataframe `df` con el comando `duplicated(df)` (devuelve un vector lógico de las filas). 

Así, también puedes utilizar el subconjunto base `[ ]` en el dataframe para ver las filas *duplicadas* con `df[duplicated(df),]` (¡no olvides la coma, que significa que quieres ver todas las columnas!) 

**Para devolver filas únicas** 

Ver las notas anteriores. Para ver las filas *únicas* se añade el negador lógico `!` delante de la función duplicated():
`df[!duplicated(df),]` 


**Para devolver las filas que son duplicados de sólo ciertas columnas** 

Subconjunta el `df` que está *dentro de los paréntesis de `duplicated()`*, para que esta función opere sólo en ciertas columnas del df. 
Para especificar las columnas, proporciona los números o nombres de las columnas después de una coma (recuerda que todo esto está *dentro* de la función `duplicated()`). 

¡Asegúrate también de mantener la coma `,` *fuera*, después de la función `duplicated()`! 

Por ejemplo, para evaluar sólo las columnas 2 a 5 en busca de duplicados: `df[!duplicated(df[, 2:5]),]`
Para evaluar sólo las columnas  `name` y `purpose` en busca de duplicados: `df[!duplicated(df[, c("name", "purpose)]),]` 





<!-- ======================================================= -->
## Recortar {#slicing}

Para "recortar" un dataframe con un filtro de filas por su número de fila/posición. Esto resulta especialmente útil si tiene varias filas por grupo funcional (por ejemplo, por "persona") y sólo quieres conservar una o algunas de ellas. 

La función básica `slice()` acepta números y devuelve filas en esas posiciones. Si los números proporcionados son positivos, sólo se devuelven éstos. Si son negativos, *no se devuelven esas filas*. Los números deben ser todos positivos o todos negativos.      

```{r}
obs %>% slice(4)  # return the 4th row
```

```{r}
obs %>% slice(c(2,4))  # return rows 2 and 4
#obs %>% slice(c(2:4))  # return rows 2 through 4
```


Ver los [datos originales](#dedup_data). 

Existen diversas variantes: Se les debe proporcionar una columna y un número de filas a devolver (a `n = `). 

* `slice_min()` y `slice_max()` mantienen sólo la(s) fila(s) con el valor(es) mínimo o máximo de la columna especificada. Esto también funciona para devolver el "min" y el "max" de los factores ordenados. 
* `slice_head()` y `slice_tail()` - mantienen sólo la *primera* o la *última* fila. 
* `slice_sample()` - mantener sólo una muestra aleatoria de las filas. 


```{r}
obs %>% slice_max(encounter, n = 1)  # return rows with the largest encounter number
```

Utiliza los argumentos `n = ` o `prop = ` para especificar el número o la proporción de filas que deben conservarse. Si no se utiliza la función en una cadena de tuberías, proporciona primero el argumento datos (por ejemplo, `slice(datos, n = 2)`). Para más información, consulta con `?slice`. 

Otros argumentos: 

`.order_by = ` utilizado en `slice_min()` y `slice_max()` esta es una columna para ordenar por antes de recortarlas.
` with_ties = ` TRUE por defecto, lo que significa que se mantienen los empates.
`.preserve = ` FALSE por defecto. Si es TRUE, la estructura de agrupación se recalcula después del recorte.
`weight_by = ` Opcional, columna numérica para ponderar por (un número mayor tiene más probabilidades de ser muestreado). También `replace = ` para saber si el muestreo se realiza con/sin reemplazo. 

<span style="color: darkgreen;">***CONSEJO:*** Al utilizar `slice_max()` y `slice_min()`, asegúrate de especificar/escribir el `n = ` (por ejemplo, `n = 2`, no simplemente 2). De lo contrario, puedes obtener un error `Error: ` ...` is not empty.`.  </span>

<span style="color: black;">***NOTA:*** Es posible que encuentres la función [`top_n()`](https://dplyr.tidyverse.org/reference/top_n.html), que ha sido sustituida por las funciones `slice`. </span>

 


<!-- ======================================================= -->
### Recortar con grupos   {.unnumbered}

Las funciones `slice_*()` pueden ser muy útiles si se aplican a un dataframe agrupado porque la operación de recorte se realiza en cada grupo por separado. Utiliza la **función** `group_by()` junto con `slice()` para agrupar los datos y tomar un corte de cada grupo. 

Esto es útil para la De-duplicación si tienes varias filas por persona pero sólo quieres mantener una de ellas. Primero se utiliza `group_by()` con columnas clave que son las mismas por persona, y luego se utiliza una función slice en una columna que será diferente entre las filas agrupadas. 

En el ejemplo siguiente, para mantener sólo el *último* encuentro *por persona*, agrupamos las filas por nombre y luego utilizamos `slice_max()` con `n = 1` en la columna de `date`. Ten en cuenta que Para aplicar una función como `slice_max() en las fechas, la columna de fecha debe ser de tipo Date. 

Por defecto, los "empates" (por ejemplo, la misma fecha en este escenario) se mantienen, y todavía obtendríamos múltiples filas para algunas personas (por ejemplo, adam). Para evitar esto, establecemos `with_ties = FALSE`. Sólo obtendremos una fila por persona. 

<span style="color: orange;">***PRECACUCIÓN:*** Si utilizas `arrange()`, especifica .`by_group = TRUE` para que los datos se ordenen dentro de cada grupo.</span>

<span style="color: red;">***PELIGRO:*** Si `with_ties = FALSE`, se mantiene la primera fila de un empate. Esto puede ser engañoso. Mira cómo para Mariah, ella tiene dos encuentros en su última fecha (6 de enero) y el primero (el más temprano) se mantuvo. Es probable que queramos mantener tu último encuentro en ese día. Mira cómo "romper" estos vínculos en el siguiente ejemplo.  </span>  




```{r, eval=F}
obs %>% 
  group_by(name) %>%       # group the rows by 'name'
  slice_max(date,          # keep row per group with maximum date value 
            n = 1,         # keep only the single highest row 
            with_ties = F) # if there's a tie (of date), take the first row
```

```{r message=FALSE, echo=F}
obs %>% 
  group_by(name) %>%       # group the rows by 'name'
  slice_max(date,          # keep row per group with maximum date value 
            n = 1,         # keep only the single highest row 
            with_ties = F) %>%  # if there's a tie (of date), take the first row
  DT::datatable(rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

Arriba, por ejemplo, podemos ver que sólo se conservó la fila de Amrish del 5 de enero, y sólo se conservó la fila de Brian del 7 de enero. Ver los [datos originales](#dedup_data).   


**Romper los "empates"** 

Se pueden ejecutar múltiples sentencias de recorte para "romper empates". En este caso, si una persona tiene varios encuentros en tu última *fecha*, se mantiene el encuentro con la última *hora* (se utiliza `lubridate::hm()` para convertir los caracteres de tiempo en tipo tiempo, ordenable).
Observa ahora cómo, la única fila que se mantiene para "Mariah" el 6 de enero es el encuentro 3 de las 08:32, no el encuentro 2 de las 07:25.  

```{r, eval=F}
# Example of multiple slice statements to "break ties"
obs %>%
  group_by(name) %>%
  
  # FIRST - slice by latest date
  slice_max(date, n = 1, with_ties = TRUE) %>% 
  
  # SECOND - if there is a tie, select row with latest time; ties prohibited
  slice_max(lubridate::hm(time), n = 1, with_ties = FALSE)
```

```{r message=FALSE, echo=F}
# Example of multiple slice statements to "break ties"
obs %>%
  group_by(name) %>%
  
  # FIRST - slice by latest date
  slice_max(date, n = 1, with_ties = TRUE) %>% 
  
  # SECOND - if there is a tie, select row with latest time; ties prohibited
  slice_max(lubridate::hm(time), n = 1, with_ties = FALSE) %>% 
  
  DT::datatable(rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

*En el ejemplo anterior, también habría sido posible realizar un recorte por número de encuentro, pero mostramos el corte por fecha y hora a modo de ejemplo.* 

***CONSEJO:*** Para utilizar `slice_max()` o `slice_min()` en una columna "carácter", ¡mútala a un tipo de factor *ordenado*! 

Ver los [datos originales](#dedup_data). 


<!-- ======================================================= -->
### Mantener todos pero marcados  {.unnumbered}

Si deseas conservar todos los registros pero marcar sólo algunos para tu análisis, considera un enfoque de dos pasos utilizando un número de registro/encuentro único: 

1.  Reduce/recorta el dataframe original a sólo las filas para el análisis. Guarda/conserva este dataframe reducido. 
2.  En el dataframe original, marca las filas según corresponda con `case_when()`, basándose en si tu identificador único de registro (recordID en este ejemplo) está presente en el dataframe reducido. 


```{r}
# 1. Define data frame of rows to keep for analysis
obs_keep <- obs %>%
  group_by(name) %>%
  slice_max(encounter, n = 1, with_ties = FALSE) # keep only latest encounter per person


# 2. Mark original data frame
obs_marked <- obs %>%

  # make new dup_record column
  mutate(dup_record = case_when(
    
    # if record is in obs_keep data frame
    recordID %in% obs_keep$recordID ~ "For analysis", 
    
    # all else marked as "Ignore" for analysis purposes
    TRUE                            ~ "Ignore"))

# print
obs_marked
```


```{r, echo=F}
DT::datatable(obs_marked, rownames = FALSE, options = list(pageLength = 8, scrollX=T), class = 'white-space: nowrap' )
```

Ver los [datos originales](#dedup_data).  

<!-- ======================================================= -->
### Calcular la exhaustividad de las filas {.unnumbered} 

Crea una columna que contenga una métrica para la exhaustividad/completitud de la fila (que no tenga  valores faltantes). Esto podría ser útil a la hora de decidir qué filas se priorizan sobre otras al de-duplicar/repartir. 

En este ejemplo, las columnas "clave" sobre las que se quiere medir la integridad se guardan en un vector de nombres de columnas. 

A continuación se crea la nueva columna `key_completeness` con `mutate()`. El nuevo valor de cada fila se define como una fracción calculada: el número de valores no ausentes en esa fila entre las columnas clave, dividido por el número de columnas clave. 

Esto implica la función `rowSums()` de R **base**. También se utiliza `.` , que dentro del piping se refiere al dataframe en ese punto (en este caso, se está subconjuntando con corchetes `[]`). 

*Desplázate a la derecha para ver más filas**. 
```{r, eval=F}
# create a "key variable completeness" column
# this is a *proportion* of the columns designated as "key_cols" that have non-missing values

key_cols = c("personID", "name", "symptoms_ever")

obs %>% 
  mutate(key_completeness = rowSums(!is.na(.[,key_cols]))/length(key_cols)) 
```

```{r message=FALSE, echo=F}
key_cols = c("personID", "name", "symptoms_ever")

obs %>% 
  mutate(key_completeness = rowSums(!is.na(.[,key_cols]))/length(key_cols)) %>% 
  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Ver los [datos originales](#dedup_data). 




<!-- ======================================================= -->
## Combinación de valores {#str_rollup}

Esta sección describe: 

1.  Cómo "combinar" valores de varias filas en una sola fila, con algunas variaciones 
2.  Una vez que se hayan "combinado" los valores, cómo sobrescribir/priorizar los valores en cada celda 

Esta sección utiliza los datos de ejemplo de la sección Preparación. 



<!-- ======================================================= -->
### Combinar los valores en una fila  {.unnumbered}  

El código de ejemplo que se muestra a continuación utiliza `group_by()` y `summarise()` para agrupar las filas por persona, y luego pega todos los valores únicos dentro de las filas agrupadas. Así, se obtiene una fila de resumen por persona. Algunas notas: 

* Se añade un sufijo a todas las nuevas columnas ("_roll" en este ejemplo) 
* Si quieres mostrar sólo los valores únicos por celda, entonces envuelve el `na.omit()` con `unique()` 
* `na.omit()` elimina los valores `NA`, pero si no se desea se puede eliminar con `paste0(.x)`... 


```{r, eval=F}
# "Roll-up" values into one row per group (per "personID") 
cases_rolled <- obs %>% 
  
  # create groups by name
  group_by(personID) %>% 
  
  # order the rows within each group (e.g. by date)
  arrange(date, .by_group = TRUE) %>% 
  
  # For each column, paste together all values within the grouped rows, separated by ";"
  summarise(
    across(everything(),                           # apply to all columns
           ~paste0(na.omit(.x), collapse = "; "))) # function is defined which combines non-NA values
```

El resultado es una fila por grupo (`ID`), con entradas ordenadas por fecha y pegadas. *Desplázate a la izquierda para ver más filas* 

```{r message=FALSE, echo=F}
# "Roll-up" values into one row per group (per "personID") 
obs %>% 
  
  # create groups by name
  group_by(personID) %>% 
  
  # order the rows within each group (e.g. by date)
  arrange(date, .by_group = TRUE) %>% 
  
  # For each column, paste together all values within the grouped rows, separated by ";"
  summarise(
    across(everything(),                                # apply to all columns
           ~paste0(na.omit(.x), collapse = "; "))) %>%  # function is defined which combines non-NA values

  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Ver los [datos originales](#dedup_data).  


**Esta variación sólo muestra valores únicos:**   

```{r}
# Variation - show unique values only 
cases_rolled <- obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                                   # apply to all columns
           ~paste0(unique(na.omit(.x)), collapse = "; "))) # function is defined which combines unique non-NA values
```

```{r message=FALSE, echo=F}
# Variation - show unique values only 
obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                                   # apply to all columns
           ~paste0(unique(na.omit(.x)), collapse = "; "))) %>%  # function is defined which combines unique non-NA values

  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


**Esta variación añade un sufijo a cada columna.**
En este caso, "_roll" para indicar que se ha combinado (roll): 

```{r, eval=F}
# Variation - suffix added to column names 
cases_rolled <- obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                
           list(roll = ~paste0(na.omit(.x), collapse = "; ")))) # _roll is appended to column names
```

```{r message=FALSE, echo=F}
# display the linelist data as a table
# Variation - suffix added to column names 
obs %>% 
  group_by(personID) %>% 
  arrange(date, .by_group = TRUE) %>% 
  summarise(
    across(everything(),                
           list(roll = ~paste0(na.omit(.x), collapse = "; ")))) %>%  # _roll is appended to column names
  DT::datatable(rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
### Sobrescribir valores/jerarquía  {.unnumbered} 

Si luego quieres evaluar todos los valores combinados, y mantener sólo un valor específico (por ejemplo, el "mejor" o el "máximo" valor), puedes utilizar `mutate()` a través de las columnas deseadas, para implementar `case_when()`, que utiliza `str_detect()` del paquete **stringr** para buscar secuencialmente patrones de cadena y sobrescribir el contenido de la celda.  

```{r}
# CLEAN CASES
#############
cases_clean <- cases_rolled %>% 
    
    # clean Yes-No-Unknown vars: replace text with "highest" value present in the string
    mutate(across(c(contains("symptoms_ever")),                     # operates on specified columns (Y/N/U)
             list(mod = ~case_when(                                 # adds suffix "_mod" to new cols; implements case_when()
               
               str_detect(.x, "Yes")       ~ "Yes",                 # if "Yes" is detected, then cell value converts to yes
               str_detect(.x, "No")        ~ "No",                  # then, if "No" is detected, then cell value converts to no
               str_detect(.x, "Unknown")   ~ "Unknown",             # then, if "Unknown" is detected, then cell value converts to Unknown
               TRUE                        ~ as.character(.x)))),   # then, if anything else if it kept as is
      .keep = "unused")                                             # old columns removed, leaving only _mod columns
```


Ahora puedes ver en la columna `symptoms_ever` que si la persona ALGUNA vez dijo "Sí" a los síntomas, entonces sólo se muestra "Sí". 

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(cases_clean, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap')
```


Ver los [datos originales](#dedup_data).   


## De-duplicación probabilística  {#probabilistic-de-duplication}

A veces, puedes querer identificar duplicados "probables" basándote en la similitud (por ejemplo, la "distancia" de la cadena) en varias columnas como el nombre, la edad, el sexo, la fecha de nacimiento, etc. Puedes aplicar un algoritmo de coincidencia probabilística para identificar duplicados probables. 

Consulta la página sobre la [unión de datos](#joining-data) para obtener una explicación sobre este método. La sección sobre Coincidencia probabilística contiene un ejemplo de aplicación de estos algoritmos para comparar un dataframe *consigo mismo*, realizando así una De-duplicación probabilística.  



<!-- ======================================================= -->
## Recursos {#resources-8}

Gran parte de la información de esta página está adaptada de estos recursos y viñetas en línea: 

[datanovia](https://www.datanovia.com/en/lessons/identify-and-remove-duplicate-data-in-r/)

[Referencia de dplyr tidyverse](https://dplyr.tidyverse.org/reference/slice.html)  

[Viñeta janitor de CRAN](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html#explore-records-with-duplicated-values-for-specific-combinations-of-variables-with-get_dupes)  

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/deduplication.Rmd-->


# Iteración, bucles y listas {#iteration-loops-and-lists} 

Con frecuencia nos enfrentamos a la repetición de análisis por subgrupos, como países, distritos o grupos de edad. Estas son sólo algunas de las situaciones frecuentes que implican la *iteración*. La codificación de sus operaciones iterativas utilizando los enfoques que se indican a continuación te ayudarán a realizar estas tareas repetitivas más rápidamente, a reducir la posibilidad de error y a reducir la longitud del código. 

Esta página presentará dos enfoques de las operaciones iterativas: el uso de *bucles for* y el uso del paquete **purrr**. 

1.  Los *bucles for* iteran el código a través de una serie de entradas, pero son menos comunes en R que en otros lenguajes de programación. No obstante, los presentamos aquí como herramienta de aprendizaje y referencia 
2.  El paquete **purrr** es el enfoque **tidyverse** de las operaciones iterativas - funciona "mapeando" una función a través de muchas entradas (valores, columnas, conjuntos de datos, etc.) 

En el camino, mostraremos ejemplos como: 

* Importación y exportación de múltiples archivos 
* Creación de epicurvas para múltiples jurisdicciones 
* Ejecución de pruebas T para varias columnas en un dataframe 

En la [sección](#iter_purrr) de **purrr** también proporcionaremos varios ejemplos de creación y manejo de listas. 

## Preparación  {#preparation-6}

### Cargar paquetes  {.unnumbered}  

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R. 

```{r}
pacman::p_load(
     rio,         # import/export
     here,        # file locator
     purrr,       # iteration
     tidyverse    # data management and visualization
)
```


### Importar datos {.unnumbered}  

Importamos los datos de casos de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica para descargar linelist "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa los datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - Mira la página de [importación y exportación](#import-and-export) para más detalles).  

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas del listado. 


```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```



<!-- ======================================================= -->

## *bucles for*  {#for-loops}

### *bucles for* en R  {#iter_loops .unnumbered}  

Los *bucles for* no se resaltan mucho en R, pero son comunes en otros lenguajes de programación. Como principiante, pueden ser útiles para aprender y practicar, ya que son más fáciles de "explorar", "depurar", y de otra manera comprender exactamente lo que está sucediendo para cada iteración, especialmente cuando todavía no te sientes cómodo escribiendo tus propias funciones. 

Puedes pasar rápidamente de los *bucles for* a la iteración con funciones mapeadas con **purrr** (véase [la sección siguiente](#iter_purrr)).  


### Componentes básicos  {.unnumbered}   

Un *bucle for* tiene tres partes fundamentales: 

1.  La **secuencia** de elementos a iterar 
2.  Las **operaciones** a realizar por cada elemento de la secuencia 
3.  El **contenedor** de los resultados (opcional) 

La sintaxis básica es: `for (para cada elemento de la secuencia) {hacer operaciones con el elemento}`. Fíjate en los paréntesis y las llaves. Los resultados pueden ser impresos en la consola, o almacenados en un objeto R contenedor. 

A continuación se muestra un sencillo ejemplo de *bucle for*.  

```{r}
for (num in c(1,2,3,4,5)) {  # La SECUENCIA está definida (números de 1 a 5) y el bucle se abre con "{"
  print(num + 2)             # Las OPERACIONES (añadir dos a cada elemento de la secuencia e imprimirlos en la consola)
}                            # El bucle se cierra con "}"                            
                             # No hay CONTENEDOR" en este ejemplo
```



### Secuencia  {.unnumbered}  

Esta es la parte "for" de un *bucle for* - las operaciones se ejecutarán "para" (for) cada elemento de la secuencia. La secuencia puede ser una serie de valores (por ejemplo, nombres de jurisdicciones, enfermedades, nombres de columnas, elementos de listas, etc.), o puede ser una serie de números consecutivos (por ejemplo, 1,2,3,4,5). Cada enfoque tiene sus propias utilidades, que se describen a continuación. 

La estructura básica de una declaración de secuencia es el `elemento en el vector`. 

* Puedes escribir cualquier carácter o palabra en lugar de "item" (por ejemplo, "i", "num", "hosp", "district", etc.). El valor de este "elemento" cambia con cada iteración del bucle, pasando por cada valor del vector. 
* El *vector* puede ser de valores de caracteres, nombres de columnas, o quizás una secuencia de números - estos son los valores que cambiarán con cada iteración. Puedes utilizarlos dentro de las operaciones del *bucle for* utilizando el término "item". 

**Ejemplo: secuencia de valores de caracteres** 

En este ejemplo, se realiza un bucle para cada valor de un vector de caracteres predefinido de nombres de hospitales. 

```{r}
# make vector of the hospital names
hospital_names <- unique(linelist$hospital)
hospital_names # print
```

Hemos elegido el término `hosp` para representar los valores del vector nombres_de_hospital. Para la primera iteración del bucle, el valor de `hosp` será `hospital_names[1]]`. Para la segunda iteración del bucle será `hospital_names[2]]`. Y así sucesivamente... 

```{r, eval=F}
# a 'for loop' with character sequence

for (hosp in hospital_names){       # sequence
  
       # OPERATIONS HERE
  }
```

**Ejemplo: secuencia de nombres de columnas** 

Se trata de una variación de la secuencia de caracteres anterior, en la que se extraen los nombres de un objeto R existente y se convierten en el vector. Por ejemplo, los nombres de las columnas de un dataframe. Convenientemente, en el código de operaciones del *bucle for*, los nombres de las columnas se pueden utilizar para *indexar* (subconjuntar) tu dataframe original 

A continuación, la secuencia son los `names()` (nombres de columnas) del dataframe `linelist`. El nombre de nuestro "elemento" es `col`, que representará el nombre de cada columna a medida que avanzan los bucles. 

A modo de ejemplo, incluimos código de operaciones dentro del *bucle for*, que se ejecuta para cada valor de la secuencia. En este código, los valores de la secuencia (nombres de las columnas) se utilizan para *indexar* (subconjuntar) `linelist`, una por una. Como se enseñó en la página de [fundamentos de R, se utilizan](#r-basics) dobles ramificaciones `[[ ]]` para el subconjunto. La columna resultante se pasa a `is.na()`, y luego a `sum()` para producir el número de valores de la columna que faltan. El resultado se imprime en la consola: un número por cada columna. 

Una nota sobre la indexación con los nombres de las columnas - ¡cuando se refiera a la propia columna *¡no escribas simplemente "col"!*  ya que representa sólo el nombre de la columna de caracteres! Para referirse a la columna completa debe utilizar el nombre de la columna como un *índice* en `linelist` a través de `linelist[[col]]`.  

```{r}
for (col in names(linelist)){        # loop runs for each column in linelist; column name represented by "col" 
  
  # Example operations code - print number of missing values in column
  print(sum(is.na(linelist[[col]])))  # linelist is indexed by current value of "col"
     
}
```



**Secuencia de números** 

En este enfoque, la secuencia es una serie de números consecutivos. Por lo tanto, el valor del "ítem" no es un valor de carácter (por ejemplo, "Hospital Central" o "fecha_de_inicio"), sino que es un número. Esto es útil para hacer un bucle a través de los dataframes, ya que puedeS utilizar el número del "ítem" dentro del *bucle for* para indexar el dataframe por *número de fila*. 

Por ejemplo, digamos que quiereS recorrer cada fila de tu dataframe y extraer cierta información. Sus "elementos" serían números de fila numéricos. A menudo, los "elementos" en este caso se escriben como `i`. 

El proceso del *bucle for* podría explicarse en palabras como "para cada elemento de una secuencia de números desde 1 hasta el número total de filas de mi dataframe, haz X". Para la primera iteración del bucle, el valor del "elemento" `i` sería 1. Para la segunda iteración, `i` sería 2, etc. 

Aquí está el aspecto de la secuencia en código: `for (i in 1:nrow(linelist)) {CODIGO DE OPERACIONES}` donde `i` representa el "elemento" y `1:nrow(linelist)` produce una secuencia de números consecutivos desde 1 hasta el número de filas en `linelist`. 


```{r, eval=F}
for (i in 1:nrow(linelist)) {  # use on a data frame
  # OPERATIONS HERE
}  
```

Si quieres que la secuencia sea numérica, pero partes de un vector (no de un dataframe), utiliza el atajo `seq_along()` para devolver una secuencia de números para cada elemento del vector. Por ejemplo, `for (i en seq_along(nombres_de_hospital) {Código_de_operaciones}`. 

El código siguiente devuelve en realidad números, que se convertirían en el valor de`i`en tu respectivo bucle.    

```{r}
seq_along(hospital_names)  # use on a named vector
```

Una ventaja de usar números en la secuencia es que es fácil usar también el número`i`para indexar un *contenedor* que almacene las salidas del bucle. Hay un ejemplo de esto en la sección de Operaciones más abajo.  

### Operaciones   {.unnumbered}  

Este es el código dentro de las llaves `{ }` del *bucle for*. Quieres que este código se ejecute para cada "elemento" de la *secuencia*. Por lo tanto, ¡ten cuidado de que cada parte de tu código que cambia por el "ítem" esté correctamente codificado de manera que realmente cambie! Por ejemplo, recuerda usar `[[ ]]` para la indexación. 

En el ejemplo siguiente, iteramos por cada fila de `linelist`. Los valores de género y edad de cada fila se pegan juntos y se almacenan en el vector de caracteres contenedor `cases_demographics`. Observa cómo también utilizamos la indexación `[[i]]` para guardar la salida del bucle en la posición correcta en el vector "contenedor". 

```{r}
# create container to store results - a character vector
cases_demographics <- vector(mode = "character", length = nrow(linelist))

# the for loop
for (i in 1:nrow(linelist)){
  
  # OPERATIONS
  # extract values from linelist for row i, using brackets for indexing
  row_gender  <- linelist$gender[[i]]
  row_age     <- linelist$age_years[[i]]    # don't forget to index!
     
  # combine gender-age and store in container vector at indexed location
  cases_demographics[[i]] <- str_c(row_gender, row_age, sep = ",") 

}  # end for loop


# display first 10 rows of container
head(cases_demographics, 10)
```


### Contenedor {.unnumbered}

A veces los resultados de tu *bucle for* se imprimirán en la consola o en el panel de gráficos de RStudio. Otras veces, querrás almacenar los resultados en un "contenedor" para su uso posterior. Dicho contenedor puede ser un vector, un dataframe o incluso una lista. 

Lo más eficiente es crear el contenedor de los resultados *antes de* comenzar el *bucle for*. En la práctica, esto significa crear un vector vacío, un dataframe o una lista. Estos pueden ser creados con las funciones `vector()` para vectores o listas, o con `matrix()` y `data.frame()` para un dataframe. 

**Vector vacío** 

Utiliza `vector()` y especifica el `mode = ` en función del tipo esperado de objetos que vas a insertar - ya sea "double" (para contener números), "carácter" o "lógico". También debes establecer la  `length = ` (longitud) por adelantado. Esta debe ser la longitud de tu secuencia de *bucle for*. 

Digamos que quieres almacenar la mediana de la demora hasta el ingreso para cada hospital. Utilizarís "double" y establecerías que la longitud fuera el número de salidas esperadas (el número de hospitales únicos en el set de datos). 

```{r}
delays <- vector(
  mode = "double",                            # we expect to store numbers
  length = length(unique(linelist$hospital))) # the number of unique hospitals in the dataset
```

**Dataframe vacío** 

Puedes hacer un dataframe vacío especificando el número de filas y columnas de esta manera:  
     
```{r, eval=F}
delays <- data.frame(matrix(ncol = 2, nrow = 3))
```


**Lista vacía** 

Es posible que desees almacenar algunos gráficos creados por un *bucle for* en una lista. Una lista es como un vector, pero contiene otros objetos R dentro de ella que pueden ser de diferente tipo. Los elementos de una lista pueden ser un solo número, un dataframe, un vector e incluso otra lista. 

En realidad, se inicializa una lista vacía utilizando el mismo comando `vector()` que el anterior, pero con  `mode = "list"`. Especifica la longitud como quieras. 

```{r, eval=F}
plots <- vector(mode = "list", length = 16)
```




### Impresión  {.unnumbered}  

Ten en cuenta que para imprimir desde dentro de un *bucle for* probablemente tendrás que envolver explícitamente con la función `print()`. 
En este ejemplo, la secuencia es un vector de caracteres explícito, que se utiliza para subsumir `linelist` en un hospital. Los resultados no se almacenan en un contenedor, sino que se imprimen en la consola con la función `print()`. 

```{r}
for (hosp in hospital_names){ 
     hospital_cases <- linelist %>% filter(hospital == hosp)
     print(nrow(hospital_cases))
}
```


###Probar tu bucle for  {.unnumbered}

Para probar tu bucle, puedes ejecutar un comando para hacer una asignación temporal del "elemento", como `i <- 10` o `hosp <- "Central Hospital "`. Haz esto *fuera del bucle* y luego ejecuta tu código de operaciones solamente (el código dentro de las llaves) para ver si se producen los resultados esperados. 




### Bucles con gráficos {.unnumbered}

Para reunir los tres componentes (contenedor, secuencia y operaciones) vamos a intentar trazar una epicurva para cada hospital (véase la página sobre [curvas epidémicas](#epidemic-curves)). 

Podemos hacer una bonita epicurva de *todos los* casos por género utilizando el paquete **incidence2** como se indica a continuación: 

```{r, warning=F, message=F}
# create 'incidence' object
outbreak <- incidence2::incidence(   
     x = linelist,                   # dataframe - complete linelist
     date_index = date_onset,        # date column
     interval = "week",              # aggregate counts weekly
     groups = gender,                # group values by gender
     na_as_group = TRUE)             # missing gender is own group

# plot epi curve
plot(outbreak,                       # name of incidence object
     fill = "gender",                # color bars by gender
     color = "black",                # outline color of bars
     title = "Outbreak of ALL cases" # title
     )
```

Para producir un gráfico separado para cada caso del hospital, podemos poner este código de epicurva dentro de un *bucle for*. 

En primer lugar, guardamos un vector con los nombres únicos de los hospitales, `hospital_names`. El *bucle for* se ejecutará una vez para cada uno de estos nombres: `for (hosp in hospital_names)`. En cada iteración del *bucle for*, el nombre actual del hospital del vector se representará como `hosp` para tu uso dentro del bucle. 

Dentro de las operaciones del bucle, puedes escribir el código R de forma normal, pero utilizando el "elemento" (`hosp ` en este caso) sabiendo que tu valor será cambiante. Dentro de este bucle: 

* Se aplica un `filter()` a `linelist`, de forma que la columna `hospital` debe ser igual al valor actual de `hosp` 
* El objeto de incidencia se crea en `linelist` filtradas 
* Se crea una gráfica del hospital actual, con un título autoajustable que utiliza `hosp` 
* El gráfico del hospital actual se guarda temporalmente y luego se imprime 
* El bucle sigue adelante para repetirse con el siguiente hospital de `hospital_names` 

```{r, out.width='50%', message = F}
# make vector of the hospital names
hospital_names <- unique(linelist$hospital)

# for each name ("hosp") in hospital_names, create and print the epi curve
for (hosp in hospital_names) {
     
     # create incidence object specific to the current hospital
     outbreak_hosp <- incidence2::incidence(
          x = linelist %>% filter(hospital == hosp),   # linelist is filtered to the current hospital
          date_index = date_onset,
          interval = "week", 
          groups = gender,
          na_as_group = TRUE
     )
     
     # Create and save the plot. Title automatically adjusts to the current hospital
     plot_hosp <- plot(
       outbreak_hosp,
       fill = "gender",
       color = "black",
       title = stringr::str_glue("Epidemic of cases admitted to {hosp}")
     )
     
     # print the plot for the current hospital
     print(plot_hosp)
     
} # end the for loop when it has been run for every hospital in hospital_names 
```



### Seguimiento del progreso de un bucle {.unnumbered} 

Un bucle con muchas iteraciones puede funcionar durante muchos minutos o incluso horas. Por lo tanto, puede ser útil imprimir el progreso en la consola de R. La sentencia `if` de abajo puede colocarse *dentro* de las operaciones del bucle para imprimir cada 100 números. Sólo tiene que ajustarla para que `i` sea el "elemento" de tu bucle. 

```{r, eval=F}
# loop with code to print progress every 100 iterations
for (i in seq_len(nrow(linelist))){

  # print progress
  if(i %% 100==0){    # The %% operator is the remainder
    print(i)

}
```





<!-- ======================================================= -->
## **purrr** y listas {#iter_purrr}

Otro enfoque de las operaciones iterativas es el paquete **purrr** - es el enfoque **tidyverse** de la iteración. 

Si tienes que realizar la misma tarea varias veces, probablemente merezca la pena crear una solución generalizada que puedas utilizar en muchas entradas. Por ejemplo, producir gráficos para múltiples jurisdicciones, o importar y combinar muchos archivos. 

También hay algunas otras ventajas de **purrr** - puedes usarlo con pipes `%>%`, que maneja los errores mejor que los *bucles for* normales, ¡y la sintaxis es bastante limpia y simple! Si estás utilizando un *bucle for*, probablemente puedas hacerlo de forma más clara y sucinta con **purrr**! 

Ten en cuenta que **purrr** es una *herramienta de programación funcional*. Es decir, las operaciones que se van a aplicar de forma iterativa están envueltas en *funciones*. Consulta la página [Escribir funciones](#writing-functions-1) para aprender a escribir tus propias funciones. 

**purrr** también se basa casi por completo en *listas* y *vectores*, así que piensa en ello como si aplicaras una función a cada elemento de esa lista/vector. 
     
### Cargar paquetes {.unnumbered}  

**purrr** forma parte de **tidyverse**, por lo que no es necesario instalar/cargar un paquete aparte. 

```{r}
pacman::p_load(
     rio,            # import/export
     here,           # relative filepaths
     tidyverse,      # data mgmt and viz
     writexl,        # write Excel file with multiple sheets
     readxl          # import Excel with multiple sheets
)
```


### `map()` {.unnumbered}  

Una de las funciones principales **de purrr** es `map()`, que "mapea" (aplica) una función a cada elemento de entrada de una lista/vector que has proporcionado. 

La sintaxis básica es `map(.x = SECUENCIA, .f = FUNCIÓN, OTROS ARGUMENTOS)`. Con un poco más de detalle: 

* `.x = ` son las *entradas* sobre las que se aplicará iterativamente la función .f - por ejemplo, un vector de nombres de jurisdicciones, columnas de un dataframe o una lista de dataframes 
* `.f = ` es la *función* a aplicar a cada elemento de la entrada `.x` - puede ser una función como `print()` que ya existe, o una función personalizada que tu definas. La función se suele escribir después de una tilde `~` (detalles más abajo). 

Algunas notas más sobre la sintaxis: 

* Si la función no necesita especificar más argumentos, puede escribirse sin paréntesis y sin tilde (por ejemplo, `.f = mean`). Para proporcionar argumentos que tendrán el mismo valor en cada iteración, escríbelos dentro de `map()` pero fuera del argumento `.f = `, como por ejemplo `na.rm = T` en `map(.x = mi_lista, .f = mean, na.rm=T)`. 
* Puedes utilizar `.x` (o simplemente `.` ) *dentro de* la función `.f = ` como marcador de posición para el valor `.x` de esa iteración 
* Utiliza la sintaxis con tilde (`~`) para tener un mayor control sobre la función - escribe la función de forma normal con paréntesis, como por ejemplo: `map(.x = mi_lista, .f = \~mean(., na.rm = T))`. Utiliza esta sintaxis sobre todo si el valor de un argumento va a cambiar en cada iteración, o si es el propio valor `.x` (véanse los ejemplos siguientes) 

**La salida al usar `map()` es una *lista* **- una lista es un tipo de objeto como un vector pero cuyos elementos pueden ser de tipo diferente. Por lo tanto, una lista producida por `map()` podría contener muchos dataframes, o muchos vectores, muchos valores individuales, ¡o incluso muchas listas! Existen versiones alternativas de `map()` que se explican a continuación y que producen otros tipos de salidas (por ejemplo, `map_dfr()` para producir un dataframe, `map_chr()` para producir vectores de caracteres y `map_dbl()` para producir vectores numéricos). 

#### Ejemplo: importar y combinar hojas de Excel  {#iter_combined .unnumbered}  

**Hagamos una demostración con una tarea epidemiológica común:** - *Quieres importar un libro de Excel con datos de casos, pero los datos están divididos en diferentes hojas en el libro. ¿Cómo puedes importar y combinar eficazmente las hojas en un dataframe?* 

Supongamos que nos envían el siguiente libro de Excel. Cada hoja contiene casos de un determinado hospital. 

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "hospital_linelists_excel_sheets.png"))
```

Este es un enfoque que utiliza `map()`: 

1.  `map()` la función `import()` para que se ejecute para cada hoja de Excel 
2.  Combinar los dataframes importados en uno solo utilizando `bind_rows()` 
3.  A lo largo del proceso, conserva el nombre original de la hoja para cada fila, almacenando esta información en una nueva columna en el dataframe final 

En primer lugar, tenemos que extraer los nombres de las hojas y guardarlos. Proporcionamos la ruta del archivo de Excel a la función `excel_sheets()` del paquete **readxl**, que extrae los nombres de las hojas. Los almacenamos en un vector de caracteres llamado `sheet_names`. 

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here::here("data", "example", "hospital_linelists.xlsx"))

```

```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx")
```

Aquí están los nombres: 

```{r}
sheet_names
```

Ahora que tenemos este vector de nombres, `map()` puede proporcionarlos uno a uno a la función `import()`. En este ejemplo, los `sheet_names` son `.x` e `import()` es la función `.f`. 

Recuerda de la página de [importación y exportación](#import-and-export) que cuando se utiliza en libros de Excel, `import()` puede aceptar el argumento `which = ` especificando la hoja a importar. Dentro de la función `.f` en  `import()`, proporcionamos `which = .x`, cuyo valor cambiará con cada iteración a través del vector `sheet_names - primero "Hospital Central", luego "Military Hospital", etc. 

Hay que tener en cuenta que, como hemos utilizado `map()`, los datos de cada hoja de Excel se guardarán como un dataframe separado dentro de una lista. Queremos que cada uno de estos elementos de la lista (dataframes) tenga un elemento *names*, así que antes de pasar `sheet_names` a `map()` lo pasamos a través de `set_names()` de **purrr**, lo que asegura que cada elemento de la lista obtenga el nombre apropiado. 

Guardamos la lista de salida como `combined`. 


```{r, echo=F}
combined <- sheet_names %>% 
  purrr::set_names() %>% 
  map(.f = ~import(here::here("data", "example", "hospital_linelists.xlsx"), which = .x))
```

```{r, eval=F}
combined <- sheet_names %>% 
  purrr::set_names() %>% 
  map(.f = ~import("hospital_linelists.xlsx", which = .x))
```

Cuando inspeccionamos la salida, vemos que los datos de cada hoja de Excel se guardan en la lista con un nombre. Esto es bueno, pero no hemos terminado. 


```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "sheets_as_list.png"))
```

Por último, utilizamos la función `bind_rows()` (de **dplyr**) que acepta la lista de dataframes de estructura similar y los combina en un dataframe. Para crear una nueva columna a partir de los *nombres* de los elementos de la lista, utilizamos el argumento `.id = `y le proporcionamos el nombre deseado para la nueva columna. 

A continuación se muestra toda la secuencia de comandos: 

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here::here("data", "example", "hospital_linelists.xlsx"))

combined <- sheet_names %>% 
  purrr::set_names() %>% 
  map(.f = ~import(here::here("data", "example", "hospital_linelists.xlsx"), which = .x)) %>% 
  bind_rows(.id = "origin_sheet")
```


```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx")  # extract sheet names
 
combined <- sheet_names %>%                                     # begin with sheet names
  purrr::set_names() %>%                                        # set their names
  map(.f = ~import("hospital_linelists.xlsx", which = .x)) %>%  # iterate, import, save in list
  bind_rows(.id = "origin_sheet") # combine list of data frames, preserving origin in new column  
```

¡Y ahora tenemos un dataframe con una columna que contiene la hoja de origen! 

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "sheets_as_df.png"))
```

Hay variaciones de `map()` que debes conocer. Por ejemplo, `map_dfr()` devuelve un dataframe, no una lista. Por lo tanto, podríamos haberla utilizado para la tarea anterior y no haber tenido que enlazar filas. Pero entonces no habríamos podido capturar de qué hoja (hospital) procedía cada caso. 

Otras variaciones son `map_chr()`, `map_dbl()`. Estas funciones son muy útiles por dos razones. En primer lugar, convierten automáticamente la salida de una función iterativa en un vector (no en una lista). En segundo lugar, pueden controlar explícitamente el tipo en el que vuelven los datos - te aseguras de que tus datos vuelven como un vector de caracteres con `map_chr()`, o vector numérico con `map_dbl()`. Volveremos a esto más adelante en la sección. 

Las funciones `map_at()` y `map_if()` también son muy útiles para la iteración - ¡permiten especificar en qué elementos de una lista se debe iterar! Funcionan simplemente aplicando un vector de índices/nombres (en el caso de `map_at()`) o una prueba lógica (en el caso de `map_if()`). 

Utilicemos un ejemplo en el que no queremos leer la primera hoja de datos del hospital. Usamos `map_at()` en lugar de `map()`, y especificamos el argumento `.at = ` a `c(-1)` que significa *no* usar el primer elemento de .x. Alternativamente, puedes proporcionar un vector de números positivos, o nombres, a `.at = ` para especificar qué elementos usar.  

```{r, echo=F}
sheet_names <- readxl::excel_sheets(here::here("data", "example", "hospital_linelists.xlsx"))

combined <- sheet_names %>% 
     purrr::set_names() %>% 
     # exclude the first sheet
     map_at(.f = ~import(here::here("data", "example", "hospital_linelists.xlsx"), which = .x),
            .at = c(-1))
```


```{r, eval=F}
sheet_names <- readxl::excel_sheets("hospital_linelists.xlsx")

combined <- sheet_names %>% 
     purrr::set_names() %>% 
     # exclude the first sheet
     map_at(.f = ~import( "hospital_linelists.xlsx", which = .x),
            .at = c(-1))
```

Ten en cuenta que el nombre de la primera hoja seguirá apareciendo como un elemento de la lista de salida, pero es sólo un nombre de un solo carácter (no un dataframe). Tendrás que eliminar este elemento antes de vincular las filas. Veremos cómo eliminar y modificar los elementos de la lista en una sección posterior. 


### Dividir los datos y exportar  {.unnumbered}  

A continuación, damos un ejemplo de cómo dividir unos datos en partes y luego utilizar la iteración `map()` para exportar cada parte como una hoja de Excel separada, o como un archivo CSV separado. 

#### Dividir los datos {.unnumbered}  

Digamos que tenemos una `lista de casos` de casos completa como un dataframe, y ahora queremos crear un listado separado para cada hospital y exportar cada una como un archivo CSV separado. A continuación, hacemos los siguientes pasos: 

Utiliza `group_split()` (de **dplyr**) para dividir el dataframe del listado por valores únicos en la columna `hospital`. La salida es una lista que contiene un dataframe por cada subconjunto de un hospital. 

```{r}
linelist_split <- linelist %>% 
     group_split(hospital)
```

Podemos ejecutar `View(linelist_split)` y ver que esta lista contiene 6 dataframes ("tibbles"), cada uno de los cuales representa los casos de un hospital. 

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_linelist_split.png"))
```

Sin embargo, ten en cuenta que los dataframes de la lista no tienen nombres por defecto. Queremos que cada uno de ellos tenga un nombre, y luego utilizar ese nombre al guardar el archivo CSV. 

Un enfoque para extraer los nombres es utilizar `pull()` (de **dplyr**) para extraer la columna `hospital` de cada dataframe de la lista. Luego, para estar seguros, convertimos los valores a caracteres y luego usamos `unique()` para obtener el nombre de ese dataframe en particular. Todos estos pasos se aplican a cada dataframe mediante `map()`. 


```{r}
names(linelist_split) <- linelist_split %>%   # Assign to names of listed data frames 
     # Extract the names by doing the following to each data frame: 
     map(.f = ~pull(.x, hospital)) %>%        # Pull out hospital column
     map(.f = ~as.character(.x)) %>%          # Convert to character, just in case
     map(.f = ~unique(.x))                    # Take the unique hospital name
```

Ahora podemos ver que cada uno de los elementos de la lista tiene un nombre. Se puede acceder a estos nombres mediante `names(linelist_split)`.   

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_linelist_split_named.png"))
```

```{r}
names(linelist_split)
```


##### Más de una columna `group_split()`  {.unnumbered}  

Si deseas dividir `linelist` por *más de una columna de agrupación*, por ejemplo, para producir un listado de subconjuntos por la intersección de hospital Y género, necesitarás un enfoque diferente para nombrar los elementos de la lista. Esto implica recoger las "claves de grupo" únicas utilizando `group_keys()` de **dplyr** - se devuelven como un dataframe. Luego puedes combinar las claves de grupo en valores con `unite()` como se muestra a continuación, y asignar estos nombres conglomerados a `linelist_split`.   

```{r}
# split linelist by unique hospital-gender combinations
linelist_split <- linelist %>% 
     group_split(hospital, gender)

# extract group_keys() as a dataframe
groupings <- linelist %>% 
     group_by(hospital, gender) %>%       
     group_keys()

groupings      # show unique groupings 
```

Ahora combinamos las agrupaciones juntas, separadas por guiones, y las asignamos como los nombres de los elementos de la lista en `linelist_split`. Esto requiere algunas líneas adicionales, ya que sustituimos `NA` por "Missing", utilizamos `unite()` de **dplyr** para combinar los valores de las columnas juntos (separados por guiones), y luego los convertimos en un vector sin nombre para poder utilizarlos como nombres de `linelist_split`.  

```{r, eval=F}
# Combine into one name value 
names(linelist_split) <- groupings %>% 
     mutate(across(everything(), replace_na, "Missing")) %>%  # replace NA with "Missing" in all columns
     unite("combined", sep = "-") %>%                         # Unite all column values into one
     setNames(NULL) %>% 
     as_vector() %>% 
     as.list()
```



#### Exportar como hojas de Excel  {.unnumbered}  

Para exportar los listados del hospital como *un libro de Excel con un listado por hoja*, podemos simplemente proporcionar la lista con nombre `linelist_split` a la función `write_xlsx()` del paquete **writexl**. Esto tiene la capacidad de guardar un libro de Excel con múltiples hojas. Los nombres de los elementos de la lista se aplican automáticamente como los nombres de las hojas.  

```{r, eval=F}
linelist_split %>% 
     writexl::write_xlsx(path = here("data", "hospital_linelists.xlsx"))
```

Ahora puedes abrir el archivo de Excel y ver que cada hospital tiene tu propia hoja. 

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_export_sheets.png"))
```

#### Exportar como archivos CSV  {.unnumbered}  

Es un comando un poco más complejo, pero también puedes exportar cada listado por hospital como un archivo CSV separado, con un nombre de archivo específico para el hospital. 

De nuevo utilizamos `map()`: tomamos el vector de nombres de elementos de la lista (mostrado arriba) y utilizamos `map()` para iterar a través de ellos, aplicando `export()` (del paquete **rio**, véase la página [Importar y exportar](#import-and-export)) en el dataframe de lista `linelist_split` que tiene ese nombre. También utilizamos el nombre para crear un nombre de archivo único. Así es como funciona: 

* Comenzamos con el vector de nombres de caracteres, pasado a `map()` como `.x` 
* La función `.f` es `export()`, que requiere un dataframe y una ruta de archivo para escribirlo 
* La entrada `.x` (el nombre del hospital) se utiliza *dentro* de `.f` para extraer/indexar ese elemento específico de la lista `linelist_split`. Esto hace que sólo se proporcione un dataframe a la vez a `export()`. 
* Por ejemplo, cuando `map()` itera por "Military Hospital", entonces `linelist`_split[.x]] es en realidad `linelist_split[["Military Hospital"]]`, devolviendo así el segundo elemento de `linelist_split` - que son todos los casos del Military Hospital. 
* La ruta del archivo proporcionada a `export()` es dinámica mediante el uso de `str_glue()` (ver página de [caracteres y cadenas](#characters-and-strings)): 
* here() se utiliza para obtener la base de la ruta del archivo y especificar la carpeta "data" (nótese las comillas simples para no interrumpir las comillas dobles de `str_glue()`) 
* A continuación, una barra `/`, y luego de nuevo el `.x` que imprime el nombre actual del hospital para que el archivo sea identificable 
* Por último, la extensión ".csv" que `export()` utiliza para crear un archivo CSV 

```{r, eval=F, message = F, warning=F}
names(linelist_split) %>%
     map(.f = ~export(linelist_split[[.x]], file = str_glue("{here('data')}/{.x}.csv")))
```

¡Ahora puedes ver que cada archivo se guarda en la carpeta "data" del proyecto R "Epi_R_handbook"."!  
     
```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "purrr_export_csv.png"))
```



### Funciones personalizadas  {.unnumbered}  

Puedes crear tu propia función para proporcionar a `map()`. 

Digamos que queremos crear curvas epidémicas para los casos de cada hospital. Para hacer esto usando **purrr**, nuestra función `.f` puede ser `ggplot()` y las extensiones con + como de costumbre. Como la salida de `map()` es siempre una lista, los gráficos se almacenan en una lista. Como son gráficos, pueden ser extraídas y trazadas con la función `ggarrange()` del paquete **ggpubr** ([documentación](https://rpkgs.datanovia.com/ggpubr/reference/ggarrange.html)).  


```{r, message = F, warning=F}

# load package for plotting elements from list
pacman::p_load(ggpubr)

# map across the vector of 6 hospital "names" (created earlier)
# use the ggplot function specified
# output is a list with 6 ggplots

hospital_names <- unique(linelist$hospital)

my_plots <- map(
  .x = hospital_names,
  .f = ~ggplot(data = linelist %>% filter(hospital == .x)) +
                geom_histogram(aes(x = date_onset)) +
                labs(title = .x)
)

# print the ggplots (they are stored in a list)
ggarrange(plotlist = my_plots, ncol = 2, nrow = 3)
```

Si este código de `map()` parece demasiado desordenado, se puede conseguir el mismo resultado guardando el comando específico de `ggplot()` como una función personalizada definida por el usuario, por ejemplo podemos llamarla `make_epicurve()`). Esta función se utiliza entonces dentro de la función `map()`. `.x` se sustituirá iterativamente por el nombre del hospital, y se utilizará como `hosp_name` en la función `make_epicurve()`. Véase la página sobre [Escribir funciones](#writing-functions-1). 

```{r, eval=F}
# Create function
make_epicurve <- function(hosp_name){
  
  ggplot(data = linelist %>% filter(hospital == hosp_name)) +
    geom_histogram(aes(x = date_onset)) +
    theme_classic()+
    labs(title = hosp_name)
  
}
```

```{r, eval=F}
# mapping
my_plots <- map(hospital_names, ~make_epicurve(hosp_name = .x))

# print the ggplots (they are stored in a list)
ggarrange(plotlist = my_plots, ncol = 2, nrow = 3)
```




### Mapear una función a través de las columnas  {.unnumbered}  

Otro caso de uso común es asignar una función a varias columnas. A continuación, `map()` la función `t.test()` a través de columnas numéricas del dataframe `linelist`, comparando los valores numéricos por género. 

Recuerda de la página sobre [Test estadísticos simples](#simple-statistical-tests) que `t.test()` puede tomar entradas en un formato de fórmula, como `t.test(columna numérica ~ columna binaria)`. En este ejemplo, hacemos lo siguiente: 

* Las columnas numéricas de interés se seleccionan del listado - éstas se convierten en las entradas `.x` de `map()` 
* La función `t.test()` se suministra como la función `.f`, que se aplica a cada columna numérica 
* Dentro del paréntesis de `t.test()`: 
  * el primer `~` precede al `.f` que `map()` iterará sobre el `.x` 
  * el `.x` representa la columna actual que se suministra a la función `t.test()` 
  * el segundo `~` es parte de la ecuación del test-t descrita anteriormente 
  * la función `t.test()` espera una columna binaria en el lado derecho de la ecuación. Suministramos el vector `linelist$gender` de forma independiente y estática (Ten en cuenta que no se incluye en `select()`). 

`map()` devuelve una lista, por lo que la salida es una lista de resultados del test-t, un elemento de la lista por cada columna numérica analizada. 

```{r}
# Results are saved as a list
t.test_results <- linelist %>% 
  select(age, wt_kg, ht_cm, ct_blood, temp) %>%  # keep only some numeric columns to map across
  map(.f = ~t.test(.x ~ linelist$gender))        # t.test function, with equation NUMERIC ~ CATEGORICAL
```

Este es el aspecto de la lista `t.test_results` cuando se abre (view) en RStudio. Hemos resaltado las partes que son importantes para los ejemplos de esta página. 

* Puedes ver en la parte superior que toda la lista se llama `t.test_results` y tiene cinco elementos. Esos cinco elementos se denominan `age`, `wt_km`, `ht_cm`, `ct_blood`, `temp` después de cada variable que se utilizó en una prueba t con el `gender` de  `linelist`. 
* Cada uno de esos cinco elementos son a su vez listas, con elementos dentro de ellas como `p.value` y `conf.int`. Algunos de estos elementos, como `p.value`, son números individuales, mientras que otros, como `conf.int`, constan de dos o más elementos (`mean in group f` y `mean in group m`). 

```{r, out.height="150%", echo=F}
knitr::include_graphics(here::here("images", "purrr_ttest.png"))
```


Nota: Recuerda que si deseas aplicar una función sólo a determinadas columnas de un dataframe, puedes utilizar simplemente `mutate()` y `across()`, como se explica en la página [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions). A continuación se muestra un ejemplo de aplicación de `as.character()` sólo a las columnas "age". Observa la colocación de los paréntesis y las comas.   

```{r, eval=F}
# convert columns with column name containing "age" to class Character
linelist <- linelist %>% 
  mutate(across(.cols = contains("age"), .fns = as.character))  
```


### Extraer de las listas  {.unnumbered}  

Como `map()` produce una salida del tipo List, dedicaremos algún tiempo a discutir cómo extraer datos de las listas utilizando las funciones **purrr** que las acompañan. Para mostrar esto, utilizaremos la lista `t.test_results` de la sección anterior. Esta es una lista de 5 listas - cada una de las 5 listas contiene los resultados de una prueba t entre una columna del dataframe `linelist` y su columna binaria `gender`. Consulta la imagen de la sección anterior para ver la estructura de la lista. 

#### Nombres de elementos {.unnumbered}  

Para extraer los nombres de los elementos en sí, basta con utilizar `names()` de R **base**. En este caso, utilizamos `names()` en `t.test_results` para devolver los nombres de cada sublista, que son los nombres de las 5 variables a las que se les realizaron test-t. 

```{r}
names(t.test_results)
```

#### Elementos por nombre o posición {.unnumbered}  

Para extraer los elementos de la lista por su nombre o su posición se pueden utilizar paréntesis `[[ ]]` como se describe en la página de [fundamentos de R](#r-basics). A continuación, utilizamos corchetes dobles para indexar la lista `t.test_results` y mostrar el primer elemento, que son los resultados del test-t sobre `age`.

```{r}
t.test_results[[1]] # first element by position
t.test_results[[1]]["p.value"] # return element named "p.value" from first element  
```

Sin embargo, a continuación mostraremos el uso de las sencillas y flexibles funciones de **purrr** `map()` y `pluck()` para lograr los mismos resultados. 

#### `pluck()` {.unnumbered}  

`pluck()` extrae elementos por nombre o por posición. Por ejemplo, para extraer los resultados del test-t para la edad, puedes utilizar `pluck()` así:   

```{r}
t.test_results %>% 
  pluck("age")        # alternatively, use pluck(1)
```

Indexa niveles más profundos especificando los niveles adicionales con comas. A continuación se extrae el elemento denominado "p.value" de la lista `age` dentro de la lista `t.test_results`. También puedes utilizar números en lugar de nombres de caracteres.   

```{r}
t.test_results %>% 
  pluck("age", "p.value")
```

Puedes extraer estos elementos internos de *todos* los elementos de primer nivel utilizando `map()` para ejecutar la función `pluck()` en cada elemento de primer nivel. Por ejemplo, el siguiente código extrae los elementos "p.value" de todas las listas dentro de `t.test_results`. La lista de resultados del test-t es el `.x` que se itera, `pluck()` es la función `.f` que se itera, y el valor "p-value" se proporciona a la función.    

```{r}
t.test_results %>%
  map(pluck, "p.value")   # return every p-value
```

Como otra alternativa, `map()` ofrece una forma abreviada en la que puedes escribir el nombre del elemento entre comillas, y lo extraerá. Si utilizas `map()` la salida será una lista, mientras que si utilizas `map_chr()` será un vector de caracteres con nombre y si utilizas `map_dbl()` será un vector numérico con nombre.

```{r}
t.test_results %>% 
  map_dbl("p.value")   # return p-values as a named numeric vector
```

Puedes leer más sobre `pluck()` en la [documentación  **purrr**](https://purrr.tidyverse.org/reference/pluck.html). Tiene una función hermana `chuck()` que devolverá un error en lugar de NULL si no existe un elemento. 



### Convertir una lista en un dataframe  {.unnumbered}  

Este es un tema complejo - Mira la sección de Recursos para tutoriales más completos. Sin embargo, vamos a mostrar la conversión de la lista de resultados del test-t en un dataframe. Crearemos un dataframe con columnas para la variable, su valor-p y las medias de los dos grupos (hombres y mujeres). 

Estos son algunos de los nuevos enfoques y funciones que se utilizarán: 

* La función `tibble()` se utilizará para crear un tibble (como un dataframe) 
  * Rodeamos la función `tibble()` con corchetes `{ }` para evitar que todo el `t.test_results` se almacene como la primera columna tibble 
* Dentro de `tibble()`, cada columna se crea explícitamente, de forma similar a la sintaxis de `mutate()`: 
  * El `.` representa `t.test_results` 
  * Para crear una columna con los nombres de las variables del test-t (los nombres de cada elemento de la lista) utilizamos `names()` como se ha descrito anteriormente 
  * Para crear una columna con los valores p utilizamos `map_dbl()` como se ha descrito anteriormente para extraer los elementos `p.value` y convertirlos en un vector numérico 

```{r}
t.test_results %>% {
  tibble(
    variables = names(.),
    p         = map_dbl(., "p.value"))
  }
```

Pero ahora vamos a añadir columnas que contengan las medias de cada grupo (hombres y mujeres). 

Tendríamos que extraer el elemento `estimate`, pero éste contiene en realidad *dos* elementos en su interior (`media en el grupo f` y `media en el grupo m`). Por lo tanto, no se puede simplificar en un vector con `map_chr()` o `map_dbl()`. En su lugar, utilizamos `map()`, que usado dentro de `tibble()` creará *una columna de tipo lista dentro del tibble*! ¡Sí, esto es posible! 

```{r}
t.test_results %>% 
  {tibble(
    variables = names(.),
    p = map_dbl(., "p.value"),
    means = map(., "estimate"))}
```

Una vez que tengas esta columna de lista, hay varias funciones de **tidyr** (parte de **tidyverse**) que te ayudan a "rectangular" o "desanidar" estas columnas de "lista anidada". Lee más sobre ellas [aquí](#iteration-loops-and-lists), o ejecutando `vignette("rectangle")`. En resumen: 

* `unnest_wider()` - da a cada elemento de una lista-columna tu propia columna 
* `unnest_longer()` - da a cada elemento de una lista-columna tu propia fila 
* `hoist()` - actúa como `unnest_wider()` pero se especifica qué elementos se van a anular 

A continuación, pasamos el tibble a `unnest_wider()` especificando la columna `means` del tibble (que es una lista anidada). El resultado es que `means` se sustituyen por dos nuevas columnas, cada una de las cuales refleja los dos elementos que había antes en cada celda `means`. 

```{r}
t.test_results %>% 
  {tibble(
    variables = names(.),
    p = map_dbl(., "p.value"),
    means = map(., "estimate")
    )} %>% 
  unnest_wider(means)
```



### Descartar, conservar y compactar  listas {.unnumbered}  

Dado que el trabajo con **purrr** implica a menudo listas, exploraremos brevemente algunas funciones de **purrr** para modificar listas. Consulta la sección de Recursos para ver tutoriales más completos sobre las funciones de **purrr**. 

* `list_modify()` tiene muchos usos, uno de los cuales puede ser eliminar un elemento de la lista 
* `keep()` conserva los elementos especificados a `.p = `, o cuando una función suministrada a `.p = ` evalúa a TRUE 
* `discard()` elimina los elementos especificados a `.p`, o cuando una función suministrada a `.p = ` evalúa a TRUE 
* `compact()` elimina todos los elementos vacíos 

Aquí hay algunos ejemplos que utilizan la lista `combined` creada en la sección anterior sobre el [uso de `map()` para importar y combinar múltiples archivos](#iter_combined) (contiene 6 dataframes de listas de casos): 

Los elementos pueden ser eliminados por su nombre con `list_modify()` y estableciendo el nombre igual a `NULL`.  

```{r, eval=F}
combined %>% 
  list_modify("Central Hospital" = NULL)   # remove list element by name
```

También puedes eliminar elementos por criterio, proporcionando una ecuación "predicada" a `.p = ` (una ecuación que evalúa a TRUE o FALSE). Coloca una tilde `~` antes de la función y utiliza `.x` para representar el elemento de la lista. Utilizando `keep()` se conservarán los elementos de la lista que se evalúen como TRUE. A la inversa, si se utiliza `discard()` se eliminarán los elementos de la lista que se evalúen como TRUE. 

```{r, eval=F}
# keep only list elements with more than 500 rows
combined %>% 
  keep(.p = ~nrow(.x) > 500)  
```

En el siguiente ejemplo, los elementos de la lista se descartan si tu tipo no son dataframes. 

```{r, eval=F}
# Discard list elements that are not data frames
combined %>% 
  discard(.p = ~class(.x) != "data.frame")
```

Su función de predicado también puede hacer referencia a elementos/columnas dentro de cada elemento de la lista. Por ejemplo, a continuación, se descartan los elementos de la lista cuya media de la columna `ct_blood` sea superior a 25.  

```{r, eval=F}
# keep only list elements where ct_blood column mean is over 25
combined %>% 
  discard(.p = ~mean(.x$ct_blood) > 25)  
```

Este comando eliminaría todos los elementos vacíos de la lista:   

```{r, eval=F}
# Remove all empty list elements
combined %>% 
  compact()
```



### `pmap()` {.unnumbered}

ESTA SECCIÓN ESTÁ EN CONSTRUCCIÓN 



## Funciones Apply  {#apply-functions}

La familia de funciones "apply" es una alternativa de R **base** a **purrr** para operaciones iterativas. Puedes leer más sobre ellas [aquí](https://www.datacamp.com/community/tutorials/r-tutorial-apply-family). 






<!-- ======================================================= -->
## Recursos  {#resources-8}

[Bucles for en Data Carpentry](https://datacarpentry.org/semester-biology/materials/for-loops-R/) 

La [página de R for Data Science en español sobre la iteración](https://es.r4ds.hadley.nz/iteración.html) 

[Viñeta sobre escritura/lectura de archivos Excel](https://martinctc.github.io/blog/vignette-write-and-read-multiple-excel-files-with-purrr/) 

Un [tutorial de purrr](https://jennybc.github.io/purrr-tutorial/index.html) por jennybc 

Otro [tutorial de purrr](http://www.rebeccabarter.com/blog/2019-08-19_purrr/) por Rebecca Barter 

Un [tutorial de purrr](http://zevross.com/blog/2019/06/11/the-power-of-three-purrr-poseful-iteration-in-r-with-map-pmap-and-imap/)  sobre map, pmap e imap 

[hoja de trucos -cheatsheet- de purrr](https://raw.githubusercontent.com/rstudio/cheatsheets/master/pngs/thumbnails/purrr-cheatsheet-thumbs.png) 

[consejos y trucos de purrr](https://www.emilhvitfeldt.com/post/2018-01-08-purrr-tips-and-tricks/)
[guardar y descartar](https://hookedondata.org/going-off-the-map/#keep-and-discard) 

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/iteration.Rmd-->

# (PART) Análisis {.unnumbered}

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_analysis.Rmd-->

# Tablas descriptivas {#descriptive-tables} 

```{r out.width = c('75%'), fig.align='center', fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "descriptive_tables.png"))
```


Esta página muestra el uso de **janitor**, **dplyr**, **gtsummary**, **rstatix** y R **base** para resumir datos y crear tablas con estadísticas descriptivas. 

En *esta página se explica cómo* crear* las tablas subyacentes, mientras que en la página [Tablas para  presentaciones](#tables-for-presentation) se explica cómo darles un buen formato e imprimirlas.* 

Cada uno de estos paquetes tiene ventajas y desventajas en cuanto a la simplicidad del código, la accesibilidad de los resultados y la calidad de los resultados impresos. Utiliza esta página para decidir qué enfoque se ajusta a tu situación. 

Tienes varias opciones para producir tablas de resumen de tabulación y tabulación cruzada. Algunos de los factores a tener en cuenta son la simplicidad del código, la posibilidad de personalización, la salida deseada (impresa en la consola de R, como dataframe, o como una imagen .png/.jpeg/.html "bonita"), y la facilidad de posprocesamiento. Ten en cuenta los siguientes puntos a la hora de elegir la herramienta para tu situación. 

* Utiliza `tabyl()` de **janitor** para producir y "adornar" tabulaciones y tabulaciones cruzadas 
* Utiliza `get_summary_stats()` de **rstatix** para generar fácilmente dataframes de estadísticas de resumen numérico para múltiples columnas y/o grupos 
* Utiliza `summarise()` y `count()` de **dplyr** para obtener estadísticas más complejas, ordenar las salidas de los dataframes o preparar los datos para `ggplot()` 
* Utiliza `tbl_summary`() de **gtsummary** para producir tablas detalladas listas para su publicación 
* Utiliza `table()` de R **base** si no tienes acceso a los paquetes anteriores 


<!-- ======================================================= -->
## Preparación  {#preparation-8}

### Cargar paquetes {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R. 


```{r, warning=F, message=F}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics 
  gtsummary,    # summary statistics and tests
  rstatix,      # summary statistics and statistical tests
  janitor,      # adding totals and percents to tables
  scales,       # easily convert proportions to percents  
  flextable     # converting tables to pretty images
  )
```

### Importar datos  {.unnumbered}

Importamos los datos de casos de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica para descargar `linelist` "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa los datos con la función `import()` del paquete **rio** (acepta muchos tipos de archivos como .xlsx, .rds, .csv - Mira la página de [importación y exportación](#import-and-export) para más detalles). 

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas del listado.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->
## Visualizar datos {#browse-data}

### Paquete **skimr** {.unnumbered}

Utilizando el paquete skimr, puedes obtener una visión detallada y estéticamente agradable de cada una de las variables de tu conjunto de datos. Lee más sobre **skimr** en su [página de github](https://github.com/ropensci/skimr). 

A continuación, se aplica la función `skim()` a todo el dataframe `linelist`. Se produce una visión general del dataframe y un resumen de cada columna (por tipo). 

```{r eval=F}
## get information about each variable in a dataset 
skim(linelist)
```

```{r  echo=F}
# sparkline histograms not showing correctly, so avoiding them.
skim_without_charts(linelist)
```



También puedes utilizar la función `summary()`, de R **base**, para obtener información completta sobre unos datos, pero esta salida puede ser más difícil de leer que utilizando **skimr**. Por eso no se muestra a continuación esta salida, para ahorrar espacio de la página.   

```{r, eval=F}
## get information about each column in a dataset 
summary(linelist)
```


### Estadísticas resumidas  {.unnumbered} 

Puedes utilizar las funciones de R **base** para producir estadísticas de resumen sobre una columna numérica. Puedes producir la mayoría de las estadísticas de resumen útiles para una columna numérica utilizando `summary()`, como se indica a continuación. Ten en cuenta que también debe especificarse el nombre del dataframe, como se muestra a continuación. 

```{r}
summary(linelist$age_years)
```

Puedes acceder y guardar una parte específica de la misma con los corchetes de índice `[ ]`: 

```{r}
summary(linelist$age_years)[[2]]            # return only the 2nd element
# equivalent, alternative to above by element name
# summary(linelist$age_years)[["1st Qu."]]  
```

Puedes mostrar estadísticas individuales con funciones de R **base** como  `max()`, `min()`, `median()`, `mean()`, `quantile()`, `sd()`, y `range()`. Consulta la página de [Fundamentos de R](#r-basics) para obtener una lista completa. 

***PRECAUCIÓN:*** Si tus datos contienen valores faltantes, R quiere que lo sepas y por ello mostrará `NA` a menos que se especifique en las funciones matemáticas anteriores que quieres que R ignore los valores faltantes, mediante el argumento `na.rm = TRUE`. 

Puedes utilizar la función `get_summary_stats()` de **rstatix** para producir las estadísticas de resumen *en un formato de dataframe*. Esto puede ser útil para realizar operaciones posteriores o trazar los números. Consulta la página [Tests estadísticos simples](#simple-statistical-tests) para obtener más detalles sobre el paquete **rstatix** y sus funciones. 

```{r}
linelist %>% 
  get_summary_stats(
    age, wt_kg, ht_cm, ct_blood, temp,  # columns to calculate for
    type = "common")                    # summary stats to return

```





## paquete **janitor** {#tbl_janitor}  

Los paquetes **janitor** ofrecen la función `tabyl()` para producir tabulaciones y tabulaciones cruzadas, que pueden ser "adornadas" o modificadas con funciones de ayuda para mostrar porcentajes, proporciones, recuentos, etc. 

A continuación, enlazamos el dataframe `linelist` con pipe a las funciones de **limpieza** e imprimimos el resultado. Si lo deseas, también puedes guardar las tablas resultantes con el operador de asignación `<-`.  

### Tabyl simple {.unnumbered}  

El uso por defecto de `tabyl()` en una columna específica produce los valores únicos, los recuentos y sus proporciones por columna. Las proporciones pueden tener muchos dígitos. Puedes ajustar el número de decimales con `adorn_rounding()` como se describe a continuación. 

```{r}
linelist %>% tabyl(age_cat)
```
Como puedes ver arriba, si hay valores que faltan se muestran en una fila etiquetada como `NA`. Puedes suprimirlos con `show_na = FALSE`. Si no hay valores faltantes, esta fila no aparecerá. Si hay valores faltantes, todas las proporciones se dan como crudas (el denominador incluye los recuentos de `NA`) y "válidas" (el denominador excluye los recuentos de `NA`). 

Si la columna es de tipo factor y sólo hay ciertos niveles en sus datos, todos los niveles seguirán apareciendo en la tabla. Puedes suprimir esta característica especificando `show_missing_levels = FALSE`. Lee más en la página de [Factores](#factors). 

### Tabulación cruzada  {.unnumbered}  

Los recuentos de tabulación cruzada se consiguen añadiendo una o más columnas adicionales dentro de `tabyl()`. Ten en cuenta que ahora sólo se muestran los recuentos - Las proporciones y porcentajes se pueden añadir con los pasos adicionales que se muestran a continuación.  

```{r}
linelist %>% tabyl(age_cat, gender)
```

### "Adornando" el tabyl {#tbl_adorn .unnumbered}  

Utiliza las funciones de "adorno" de **janitor** para añadir totales o convertir a proporciones, porcentajes, o ajustar la visualización de otro modo. A menudo, enlazarás el tabyl con pipe a través de varias de estas funciones. 


Función            | Resultado                         
-------------------|--------------------------------
`adorn_totals()`   | Añade los totales (`where = ` "row", "col", o "both"). Establecer `name =` para el "Total".     
`adorn_percentages()` | Convierte los recuentos en proporciones, con `denominator = ` "row", "col", o "all"  
`adorn_pct_formatting()` | Convierte las proporciones en porcentajes. Especifica los `digits =`. Elimina el símbolo "%" con `affix_sign = FALSE`.  
`adorn_rounding()` | Para redondear proporciones a `digits =`. Para redondear porcentajes utiliza `adorn_pct_formatting()` con `digits =`.   
`adorn_ns()` | Añade recuentos a una tabla de proporciones o porcentajes. indica la `position =` "rear" para mostrar los recuentos entre paréntesis, o "front" para poner los porcentajes entre paréntesis.    
`adorn_title()` | Añade una cadena mediante los argumentos `row_name = ` y/o `col_name = ` 

Se consciente del orden en que se aplican las funciones anteriores. A continuación, algunos ejemplos. 

Una simple tabla unidireccional con porcentajes en lugar de las proporciones por defecto. 

```{r}
linelist %>%               # case linelist
  tabyl(age_cat) %>%       # tabulate counts and proportions by age category
  adorn_pct_formatting()   # convert proportions to percents
```

Una tabulación cruzada con un total de filas y porcentajes de filas.  

```{r}
linelist %>%                                  
  tabyl(age_cat, gender) %>%                  # counts by age and gender
  adorn_totals(where = "row") %>%             # add total row
  adorn_percentages(denominator = "row") %>%  # convert counts to proportions
  adorn_pct_formatting(digits = 1)            # convert proportions to percents
```

Una tabulación cruzada ajustada para que aparezcan tanto los recuentos como los porcentajes.   

```{r}
linelist %>%                                  # case linelist
  tabyl(age_cat, gender) %>%                  # cross-tabulate counts
  adorn_totals(where = "row") %>%             # add a total row
  adorn_percentages(denominator = "col") %>%  # convert to proportions
  adorn_pct_formatting() %>%                  # convert to percents
  adorn_ns(position = "front") %>%            # display as: "count (percent)"
  adorn_title(                                # adjust titles
    row_name = "Age Category",
    col_name = "Gender")
```



### Impresión del tabyl {.unnumbered}

Por defecto, el tabyl se imprimirá en crudo en la consola de R. 

Alternativamente, puedes pasar el tabyl a **flextable** o un paquete similar para imprimirlo como una imagen "bonita" en el visor de RStudio, que podría exportarse como .png, .jpeg, .html, etc. Esto se discute en la página [Tablas para  presentaciones](#tables-for-presentation). Ten en cuenta que si imprimes de esta manera y utilizas `adorn_titles()`, debes especificar `placement = "combined"`. 

```{r}
linelist %>%
  tabyl(age_cat, gender) %>% 
  adorn_totals(where = "col") %>% 
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front") %>% 
  adorn_title(
    row_name = "Age Category",
    col_name = "Gender",
    placement = "combined") %>% # this is necessary to print as image
  flextable::flextable() %>%    # convert to pretty image
  flextable::autofit()          # format to one line per row 

```


### Uso en otras tablas {.unnumbered}  

Puedes utilizar las funciones `adorn_()` de **janitor** en otras tablas, como las creadas por `summarise()` y `count()` de **dplyr**, o `table()` de R **base**. Por ejemplo: 

```{r}
linelist %>% 
  count(hospital) %>%   # dplyr function
  adorn_totals()        # janitor function
```


### Guardar el tabyl  {.unnumbered}  

Si conviertes la tabla en una imagen "bonita" con un paquete como **flextable**, puedes guardarla con funciones de ese paquete - como `save_as_html()`, `save_as_word()`, `save_as_ppt()`, y `save_as_image()`  de **flextable** (como se discute más ampliamente en la página [Tablas para presentaciones](#tables-for-presentation)). A continuación, la tabla se guarda como un documento de Word, en el que se puede seguir editando a mano.  

```{r, eval=F}
linelist %>%
  tabyl(age_cat, gender) %>% 
  adorn_totals(where = "col") %>% 
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front") %>% 
  adorn_title(
    row_name = "Age Category",
    col_name = "Gender",
    placement = "combined") %>% 
  flextable::flextable() %>%                     # convert to image
  flextable::autofit() %>%                       # ensure only one line per row
  flextable::save_as_docx(path = "tabyl.docx")   # save as Word document to filepath
```

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "tabyl_word.png"))
```

### Estadísticas {#janitor_age_out_stats .unnumbered}  

Puedes aplicar a las tabyl tests estadísticos como `chisq.test()` o `fisher.test()` del paquete **stats**, como se muestra a continuación. Ten en cuenta que los valores faltantes no están permitidos, por lo que se excluyen de la tabulación con `show_na = FALSE`. 

```{r, warning=F, message=F}
age_by_outcome <- linelist %>% 
  tabyl(age_cat, outcome, show_na = FALSE) 

chisq.test(age_by_outcome)
```

Consulta la página sobre [Tests estadísticos sencillos](#simple-statistical-tests) para obtener más código y consejos sobre estadística. 

### Otros consejos  {.unnumbered}  

* Incluye el argumento `na.rm = TRUE` para excluir los valores faltantes de cualquiera de los cálculos anteriores. 
* Si aplicas cualquier función de ayuda `adorn_*()` a tablas no creadas por `tabyl()`, puedes especificar una(s) columna(s) particular(es) para aplicarlas como `adorn_percentage(,,,c(cases,deaths))` (especifícalos en el cuarto argumento sin nombre). La sintaxis no es sencilla. Considera la posibilidad de utilizar `summarise()` en su lugar. 
* Puedes leer más detalles en la [página de janitor](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html) y en esta [viñeta de tabyl](https://cran.r-project.org/web/packages/janitor/vignettes/tabyls.html). 




## paquete **dplyr** {#dplyr-package} 

**dplyr** forma parte de los paquetes **tidyverse** y es una herramienta de gestión de datos muy común. La creación de tablas con las funciones de **dplyr** `summarise()` y `count()` es un enfoque útil para calcular estadísticas de resumen, resumir *por grupos* o pasar tablas a `ggplot()`. 

`summarise()` crea un *nuevo dataframe de resumen*. Si los datos *no están agrupados*, se producirá un dataframe de una fila con las estadísticas de resumen especificadas de todo el dataframe. Si los datos están *agrupados*, el nuevo dataframe tendrá una fila por *grupo* (véase la página [Agrupar datos](#grouping-data)). 

Dentro del paréntesis de `summarise()`, se proporcionan los nombres de cada nueva columna de resumen, seguidos de un signo de igualdad y de una función estadística a aplicar. 

<span style="color: darkgreen;">***SUGERENCIA:*** La función summarise funciona tanto con la ortografía británica como con la estadounidense (summarise() y `summarize()`). </span>

### Obtener recuentos  {.unnumbered}  

La función más sencilla de aplicar dentro de `summarise()` es `n()`. Deja los paréntesis vacíos para contar el número de filas. 

```{r}
linelist %>%                 # begin with linelist
  summarise(n_rows = n())    # return new summary dataframe with column n_rows
```

Esto se vuelve más interesante si hemos agrupado los datos de antemano.  

```{r}
linelist %>% 
  group_by(age_cat) %>%     # group data by unique values in column age_cat
  summarise(n_rows = n())   # return number of rows *per group*
```

El comando anterior se puede acortar utilizando la función `count()` en su lugar. `count()` hace lo siguiente: 

1.  Agrupa los datos por las columnas que se le proporcionan 
2.  Los resume con `n()` (creando la columna `n`) 
3.  Desagrupa los datos 

```{r}
linelist %>% 
  count(age_cat)
```

Puedes cambiar el nombre de la columna de recuentos de la `n` por defecto a otra cosa especificando a `name = `. 

Los recuentos tabulados de dos o más columnas de agrupación se siguen devolviendo en formato "largo", con los recuentos en la columna `n`. Consulta la página sobre [Pivotar datos](#pivoting-data) para conocer los formatos de datos "long" y "wide". 

```{r}
linelist %>% 
  count(age_cat, outcome)
```


### Mostrar todos los niveles {.unnumbered}  
 
Si estás tabulando una columna de tipo *factor*, puedes asegurarte de que se muestren *todos* los niveles (no sólo los niveles con valores en los datos) añadiendo `.drop = FALSE` en el comando `summarise()` o `count()`. 

Esta técnica es útil para estandarizar sus tablas/gráficos. Por ejemplo, si está creando cifras para varios subgrupos, o creando repetidamente la cifra para informes de rutina. En cada una de estas circunstancias, la presencia de valores en los datos puede fluctuar, pero puedes definir niveles que permanezcan constantes. 

Para más información, consulta la página sobre [factores](#factors). 



### Proporciones {#tbl_dplyr_prop .unnumbered}  
 
Las proporciones pueden añadirse pasando la tabla por `mutate()` para crear una nueva columna. Define la nueva columna como la columna de recuentos (`n` por defecto) dividida por la `sum()` de la columna de recuentos (esto producirá una proporción). 

Ten en cuenta que en este caso, `sum()` en el comando `mutate()` producirá la suma de toda la columna `n` para utilizarla como denominador de la proporción. Como se explica en la página [Agrupar datos](#group_summarise), *si* `sum()` se utiliza en datos *agrupados* (por ejemplo, si el comando `mutate()` sigue inmediatamente a un comando `group_by()`), producirá sumas *por grupo*. Como se acaba de indicar, `count()` termina sus acciones *desagrupando*. Por lo tanto, en este escenario obtenemos proporciones de columnas completas. 

Para mostrar fácilmente los porcentajes, puedes envolver la proporción en la función `percent()` del paquete **scales** (Ten en cuenta que se convierte en tipo carácter). 

```{r}
age_summary <- linelist %>% 
  count(age_cat) %>%                     # group and count by gender (produces "n" column)
  mutate(                                # create percent of column - note the denominator
    percent = scales::percent(n / sum(n))) 

# print
age_summary
```

A continuación se presenta un método para calcular las proporciones *dentro de los grupos*. Se basa en diferentes niveles de agrupación de datos que se aplican y eliminan selectivamente. En primer lugar, los datos se agrupan en función del `outcome` mediante `group_by()`. A continuación, se aplica `count()`. Esta función agrupa además los datos por `age_cat` y devuelve los recuentos para cada combinación de `outcome`-`age-cat`. Es importante destacar que, al finalizar tu proceso, `count()` también *desagrupa* la agrupación `age_cat`, por lo que la única agrupación de datos que queda es la agrupación original por `outcome`. Por lo tanto, el paso final del cálculo de las proporciones (denominador sum(n)) sigue estando agrupado por `outcome`.  

```{r}
age_by_outcome <- linelist %>%                  # begin with linelist
  group_by(outcome) %>%                         # group by outcome 
  count(age_cat) %>%                            # group and count by age_cat, and then remove age_cat grouping
  mutate(percent = scales::percent(n / sum(n))) # calculate percent - note the denominator is by outcome group
```

```{r, echo=F}
DT::datatable(age_by_outcome, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```




### Gráficas {.unnumbered}  

Mostrar una tabla "larga" como la anterior con `ggplot()` es relativamente sencillo. Los datos están naturalmente en formato "largo", que es aceptado naturalmente por `ggplot()`. Mira más ejemplos en las páginas [Conceptos básicos de ggplot](#ggplot-basics) y [consejos de ggplot](#ggplot-tips). 


```{r, warning=F, message=F}
linelist %>%                      # begin with linelist
  count(age_cat, outcome) %>%     # group and tabulate counts by two columns
  ggplot()+                       # pass new data frame to ggplot
    geom_col(                     # create bar plot
      mapping = aes(   
        x = outcome,              # map outcome to x-axis
        fill = age_cat,           # map age_cat to the fill
        y = n))                   # map the counts column `n` to the height
```


### Estadísticas resumidas  {.unnumbered}  

Una de las principales ventajas de **dplyr** y de `summarise()` es la capacidad de producir resúmenes estadísticos más avanzados como `median()`, `mean()`, `max()`, `min()`, `sd()` (desviación estándar) y percentiles. También puedes utilizar `sum()` para mostrar el número de filas que cumplen ciertos criterios lógicos. Al igual que en el caso anterior, estas salidas pueden producirse para todo el conjunto de dataframes o por grupos. 

La sintaxis es la misma: dentro de los paréntesis de `summarise()` se proporcionan los nombres de cada nueva columna de resumen, seguidos de un signo de igualdad y de una función estadística para aplicar. Dentro de la función estadística, indica la(s) columna(s) con la(s) que se va a operar y cualquier argumento relevante (por ejemplo, `na.rm =` TRUE para la mayoría de las funciones matemáticas). 

También puedes utilizar `sum()` para mostrar el número de filas que cumplen un criterio lógico. La expresión que contiene se cuenta si se evalúa como `TRUE`. Por ejemplo: 

* `sum(age_years < 18, na.rm=T)`  
* `sum(gender == "male", na.rm=T)`  
* `sum(response %in% c("Likely", "Very Likely"))`   

A continuación, se resumen los datos de `linelist` para describir los días de retraso desde el inicio de los síntomas hasta el ingreso en el hospital (columna `days_onset_hosp`), por hospital. 

```{r}
summary_table <- linelist %>%                                        # begin with linelist, save out as new object
  group_by(hospital) %>%                                             # group all calculations by hospital
  summarise(                                                         # only the below summary columns will be returned
    cases       = n(),                                                # number of rows per group
    delay_max   = max(days_onset_hosp, na.rm = T),                    # max delay
    delay_mean  = round(mean(days_onset_hosp, na.rm=T), digits = 1),  # mean delay, rounded
    delay_sd    = round(sd(days_onset_hosp, na.rm = T), digits = 1),  # standard deviation of delays, rounded
    delay_3     = sum(days_onset_hosp >= 3, na.rm = T),               # number of rows with delay of 3 or more days
    pct_delay_3 = scales::percent(delay_3 / cases)                    # convert previously-defined delay column to percent 
  )

summary_table  # print
```


Algunos consejos: 

* Utilizar `sum()` con una sentencia lógica para "contar" las filas que cumplen ciertos criterios (`==`) 
* Ten en cuenta el uso de `na.rm = TRUE` dentro de funciones matemáticas como `sum()`, de lo contrario se mostrará `NA` si hay algún valor faltante 
* Utiliza la función `percent()` del paquete **scales** para convertir fácilmente a porcentajes 
* Ajusta la `accuracy = ` (precisión) a 0,1 o 0,01 para garantizar 1 o 2 decimales respectivamente 
* Utilizar `round()` de R **base** para especificar los decimales 
* Para calcular estas estadísticas en todo el set de datos, utiliza `summarise()` sin `group_by()` 
* Puedes crear columnas para los propósitos de cálculos posteriores (por ejemplo, denominadores) que eventualmente se eliminan de tu dataframe con `select()`. 


### Estadísticas condicionales  {.unnumbered}  

Es posible que desees producir *estadísticas condicionales*, por ejemplo, el máximo de filas que cumplen ciertos criterios. Esto se puede hacer sub-configurando la columna con corchetes [ ]. El ejemplo siguiente devuelve la temperatura máxima de los pacientes clasificados con o sin fiebre. Sin embargo, ten en cuenta que puede ser más adecuado añadir otra columna al comando `group_by()` y `pivot_wider()` (como se demuestra [a continuación](#tbls_pivot_wider)). 


```{r}
linelist %>% 
  group_by(hospital) %>% 
  summarise(
    max_temp_fvr = max(temp[fever == "yes"], na.rm = T),
    max_temp_no = max(temp[fever == "no"], na.rm = T)
  )
```



### Pegar valores  {.unnumbered}  

La función `str_glue()` de **stringr** es útil para combinar valores de varias columnas en una nueva columna. En este contexto, se suele utilizar *después* del comando `summarise()`. 

En la página [Caracteres y cadenas](#characters-and-strings), se discuten varias opciones para combinar columnas, incluyendo `unite()`, y `paste0()`. En este caso de uso, abogamos por `str_glue()` porque es más flexible que `unite()` y tiene una sintaxis más sencilla que `paste0()`. 

A continuación, el dataframe de `summary_table` (creado anteriormente) se muta de manera que las columnas `delay_mean` y `delay_sd` se combinan, se añade el formato de paréntesis a la nueva columna y se eliminan sus respectivas columnas antiguas. 

Luego, para hacer la tabla más presentable, se añade una fila de totales con `adorn_totals()` de **janitor** (que ignora las columnas no numéricas). Por último, utilizamos `select()` de **dplyr** para reordenar y renombrar los nombres de las columnas. 

Ahora puedes pasar a **flextable** e imprimir la tabla a Word, .png, .jpeg, .html, Powerpoint, RMarkdown, etc. (ver la página de [Tablas para presentaciones](#tables-for-presentation)). 

```{r}
summary_table %>% 
  mutate(delay = str_glue("{delay_mean} ({delay_sd})")) %>%  # combine and format other values
  select(-c(delay_mean, delay_sd)) %>%                       # remove two old columns   
  adorn_totals(where = "row") %>%                            # add total row
  select(                                                    # order and rename cols
    "Hospital Name"   = hospital,
    "Cases"           = cases,
    "Max delay"       = delay_max,
    "Mean (sd)"       = delay,
    "Delay 3+ days"   = delay_3,
    "% delay 3+ days" = pct_delay_3
    )
```

#### Percentiles {.unnumbered}  

Los *percentiles* y cuartiles en **dplyr** merecen una mención especial. Para mostrar los cuantiles, utiliza `quantile()` con los valores predeterminados o especifica el valor o los valores que deseas con `probs =`. 

```{r}
# get default percentile values of age (0%, 25%, 50%, 75%, 100%)
linelist %>% 
  summarise(age_percentiles = quantile(age_years, na.rm = TRUE))

# get manually-specified percentile values of age (5%, 50%, 75%, 98%)
linelist %>% 
  summarise(
    age_percentiles = quantile(
      age_years,
      probs = c(.05, 0.5, 0.75, 0.98), 
      na.rm=TRUE)
    )
```

Si deseas mostrar cuantiles *por grupo*, puedes encontrar salidas largas y menos útiles si simplemente añades otra columna a `group_by()`. Por lo tanto, prueba este enfoque en su lugar: crea una columna para cada nivel de cuantil deseado. 

```{r}
# get manually-specified percentile values of age (5%, 50%, 75%, 98%)
linelist %>% 
  group_by(hospital) %>% 
  summarise(
    p05 = quantile(age_years, probs = 0.05, na.rm=T),
    p50 = quantile(age_years, probs = 0.5, na.rm=T),
    p75 = quantile(age_years, probs = 0.75, na.rm=T),
    p98 = quantile(age_years, probs = 0.98, na.rm=T)
    )
```

Aunque `summarise()` de **dplyr** ofrece ciertamente un control más fino, puedes encontrar que todas las estadísticas de resumen que necesitas pueden producirse con `get_summary_stat()` del paquete **rstatix**. Si se opera con datos agrupados, if mostrará 0%, 25%, 50%, 75% y 100%. Si se aplica a datos no agrupados, puedes especificar los percentiles con `probs = c(.05, .5, .75, .98)`. 


```{r}
linelist %>% 
  group_by(hospital) %>% 
  rstatix::get_summary_stats(age, type = "quantile")
```

```{r}
linelist %>% 
  rstatix::get_summary_stats(age, type = "quantile")
```



### Resumir datos agregados {.unnumbered}  
 
*Si comienza con datos agregados*, al utilizar `n()` devuelve el número de *filas*, no la suma de los recuentos agregados. Para obtener sumas, utiliza `sum()` en la columna de recuentos de los datos. 

Por ejemplo, digamos que se empieza con el dataframe de recuentos que se muestra a continuación, llamado `linelist_agg` - muestra en formato "largo" los recuentos de casos por resultado y género. 

A continuación creamos este dataframe de ejemplo de recuentos de casos de `linelist` por resultado y sexo (se eliminan los valores faltantes para mayor claridad). 

```{r}
linelist_agg <- linelist %>% 
  drop_na(gender, outcome) %>% 
  count(outcome, gender)

linelist_agg
```

Para sumar los recuentos (en la columna n) por grupo, puedes utilizar `summarise()` pero establecer la nueva columna igual a sum(n, na.rm=T)`. Para añadir un elemento condicional a la operación de suma, puedes utilizar la sintaxis del subconjunto [ ] en la columna de recuentos. 

```{r}
linelist_agg %>% 
  group_by(outcome) %>% 
  summarise(
    total_cases  = sum(n, na.rm=T),
    male_cases   = sum(n[gender == "m"], na.rm=T),
    female_cases = sum(n[gender == "f"], na.rm=T))
```




### `across()` varias columnas {.unnumbered}  

Puedes utilizar `summarise()` en varias columnas utilizando `across()`. Esto facilita la vida cuando se desea calcular las mismas estadísticas para muchas columnas. Escribe `across()` dentro de `summarise()` y especifica lo siguiente: 

* `.cols = ` como un vector de nombres de columnas `c()` o funciones de ayuda "tidyselect" (explicadas más adelante) 

* `.fns = ` la función a realizar (sin paréntesis) - puedes proporcionar varias dentro de una `list()`

A continuación, `mean()` se aplica a varias columnas numéricas. Se nombra explícitamente un vector de columnas a `.cols = ` y se especifica una única función `mean` (sin paréntesis) a `.fns = `. Cualquier argumento adicional para la función (por ejemplo, `na.rm=TRUE`) se proporciona después de `.fns = `, separado por una coma. 

Puede ser difícil conseguir el orden correcto de los paréntesis y las comas cuando se utiliza `across()`. Recuerda que dentro de `across()` debes incluir las columnas, las funciones y cualquier argumento extra necesario para las funciones. 

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm),  # columns
                   .fns = mean,                               # function
                   na.rm=T))                                  # extra arguments
```

Se pueden ejecutar varias funciones a la vez. A continuación se proporcionan las funciones `mean` y `sd` a `.fns = ` dentro de una `list()`. Tienes la oportunidad de proporcionar nombres de caracteres (por ejemplo, "mean" y "sd") que se añaden en los nuevos nombres de columna.   

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm), # columns
                   .fns = list("mean" = mean, "sd" = sd),    # multiple functions 
                   na.rm=T))                                 # extra arguments
```

Aquí están esas funciones de ayuda "tidyselect" que puedes proporcionar a `.cols = ` para seleccionar columnas: 

* `everything()` - todas las demás columnas no mencionadas 
* `last_col()` - la última columna 
* `where()` - aplica una función a todas las columnas y selecciona las que son TRUE 
* `starts_with()` - coincide con un prefijo especificado. Ejemplo: `starts_with("date") `
* `ends_with()` - coincide con un sufijo especificado. Ejemplo: `ends_with("_end")` 
* `contains()` - columnas que contienen una cadena de caracteres. Ejemplo: `contains("time")`
* `matches()` - para aplicar una expresión regular (regex). Ejemplo: `contains("[pt]al")`
* `num_range()` - 
* `any_of()` - coincide con el nombre de la columna. Es útil si el nombre puede no existir. Ejemplo: `any_of(date_onset, date_death, cardiac_arrest)` 

Por ejemplo, para producir la media de cada columna numérica utiliza where() y proporciona la función `as.numeric()` (sin paréntesis). Todo esto queda dentro del comando `across()`. 

```{r}
linelist %>% 
  group_by(outcome) %>% 
  summarise(across(
    .cols = where(is.numeric),  # all numeric columns in the data frame
    .fns = mean,
    na.rm=T))
```


### Pivote más amplio  {#tbls_pivot_wider .unnumbered}

Si prefieres tu tabla en formato "ancho" puedes transformarla utilizando la función `pivot_wider()` de **tidyr**. Es probable que tengas que renombrar las columnas con `rename()`. Para más información, consulta la página sobre [Pivotar datos](#pivoting-data). 

El ejemplo siguiente comienza con la tabla "larga" `age_by_outcome` de la [sección de proporciones](#tbl_dplyr_prop). La creamos de nuevo y la imprimimos, para mayor claridad: 

```{r}
age_by_outcome <- linelist %>%                  # begin with linelist
  group_by(outcome) %>%                         # group by outcome 
  count(age_cat) %>%                            # group and count by age_cat, and then remove age_cat grouping
  mutate(percent = scales::percent(n / sum(n))) # calculate percent - note the denominator is by outcome group
```

```{r, echo=F}
DT::datatable(age_by_outcome, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Para pivotar más ampliamente, creamos las nuevas columnas a partir de los *valores* de la columna existente `age_cat` (estableciendo `names_from = age_cat`). También especificamos que los nuevos valores de la tabla provendrán de la columna existente `n`, con `values_from = n`. Las columnas no mencionadas en nuestro comando de pivoteo (`outcome`) permanecerán sin cambios en el extremo izquierdo. 

```{r}
age_by_outcome %>% 
  select(-percent) %>%   # keep only counts for simplicity
  pivot_wider(names_from = age_cat, values_from = n)  
```


### Total de filas {#tbl_dplyr_totals .unnumbered}  

Cuando `summarise()` opera con datos agrupados no produce automáticamente estadísticas "totales". A continuación, se presentan dos enfoques para añadir una fila de totales:  

#### adorn_totals() de **janitor** {.unnumbered}  

Si tu tabla consiste sólo en recuentos o proporciones/porcentajes que pueden sumarse en un total, entonces puedes añadir totales de *suma* usando `adorn_totals()` de **janitor** como se describe en la sección anterior. Ten en cuenta que esta función sólo puede sumar las columnas numéricas - si deseas calcular otras estadísticas de resumen total, mira el siguiente enfoque con **dplyr**. 

A continuación, `linelist` se agrupa por género y se resume en una tabla que describe el número de casos con resultado conocido, los fallecidos y los recuperados. Al pasar la tabla por `adorn_totals()` se añade una fila total en la parte inferior que refleja la suma de cada columna. Las funciones posteriores `adorn_*()` ajustan la visualización como se indica en el código. 

```{r}
linelist %>% 
  group_by(gender) %>%
  summarise(
    known_outcome = sum(!is.na(outcome)),           # Number of rows in group where outcome is not missing
    n_death  = sum(outcome == "Death", na.rm=T),    # Number of rows in group where outcome is Death
    n_recover = sum(outcome == "Recover", na.rm=T), # Number of rows in group where outcome is Recovered
  ) %>% 
  adorn_totals() %>%                                # Adorn total row (sums of each numeric column)
  adorn_percentages("col") %>%                      # Get column proportions
  adorn_pct_formatting() %>%                        # Convert proportions to percents
  adorn_ns(position = "front")                      # display % and counts (with counts in front)
```

#### `summarise()` en los datos "totales" y luego `bind_rows()`  {.unnumbered}  

Si tu tabla consta de estadísticas de resumen como `median()`, `mean()`,, etc., el enfoque `adorn_totals()` mostrado anteriormente *no* será suficiente. En tu lugar, para obtener los estadísticos de resumen de todo el set de datos debe calcularlos con un comando `summarise()` separado y luego vincular los resultados a la tabla de resumen agrupada original. Para hacer el enlace puedes utilizar `bind_rows()` de **dplyr** descrito en la página de [unión de datos](#joining-data). A continuación se muestra un ejemplo: 

Se puede hacer una tabla resumen de resultados *por hospital* con `group_by()` y `summarise()` así: 

```{r, warning=F, message=F}
by_hospital <- linelist %>% 
  filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T))               # median CT value per group
  
by_hospital # print table
```

Para obtener los totales, ejecuta el mismo comando `summarise()` pero agrupando los datos sólo por resultado (no por hospital), de la siguiente manera:   

```{r}
totals <- linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # These statistics are now by outcome only     
        ct_value = median(ct_blood, na.rm=T))

totals # print table
```

Podemos unir estos dos dataframes. Ten en cuenta que `by_hospital` tiene 4 columnas, mientras que `totals` tiene 3 columnas. Al utilizar `bind_rows()`, las columnas se combinan por nombre, y cualquier espacio extra se rellena con `NA` (por ejemplo, los valores de la columna `hospital` para las dos nuevas filas de `totals`). Después de enlazar las filas, convertimos estos espacios vacíos en "Total" utilizando `replace_na()` (véase la página de [limpieza de datos y funciones básicas](#cleaning-data-and-core-functions)).  

```{r}
table_long <- bind_rows(by_hospital, totals) %>% 
  mutate(hospital = replace_na(hospital, "Total"))
```

Aquí está la nueva tabla con las filas "Total" en la parte inferior. 

```{r, message=FALSE, echo=F}
DT::datatable(table_long, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```

Esta tabla tiene un formato "largo", que puede ser lo que quieres. *Opcionalmente*, puedes *pivotar* esta tabla *más ampliamente* para hacerla más legible. Mira la sección sobre pivoteo más amplio arriba, y la página [Pivotar datos](#pivoting-data). También puedes añadir más columnas, y organizarla de forma agradable. Este código está abajo.   

```{r}
table_long %>% 
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known)                                  # Arrange rows from lowest to highest (Total row at bottom)

```

Y luego puedes imprimir esto muy bien como una imagen - abajo está la salida impresa con **flextable**. Puedes leer más en profundidad sobre este ejemplo y cómo lograr esta tabla "bonita" en la página [Tablas para presentaciones](#tables-for-presentation). 

```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}

linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) 

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make pretty images of tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

table <- linelist %>% 
  # filter
  ########
  #filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known) %>%                                 # Arrange rows from lowest to highest (Total row at bottom)

  # formatting
  ############
  flextable() %>% 
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  colformat_num(., j = c(4,7), digits = 1) %>% 
  bold(i = 1, bold = TRUE, part = "header") %>% 
  bold(i = 6, bold = TRUE, part = "body")


table
```



## Paquete **gtsummary** {#tbl_gt}   

Si deseas imprimir tus estadísticas de resumen en un gráfico bonito y listo para tu publicación, puedes utilizar el paquete **gtsummary** y tu función `tbl_summary`(). El código puede parecer complejo al principio, pero los resultados se ven muy bien y se imprimen en tu panel de RStudio Viewer como una imagen HTML. Lea [esta viñeta](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html). 

También puedes añadir los resultados de las pruebas estadísticas a las tablas de gtsummary. Este proceso se describe en la sección **gtsummary de la** página [Tests estadísticos simples](#stats_gt). 

Para introducir `tbl_summary`() mostraremos primero el comportamiento más básico, que realmente produce una tabla grande y bonita. Luego, examinaremos en detalle cómo hacer ajustes y tablas más a medida. 



### Tabla resumen  {.unnumbered}

El comportamiento por defecto de `tbl_summary`() es bastante increíble: toma las columnas que proporcionas y crea una tabla de resumen en un solo comando. La función imprime las estadísticas apropiadas para el tipo de columna: mediana y rango intercuartil (IQR) para las columnas numéricas, y recuentos (%) para las columnas categóricas. Los valores faltantes se convierten en "Missing". Se añaden notas a pie de página para explicar las estadísticas, mientras que el N total se muestra en la parte superior. 

```{r, warning=F, message=F}
linelist %>% 
  select(age_years, gender, outcome, fever, temp, hospital) %>%  # keep only the columns of interest
  tbl_summary()                                                  # default
```


### Ajustes  {.unnumbered}  

Ahora explicaremos cómo trabaja la función y cómo hacer los ajustes. Los argumentos clave se detallan a continuación: 

**`by = `**
Puedes estratificar tu tabla por una columna (por ejemplo, por resultado), creando una tabla de dos vías. 

**`statistic = ` **
Usa una ecuación para especificar qué estadísticas mostrar y cómo mostrarlas. La ecuación tiene dos lados, separados por una tilde `~`. En el lado derecho, entre comillas, está la visualización estadística deseada, y en el izquierdo están las columnas a las que se aplicará esa visualización. 

* El lado derecho de la ecuación utiliza la sintaxis de `str_glue()` de **stringr** (véase [Caracteres y cadenas](#characters-and-strings)), con la cadena de visualización deseada entre comillas y los propios estadísticos entre llaves. Puedes incluir estadísticas como "n" (para los recuentos), "N" (para el denominador), "mean", "median", "sd", "max", "min", percentiles como "p##" como "p25", o porcentaje del total como "p". Consulta `?tbl_summary` para obtener más detalles. 
* Para el lado izquierdo de la ecuación, puedes especificar las columnas por su nombre (por ejemplo, `age` o `c(age, gender)`) o utilizando ayudantes como `all_continuous()`, `all_categorical()`, `contains()`, `starts_with()`, etc.  

Un ejemplo sencillo de una ecuación `statistic = ` podría ser como el siguiente, para imprimir sólo la media de la columna `age_years`:

```{r}
linelist %>% 
  select(age_years) %>%         # keep only columns of interest 
  tbl_summary(                  # create summary table
    statistic = age_years ~ "{mean}") # print mean of age
```

Una ecuación un poco más compleja podría tener el aspecto de `"({min}, {max})"`, incorporando los valores máximo y mínimo entre paréntesis y separados por una coma: 

```{r}
linelist %>% 
  select(age_years) %>%                       # keep only columns of interest 
  tbl_summary(                                # create summary table
    statistic = age_years ~ "({min}, {max})") # print min and max of age
```

También puedes diferenciar la sintaxis para columnas separadas o tipos de columnas. En el ejemplo más complejo de abajo, el valor proporcionado a  `statistc = ` es una **lista** que indica que para todas las columnas continuas la tabla debe imprimir la media con la desviación estándar entre paréntesis, mientras que para todas las columnas categóricas debe imprimir el n, el denominador y el porcentaje. 

**`digits = `**  
Ajusta los dígitos y el redondeo. Opcionalmente, se puede especificar que sea sólo para columnas continuas (como a continuación). 

**`label = `**  
Ajustar cómo debe mostrarse el nombre de la columna. Proporciona el nombre de la columna y la etiqueta deseada separados por una tilde. El valor por defecto es el nombre de la columna. 

**`missing_text = `**  
Ajustar cómo se muestran los valores faltantes. El valor por defecto es "Unknown". 

**`type = `** 
Se utiliza para ajustar cuántos niveles de la estadística se muestran. La sintaxis es similar a `statistic = ` en el sentido de que se proporciona una ecuación con columnas a la izquierda y un valor a la derecha. Dos escenarios comunes incluyen: 

* `type = all_categorical() ~ "categorical"`  Fuerza a las columnas dicotómicas (por ejemplo, `fever` sí/no) a mostrar todos los niveles en lugar de sólo la fila "sí"

* `type = all_continuous() ~ "continuous2"` Permite estadísticas de varias líneas por variable, como se muestra en una sección posterior 

En el siguiente ejemplo, cada uno de estos argumentos se utiliza para modificar la tabla resumen original: 

```{r}
linelist %>% 
  select(age_years, gender, outcome, fever, temp, hospital) %>% # keep only columns of interest
  tbl_summary(     
    by = outcome,                                               # stratify entire table by outcome
    statistic = list(all_continuous() ~ "{mean} ({sd})",        # stats and format for continuous columns
                     all_categorical() ~ "{n} / {N} ({p}%)"),   # stats and format for categorical columns
    digits = all_continuous() ~ 1,                              # rounding for continuous columns
    type   = all_categorical() ~ "categorical",                 # force all categorical levels to display
    label  = list(                                              # display labels for column names
      outcome   ~ "Outcome",                           
      age_years ~ "Age (years)",
      gender    ~ "Gender",
      temp      ~ "Temperature",
      hospital  ~ "Hospital"),
    missing_text = "Missing"                                    # how missing values should display
  )
```



### Estadísticas de varias líneas para variables continuas  {.unnumbered}  

Si deseas imprimir varias líneas de estadísticas para variables continuas, puedes indicarlo estableciendo `type = ` a "continuous2". Puedes combinar todos los elementos mostrados anteriormente en una tabla eligiendo qué estadísticas quiere mostrar. Para ello, debes indicar a la función que deseas obtener una tabla escribiendo el tipo como "continuous2". El número de valores faltantes se muestra como "Desconocido". 


```{r}
linelist %>% 
  select(age_years, temp) %>%                      # keep only columns of interest
  tbl_summary(                                     # create summary table
    type = all_continuous() ~ "continuous2",       # indicate that you want to print multiple statistics 
    statistic = all_continuous() ~ c(
      "{mean} ({sd})",                             # line 1: mean and SD
      "{median} ({p25}, {p75})",                   # line 2: median and IQR
      "{min}, {max}")                              # line 3: min and max
    )
```
Hay muchas otras formas de modificar estas tablas, incluyendo la adición de valores p, el ajuste del color y los títulos, etc. Muchas de ellas se describen en la documentación (escribe `?tbl_summary` en la Consola), y algunas se dan en la sección de [Tests estadísticos sencillos](https://epirhandbook.com/simple-statistical-tests.html).







## R **base**   {#base-r-1}

Puedes utilizar la función `table()` para tabular y cruzar las columnas. A diferencia de las opciones anteriores, debes especificar el dataframe cada vez que haga referencia a un nombre de columna, como se muestra a continuación. 

<span style="color: orange;">***ATENCIÓN:*** Los valores `NA` (missing) **no** se tabularán a menos que se incluya el argumento `useNA = "always"` (que también podría establecerse como "no" o "ifany"). .</span>

<span style="color: darkgreen;">***CONSEJO:*** Puedes utilizar el `%$%` de **magrittr** para eliminar la necesidad de repetir las llamadas al dataframe dentro de las funciones de R **base**. Por ejemplo, lo siguiente podría escribirse `linelist %$% table(outcome, useNA = "always")`  </span>

```{r}
table(linelist$outcome, useNA = "always")
```

Se pueden cruzar varias columnas enumerándolas una tras otra, separadas por comas. Opcionalmente, se puede asignar a cada columna un "nombre" como `Outcome = linelist$outcome`.  

```{r}
age_by_outcome <- table(linelist$age_cat, linelist$outcome, useNA = "always") # save table as object
age_by_outcome   # print table
```

### Proporciones  {.unnumbered}  

Para producir las proporciones, pasa la tabla anterior a la función `prop.table()`. Utiliza el argumento `margins = ` para especificar si deseas que las proporciones sean de filas (1), de columnas (2) o de toda la tabla (3). Para mayor claridad, eniazamos la tabla con pipe a la función `round()` de R **base**, especificando 2 dígitos. 

```{r}
# get proportions of table defined above, by rows, rounded
prop.table(age_by_outcome, 1) %>% round(2)
```

### Totales {.unnumbered}  

Para añadir los totales de filas y columnas, pasa la tabla a `addmargins()`. Esto funciona tanto para recuentos como para proporciones. 

```{r}
addmargins(age_by_outcome)
```

### Convertir en dataframe  {.unnumbered}  

Convertir un objeto `table()` directamente en un dataframe no es sencillo. A continuación se muestra un enfoque: 

1)  Crea la tabla, *sin utilizar* `useNA = "always"`. En su lugar, convierte los valores `NA` en "(Missing)" con `fct_explicit_na()` de **forcats**. 
2)  Añade los totales (opcional) pasando por `addmargins()` 
3)  Pipe a la función R **base** `as.data.frame.matrix()` 
4)  Enviar la tabla a la función `rownames_to_column()` de **tibble**, especificando el nombre de la primera columna 
5)  Imprime, visualiza o exporta según desees. En este ejemplo utilizamos `flextable()` del paquete **flextable** como se describe en la página [Tablas para presentaciones](#tables-for-presentation). Esto imprimirá en el panel de visualización de RStudio como una bonita imagen HTML. 


```{r, warning=F, message=F}
table(fct_explicit_na(linelist$age_cat), fct_explicit_na(linelist$outcome)) %>% 
  addmargins() %>% 
  as.data.frame.matrix() %>% 
  tibble::rownames_to_column(var = "Age Category") %>% 
  flextable::flextable()
```




<!-- ======================================================= -->

## Recursos  {#resources-10}

Gran parte de la información de esta página está adaptada de estos recursos y viñetas en línea: 

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)  

[dplyr](https://dplyr.tidyverse.org/articles/grouping.html)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/tables_descriptive.Rmd-->

# Tests estadísticos sencillos {#simple-statistical-tests}

Esta página muestra cómo realizar tests estadísticos sencillos utilizando **R base**, **rstatix** y **gtsummary**.

* Prueba o Test T de Student
* Prueba o Test de Shapiro-Wilk
* Prueba o Test de suma de rangos de Wilcoxon
* Prueba o Test de Kruskal-Wallis
* Prueba o Test de Chi-cuadrado
* Correlaciones entre variables numéricas

...Se pueden realizar otras muchas pruebas. Solo mostraremos éstas, las más comunes y enlazaremos con más documentación.

Cada uno de los paquetes mencionados tienen unos usos específicos:

* Utiliza las funciones  de **R base** para imprimir una salida estadística en la consola de R
* Utiliza las funciones **rstatix** para devolver los resultados en un dataframe, o si deseas que las pruebas se ejecuten por grupos
* Utiliza **gtsummary** si tienes interés en rápidamente tablas listas para su publicación



<!-- ======================================================= -->
## Preparación {#preparation-9}


### Cargar paquetes {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos la función `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes ya instalados con el comando `library()` de **R base** Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.


```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics, 
  gtsummary,    # summary statistics and tests
  rstatix,      # statistics
  corrr,        # correlation analayis for numeric variables
  janitor,      # adding totals and percents to tables
  flextable     # converting tables to HTML
  )
```

### Importar datos {.unnumbered}

Importaremos los datos de casos de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica para descargar linelist "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (archivo linelist_cleaned.rds). Importa tus datos con la función `import()` del paquete **rio** (acepta muchos tipos de archivos como .xlsx, .rds, .csv - Mira la página de [importación y exportación](#import-and-export) para más detalles).


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas del listado.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





## ** R base** {#base-r-2}

Puedes utilizar las funciones de ** R base** para realizar pruebas estadísticas. Los comandos son relativamente sencillos y los resultados se imprimen en la consola de R para su visualización. Sin embargo, las salidas suelen ser listas y, por lo tanto, son más difíciles de manipular en el caso que se desee utilizar los resultados en operaciones posteriores.

### Tests-T {.unnumbered} 

Un [test-t](https://es.wikipedia.org/wiki/Prueba_t_de_Student), también llamado "Test t de Student" o "Prueba t de Student", se utiliza normalmente para determinar si existe una diferencia significativa entre las medias de alguna variable numérica entre dos grupos. Aquí mostraremos la sintaxis para hacer esta prueba dependiendo de si las columnas se encuentran o no en el mismo dataframe.

**Sintaxis 1:** Esta es la sintaxis cuando las columnas numéricas y categóricas están en el mismo dataframe. Sitúa la columna numérica en el lado izquierdo de la ecuación y la columna categórica en el lado derecho. Especifica los datos en `data =`. Opcionalmente, establece `paired = TRUE`, `conf.level = ` (0.95 por defecto), y `alternative = ` (ya sea "two.sided", "less", o "greater"). Escribe `?t.test` para obtener más detalles.

```{r}
## compare mean age by outcome group with a t-test
t.test(age_years ~ gender, data = linelist)
```

**Sintaxis 2:** Puedes comparar dos vectores numéricos separados utilizando esta sintaxis alternativa. Por ejemplo, si las dos columnas están en dataframes diferentes.
  

```{r, eval=F}
t.test(df1$age_years, df2$age_years)
```

También se puede utilizar una prueba t de Student para determinar si la media de una muestra es significativamente diferente de algún valor específico. Aquí realizamos una prueba t de una muestra con la media poblacional conocida/hipotética como `mu = `:

```{r, eval=F}
t.test(linelist$age_years, mu = 45)
```

### Prueba de Shapiro-Wilk {.unnumbered}  

El [test de Shapiro-Wilk](https://es.wikipedia.org/wiki/Test_de_Shapiro–Wilk) puede utilizarse para determinar si una muestra procede de una población distribuida normalmente (un supuesto en muchas otras pruebas y análisis, como la prueba t). Sin embargo, sólo puede utilizarse en una muestra de entre 3 y 5000 observaciones. Para muestras más grandes puede ser útil un [gráfico de cuantiles](https://ggplot2.tidyverse.org/reference/geom_qq.html).


```{r, eval=F}
shapiro.test(linelist$age_years)
```

### Test de suma de rangos de Wilcoxon {.unnumbered}

El test de suma de rangos de Wilcoxon, también llamada [test U de Mann-Whitney](https://en.wikipedia.org/wiki/Mann–Whitney_U_test), se utiliza a menudo para ayudar a determinar si dos muestras numéricas proceden de la misma distribución cuando tus poblaciones no se distribuyen normalmente o tienen una varianza desigual.

```{r wilcox_base}

## compare age distribution by outcome group with a wilcox test
wilcox.test(age_years ~ outcome, data = linelist)

```


### Test de Kruskal-Wallis {.unnumbered}

El test de [Kruskal-Wallis](https://en.wikipedia.org/wiki/Kruskal–Wallis_one-way_analysis_of_variance) es una extensión del test de suma de rangos de Wilcoxon. Puede utilizarse para comprobar las diferencias en la distribución de más de dos muestras. Cuando sólo se utilizan dos muestras, los resultados son idénticos a los del test de suma de rangos de Wilcoxon.

```{r }

## compare age distribution by outcome group with a kruskal-wallis test
kruskal.test(age_years ~ outcome, linelist)

```

### Test de Chi-cuadrado {.unnumbered} 

El [test de Chi-cuadrado de Pearson](https://en.wikipedia.org/wiki/Chi-squared_test) se utiliza para comprobar las diferencias significativas entre grupos categóricos.

```{r}

## compare the proportions in each group with a chi-squared test
chisq.test(linelist$gender, linelist$outcome)

```



## Paquete **rstatix** {#rstatix-package}

El paquete **rstatix** ofrece la posibilidad de ejecutar pruebas estadísticas y recuperar los resultados en un formato "amigable". Los resultados se encuentran automáticamente en un dataframe para que puedan realizar operaciones posteriores con los resultados. También es fácil agrupar los datos que se pasan a las funciones, de modo que las estadísticas se ejecutan para cada grupo.


### Estadísticas resumidas {.unnumbered}  

La función `get_summary_stats()` es una forma rápida de generar estadísticas de resumen. Únicamente tienes que tienes que seleccionar tu dataframe al aplicar esta función así como especificar las columnas que deseas analizar. Si no se especifica ninguna columna, las estadísticas se calculan para todas ellas.

Por defecto, la función devuelve una gama completa de estadísticas de resumen: n, max, min, mediana, cuartil 25%, cuartil 75%, IQR, desviación absoluta mediana (mad), media, desviación estándar, error estándar y un intervalo de confianza de la media.


```{r}
linelist %>%
  rstatix::get_summary_stats(age, temp)
```

Puedes especificar un subconjunto de estadísticas de resumen a calcular proporcionando uno de los siguientes valores a `type = `: "full", "common", "robust", "five_number", "mean_sd", "mean_se", "mean_ci", "median_iqr", "median_mad", "quantile", "mean", "median", "min", "max".  

También puede utilizarse con datos agrupados, de forma que se devuelva una fila por cada variable de agrupación:

```{r}
linelist %>%
  group_by(hospital) %>%
  rstatix::get_summary_stats(age, temp, type = "common")
```

Por último, también se puede utilizar **rstatix** para realizar las siguientes pruebas estadísticas:

### Test-T {.unnumbered}  

Utiliza una sintaxis de fórmula para especificar las columnas numéricas y categóricas:

```{r}
linelist %>% 
  t_test(age_years ~ gender)
```

Utiliza `~ 1` y especifica `mu = ` para un test-T de una muestra. Esto también puede hacerse por grupo.

```{r}
linelist %>% 
  t_test(age_years ~ 1, mu = 30)
```

Si procede, las pruebas estadísticas pueden realizarse por grupos, como se muestra a continuación:

```{r}
linelist %>% 
  group_by(gender) %>% 
  t_test(age_years ~ 1, mu = 18)
```

### Prueba de Shapiro-Wilk {.unnumbered}  

Como ya se ha dicho, el tamaño de la muestra debe estar entre 3 y 5000.

```{r}
linelist %>% 
  head(500) %>%            # first 500 rows of case linelist, for example only
  shapiro_test(age_years)
```

### Prueba de suma de rangos de Wilcoxon {.unnumbered}  

```{r}
linelist %>% 
  wilcox_test(age_years ~ gender)
```


### Prueba de Kruskal-Wallis {.unnumbered}  

También conocida como la prueba U de Mann-Whitney.

```{r}
linelist %>% 
  kruskal_test(age_years ~ outcome)
```


### Prueba de Chi-cuadrado {.unnumbered}  

La función para la prueba de chi-cuadrado funciona con tablas, así que primero creamos una tabulación cruzada. Hay muchas formas de crear una tabulación cruzada (véase [Tablas descriptivas](#descriptive-tables)), pero aquí utilizamos `tabyl()` de **janitor** y eliminamos la columna más a la izquierda de las etiquetas de valores antes de pasarla a `chisq_test()`.

```{r}
linelist %>% 
  tabyl(gender, outcome) %>% 
  select(-1) %>% 
  chisq_test()

```

Se pueden ejecutar muchas más funciones y pruebas estadísticas con las funciones de rstatix. Consulta [la documentación](https://github.com/kassambara/rstatix) de **rstatix** o escribiendo `?rstatix`.




## Paquete `gtsummary` {#stats_gt}

Utilizaa **gtsummary** si quieres añadir los resultados de una prueba estadística a una tabla estéticamente presentada, creada con este paquete (como se describe en la sección **gtsummary** del capítulo [Tablas descriptivas](#tbl_gt)).

La realización de pruebas estadísticas de comparación con `tbl_summary` se lleva a cabo añadiendo la función `add_p` a una tabla y especificando qué prueba utilizar. Es posible obtener p-valores corregidos para múltiples pruebas utilizando la función `add_q`. Ejecuta `?tbl_summary` para obtener más detalles.

### Prueba de Chi-cuadrado {.unnumbered}

Compara las proporciones de una variable categórica en dos grupos. La prueba estadística por defecto de `add_p`(), cuando se aplica a una variable categórica es realizar una prueba de independencia de chi-cuadrado con corrección de continuidad, pero si algúna celda de valores esperados es inferior a 5, se utiliza una prueba exacta de Fisher.
```{r chi_gt}
linelist %>% 
  select(gender, outcome) %>%    # keep variables of interest
  tbl_summary(by = outcome) %>%  # produce summary table and specify grouping variable
  add_p()                        # specify what test to perform
```


### Tests-T {.unnumbered} 

Compara la diferencia de medias de una variable continua en dos grupos. Por ejemplo, hace la comparación de la media de edad por resultado del paciente.

```{r ttest_gt}

linelist %>% 
  select(age_years, outcome) %>%             # keep variables of interest
  tbl_summary(                               # produce summary table
    statistic = age_years ~ "{mean} ({sd})", # specify what statistics to show
    by = outcome) %>%                        # specify the grouping variable
  add_p(age_years ~ "t.test")                # specify what tests to perform


```

### Test de suma de rangos de Wilcoxon {.unnumbered}

Compara la distribución de una variable continua en dos grupos. Por defecto se utiliza la prueba de suma de rangos de Wilcoxon y la mediana (IQR) cuando se comparan dos grupos. Sin embargo, para datos no distribuidos normalmente o para comparar varios grupos, la prueba de Kruskal-wallis es más apropiada.

```{r wilcox_gt}

linelist %>% 
  select(age_years, outcome) %>%                       # keep variables of interest
  tbl_summary(                                         # produce summary table
    statistic = age_years ~ "{median} ({p25}, {p75})", # specify what statistic to show (this is default so could remove)
    by = outcome) %>%                                  # specify the grouping variable
  add_p(age_years ~ "wilcox.test")                     # specify what test to perform (default so could leave brackets empty)


```

### Test de Kruskal-Wallis {.unnumbered}

Se usa para comparar la distribución de una variable continua en dos o más grupos, independientemente de que los datos se distribuyan normalmente.

```{r kruskal_gt}

linelist %>% 
  select(age_years, outcome) %>%                       # keep variables of interest
  tbl_summary(                                         # produce summary table
    statistic = age_years ~ "{median} ({p25}, {p75})", # specify what statistic to show (default, so could remove)
    by = outcome) %>%                                  # specify the grouping variable
  add_p(age_years ~ "kruskal.test")                    # specify what test to perform


```




<!-- ## `dplyr` package {} -->

<!-- Performing statistical tests in `dplyr` alone is very dense, again because it  -->
<!-- does not fit within the tidy-data framework. It requires using `purrr` to create -->
<!-- a list of dataframes for each of the subgroups you want to compare. See the page on [Iteration, loops, and lists] to learn about **purrr**.   -->

<!-- An easier alternative may be the `rstatix` package.  -->

<!-- ### T-tests {.unnumbered}  -->

<!-- ```{r ttest_dplyr} -->

<!-- linelist %>%  -->
<!--   ## only keep variables of interest -->
<!--   select(age, outcome) %>%  -->
<!--   ## drop those missing outcome  -->
<!--   filter(!is.na(outcome)) %>%  -->
<!--   ## specify the grouping variable -->
<!--   group_by(outcome) %>%  -->
<!--   ## create a subset of data for each group (as a list) -->
<!--   nest() %>%  -->
<!--   ## spread in to wide format -->
<!--   pivot_wider(names_from = outcome, values_from = data) %>%  -->
<!--   mutate( -->
<!--     ## calculate the mean age for the death group -->
<!--     Death_mean = map(Death, ~mean(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the sd among dead  -->
<!--     Death_sd = map(Death, ~sd(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the mean age for the recover group -->
<!--     Recover_mean = map(Recover, ~mean(.x$age, na.rm = TRUE)),  -->
<!--     ## calculate the sd among recovered  -->
<!--     Recover_sd = map(Recover, ~sd(.x$age, na.rm = TRUE)), -->
<!--     ## using both grouped data sets compare mean age with a t-test -->
<!--     ## keep only the p.value -->
<!--     t_test = map2(Death, Recover, ~t.test(.x$age, .y$age)$p.value) -->
<!--   ) %>%  -->
<!--   ## drop datasets  -->
<!--   select(-Death, -Recover) %>%  -->
<!--   ## return a dataset with the medians and p.value (drop missing) -->
<!--   unnest(cols = everything()) -->

<!-- ``` -->


<!-- ### Wilcoxon rank sum test {.unnumbered} -->

<!-- ```{r wilcox_dplyr} -->

<!-- linelist %>%  -->
<!--   ## only keep variables of interest -->
<!--   select(age, outcome) %>%  -->
<!--   ## drop those missing outcome  -->
<!--   filter(!is.na(outcome)) %>%  -->
<!--   ## specify the grouping variable -->
<!--   group_by(outcome) %>%  -->
<!--   ## create a subset of data for each group (as a list) -->
<!--   nest() %>%  -->
<!--   ## spread in to wide format -->
<!--   pivot_wider(names_from = outcome, values_from = data) %>%  -->
<!--   mutate( -->
<!--     ## calculate the median age for the death group -->
<!--     Death_median = map(Death, ~median(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the sd among dead  -->
<!--     Death_iqr = map(Death, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## calculate the median age for the recover group -->
<!--     Recover_median = map(Recover, ~median(.x$age, na.rm = TRUE)),  -->
<!--     ## calculate the sd among recovered  -->
<!--     Recover_iqr = map(Recover, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## using both grouped data sets compare age distribution with a wilcox test -->
<!--     ## keep only the p.value -->
<!--     wilcox = map2(Death, Recover, ~wilcox.test(.x$age, .y$age)$p.value) -->
<!--   ) %>%  -->
<!--   ## drop datasets  -->
<!--   select(-Death, -Recover) %>%  -->
<!--   ## return a dataset with the medians and p.value (drop missing) -->
<!--   unnest(cols = everything()) -->

<!-- ``` -->

<!-- ### Kruskal-wallis test {.unnumbered} -->


<!-- ```{r kruskal_dplyr} -->

<!-- linelist %>%  -->
<!--   ## only keep variables of interest -->
<!--   select(age, outcome) %>%  -->
<!--   ## drop those missing outcome  -->
<!--   filter(!is.na(outcome)) %>%  -->
<!--   ## specify the grouping variable -->
<!--   group_by(outcome) %>%  -->
<!--   ## create a subset of data for each group (as a list) -->
<!--   nest() %>%  -->
<!--   ## spread in to wide format -->
<!--   pivot_wider(names_from = outcome, values_from = data) %>%  -->
<!--   mutate( -->
<!--     ## calculate the median age for the death group -->
<!--     Death_median = map(Death, ~median(.x$age, na.rm = TRUE)), -->
<!--     ## calculate the sd among dead  -->
<!--     Death_iqr = map(Death, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## calculate the median age for the recover group -->
<!--     Recover_median = map(Recover, ~median(.x$age, na.rm = TRUE)),  -->
<!--     ## calculate the sd among recovered  -->
<!--     Recover_iqr = map(Recover, ~str_c( -->
<!--       quantile(.x$age, probs = c(0.25, 0.75), na.rm = TRUE),  -->
<!--       collapse = ", " -->
<!--       )), -->
<!--     ## using the original data set compare age distribution with a kruskal test -->
<!--     ## keep only the p.value -->
<!--     kruskal = kruskal.test(linelist$age, linelist$outcome)$p.value -->
<!--   ) %>%  -->
<!--   ## drop datasets  -->
<!--   select(-Death, -Recover) %>%  -->
<!--   ## return a dataset with the medians and p.value (drop missing) -->
<!--   unnest(cols = everything()) -->

<!-- ``` -->

<!-- ### Chi-squared test {.unnumbered}  -->


<!-- ```{r} -->
<!-- linelist %>%  -->
<!--   ## do everything by gender  -->
<!--   group_by(outcome) %>%  -->
<!--   ## count the variable of interest -->
<!--   count(gender) %>%  -->
<!--   ## calculate proportion  -->
<!--   ## note that the denominator here is the sum of each gender -->
<!--   mutate(percentage = n / sum(n) * 100) %>%  -->
<!--   pivot_wider(names_from = outcome, values_from = c(n, percentage)) %>%  -->
<!--   filter(!is.na(gender)) %>%  -->
<!--   mutate(pval = chisq.test(linelist$gender, linelist$outcome)$p.value) -->
<!-- ``` -->


<!-- ======================================================= -->

## Correlaciones {#correlations}

La correlación entre variables numéricas puede investigarse con el paquete **corrr** de **tidyverse**. Permite calcular las correlaciones mediante los test de Pearson, tau de Kendall o rho de Spearman. El paquete crea una tabla y también tiene una función para representar automáticamente los valores. 

```{r, warning=F, message=F}

correlation_tab <- linelist %>% 
  select(generation, age, ct_blood, days_onset_hosp, wt_kg, ht_cm) %>%   # keep numeric variables of interest
  correlate()      # create correlation table (using default pearson)

correlation_tab    # print

## remove duplicate entries (the table above is mirrored) 
correlation_tab <- correlation_tab %>% 
  shave()

## view correlation table 
correlation_tab

## plot correlations 
rplot(correlation_tab)
```


<!-- ======================================================= -->

## Recursos {#resources-11}

Gran parte de la información de esta página está adaptada de los siguientes recursos y viñetas en línea:

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)

[dplyr](https://dplyr.tidyverse.org/articles/grouping.html) 

[corrr](https://corrr.tidymodels.org/articles/using-corrr.html) 

[correlaciones en sthda ](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/stat_tests.Rmd-->

# Regresión univariante y multivariable {#univariate-and-multivariable-regression}

<!-- ======================================================= -->

Esta página muestra como se pueden emplear las funciones de regresión de R **base** , como `glm()` y el paquete **gtsummary** para observar las asociaciones entre variables (por ejemplo, odds ratios, risk ratios y hazard ratios). También utiliza funciones como `tidy()` del paquete **broom** para limpiar los resultados de la regresión.

1.  Univariante: tablas de dos por dos
2.  Estratificado: estimaciones mantel-haenszel
3.  Multivariable: selección de variables, selección de modelos, tabla final
4.  Forest plots

Para la regresión de riesgos proporcionales de Cox, véase la página de [análisis de supervivencia](#survival-analysis).

***NOTA:*** Utilizamos el término *multivariable* para referirnos a una regresión con múltiples variables explicativas. En este sentido, un modelo multivariante sería una regresión con varios resultados - véase este [editorial](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3518362/) para más detalles.


<!-- ======================================================= -->

## Preparación {#preparation-9}


### Cargar paquetes {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para realizar los análisis. En este manual se hace énfasis en en el empleo de  `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para tu uso. Los paquetes ya instalados también pueden cargarse empleando  `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  tidyverse,    # data management + ggplot2 graphics, 
  stringr,      # manipulate text strings 
  purrr,        # loop over objects in a tidy way
  gtsummary,    # summary statistics and tests 
  broom,        # tidy up results from regressions
  lmtest,       # likelihood-ratio tests
  parameters,   # alternative to tidy up results from regressions
  see          # alternative to visualise forest plots
  )
```

### Importar datos {.unnumbered}

Importaremos los datos de casos de una epidemia de ébola simulada. Para seguir el proceso, [clica aquí para descargar la base de datos `linelist` "limpia"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa tus datos con la función `import()` del paquete **rio** (la cual acepta múltiples tipos de archivos como .xlsx, .rds, .csv - Checa la página de [importación y exportación](#import-and-export) para más detalles). 


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas de la base de datos linelist.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
```

### Datos limpios {.unnumbered}

#### Almacenar las variables explicativas {.unnumbered}  

Almacenamos en un vector de caracteres los nombres de las columnas explicativas. Esto se explicará más adelante.

```{r}
## define variables of interest 
explanatory_vars <- c("gender", "fever", "chills", "cough", "aches", "vomit")
```


#### Convertir a 1's y 0's  {.unnumbered}   

A continuación convertimos las columnas explicativas de "sí"/"no", "m"/"f", y "muerto"/"vivo" a **1 / 0**, para cumplir con las expectativas de los modelos de regresión logística. Para hacer esto de manera eficiente, utilizaremos `across()` de **dplyr** para transformar varias columnas a la vez. La función que aplicamos a cada columna es `case_when()` (también de **dplyr**) que aplica la lógica para convertir los valores especificados en 1's y 0's. Mira las secciones sobre `across()` y `case_when()` en la página de [Limpieza de datos y funciones básicas](#clean_across)).

Nota: el "." que aparece a continuación representa la columna que está siendo procesada por `across()` en ese momento.

```{r}
## convert dichotomous variables to 0/1 
linelist <- linelist %>%  
  mutate(across(                                      
    .cols = all_of(c(explanatory_vars, "outcome")),  ## for each column listed and "outcome"
    .fns = ~case_when(                              
      . %in% c("m", "yes", "Death")   ~ 1,           ## recode male, yes and death to 1
      . %in% c("f", "no",  "Recover") ~ 0,           ## female, no and recover to 0
      TRUE                            ~ NA_real_)    ## otherwise set to missing
    )
  )

       
      
```

#### Eliminar las filas con valores perdidos {.unnumbered}  

Para eliminar las filas con valores perdidos, se puede utilizar la función `drop_na()` de **tidyr**. Sin embargo, sólo queremos hacer esto para las filas a las que les faltan valores en las columnas de interés.

Lo primero que debemos hacer es asegurarnos de que nuestro vector `explanatory_vars` incluye la columna `age` (`age` habría producido un error en la operación anterior `case_when()`, que sólo era para variables dicotómicas). A continuación, escribimos un pipe uniendo `linelist` con `drop_na()` para eliminar cualquier fila con valores perdidos en la columna `outcome` o en cualquiera de las columnas `explanatory_vars`.

Antes de ejecutar el código, podemos comprobar el número de filas inicial de `linelist` empleando ` nrow(linelist)`.

```{r}
## add in age_category to the explanatory vars 
explanatory_vars <- c(explanatory_vars, "age_cat")

## drop rows with missing information for variables of interest 
linelist <- linelist %>% 
  drop_na(any_of(c("outcome", explanatory_vars)))

```

Podremos checar el número de filas que quedan en `linelist` tras la operación empleando `nrow(linelist)`.


<!-- ======================================================= -->

## Univariante {#univariate}

Al igual que en la página sobre [Tablas descriptivas](#descriptive-tables), en función de la tarea que vayas a realizar, podremos elegir que función emplear. A continuación presentamos dos opciones para realizar análisis univariantes:

* Puedes utilizar las funciones disponibles en R **base** para imprimir rápidamente los resultados en la consola. Después, puedes utilizar el paquete **broom** para convertir esos outputs a formato *tidy*.

* Puedes utilizar el paquete **gtsummary** para modelar y obtener resultados en tablas listas para su publicación. 



<!-- ======================================================= -->

### R **base** {.unnumbered}

#### Regresión lineal {.unnumbered}  

La función `lm()` de  R **base** realiza una regresión lineal, evaluando la relación entre la respuesta numérica y las variables explicativas que se supone tienen una relación lineal.

Para ello, proporciona la ecuación como una fórmula, con los nombres de las columnas de respuesta y explicativa separados por una tilde `~`. Además, especifica la base de datos a `data = `. Finalmente, define los resultados del modelo como un objeto R, para poder utilizarlos más tarde.  

```{r lin_reg}
lm_results <- lm(ht_cm ~ age, data = linelist)
```

A continuación, puedes ejecutar `summary()` en los resultados del modelo para ver los coeficientes (estimaciones), el valor P, los residuos y otras medidas.

```{r lin_reg_res}
summary(lm_results)
```

También se puede utilizar la función `tidy()` del paquete **broom** para obtener los resultados en una tabla. Lo que nos dicen los resultados es que por cada año de aumento de la edad la altura aumenta 3,5 cm y esto es estadísticamente significativo.

```{r lin_reg_res_tidy}
tidy(lm_results)
```

También podemos utilizar esta regresión para añadirla a un **ggplot**, para hacer esto, primero juntamos los puntos de los datos observados y la línea ajustada en un dataframe utilizando la función `augment()` de **broom**.

```{r lin_reg_res_plot}

## pull the regression points and observed data in to one dataset
points <- augment(lm_results)

## plot the data using age as the x-axis 
ggplot(points, aes(x = age)) + 
  ## add points for height 
  geom_point(aes(y = ht_cm)) + 
  ## add your regression line 
  geom_line(aes(y = .fitted), colour = "red")

```

También es posible añadir una recta de regresión lineal en **ggplot** utilizando la función `geom_smooth()`. 

```{r geom_smooth}

## add your data to a plot 
 ggplot(linelist, aes(x = age, y = ht_cm)) + 
  ## show points
  geom_point() + 
  ## add a linear regression 
  geom_smooth(method = "lm", se = FALSE)
```

Consulta la sección de recursos al final de este capítulo para consultar tutoriales más detallados. 


#### Regresión logística{.unnumbered}  

La función `glm()` del paquete **stats** (parte de R **base**) se utiliza para ajustar los modelos lineales generalizados (GLM).

`glm()` puede utilizarse para la regresión logística univariante y multivariable (por ejemplo, para obtener Odds Ratios). Aquí están las partes principales:

```{r, eval=F}
# arguments for glm()
glm(formula, family, data, weights, subset, ...)
```

* `formula = ` El modelo se proporciona a `glm()` como una ecuación, con el resultado a la izquierda y las variables explicativas a la derecha de una tilde \~.
* `family = ` Determina el tipo de modelo a ejecutar. Para la regresión logística, utiliza `family = "binomial"`, para poisson utiliza `family = "poisson"`. Otros ejemplos se encuentran en la tabla siguiente.
* `data = ` Especifica tu base de datos. 

Si es necesario, también puede especificar la función de enlace mediante la sintaxis `family = familytype(link = "linkfunction"))`. Puedes leer más en la documentación sobre otras familias y argumentos opcionales como  `weights = ` y `subset = ` (`?glm`).


Familia                |Función de enlace por defecto
-----------------------|-------------------------------------------  
`"binomial"` | `(link = "logit")`  
`"gaussian"` | `(link = "identity")`  
`"Gamma"` | `(link = "inverse")`  
`"inverse.gaussian"` | `(link = "1/mu^2")`  
`"poisson"` | `(link = "log")`  
`"quasi"` | `(link = "identity", variance = "constant")`  
`"quasibinomial"` | `(link = "logit")`  
`"quasipoisson"` | `(link = "log")`  


Cuando se ejecuta `glm()` lo más habitual es guardar los resultados como un objeto R. A continuación, se pueden mostrar los resultados en la consola utilizando `summary()` como se muestra a continuación, o realizar otras operaciones con los resultados (por ejemplo, exponenciar).

Si necesitas ejecutar una regresión binomial negativa, puede utilizar el paquete **MASS**;  el cual contiene la función `glm.nb()` que utiliza la misma sintaxis que `glm()`. 

Para un recorrido por diferentes regresiones, consulta la [página de estadísticas de UCLA](https://stats.idre.ucla.edu/other/dae/).

#### Univariante `glm()` {.unnumbered}

En este ejemplo estamos evaluando la asociación entre diferentes categorías de edad y el resultado de muerte (codificado como 1 en la sección anterior "Preparación"). A continuación se muestra un modelo univariante de `outcome` por age_cat. Guardamos la salida del modelo como `model` y luego la imprimimos con `summary()` en la consola. Observa que las estimaciones proporcionadas son las *probabilidades logarítmicas (log odds)* y que el nivel de referencia es el primer nivel del factor `age_cat` ("0-4").

```{r}
model <- glm(outcome ~ age_cat, family = "binomial", data = linelist)
summary(model)
```

Para modificar el nivel de referencia de una variable determinada, asegúrate de que la columna es del tipo Factor y mueve el nivel deseado a la primera posición con `fct_relevel()` (véase la página sobre [Factores](#factors)). Por ejemplo, a continuación tomamos la columna `age_cat` y establecemos "20-29" como línea de base antes de conectar mediante pipes el dataframe modificado con `glm()`.

```{r}
linelist %>% 
  mutate(age_cat = fct_relevel(age_cat, "20-29", after = 0)) %>% 
  glm(formula = outcome ~ age_cat, family = "binomial") %>% 
  summary()
```

#### Imprimir resultados {.unnumbered}

En la mayoría de los casos, para su empleo posterior, es necesario hacer modificaciones a los resultados obtenidos anteriormente. La función `tidy()` del paquete **broom** es  útil de cara a hacer más presentables los resultados de nuestros modelos.

Aquí demostramos cómo combinar los resultados del modelo con una tabla de recuento.

1.  Obtén las estimaciones de log odds ratio *exponenciadas* y los intervalos de confianza pasando el modelo a `tidy()` y estableciendo `exponentiate = TRUE` y `conf.int = TRUE`.

```{r odds_base_single}

model <- glm(outcome ~ age_cat, family = "binomial", data = linelist) %>% 
  tidy(exponentiate = TRUE, conf.int = TRUE) %>%        # exponentiate and produce CIs
  mutate(across(where(is.numeric), round, digits = 2))  # round all numeric columns
```

A continuación, se muestra el objeto tibble `model`  resultante:

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(model, rownames = FALSE, options = list(pageLength = nrow(model), scrollX=T), class = 'white-space: nowrap' )
```

2.  Combina estos resultados del modelo con una tabla de recuentos. A continuación, creamos la tabla cruzada de recuentos con la función `tabyl()` de **janitor**, como se explica en la página de [tablas descriptivas](#descriptive-tables).

```{r}
counts_table <- linelist %>% 
  janitor::tabyl(age_cat, outcome)
```


<!-- * Group rows by outcome, and get counts by age category   -->
<!-- * Pivot wider so the column are `age_cat`, `0`, and `1`   -->
<!-- * Remove row for `NA` `age_cat`, if applicable, to align with the model results   -->

<!-- ```{r} -->
<!-- counts_table <- linelist %>%  -->
<!--   filter(!is.na(outcome) & !is.na(age_cat)) %>%    # ensure outcome and age_cat are present  -->
<!--   group_by(outcome) %>%                            # get counts of variable of interest grouped by outcome -->
<!--   count(age_cat) %>%   ## gets number or rows by unique outcome-age category combinations   -->
<!--   pivot_wider(names_from = outcome, values_from = n)    ## spread data to wide format (as in cross-tabulation) -->

<!-- ``` -->


Este es el aspecto de este dataframe `counts_table`: 

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(counts_table, rownames = FALSE, options = list(pageLength = nrow(counts_table), scrollX=T), class = 'white-space: nowrap' )
```

Ahora podemos unir `counts_table` y los resultados del `model` horizontalmente con `bind_cols()` (**dplyr**). Recuerda que con `bind_cols()` las filas de los dos dataframes deben estar perfectamente alineadas. En este código, como estamos enlazando mediante pipes, utilizamos `.` para representar el objeto `counts_table` mientras lo enlazamos con el modelo. Para terminar el proceso, utilizamos `select()` para elegir las columnas deseadas y determinar su orden, y finalmente aplicamos la función `round()`  de R **base** en todas las columnas numéricas para especificar 2 decimales.

```{r, message=F, warning=F}
combined <- counts_table %>%           # begin with table of counts
  bind_cols(., model) %>%              # combine with the outputs of the regression 
  select(term, 2:3, estimate,          # select and re-order cols
         conf.low, conf.high, p.value) %>% 
  mutate(across(where(is.numeric), round, digits = 2)) ## round to 2 decimal places
```

Este es el aspecto del dataframe combinado, impreso de forma agradable como una imagen con una función de **flextable**. En [Tablas para presentación](#tables-for-presentation) se explica cómo personalizar dichas tablas con **flextable**, o bien puede utilizar otros paquetes como **knitr** o **GT**. 

```{r}
combined <- combined %>% 
  flextable::qflextable()
```


#### Loops con múltiples modelos univariantes {.unnumbered}  

A continuación presentamos un método que utiliza `glm()` y `tidy()`.  Para un enfoque más sencillo, véase la sección sobre **gtsummary**.

Para ejecutar los modelos en varias variables de exposición para producir odds ratios univariantes (es decir, sin controlar entre sí), se puede utilizar el enfoque siguiente. Utiliza `str_c()` de **stringr** para crear fórmulas univariantes (véase [Caracteres y cadenas](#characters-and-strings)), ejecuta la regresión `glm()` en cada fórmula, pasa cada resultado de `glm()` a `tidy()` y finalmente junta todos los resultados de los modelos resultantes con `bind_rows()` de **tidyr**. Este enfoque utiliza `map()` del paquete **purrr** para iterar - véase la página sobre [Iteración, bucles y listas](#iteration-loops-and-lists) para más información sobre esta herramienta.

1.  Crea un vector de nombres de columnas de las variables explicativas. Ya lo tenemos como `explanatory_vars` de la sección de preparación de esta página.

2.  Utiliza `str_c()` para crear múltiples fórmulas de cadena, con el resultado a la izquierda, y un nombre de columna de `explanatory_vars` a la derecha. El punto `.` sustituye al nombre de la columna en `explanatory_vars`.

```{r}
explanatory_vars %>% str_c("outcome ~ ", .)
```

3.  Pasa estas fórmulas de cadena a `map()` y establece `~glm()` como la función a aplicar a cada entrada. Dentro de `glm()`, establece la fórmula de regresión como `as.formula(.x)`, donde `.x` se sustituirá por la fórmula de cadena definida en el paso anterior. `map()` realizará un bucle sobre cada una de las fórmulas de cadena, ejecutando regresiones para cada una.

4.  Los resultados de este primer `map()` se pasan a un segundo comando `map()`, que aplica `tidy()` a los resultados de la regresión.

5.  Por último, la salida de la segunda función `map()` (una lista de dataframes ordenados) se condensa con `bind_rows()`, dando lugar a un dataframe con todos los resultados univariantes. 


```{r odds_base_multiple}

models <- explanatory_vars %>%       # begin with variables of interest
  str_c("outcome ~ ", .) %>%         # combine each variable into formula ("outcome ~ variable of interest")
  
  # iterate through each univariate formula
  map(                               
    .f = ~glm(                       # pass the formulas one-by-one to glm()
      formula = as.formula(.x),      # within glm(), the string formula is .x
      family = "binomial",           # specify type of glm (logistic)
      data = linelist)) %>%          # dataset
  
  # tidy up each of the glm regression outputs from above
  map(
    .f = ~tidy(
      .x, 
      exponentiate = TRUE,           # exponentiate 
      conf.int = TRUE)) %>%          # return confidence intervals
  
  # collapse the list of regression outputs in to one data frame
  bind_rows() %>% 
  
  # round all numeric columns
  mutate(across(where(is.numeric), round, digits = 2))
```

Esta vez, el objeto final `models` es más largo porque ahora representa los resultados combinados de varias regresiones univariantes. Clica para ver todas las filas de `model`.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(models, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Como antes, podemos crear una tabla de recuentos a partir de `linelist` para cada variable explicativa, vincularla a `models` y hacer una bonita tabla. Comenzamos con las variables, e iteramos a través de ellas con `map()`. Iteramos a través de una función definida por el usuario que implica la creación de una tabla de recuentos con funciones **dplyr**. Luego se combinan los resultados y se vinculan con los resultados del modelo `models`.  


```{r, warning=F, message=F}

## for each explanatory variable
univ_tab_base <- explanatory_vars %>% 
  map(.f = 
    ~{linelist %>%                ## begin with linelist
        group_by(outcome) %>%     ## group data set by outcome
        count(.data[[.x]]) %>%    ## produce counts for variable of interest
        pivot_wider(              ## spread to wide format (as in cross-tabulation)
          names_from = outcome,
          values_from = n) %>% 
        drop_na(.data[[.x]]) %>%         ## drop rows with missings
        rename("variable" = .x) %>%      ## change variable of interest column to "variable"
        mutate(variable = as.character(variable))} ## convert to character, else non-dichotomous (categorical) variables come out as factor and cant be merged
      ) %>% 
  
  ## collapse the list of count outputs in to one data frame
  bind_rows() %>% 
  
  ## merge with the outputs of the regression 
  bind_cols(., models) %>% 
  
  ## only keep columns interested in 
  select(term, 2:3, estimate, conf.low, conf.high, p.value) %>% 
  
  ## round decimal places
  mutate(across(where(is.numeric), round, digits = 2))

```

A continuación se muestra el aspecto del dataframe. Consulta la página sobre [Tablas para presentación](#tables-for-presentation) para obtener ideas sobre cómo convertir esta tabla en una bonita tabla HTML (por ejemplo, con **flextable**).

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(univ_tab_base, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->

### Paquete **gtsummary** {#reg_gt_uni .unnumbered}

A continuación presentamos el uso de `tbl_uvregression()` del paquete **gtsummary**. Al igual que en la página sobre [Tablas descriptivas](#descriptive-tables), las funciones de gtsummary hacen un buen trabajo a la hora de realizar estadísticas *y* producir outputs con aspecto profesional. Esta función produce una tabla de resultados de regresión univariante.

Seleccionamos sólo las columnas necesarias de `linelist` (variables explicativas y la variable de resultado) y las introducimos en `tbl_uvregression()`. Vamos a ejecutar una regresión univariante en cada una de las columnas que definimos como `explanatory_vars` en la sección de preparación de datos (sexo, fiebre, escalofríos, tos, dolores, vómitos y age_cat).

Dentro de la propia función, proporcionamos el `method = ` como `glm` (sin comillas), la columna de resultado `y = ` (`outcome`), especificamos a `method.args = ` que queremos ejecutar la regresión logística a través de  `family = binomial`, y le decimos que exponencie los resultados.

La salida es HTML y contiene el recuento de cada variable. 

```{r odds_gt, message=F, warning=F}

univ_tab <- linelist %>% 
  dplyr::select(explanatory_vars, outcome) %>% ## select variables of interest

  tbl_uvregression(                         ## produce univariate table
    method = glm,                           ## define regression want to run (generalised linear model)
    y = outcome,                            ## define outcome variable
    method.args = list(family = binomial),  ## define what type of glm want to run (logistic)
    exponentiate = TRUE                     ## exponentiate to produce odds ratios (rather than log odds)
  )

## view univariate results table 
univ_tab
```

Hay muchas modificaciones que se pueden hacer al output de esta tabla, como ajustar las etiquetas de texto, poner en negrita las filas por tu valor p, etc. Puedes consultar tutoriales [aquí](http://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html) y en internet. 



<!-- ======================================================= -->

## Estratificado {#stratified}

Actualmente, el análisis estratificado para **gtsummary** se está desarrollando. Esta página se actualizará a su debido tiempo.



## Multivariable  {#multivariable}

Para el análisis multivariable, volvemos a presentar dos enfoques:

* `glm()` y `tidy()`  
* Paquete **gtsummary** 

El flujo de trabajo es similar para cada uno de ellos, siendo diferente el último paso al elaborar una tabla final.


### Realizar análisis multivariable {.unnumbered}  

Aquí utilizamos `glm()` pero en este caso, añadiremos más variables al lado derecho de la ecuación, separadas por símbolos de suma (`+`).

Para ejecutar el modelo con todas nuestras variables explicativas ejecutaríamos: 

```{r}
mv_reg <- glm(outcome ~ gender + fever + chills + cough + aches + vomit + age_cat, family = "binomial", data = linelist)

summary(mv_reg)
```

Si quieres incluir dos variables y una interacción entre ellas puede separarlas con un asterisco `*` en lugar de un `+`. Si sólo especifica la interacción, sepáralas con dos puntos `:`. Por ejemplo: 

```{r, eval=F}
glm(outcome ~ gender + age_cat * fever, family = "binomial", data = linelist)
```


*Opcionalmente*, puedes utilizar este código para aprovechar el vector predefinido de nombres de columnas y volver a crear el comando anterior utilizando `str_c()`. Esto puede ser útil si los nombres de sus variables explicativas cambian, o si no quieres escribirlas todos de nuevo.

```{r mv_regression}

## run a regression with all variables of interest 
mv_reg <- explanatory_vars %>%  ## begin with vector of explanatory column names
  str_c(collapse = "+") %>%     ## combine all names of the variables of interest separated by a plus
  str_c("outcome ~ ", .) %>%    ## combine the names of variables of interest with outcome in formula style
  glm(family = "binomial",      ## define type of glm as logistic,
      data = linelist)          ## define your dataset
```


#### Construir el modelo {.unnumbered}  

Puedes construir tu modelo paso a paso, guardando varios modelos que incluyan determinadas variables explicativas. Puedes comparar estos modelos con pruebas de razón de verosimilitud utilizando `lrtest()` del paquete **lmtest**, como se indica a continuación:

<span style="color: black;">***NOTA:*** El uso de `anova(model1, model2, test = "Chisq")` de R **base** produce los mismos resultados </span> 

```{r}
model1 <- glm(outcome ~ age_cat, family = "binomial", data = linelist)
model2 <- glm(outcome ~ age_cat + gender, family = "binomial", data = linelist)

lmtest::lrtest(model1, model2)
```

Otra opción es tomar el objeto que contiene el modelo y aplicar la función `step()` del paquete **stats**. Especifica qué dirección de selección de variables deseas utilizar al construir el modelo.  

```{r}
## choose a model using forward selection based on AIC
## you can also do "backward" or "both" by adjusting the direction
final_mv_reg <- mv_reg %>%
  step(direction = "forward", trace = FALSE)
```


Para mayor claridad, también puedes desactivar la notación científica en tu sesión de R. 

```{r}
options(scipen=999)
```

Como se describe en la sección sobre el análisis univariante, pasamos la salida del modelo a `tidy()` para exponenciar las probabilidades logarítmicas y los IC. Finalmente, redondeamos todas las columnas numéricas a dos decimales. Haz scroll para ver el resultado. 

```{r mv_regression_base}

mv_tab_base <- final_mv_reg %>% 
  broom::tidy(exponentiate = TRUE, conf.int = TRUE) %>%  ## get a tidy dataframe of estimates 
  mutate(across(where(is.numeric), round, digits = 2))          ## round 
```

Este es el aspecto del dataframe resultante:

```{r, message=FALSE, echo=F}
DT::datatable(mv_tab_base, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```





<!-- ======================================================= -->

### Combinar regresiones univariantes y multivariables {.unnumbered}

#### Combinar con **gtsummary**  {.unnumbered}  

El paquete **gtsummary** proporciona la función `tbl_regression()`, que toma los resultados de una regresión (`glm()` en este caso) y produce una bonita tabla resumen.

```{r mv_regression_gt}
## show results table of final regression 
mv_tab <- tbl_regression(final_mv_reg, exponentiate = TRUE)
```

Veamos la tabla:

```{r}
mv_tab
```

También puedes combinar varias tablas producidas por **gtsummary** con la función `tbl_merge()`. En este ejemplo combinaremos los resultados multivariables con los resultados *univariantes* de **gtsummary** que creamos [anteriormente](#reg_gt_uni):

```{r}
## combine with univariate results 
tbl_merge(
  tbls = list(univ_tab, mv_tab),                          # combine
  tab_spanner = c("**Univariate**", "**Multivariable**")) # set header names
```



#### Combinar con **dplyr** {.unnumbered}  

Una forma alternativa de combinar los resultados univariables y multivariables de `glm()`/`tidy()` es con las funciones join de **dplyr**.

* Unimos los resultados univariantes obtenidos anteriormente (`univ_tab_base`, que contiene los recuentos) con los resultados multivariables en formato tidy de `mv_tab_base`.
* Utilizamos `select()` para mantener sólo las columnas que queremos, especificar su orden y renombrarlas
* Empleamos `round()` con dos decimales en todas las columnas que sean de tipo "Double".  

```{r, warning=F, message=F}
## combine univariate and multivariable tables 
left_join(univ_tab_base, mv_tab_base, by = "term") %>% 
  ## choose columns and rename them
  select( # new name =  old name
    "characteristic" = term, 
    "recovered"      = "0", 
    "dead"           = "1", 
    "univ_or"        = estimate.x, 
    "univ_ci_low"    = conf.low.x, 
    "univ_ci_high"   = conf.high.x,
    "univ_pval"      = p.value.x, 
    "mv_or"          = estimate.y, 
    "mvv_ci_low"     = conf.low.y, 
    "mv_ci_high"     = conf.high.y,
    "mv_pval"        = p.value.y 
  ) %>% 
  mutate(across(where(is.double), round, 2))   

```




<!-- ======================================================= -->

## Forest plot {#forest-plot}

Esta sección muestra cómo producir un gráfico con los resultados de tu regresión. 
Hay dos opciones, puedes construir un gráfico tú mismo usando **ggplot2** o usar un metapaquete llamado **easystats** (un paquete que incluye muchos paquetes).

Consulta la página sobre [Conceptos básicos de ggplot](#ggplot-basics) si no estás familiarizado con el paquete de gráficos **ggplot2**.


<!-- ======================================================= -->

### Paquete **ggplot2** {.unnumbered}

Puedes construir un gráfico de bosque con `ggplot()` trazando elementos de los resultados de la regresión multivariable. Añade las capas de los gráficos utilizando estos "geoms":

* Añadimos estimaciones con `geom_point()`
* Añadimos intervalos de confianza con `geom_errorbar()`
* Ploteamos una línea vertical en OR = 1 con `geom_vline()`

Antes de empezar a plotear, es posible que sea necesario  utilizar `fct_relevel()` del paquete **forcats** para establecer el orden de las variables/niveles en el eje y. De no establecer un orden en las variables,  `ggplot()` podría mostrar las variables en orden alfanumérico, lo que no funcionaría bien para los valores de categoría de edad ("30" aparecería antes de "5"). Mira la página sobre [Factores](#factors) para más detalles.

```{r ggplot_forest}

## remove the intercept term from your multivariable results
mv_tab_base %>% 
  
  #set order of levels to appear along y-axis
  mutate(term = fct_relevel(
    term,
    "vomit", "gender", "fever", "cough", "chills", "aches",
    "age_cat5-9", "age_cat10-14", "age_cat15-19", "age_cat20-29",
    "age_cat30-49", "age_cat50-69", "age_cat70+")) %>%
  
  # remove "intercept" row from plot
  filter(term != "(Intercept)") %>% 
  
  ## plot with variable on the y axis and estimate (OR) on the x axis
  ggplot(aes(x = estimate, y = term)) +
  
  ## show the estimate as a point
  geom_point() + 
  
  ## add in an error bar for the confidence intervals
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) + 
  
  ## show where OR = 1 is for reference as a dashed line
  geom_vline(xintercept = 1, linetype = "dashed")
  
```


<!-- ======================================================= -->

### Paquetes **easystats** {.unnumbered}

Una alternativa, si no deseas el nivel de precisión y control que proporciona **ggplot2**, es utilizar la combinación de paquetes **easystats**.

La función `model_parameters()` del paquete **parameters** hace el equivalente de la función `tidy()` del paquete **broom**. El paquete **see** acepta esos resultados y crea por defecto un forest plot, dándo como output un objeto `ggplot()`.

```{r easystats_forest}
pacman::p_load(easystats)

## remove the intercept term from your multivariable results
final_mv_reg %>% 
  model_parameters(exponentiate = TRUE) %>% 
  plot()
  
```


<!-- ======================================================= -->

## Recursos {#resources-11}

El contenido de esta página se ha basado en estos recursos y viñetas:  

[Regresión lineal en R](https://www.datacamp.com/community/tutorials/linear-regression-R)

[gtsummary](http://www.danieldsjoberg.com/gtsummary/articles/tbl_regression.html)

[Página de estadísticas de la UCLA](https://stats.idre.ucla.edu/other/dae/)

[regresión escalonada sthda](http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/)

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/regression.Rmd-->


# Valores faltantes {#missing-data}

```{r, out.width=c("50%"), echo=F}
knitr::include_graphics(here::here("images", "missingness.png"))
knitr::include_graphics(here::here("images", "missingness_overview.png"))
```


En esta página se explica cómo:

1.  Evaluar la falta de información
2.  Filtrar las filas por valores faltantes
3.  Representar la falta de datos a lo largo del tiempo
4.  Manejar cómo se muestra `NA` en los gráficos
5.  Realizar la imputación de valores faltantes: MCAR, MAR, MNAR



<!-- ======================================================= -->
## Preparación {#preparation-11 }

### Cargar paquetes {.unnumbered}  

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de de R **base** Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r}
pacman::p_load(
  rio,           # import/export
  tidyverse,     # data mgmt and viz
  naniar,        # assess and visualize missingness
  mice           # missing data imputation
)
```


### Importar datos {.unnumbered}

Importamos los datos de casos de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica para descargar `linelist` "limpioa"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa tus datos con la función `import()` del paquete **rio** (acepta muchos tipos de archivos como .xlsx, .rds, .csv - Mira la página de [importación y exportación](#import-and-export) para más detalles).

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas de `linelist`.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Convertir valores faltantes en la importación {.unnumbered}  

Al importar los datos, ten en cuenta los valores que deben clasificarse como faltantes. Por ejemplo, 99, 999, "Missing", celdas en blanco ("") o celdas con un espacio vacío (" "). Puedes convertirlos en `NA` (la versión de R de los valores faltantes) con el comando de importación de datos.
Consulta la sección de [datos faltantes](#import_missing) de la página de Importación para obtener más detalles, ya que la sintaxis exacta varía según el tipo de archivo. 


<!-- ======================================================= -->
## Valores faltantes en R {#missing-values-in-r}

A continuación, exploramos las formas en que se presenta y evalúa los datos faltantes en R, junto con algunos valores y funciones adyacentes.  

### `NA` {.unnumbered}  

En R, los valores faltantes se representan con un valor reservado (especial): `NA`. Ten en cuenta que se escribe *sin* comillas. "NA" es diferente y es sólo un valor de carácter normal (también una letra de los Beatles de la canción Hey Jude).

Tus datos pueden tener otras formas de representar la falta de información, como "99", o "Missing", o "Desconocido" - incluso puedes tener el valor de carácter vacío "" que parece "en blanco", o un solo espacio " ". Se consciente de ello y considera la posibilidad de [convertirlos en `NA` durante la importación](#import_missing) o durante la limpieza de datos con `na_if()`.

En tu limpieza de datos, también puedes convertir en el otro sentido - cambiando todos los `NA` a "Missing" o similar con `replace_na()` o con `fct_explicit_na()` para los factores.


### Versiones de `NA` {.unnumbered}  

La mayoría de las veces, `NA` representa un valor que falta y todo funciona bien. Sin embargo, en algunas circunstancias puedes encontrar la necesidad de *variaciones* de `NA` específicas para un tipo de objeto (carácter, numérico, etc.). Esto será poco frecuente, pero debes tenerlo en cuenta.

El escenario típico para esto es cuando se crea una nueva columna con la función **dplyr** `case_when()`. Como se describe en la página de [Limpieza de datos y funciones básicas](#clean_case_when), esta función evalúa cada fila del dataframe, valora si las filas cumplen con los criterios lógicos especificados (lado derecho del código), y asigna el nuevo valor correcto (lado izquierdo del código). *Importante: todos los valores del lado derecho deben ser del mismo tipo*. 

```{r, eval=F}
linelist <- linelist %>% 
  
  # Create new "age_years" column from "age" column
  mutate(age_years = case_when(
    age_unit == "years"  ~ age,       # if age is given in years, assign original value
    age_unit == "months" ~ age/12,    # if age is given in months, divide by 12
    is.na(age_unit)      ~ age,       # if age UNIT is missing, assume years
    TRUE                 ~ NA_real_)) # any other circumstance, assign missing
```

Si deseas `NA` en el lado derecho, es posible que tengas que especificar una de las opciones especiales de `NA` que se indican a continuación. Si los otros valores del lado derecho son caracteres, considera usar "Missing" en su lugar o, de lo contrario, usa `NA_character_`. Si todos son numéricos, utiliza `NA_real_`. Si todos son fechas o lógicos, puedes utilizar `NA`.

* `NA` - utilizar para fechas o TRUE/FALSE lógico
* `NA_character_` - utilizar para caracteres
* `NA_real_` - uso para numérico

De nuevo, no es probable que te encuentres con estas variaciones *a menos que* estés utilizando `case_when()` para crear una nueva columna. Consulta la [documentación de R sobre NA](https://stat.ethz.ch/R-manual/R-devel/library/base/html/NA.html) para obtener más información.





### `NULL` {.unnumbered}  

`NULL` es otro valor reservado en R. Es la representación lógica de una declaración que no es ni verdadera ni falsa. Es devuelto por expresiones o funciones cuyos valores son indefinidos. Generalmente no asignes NULL como valor, a menos que escribas funciones o si escribes una [aplicación **Shiny**](#dashboards-with-shiny) para devolver `NULL` en escenarios específicos.

La nulidad puedes evaluarse con `is.null()` y la conversión puedes hacerse con `as.null()`.

Véase esta [entrada del blog](https://www.r-bloggers.com/2010/04/r-na-vs-null/) sobre la diferencia entre `NULL` y `NA`.




### `NaN` {.unnumbered}  

Los valores imposibles se representan con el valor especial `NaN`. Un ejemplo de esto es cuando se fuerza a R a dividir 0 entre 0. Puedes evaluar esto con `is.nan()`. También puedes encontrar funciones complementarias incluyendo `is.infinite()` y `is.finite()`.

### `Inf` {.unnumbered}  

`Inf` representa un valor infinito, como cuando se divide un número por 0.

Como ejemplo de cómo podría afectar esto a tu trabajo: digamos que tienes un vector/columna z que contiene estos valores: `z <- c(1, 22, NA, Inf, NaN, 5)`

Si deseas utilizar `max()` en la columna para encontrar el valor más alto, puedes utilizar el `na.rm = TRUE` para eliminar el `NA` del cálculo, pero el `Inf` y el` NaN` permanecen y se devolverá `Inf`. Para resolver esto, puedes utilizar los corchetes `[ ]` y `is.finite()` para subconjuntar de manera que sólo se utilicen valores finitos para el cálculo: `max(z[is.finite(z)])`.

```{r, eval=F}
z <- c(1, 22, NA, Inf, NaN, 5)
max(z)                           # returns NA
max(z, na.rm=T)                  # returns Inf
max(z[is.finite(z)])             # returns 22
```


### Ejemplos {.unnumbered}  


Comando R | Resultado
----------|--------------
`5 / 0` | `Inf`  
`0 / 0` | `NaN`  
`5 / NA` | `NA`  
`5 / Inf | `0`  
`NA - 5` | `NA`  
`Inf / 5` | `Inf`  
`class(NA)` | "logical"  
`class(NaN)` | "numeric"  
`class(Inf)` | "numeric"  
`class(NULL)` | "NULL"  

"NAs introduced by coercion" es un mensaje de aviso común. Esto puede ocurrir si se intenta hacer una conversión ilegal como insertar un valor de carácter en un vector que de otra manera es numérico.

```{r}
as.numeric(c("10", "20", "thirty", "40"))
```

`NULL` se ignora en un vector.

```{r}
my_vector <- c(25, NA, 10, NULL)  # define
my_vector                         # print
```


La varianza de un número da como resultado `NA`.

```{r}
var(22)
```


<!-- ======================================================= -->
## Funciones útiles {#useful-functions}

Las siguientes funciones de R **base** son muy útiles a la hora de evaluar o manejar los valores faltantes:


### `is.na()` y `!is.na()` {.unnumbered}  

Utiliza `is.na()` para identificar los valores que faltan, o utiliza su opuesto (con `!` delante) para identificar los valores que no faltan. Ambos devuelven un valor lógico (`TRUE` o `FALSE`). Recuerda que puedes `sum()` el vector resultante para contar el número de `TRUE`, por ejemplo, `sum(is.na(linelist$date_outcome))`.   

```{r}
my_vector <- c(1, 4, 56, NA, 5, NA, 22)
is.na(my_vector)
!is.na(my_vector)
sum(is.na(my_vector))
```


### `na.omit()` {.unnumbered}  

Esta función, si se aplica a un dataframe, eliminará *las* filas con valores faltantes. También es de R **base**. Si se aplica a un vector, eliminará los valores `NA` del vector al que se aplica. Por ejemplo:

```{r}
na.omit(my_vector)
```

### `drop_na()` {.unnumbered}  

Esta es una función **tidyr** que es útil en un [proceso de limpieza de datos](#cleaning-data-and-core-functions). Si se ejecuta con los paréntesis vacíos, elimina *las* filas con valores faltantes. Si se especifican los nombres de las columnas en los paréntesis, se eliminarán las filas con valores faltantes en esas columnas. También puedes utilizar la sintaxis "tidyselect" para especificar las columnas. 

```{r, eval=F}
linelist %>% 
  drop_na(case_id, date_onset, age) # drops rows missing values for any of these columns
```


### `na.rm = TRUE` {.unnumbered}  

Cuando se ejecuta una función matemática como `max()`, `min()`, `sum()` o `mean()`, si hay algún valor `NA` presente el valor devuelto será NA. Este comportamiento por defecto es intencionado, para que avise si falta algún dato.

Puedes evitarlo eliminando los valores faltantes del cálculo. Para ello, incluye el argumento `na.rm = TRUE` ("na.rm" significa "eliminar NA").


```{r}
my_vector <- c(1, 4, 56, NA, 5, NA, 22)

mean(my_vector)     

mean(my_vector, na.rm = TRUE)
```



<!-- ======================================================= -->
## Evaluar la ausencia de datos en un dataframe {#assess-missingness-in-a-data-frame}

Puedes utilizar el paquete **naniar** para evaluar y visualizar la falta de datos del dataframe `linelist`. 

```{r}
# install and/or load package
pacman::p_load(naniar)
```

### Cuantificación de la ausencia de datos {.unnumbered}

Para encontrar el porcentaje de todos los valores que faltan utiliza `pct_miss()`. Utiliza `n_miss()` para obtener el número de valores faltantes.  

```{r}
# percent of ALL data frame values that are missing
pct_miss(linelist)
```

Las dos funciones siguientes devuelven el porcentaje de filas con algún valor ausente, o que están totalmente completas, respectivamente. Recuerda que `NA` significa que falta, y que `""` o `" "` no se contarán como faltantes.

```{r}
# Percent of rows with any value missing
pct_miss_case(linelist)   # use n_complete() for counts
```

```{r}
# Percent of rows that are complete (no values missing)  
pct_complete_case(linelist) # use n_complete() for counts
```



### Visualización de faltantes {.unnumbered}  

La función `gg_miss_var()` mostrará el número (o el %) de valores faltantes en cada columna. Algunos matices:

* Puedes añadir un nombre de columna (no entre comillas) al argumento `facet = ` para ver el gráfico por grupos
* Por defecto, se muestran los recuentos en lugar de los porcentajes, cámbialo con `show_pct = TRUE`
* Puedes añadir etiquetas de eje y de título como para un `ggplot()` normal con `+ labs(...)` 


```{r}
gg_miss_var(linelist, show_pct = TRUE)
```

Aquí los datos están conectados con `%>%` en la función. El argumento `facet = ` también se utiliza para dividir los datos.

```{r}
linelist %>% 
  gg_miss_var(show_pct = TRUE, facet = outcome)
```


Puedes utilizar `vis_miss()` para visualizar el dataframe como un mapa de calor, mostrando si cada valor falta o no. También puedes `select()` determinadas columnas del dataframe y proporcionar sólo esas columnas a la función.

```{r}
# Heatplot of missingness across the entire data frame  
vis_miss(linelist)
```


### Explorar y visualizar las relaciones de datos faltantes {.unnumbered} 

¿Cómo se visualiza algo que no existe? Por defecto, `ggplot()` elimina los puntos con valores faltantes de los gráficos.

**naniar** ofrece una solución mediante `geom_miss_point()`. Al crear un gráfico de dispersión de dos columnas, los registros con uno de los valores ausentes y el otro valor presente se muestran estableciendo los valores ausentes en un 10% más bajo que el valor más bajo de la columna, y coloreándolos de forma distinta.

En el gráfico de dispersión que aparece a continuación, los puntos rojos son registros en los que el valor de una columna está presente pero falta el valor de la otra columna. Esto permite ver la distribución de los valores que faltan en relación con los valores que no faltan.



```{r}
ggplot(
  data = linelist,
  mapping = aes(x = age_years, y = temp)) +     
  geom_miss_point()
```

Para evaluar la ausencia en el dataframe *estratificado por otra columna*, puedes utilizar `gg_miss_fct()`, que devuelve un mapa de calor del porcentaje de ausencia en el dataframe *por una columna de factor/categoría (o fecha)*:

```{r}
gg_miss_fct(linelist, age_cat5)
```


Esta función también se puede utilizar con una columna de fechas para ver cómo ha cambiado la falta de datos en el tiempo:  

```{r}
gg_miss_fct(linelist, date_onset)
```




### "Sombra" de las columnas {.unnumbered}

Otra forma de visualizar la ausencia de valores en una columna es mediante una segunda columna que sea como una "sombra" de esta  que puede crear **naniar**. `bind_shadow()` crea una columna binaria `NA`/no `NA` para cada columna existente, y vincula todas estas nuevas columnas al conjunto de datos original con el apéndice "_NA". Esto duplica el número de columnas - ver más abajo:


```{r}
shadowed_linelist <- linelist %>% 
  bind_shadow()

names(shadowed_linelist)
```

Estas  "sombras" de las columnas pueden utilizarse para trazar la proporción de valores que faltan, por cualquier otra columna.

Por ejemplo, el siguiente gráfico muestra la proporción de registros que carecen de `days_onset_hosp` (número de días desde el inicio de los síntomas hasta la hospitalización), según el valor de ese registro en `date_hospitalisation`. Esencialmente, se está trazando la densidad de la columna del eje x, pero estratificando los resultados (`color =` ) por una columna de sombra de interés. Este análisis funciona mejor si el eje-x es una columna numérica o de fecha.


```{r, message = F}
ggplot(data = shadowed_linelist,          # data frame with shadow columns
  mapping = aes(x = date_hospitalisation, # numeric or date column
                colour = age_years_NA)) + # shadow column of interest
  geom_density()                          # plots the density curves
```

También puedes utilizar estas columnas "sombra" para estratificar un resumen estadístico, como se muestra a continuación:

```{r}
linelist %>%
  bind_shadow() %>%                # create the shows cols
  group_by(date_outcome_NA) %>%    # shadow col for stratifying
  summarise(across(
    .cols = age_years,             # variable of interest for calculations
    .fns = list("mean" = mean,     # stats to calculate
                "sd" = sd,
                "var" = var,
                "min" = min,
                "max" = max),  
    na.rm = TRUE))                 # other arguments for the stat calculations
```


A continuación se muestra una forma alternativa de trazar la proporción de los valores de una columna que faltan a lo largo del tiempo. *No* implica **naniar**. Este ejemplo muestra el porcentaje de observaciones semanales que faltan).

1.  Agrega los datos en una unidad de tiempo útil (días, semanas, etc.), resumiendo la proporción de observaciones con `NA` (y cualquier otro valor de interés)
2.  Representa la proporción que falta como una línea usando `ggplot()`

A continuación, tomamos `linelist`, añadimos una nueva columna para la semana, agrupamos los datos por semana y luego calculamos el porcentaje de registros de esa semana en los que falta el valor. (Nota: si se desea el porcentaje de 7 días el cálculo sería ligeramente diferente).

```{r}
outcome_missing <- linelist %>%
  mutate(week = lubridate::floor_date(date_onset, "week")) %>%   # create new week column
  group_by(week) %>%                                             # group the rows by week
  summarise(                                                     # summarize each week
    n_obs = n(),                                                  # number of records
    
    outcome_missing = sum(is.na(outcome) | outcome == ""),        # number of records missing the value
    outcome_p_miss  = outcome_missing / n_obs,                    # proportion of records missing the value
  
    outcome_dead    = sum(outcome == "Death", na.rm=T),           # number of records as dead
    outcome_p_dead  = outcome_dead / n_obs) %>%                   # proportion of records as dead
  
  tidyr::pivot_longer(-week, names_to = "statistic") %>%         # pivot all columns except week, to long format for ggplot
  filter(stringr::str_detect(statistic, "_p_"))                  # keep only the proportion values
```

Entonces, representamos la proporción que falta como una línea, por semana. Si no estás familiarizado con el paquete de gráficas **ggplot2**, consulta la página de [fundamentos de ggplot](#ggplot-basics).

```{r, message=F, warning=F}
ggplot(data = outcome_missing)+
    geom_line(
      mapping = aes(x = week, y = value, group = statistic, color = statistic),
      size = 2,
      stat = "identity")+
    labs(title = "Weekly outcomes",
         x = "Week",
         y = "Proportion of weekly records") + 
     scale_color_discrete(
       name = "",
       labels = c("Died", "Missing outcome"))+
    scale_y_continuous(breaks = c(seq(0,1,0.1)))+
  theme_minimal()+
  theme(legend.position = "bottom")
```





<!-- ======================================================= -->
## Uso de datos con valores faltantes {#using-data-with-missing-values} 

### Filtrar las filas con valores faltantes {.unnumbered}

Para eliminar rápidamente las filas con valores faltantes, utiliza la función **dplyr** `drop_na()`.

`linelist` original tiene ` nrow(linelist)` filas. El número ajustado de filas se muestra a continuación:

```{r}
linelist %>% 
  drop_na() %>%     # remove rows with ANY missing values
  nrow()
```

Puedes especificar que se eliminen las filas que faltan en determinadas columnas:

```{r}
linelist %>% 
  drop_na(date_onset) %>% # remove rows missing date_onset 
  nrow()
```

Puedes listar las columnas una tras otra, o utilizar [las funciones de ayuda "tidyselect"](#clean_tidyselect):

```{r}
linelist %>% 
  drop_na(contains("date")) %>% # remove rows missing values in any "date" column 
  nrow()
```



<!-- ======================================================= -->
### Manejo de `NA` en `ggplot()` {.unnumbered}

A menudo es conveniente informar del número de valores excluidos de un gráfico en un pie de foto. A continuación se muestra un ejemplo:

En `ggplot()`, puedes añadir `labs()` y dentro de él un `caption = `. En el pie, puedes usar `str_glue()` del paquete **stringr** para pegar los valores juntos en una frase de forma dinámica para que se ajusten a los datos. Un ejemplo es el siguiente:

* Observa el uso de `\n` para una nueva línea.
* Ten en cuenta que si varias columnas contribuyen a que los valores no se muestren (por ejemplo, la edad o el sexo si se reflejan en el gráfico), deberás filtrar también esas columnas para calcular correctamente el número no mostrado.

```{r, eval=F}
labs(
  title = "",
  y = "",
  x = "",
  caption  = stringr::str_glue(
  "n = {nrow(central_data)} from Central Hospital;
  {nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown."))  
```

A veces, puede ser más fácil guardar la cadena como un objeto en comandos anteriores al comando `ggplot()`, y simplemente referenciar el objeto de cadena nombrado dentro de `str_glue()`.


<!-- ======================================================= -->
### `NA` en los factores {.unnumbered}

Si tu columna de interés es un factor, utiliza `fct_explicit_na()` del paquete **forcats** para convertir los valores `NA` en un valor de carácter. Mira más detalles en la página de [Factores](#factors). Por defecto, el nuevo valor es "(Missing)" pero puede ajustarse mediante el argumento `na_level = `.

```{r}
pacman::p_load(forcats)   # load package

linelist <- linelist %>% 
  mutate(gender = fct_explicit_na(gender, na_level = "Missing"))

levels(linelist$gender)
```



<!-- ======================================================= -->
## Imputación {#imputation}

A veces, al analizar los datos, será importante "rellenar los huecos" e imputar los datos que faltan Aunque siempre se puede analizar simplemente unos datos después de eliminar todos los valores que faltan, esto puede causar problemas de muchas maneras. He aquí dos ejemplos:

1)  Al eliminar todas las observaciones con valores faltantes o las variables con una gran cantidad de valores faltantes, podría reducir su potencia o la capacidad para realizar algunos tipos de análisis. Por ejemplo, como descubrimos antes, sólo una pequeña fracción de las observaciones de nuestro conjunto de datos de linelist no tiene valores faltantes en todas nuestras variables. Si elimináramos la mayor parte de nuestro conjunto de datos, perderíamos mucha información. Además, la mayoría de nuestras variables tienen una cierta cantidad de valores faltantes; para la mayoría de los análisis, probablemente no sea razonable eliminar todas las variables que tienen muchos valores faltantes.

2)  Dependiendo de la razón por la que faltan datos, el análisis de los datos que no faltan podría conducir a resultados sesgados o engañosos. Por ejemplo, como hemos sabido antes, nos faltan datos de algunos pacientes sobre si han tenido algunos síntomas importantes como fiebre o tos. Pero, como una posibilidad, tal vez esa información no se registró para las personas que obviamente no estaban muy enfermas. En ese caso, si elimináramos estas observaciones, estaríamos excluyendo a algunas de las personas más sanas de nuestro conjunto de datos, lo que podría sesgar los resultados.

Es importante pensar en la razón por la que pueden faltar datos, además de ver cuántos faltan. Esto puede ayudarte a decidir la importancia de imputar los datos que faltan, así como el método de imputación de los datos que faltan que pueda ser mejor en esa situación.

### Tipos de datos faltantes {.unnumbered}

A continuación se presentan tres tipos generales de datos faltantes:

1)  **Falta completamente al azar** (MCAR Missing Completely at Random). Esto significa que no existe ninguna relación entre la probabilidad de que falten datos y cualquiera de las otras variables de los datos. La probabilidad de que falte es la misma para todos los casos. Pero, si tienes una fuerte razón para creer que tus datos son MCAR, analizar sólo los datos no ausentes sin imputar no sesgará los resultados (aunque puede perder algo de potencia). [PENDIENTE: considerar la discusión de las pruebas estadísticas para MCAR].

2)  **Falta al azar** (MAR Missing at Random). Este nombre es, en realidad, un poco engañoso, ya que MAR significa que los datos faltan de forma sistemática y predecible en función del resto de la información que se tiene. Por ejemplo, puede que todas las observaciones de nuestro conjunto de datos con un valor ausente de fiebre no se hayan registrado porque se asumió que todos los pacientes con escalofríos y dolores tenían fiebre, por lo que nunca se les tomó la temperatura. Si es cierto, podríamos predecir fácilmente que cada observación que falta con escalofríos y dolores también tiene fiebre y utilizar esta información para imputar nuestros datos que faltan. En la práctica, esto es más bien un espectro. Quizá si un paciente tiene escalofríos y dolores, es más probable que también tenga fiebre si no se le toma la temperatura, pero no siempre. Esto sigue siendo predecible aunque no sea perfectamente predecible. Este es un tipo común de valores faltantes

3)  **Desaparición no aleatoria** (MNAR Missing not at Random). A veces, también se denomina Falta no aleatoria (**NMAR**). Esto supone que la probabilidad de que falte un valor NO es sistemática ni predecible utilizando el resto de la información que tenemos, pero tampoco falta al azar. En esta situación, los datos faltan por razones desconocidas o por razones de las que no se tiene ninguna información. Por ejemplo, en nuestro conjunto de datos puede faltar información sobre la edad porque algunos pacientes muy mayores no saben o se niegan a decir su edad. En esta situación, los datos que faltan sobre la edad están relacionados con el propio valor (y, por tanto, no son aleatorios) y no son predecibles en función del resto de la información que tenemos. El MNAR es complejo y, a menudo, la mejor manera de afrontarlo es intentar recopilar más datos o información sobre el motivo por el que faltan los datos en lugar de intentar imputarlos.

En general, la imputación de datos MCAR suele ser bastante sencilla, mientras que la MNAR es muy difícil, si no imposible. Muchos de los métodos comunes de imputación de datos asumen MAR.

### Paquetes útiles {.unnumbered}

Algunos paquetes útiles para la imputación de valores faltantes son Mmisc, missForest (que utiliza bosques aleatorios para imputar los valores faltantes) y mice (Imputación multivariada por ecuaciones encadenadas). Para esta sección sólo utilizaremos el paquete mice, que implementa una variedad de técnicas. El mantenedor del paquete mice ha publicado un libro en línea sobre [la imputación Flexible de valores faltantes](https://stefvanbuuren.name/fimd/).

Este es el código para cargar el paquete **mice** :

```{r}
pacman::p_load(mice)
```

### Imputación de la media {.unnumbered}

A veces, si estás haciendo un análisis simple o tienes una razón de peso para pensar que puede asumir MCAR, puedes simplemente establecer los valores numéricos que faltan a la media de esa variable. Tal vez podamos asumir que las mediciones de temperatura que faltan en nuestro conjunto de datos eran MCAR o eran simplemente valores normales. Aquí está el código para crear una nueva variable que reemplaza los valores de temperatura faltantes con el valor medio de la temperatura en nuestro conjunto de datos. Sin embargo, en muchas situaciones reemplazar los datos con la media puede conducir a un sesgo, así que ten cuidado.

```{r}
linelist <- linelist %>%
  mutate(temp_replace_na_with_mean = replace_na(temp, mean(temp, na.rm = T)))
```

También puedes realizar un proceso similar para sustituir los datos categóricos por un valor específico. Para nuestro conjunto de datos, imagina que sabes que todas las observaciones con un valor faltante para tu resultado (que puede ser "Muerte" o "Recuperación") son en realidad personas que han muerto (nota: esto no es realmente cierto para este conjunto de datos):

```{r}
linelist <- linelist %>%
  mutate(outcome_replace_na_with_death = replace_na(outcome, "Death"))
```

### Imputación de la regresión {.unnumbered}

Un método algo más avanzado consiste en utilizar algún tipo de modelo estadístico para predecir cuál es el valor que falta y sustituirlo por el valor predicho. Este es un ejemplo de creación de valores predichos para todas las observaciones en las que falta la temperatura, pero no la edad ni la fiebre, mediante una simple regresión lineal que utiliza el estado de la fiebre y la edad en años como predictores. En la práctica, es conveniente utilizar un modelo mejor que este tipo de enfoque simple.

```{r, warning=F, message=F}
simple_temperature_model_fit <- lm(temp ~ fever + age_years, data = linelist)

#using our simple temperature model to predict values just for the observations where temp is missing
predictions_for_missing_temps <- predict(simple_temperature_model_fit,
                                        newdata = linelist %>% filter(is.na(temp))) 
```

O bien, utilizando el mismo enfoque de modelización a través del paquete **mice** para crear valores imputados para las observaciones de temperatura que faltan:

```{r}
model_dataset <- linelist %>%
  select(temp, fever, age_years)  

temp_imputed <- mice(model_dataset,
                            method = "norm.predict",
                            seed = 1,
                            m = 1,
                            print = F)

temp_imputed_values <- temp_imputed$imp$temp

```


Este es el mismo tipo de enfoque de algunos métodos más avanzados, como el uso del paquete **missForest** para sustituir los datos que faltan por valores predichos. En ese caso, el modelo de predicción es un bosque aleatorio en lugar de una regresión lineal. También se pueden utilizar otros tipos de modelos para hacer esto. Sin embargo, aunque este enfoque funciona bien con MCAR, debes tener un poco de cuidado si crees que MAR o MNAR describen con más precisión tu situación. La calidad de tu imputación dependerá de lo bueno que sea tu modelo de predicción e incluso con un modelo muy bueno la variabilidad de los datos imputados puede estar subestimada.

### LOCF y BOCF {.unnumbered}

La última observación trasladada (LOCF) y la observación de referencia trasladada (BOCF) son métodos de imputación para datos de series temporales/longitudinales. La idea es tomar el valor observado anterior como reemplazo de los datos que faltan. Cuando faltan varios valores sucesivamente, el método busca el último valor observado.

La función `fill()` del paquete **tidyr** puede utilizarse para la imputación LOCF y BOCF (sin embargo, otros paquetes como **HMISC**, **zoo** y **data.table** también incluyen métodos para hacerlo). Para mostrar la sintaxis de `fill()`, crearemos un sencillo conjunto de datos de series temporales que contenga el número de casos de una enfermedad para cada trimestre de los años 2000 y 2001. Sin embargo, falta el valor del año para los trimestres posteriores al primero, por lo que tendremos que imputarlos. La unión `fill()` también se demuestra en la página [Pivotar datos](#pivoting-data).

```{r}
#creating our simple dataset
disease <- tibble::tribble(
  ~quarter, ~year, ~cases,
  "Q1",    2000,    66013,
  "Q2",      NA,    69182,
  "Q3",      NA,    53175,
  "Q4",      NA,    21001,
  "Q1",    2001,    46036,
  "Q2",      NA,    58842,
  "Q3",      NA,    44568,
  "Q4",      NA,    50197)

#imputing the missing year values:
disease %>% fill(year)

```

Nota: asegúrate de que tus datos están correctamente ordenados antes de utilizar la función `fill()`. `fill()` rellena por defecto "hacia abajo", pero también puedes imputar valores en diferentes direcciones cambiando el parámetro `.direction`. Podemos hacer unos datos similares en el que el valor del año se registra sólo al final del año y falta para los trimestres anteriores:

```{r}
#creating our slightly different dataset
disease <- tibble::tribble(
  ~quarter, ~year, ~cases,
  "Q1",      NA,    66013,
  "Q2",      NA,    69182,
  "Q3",      NA,    53175,
  "Q4",    2000,    21001,
  "Q1",      NA,    46036,
  "Q2",      NA,    58842,
  "Q3",      NA,    44568,
  "Q4",    2001,    50197)

#imputing the missing year values in the "up" direction:
disease %>% fill(year, .direction = "up")

```
En este ejemplo, LOCF y BOCF son claramente lo correcto, pero en situaciones más complicadas puede ser más difícil decidir si estos métodos son apropiados. Por ejemplo, es posible que falten valores de laboratorio para un paciente del hospital después del primer día. A veces, esto puede significar que los valores de laboratorio no cambiaron... ¡pero también podría significar que el paciente se recuperó y sus valores serían muy diferentes después del primer día! Utiliza estos métodos con precaución.


### Imputación múltiple {.unnumbered}

El [libro en línea que mencionamos antes](https://stefvanbuuren.name/fimd/), escrito por el autor del paquete **mice**  contiene una explicación detallada de la imputación múltiple y de los motivos por los que conviene utilizarla. Pero, aquí hay una explicación básica del método:

Cuando se realiza una imputación múltiple, se crean múltiples conjuntos de datos con los valores faltantes imputados a valores de datos plausibles (dependiendo de los datos de tu investigación, puedes querer crear más o menos de estos conjuntos de datos imputados, pero el paquete **mice** establece el número por defecto en 5). La diferencia es que, en lugar de un valor único y específico, cada valor imputado se extrae de una distribución estimada (por lo que incluye cierta aleatoriedad). Como resultado, cada uno de estos conjuntos de datos tendrá valores imputados ligeramente diferentes (sin embargo, los datos no ausentes serán los mismos en cada uno de estos conjuntos de datos imputados). Todavía se utiliza algún tipo de modelo predictivo para hacer la imputación en cada uno de estos nuevos conjuntos de datos (mice tiene muchas opciones para los métodos de predicción, incluyendo *Predictive Mean Matching*, *regresión logística* y *random forest*), pero el paquete mice puede encargarse de muchos de los detalles del modelado.

Entonces, una vez que hayas creado estos nuevos conjuntos de datos imputados, puedes aplicar cualquier modelo estadístico o análisis que estuviera planeando hacer para cada uno de estos nuevos conjuntos de datos imputados y juntar los resultados de estos modelos. Esto funciona muy bien para reducir el sesgo tanto en MCAR como en muchas configuraciones de MAR y a menudo resulta en estimaciones de error estándar más precisas.

He aquí un ejemplo de aplicación del proceso de Imputación Múltiple para predecir la temperatura en nuestro conjunto de datos de `linelist` utilizando una edad y un estado de fiebre (nuestro conjunto de datos modelo simplificado de arriba): 

```{r}
# imputing missing values for all variables in our model_dataset, and creating 10 new imputed datasets
multiple_imputation = mice(
  model_dataset,
  seed = 1,
  m = 10,
  print = FALSE) 

model_fit <- with(multiple_imputation, lm(temp ~ age_years + fever))

base::summary(mice::pool(model_fit))
```

En este caso, utilizamos el método de imputación por defecto de mice, que es el de Coincidencia de Medias Predictivas. A continuación, utilizamos estos conjuntos de datos imputados para estimar por separado y luego agrupar los resultados de las regresiones lineales simples en cada uno de estos conjuntos de datos. Hay muchos detalles que hemos pasado por alto y muchas configuraciones que puedes ajustar durante el proceso de Imputación Múltiple mientras utilizas el paquete **mice**. Por ejemplo, no siempre tendrá datos numéricos y podría necesitar utilizar otros métodos de imputación (puedes seguir utilizando el paquete mice para muchos otros tipos de datos y métodos). Pero, para un análisis más robusto cuando los datos faltantes son una preocupación significativa, la Imputación Múltiple es una buena solución que no siempre es mucho más trabajo que hacer un análisis de caso completo.





<!-- ======================================================= -->
## Recursos {#resources-13}

Viñeta sobre el [paquete naniar](https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html)

Galería de [visualizaciones de valores faltantes](https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html)

[Libro en línea](https://stefvanbuuren.name/fimd/) sobre imputación múltiple en R por el mantenedor del paquete **mice**
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/missing_data.Rmd-->


# Tasas estandarizadas {#standardised-rates}

Esta página te mostrará dos formas de estandarizar un resultado, como las hospitalizaciones o la mortalidad, por características como la edad y el sexo.

* Uso del paquete **dsr**
* Uso del paquete **PHEindicatormethods**

Comenzamos demostrando ampliamente los procesos de preparación/limpieza/unión de datos, ya que esto es común cuando se combinan datos de población de múltiples países, datos de población estándar, defunciones, etc.

## Resumen {#overview-1}  

Hay dos formas principales de estandarizar: la estandarización directa y la indirecta. Supongamos que queremos estandarizar la tasa de mortalidad por edad y sexo para el país A y el país B, y comparar las tasas estandarizadas entre estos países.

* Para la estandarización directa, tendrás que conocer el número de la población de riesgo y el número de defunciones para cada estrato de edad y sexo, para el país A y el país B. Un estrato en nuestro ejemplo podría ser el de las mujeres entre 15-44 años.
* Para la estandarización indirecta, sólo es necesario conocer el número total de defunciones y la estructura por edad y sexo de cada país. Por tanto, esta opción es factible si no se dispone de tasas de mortalidad específicas por edad y sexo o de cifras de población. La estandarización indirecta es, además, preferible en caso de números pequeños por estrato, ya que las estimaciones en la estandarización directa estarían influenciadas por una variación sustancial del muestreo.

<!-- ======================================================= -->
## Preparación {#preparation-12}

Para mostrar cómo se realiza la estandarización, utilizaremos recuentos ficticios de la población y de las defunciones del país A y del país B, por edad (en categorías de 5 años) y por sexo (femenino, masculino). Para que los datos estén listos para su uso, realizaremos los siguientes pasos de preparación:

1.  Cargar paquetes
2.  Cargar datos
3.  Unir los datos de población y mortalidad de los dos países
4.  Pivotar largo para que haya una fila por estrato de edad y sexo
5.  Limpiar la población de referencia (población estándar mundial) y unirla a los datos del país

En tu caso, los datos pueden tener un formato diferente. Tal vez esos datos sean por provincia, ciudad u otra zona de captación. Puede que tengas una fila para cada defunció e información sobre la edad y el sexo de cada una (o de una proporción significativa) de estas defunciones. En este caso, consulta las páginas sobre [Agrupar de datos](#grouping-data), [Pivotar de datos](#pivoting-data) y [Tablas descriptivas](#descriptive-tables) para crear unos datos con recuentos de eventos y población por estrato de edad y sexo.

También necesitamos una población de referencia, la población estándar. Para los fines de este ejercicio utilizaremos la `world_standard_population_by_sex`. La población estándar mundial se basa en las poblaciones de 46 países y se elaboró en 1960. Hay muchas poblaciones "estándar"; por ejemplo, el sitio web [del NHS de Escocia](https://www.opendata.nhs.scot/dataset/standard-populations) ofrece bastante información sobre la población estándar europea, la población estándar mundial y la población estándar de Escocia.

<!-- ======================================================= -->
### Cargar paquetes {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R. 

```{r}
pacman::p_load(
     rio,                 # import/export data
     here,                # locate files
     tidyverse,           # data management and visualization
     stringr,             # cleaning characters and strings
     frailtypack,         # needed for dsr, for frailty models
     dsr,                 # standardise rates
     PHEindicatormethods) # alternative for rate standardisation
```


<span style="color: orange;">***PRECAUCIÓN:*** Si tienes una versión más reciente de R, el paquete **dsr** no puede descargarse directamente de CRAN. Sin embargo, todavía está disponible en el archivo CRAN. Puedes instalar y utilizar éste.</span>

Para los que no son usuarios de Mac:

```{r, eval=F} 
packageurl <- "https://cran.r-project.org/src/contrib/Archive/dsr/dsr_0.2.2.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
```

```{r, eval=FALSE}
# Other solution that may work
require(devtools)
devtools::install_version("dsr", version="0.2.2", repos="http:/cran.us.r.project.org")
```

Para los usuarios de Mac: 

```{r, eval=FALSE}
require(devtools)
devtools::install_version("dsr", version="0.2.2", repos="https://mac.R-project.org")
```




### Cargar los datos de población {.unnumbered}  

Consulta la página de [descargando el manual y los datos](#download-handbook-and-data) para obtener instrucciones sobre cómo descargar todos los datos de ejemplo del manual. Puedes importar los datos de la página de estandarización directamente a R desde nuestro repositorio de Github ejecutando los siguientes comandos `import()`:

```{r, eval=F}
# import demographics for country A directly from Github
A_demo <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/country_demographics.csv")

# import deaths for country A directly from Github
A_deaths <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/deaths_countryA.csv")

# import demographics for country B directly from Github
B_demo <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/country_demographics_2.csv")

# import deaths for country B directly from Github
B_deaths <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/deaths_countryB.csv")

# import demographics for country B directly from Github
standard_pop_data <- import("https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/standardization/world_standard_population_by_sex.csv")

```


En primer lugar, cargamos los datos demográficos (recuentos de hombres y mujeres por categoría de edad de 5 años) de los dos países que vamos a comparar, "Country A" y "Country B".

```{r, echo=F}
# Country A
A_demo <- rio::import(here::here("data", "standardization", "country_demographics.csv")) %>% 
     mutate(Country = "A") %>% 
     select(Country, everything()) %>% # re-arrange
     mutate(age_cat5 = str_replace_all(age_cat5, "\\+", "")) # remove + symbols
```

```{r, eval=F}
# Country A
A_demo <- import("country_demographics.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(A_demo, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


```{r, echo=F}
# Country B
B_demo <- rio::import(here::here("data", "standardization", "country_demographics_2.csv")) %>% 
     mutate(Country = "B") %>% 
     select(Country, everything()) # re-arrange
```

```{r, eval=F}
# Country B
B_demo <- import("country_demographics_2.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(B_demo, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```





### Cargar datos de defunciones {.unnumbered}  

Convenientemente, también tenemos los recuentos de las defunciones durante el período de interés, por edad y sexo. Los recuentos de cada país están en un archivo separado, que se muestra a continuación.

```{r, echo=F}
A_males <- c(224, 257, 251, 245, 334, 245, 154, 189, 334, 342, 565, 432, 543, 432, 245, 543, 234, 354) # for males of country A
B_males <- c(34, 37, 51, 145, 434, 120, 100, 143, 307, 354, 463, 639, 706, 232, 275, 543, 234, 274) # for males of country B
A_females <- c(194, 254, 232, 214, 316, 224, 163, 167, 354, 354, 463, 574, 493, 295, 175, 380, 177, 392) # for females of country A
B_females <- c(54, 24, 32, 154, 276, 254, 123, 164, 254, 354, 453, 654, 435, 354, 165, 432, 287, 395) # for females of country B

age_cat5 <- c("0-4", "5-9", "10-14", "15-19", "20-24", "25-29",  "30-34", "35-39", "40-44",
                                                                                "45-49", "50-54", "55-59",
                                                                                "60-64", "65-69", "70-74",
                                                                                "75-79", "80-84", "85")
A_deaths <- data.frame(Country = "A", AgeCat = age_cat5, Male = A_males, Female = A_females)
B_deaths <- data.frame(Country = "B", AgeCat = age_cat5, Male = B_males, Female = B_females)
```

Defunciones en Country A
```{r message=FALSE, echo=F}
DT::datatable(A_deaths, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Defunciones en Country B

```{r message=FALSE, echo=F}
DT::datatable(B_deaths, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


```{r, echo=F}
rio::export(A_deaths, here::here("data", "standardization", "deaths_countryA.csv"))
rio::export(B_deaths, here::here("data", "standardization", "deaths_countryB.csv"))
```



### Poblaciones y defunciones limpias {.unnumbered}  

Necesitamos unir y transformar estos datos de la siguiente manera:

* Combinar las poblaciones de los países en un solo conjunto de datos y hacer un pivote "largo" para que cada estrato de edad y sexo sea una fila
* Combinar los recuentos de defunciones por país en un solo conjunto de datos y hacer un pivote "largo" para que cada estrato de edad y sexo sea una fila
* Unir las defunciones a las poblaciones

En primer lugar, combinamos los datos de las poblaciones de los países, los pivotamos "largo" y realizamos una pequeña limpieza. Para más detalles, consulta la página sobre [pivotar datos](#pivoting-data).

```{r}
pop_countries <- A_demo %>%  # begin with country A dataset
     bind_rows(B_demo) %>%        # bind rows, because cols are identically named
     pivot_longer(                       # pivot longer
          cols = c(m, f),                   # columns to combine into one
          names_to = "Sex",                 # name for new column containing the category ("m" or "f") 
          values_to = "Population") %>%     # name for new column containing the numeric values pivoted
     mutate(Sex = recode(Sex,            # re-code values for clarity
          "m" = "Male",
          "f" = "Female"))
```

Los datos de población combinados tienen ahora este aspecto (clica para ver los países A y B):

```{r message=FALSE, echo=F}
DT::datatable(pop_countries, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Y ahora realizamos operaciones similares en los dos conjuntos de datos de defunciones.

```{r}
deaths_countries <- A_deaths %>%    # begin with country A deaths dataset
     bind_rows(B_deaths) %>%        # bind rows with B dataset, because cols are identically named
     pivot_longer(                  # pivot longer
          cols = c(Male, Female),        # column to transform into one
          names_to = "Sex",              # name for new column containing the category ("m" or "f") 
          values_to = "Deaths") %>%      # name for new column containing the numeric values pivoted
     rename(age_cat5 = AgeCat)      # rename for clarity
```

Los datos de las defunciones tienen ahora este aspecto, y contienen datos de ambos países:

```{r message=FALSE, echo=F}
DT::datatable(deaths_countries, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Ahora unimos los datos de defunciones y población basándonos en las columnas comunes  `Country`, `age_cat5`, y `Sex`. Esto añade la columna `Deaths`.

```{r}
country_data <- pop_countries %>% 
     left_join(deaths_countries, by = c("Country", "age_cat5", "Sex"))
```

Ahora podemos clasificar `Sex`, `age_cat5`, y `Country` como factores y establecer el orden de los niveles utilizando la función `fct_relevel()` del paquete **forcats**, como se describe en la página sobre [Factores](#factors). Ten en cuenta que la clasificación de los niveles de los factores no cambia visiblemente los datos, pero el comando `arrange()` los ordena por Country, age category, y sex.

```{r, warning=F, message=F}
country_data <- country_data %>% 
  mutate(
    Country = fct_relevel(Country, "A", "B"),
      
    Sex = fct_relevel(Sex, "Male", "Female"),
        
    age_cat5 = fct_relevel(
      age_cat5,
      "0-4", "5-9", "10-14", "15-19",
      "20-24", "25-29",  "30-34", "35-39",
      "40-44", "45-49", "50-54", "55-59",
      "60-64", "65-69", "70-74",
      "75-79", "80-84", "85")) %>% 
          
  arrange(Country, age_cat5, Sex)

```

```{r message=FALSE, echo=F}
DT::datatable(country_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<span style="color: orange;">***PRECAUCIÓN:*** Si tienes pocas defunciones por estrato, considera la posibilidad de utilizar categorías de 10 o 15 años, en lugar de categorías de 5 años para la edad.</span>




### Carga de la población de referencia {.unnumbered}  

Por último, para la estandarización directa, importamos la población de referencia (la "población estándar" mundial por sexo)

```{r, echo=F}
# Reference population
standard_pop_data <- rio::import(here::here("data", "standardization", "world_standard_population_by_sex.csv")) %>% 
     rename(age_cat5 = AgeGroup)
```

```{r, eval=F}
# Reference population
standard_pop_data <- import("world_standard_population_by_sex.csv")
```

```{r message=FALSE, echo=F}
DT::datatable(standard_pop_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
### Población de referencia limpia {.unnumbered}

Los valores de la categoría de edad en los dataframes `country_data` y `standard_pop_data` tendrán que estar alineados.

Actualmente, los valores de la columna age_cat5 del dataframe `standard_pop_data` contienen la palabra "years" y "plus", mientras que los del dataframe `country_data` no. Tendremos que hacer coincidir los valores de la categoría de edad. Usamos `str_replace_all()` del paquete **stringr**, como se describe en la página sobre [Caracteres y cadenas](#characters-and-strings), para reemplazar estos patrones sin espacio `""`.

Además, el paquete **dsr** espera que en la población estándar, la columna que contiene los recuentos se llame `"pop"`. Así que cambiaremos el nombre de esa columna.

```{r}
# Remove specific string from column values
standard_pop_clean <- standard_pop_data %>%
     mutate(
          age_cat5 = str_replace_all(age_cat5, "years", ""),   # remove "year"
          age_cat5 = str_replace_all(age_cat5, "plus", ""),    # remove "plus"
          age_cat5 = str_replace_all(age_cat5, " ", "")) %>%   # remove " " space
     
     rename(pop = WorldStandardPopulation)   # change col name to "pop", as this is expected by dsr package
```

<span style="color: orange;">***PRECAUCIÓN:*** Si intentas utilizar `str_replace_all()` para eliminar un *símbolo* de suma, no funcionará porque es un símbolo especial. "Escapa" de los símbolos especiales poniendo dos barras invertidas delante, como en `str_replace_call(columna, "\\+", "")`. </span>

### Crear un conjunto de datos con una población estándar {#standard_all .unnumbered}  

Por último, el paquete **PHEindicatormethods**, que se detalla [a continuación](#standard_phe), espera que las poblaciones estándar se unan a los recuentos de eventos y poblaciones del país. Por lo tanto, crearemos un conjunto de datos `all_data` con ese fin. 

```{r}
all_data <- left_join(country_data, standard_pop_clean, by=c("age_cat5", "Sex"))
```

Este conjunto de datos completo tiene el siguiente aspecto: 

```{r message=FALSE, echo=F}
DT::datatable(all_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## paquete **dsr** {#dsr-package}

A continuación mostramos el cálculo y la comparación de tasas estandarizadas directamente utilizando el paquete **dsr**. El paquete **dsr** permite calcular y comparar tasas estandarizadas directamente (¡no hay tasas estandarizadas indirectamente!).

En la sección de preparación de datos, hemos creado conjuntos de datos separados para los recuentos de países y la población estándar:

1.  el objeto `country_data`, que es una tabla de población con el número de habitantes y el número de defunciones por estrato por país
2.  el objeto `standard_pop_clean`, que contiene el número de población por estrato para nuestra población de referencia, la población estándar mundial

Utilizaremos estos conjuntos de datos separados para el enfoque **dsr**.  


<!-- ======================================================= -->
### Tasas estandarizadas {.unnumbered}

A continuación, calculamos las tasas por país directamente estandarizadas por edad y sexo. Utilizamos la función `dsr()`.

Cabe destacar que `dsr()` espera un dataframe para las poblaciones de los países y los recuentos de eventos (defunciones), *y un dataframe **separado** con la población de referencia*. También espera que en este conjunto de datos de la población de referencia el nombre de la columna de la unidad de tiempo sea "pop" (lo aseguramos en la sección de preparación de datos).

Hay muchos argumentos, como se anota en el código siguiente. En particular, el `event = ` se establece en la columna  `Deaths`, y `fu = ` ("seguimiento") con la columna  `Population`. Establecemos los subgrupos de comparación como la columna `Country` y estandarizamos en base a `age_cat5` y `Sex`. A estas dos últimas columnas no se les asigna un argumento con nombre concreto. Consulta `?dsr` para obtener más detalles.

```{r, warning=F, message=F}
# Calculate rates per country directly standardized for age and sex
mortality_rate <- dsr::dsr(
     data = country_data,  # specify object containing number of deaths per stratum
     event = Deaths,       # column containing number of deaths per stratum 
     fu = Population,      # column containing number of population per stratum
     subgroup = Country,   # units we would like to compare
     age_cat5,             # other columns - rates will be standardized by these
     Sex,
     refdata = standard_pop_clean, # reference population data frame, with column called pop
     method = "gamma",      # method to calculate 95% CI
     sig = 0.95,            # significance level
     mp = 100000,           # we want rates per 100.000 population
     decimals = 2)          # number of decimals)


# Print output as nice-looking HTML table
knitr::kable(mortality_rate) # show mortality rate before and after direct standardization
```

Arriba, vemos que mientras Country A tenía una tasa de mortalidad bruta más baja que Country B, ahora tiene una tasa estandarizada más alta después de la estandarización directa por edad y sexo.




<!-- ======================================================= -->
### Razón de tasas estandarizadas {.unnumbered}

```{r,warning=F, message=F}
# Calculate RR
mortality_rr <- dsr::dsrr(
     data = country_data, # specify object containing number of deaths per stratum
     event = Deaths,      # column containing number of deaths per stratum 
     fu = Population,     # column containing number of population per stratum
     subgroup = Country,  # units we would like to compare
     age_cat5,
     Sex,                 # characteristics to which we would like to standardize 
     refdata = standard_pop_clean, # reference population, with numbers in column called pop
     refgroup = "B",      # reference for comparison
     estimate = "ratio",  # type of estimate
     sig = 0.95,          # significance level
     mp = 100000,         # we want rates per 100.000 population
     decimals = 2)        # number of decimals

# Print table
knitr::kable(mortality_rr) 
```

La tasa de mortalidad estandarizada es 1,22 veces mayor en Country A en comparación con Country B (IC del 95%: 1,17-1,27).

<!-- ======================================================= -->
### Diferencia de tasas estandarizadas {.unnumbered}

```{r, warning=F, message=F}
# Calculate RD
mortality_rd <- dsr::dsrr(
     data = country_data,       # specify object containing number of deaths per stratum
     event = Deaths,            # column containing number of deaths per stratum 
     fu = Population,           # column containing number of population per stratum
     subgroup = Country,        # units we would like to compare
     age_cat5,                  # characteristics to which we would like to standardize
     Sex,                        
     refdata = standard_pop_clean, # reference population, with numbers in column called pop
     refgroup = "B",            # reference for comparison
     estimate = "difference",   # type of estimate
     sig = 0.95,                # significance level
     mp = 100000,               # we want rates per 100.000 population
     decimals = 2)              # number of decimals

# Print table
knitr::kable(mortality_rd) 
```

El país A tiene 4,24 defunciones adicionales por cada 100.000 habitantes (IC del 95%: 3,24-5,24) en comparación con el país A.







<!-- ======================================================= -->
## Paquete **PHEindicatormethods** {#standard_phe  }

Otra forma de calcular las tasas estandarizadas es con el paquete **PHEindicatormethods**. Este paquete permite calcular las tasas estandarizadas tanto directa como indirectamente. Mostraremos ambos métodos.

En esta sección se utilizará el dataframe `all_data` creado al final de la sección Preparación. Este dataframe incluye las poblaciones de los países, los eventos de defunciones y la población de referencia mundial estándar. Puedes verlo [aquí](#standard_all).



<!-- ======================================================= -->
### Tasas estandarizadas directamente {.unnumbered}

A continuación, primero agrupamos los datos por país y luego los pasamos a la función `phe_dsr()` para obtener directamente las tasas estandarizadas por país.

Cabe destacar que la población de referencia (estándar) puede proporcionarse como una **columna dentro del dataframe específico del país** o como un **vector separado**. Si se proporciona dentro del dataframe específico del país, hay que establecer `stdpoptype = "field"`. Si se proporciona como un vector, hay que establecer `stdpoptype = "vector"`. En este último caso, hay que asegurarse de que el orden de las filas por estratos es similar tanto en el dataframe específico del país como en la población de referencia, ya que los registros se emparejarán por posición. En nuestro ejemplo siguiente, proporcionamos la población de referencia como una columna dentro del dataframe específico del país.

Consulta la ayuda de `?phr_dsr` o los enlaces de la sección Referencias para obtener más información. 

```{r}
# Calculate rates per country directly standardized for age and sex
mortality_ds_rate_phe <- all_data %>%
     group_by(Country) %>%
     PHEindicatormethods::phe_dsr(
          x = Deaths,                 # column with observed number of events
          n = Population,             # column with non-standard pops for each stratum
          stdpop = pop,               # standard populations for each stratum
          stdpoptype = "field")       # either "vector" for a standalone vector or "field" meaning std populations are in the data  

# Print table
knitr::kable(mortality_ds_rate_phe)
```

<!-- ======================================================= -->
### Tasas estandarizadas indirectamente {#standard_indirect .unnumbered}

Para la estandarización indirecta, se necesita una población de referencia con el número de defunciones y el número de población por estrato. En este ejemplo, calcularemos las tasas del país A *utilizando el país B como población de referencia*, ya que la población de referencia de `standard_pop_clean` no incluye el número de defunciones por estrato.

A continuación, creamos primero la población de referencia del país B. Luego, pasamos los datos de mortalidad y población del país A, los combinamos con la población de referencia y los pasamos a la función `phe_isr()`, para obtener tasas estandarizadas indirectamente. Por supuesto, también se puede hacer a la inversa.

En nuestro ejemplo, la población de referencia se proporciona como un dataframe separado. En este caso, nos aseguraremos que los vectores `x = `, `n = `, `x_ref = ` y `n_ref = ` estén ordenados por los mismos valores de categoría de normalización (estrato) que los de nuestro dataframe específico del país, ya que los registros se emparejarán por posición.

Consulta la ayuda de `?phr_isr` o los enlaces de la sección Referencias para obtener más información. 

```{r}
# Create reference population
refpopCountryB <- country_data %>% 
  filter(Country == "B") 

# Calculate rates for country A indirectly standardized by age and sex
mortality_is_rate_phe_A <- country_data %>%
     filter(Country == "A") %>%
     PHEindicatormethods::phe_isr(
          x = Deaths,                 # column with observed number of events
          n = Population,             # column with non-standard pops for each stratum
          x_ref = refpopCountryB$Deaths,  # reference number of deaths for each stratum
          n_ref = refpopCountryB$Population)  # reference population for each stratum

# Print table
knitr::kable(mortality_is_rate_phe_A)
```

<!-- ======================================================= -->
## Recursos {#resources-14}

Si deseas ver otro ejemplo reproducible utilizando **dsr**, consulta [esta viñeta](https://mran.microsoft.com/snapshot/2020-02-12/web/packages/dsr/vignettes/dsr.html)

Si deseas ver otro ejemplo en el que se utilizan **los métodos de PHEindicator**, visita [este sitio web](https://mran.microsoft.com/snapshot/2018-10-22/web/packages/PHEindicatormethods/vignettes/IntroductiontoPHEindicatormethods.html)

Ver el archivo pdf de referencia de [**PHEindicatormethods**](https://cran.r-project.org/web/packages/PHEindicatormethods/PHEindicatormethods.pdf) 


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/standardization.Rmd-->


# Medias Móviles {#moving-averages}

```{r, out.width=c("100%"), echo=F}
knitr::include_graphics(here::here("images", "moving_avg_epicurve.png"))
```


Esta página cubrirá dos métodos para calcular y visualizar las medias móviles:  

1) Calcular con el paquete **slider**.  
2) Calcular *dentro* de un comando `ggplot()` con el paquete **tidyquant**. 


<!-- ======================================================= -->
## Preparación {#preparation-12}

### Cargar paquetes {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con library() de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.


```{r}
pacman::p_load(
  tidyverse,      # for data management and viz
  slider,         # for calculating moving averages
  tidyquant       # for calculating moving averages within ggplot
)
```


### Importar datos {.unnumbered}

Importamos los datos de casos de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica para descargar linelist "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa los datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - mira la página de [importación y exportación](#import-and-export) para más detalles).  


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

A continuación se muestran las primeras 50 filas del listado.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<!-- ======================================================= -->
## Calcular con **slider** {#calculate-with-slider}

**Utiliza este enfoque para calcular una media móvil en un dataframe antes de representarla**

El paquete **slider** proporciona varias funciones de "ventana deslizante" para calcular medias móviles, sumas acumulativas, regresiones móviles, etc. Trata un dataframe como un vector de filas, permitiendo la iteración por filas sobre un dataframe.

Estas son algunas de las funciones más comunes:

* `slide_dbl()` - itera a través de una columna *numérica* (de ahí "_dbl") realizando una operación mediante una ventana deslizante
  * `slide_sum()` - función abreviada de suma móvil para `slide_dbl()`
  * `slide_mean()` - función abreviada de media móvil para `slide_dbl()`
* `slide_index_dbl()` - aplica la ventana móvil en una columna numérica utilizando una columna separada para *indexar* la progresión de la ventana (útil si se rueda por fecha con algunas fechas ausentes)
  * `slide_index_sum()` - Función abreviada de suma móvil con indexación
  * `slide_index_mean()` - Función de acceso directo a la media móvil con indexación

El paquete **slider** tiene muchas otras funciones que se tratan en la sección de Recursos de esta página. Tocamos brevemente las más comunes.

**Argumentos básicos**

* `.x`, el primer argumento por defecto, es el vector sobre el que iterar y al que aplicar la función
* `.i = ` para las versiones de "índice" de las funciones de **deslizamiento** - proporciona una columna para "indexar" el rollo (véase la sección [siguiente](#roll_index))
* `.f =` , el segundo argumento por defecto, bien:
  * Una función, escrita sin paréntesis, como `mean`, o
  * Una fórmula, que se convertirá en una función. Por ejemplo `~ .x - mean(.x)` devolverá el resultado del valor actual menos la media del valor de la ventana

* Para más detalles, consulta este [material de referencia](https://davisvaughan.github.io/slider/reference/slide.html)



**Tamaño de la ventana**

Especifica el tamaño de la ventana utilizando los argumentos `.before`, `.after`, o ambos:

* `.before = ` - Proporcionar un número entero
* `.after = `- Proporcionar un número entero
* `.complete = `- Pon este valor a `TRUE` si sólo quieres que se realicen cálculos en ventanas completas

Por ejemplo, para conseguir una ventana de 7 días que incluya el valor actual y los seis anteriores, utiliza `.before = 6`. Para conseguir una ventana "centrada" proporciona el mismo número tanto a `.before = ` como a `.after = `.

Por defecto, `.complete = ` será FALSE por lo que si la ventana completa de filas no existe, las funciones utilizarán las filas disponibles para realizar el cálculo. Si se ajusta a `TRUE`, los cálculos sólo se realizan en ventanas completas.

**Ventana expansiva**

Para lograr operaciones *acumulativas*, establece el argumento `.before = ` en `Inf`. Esto realizará la operación sobre el valor actual y todos los que vengan antes. 





### Balancear por fecha  {#roll_index .unnumbered}  

El caso más probable de uso de un cálculo rotativo en epidemiología aplicada es examinar una medida a lo *largo del tiempo*. Por ejemplo, una medición continua de la incidencia de casos, basada en el recuento diario de casos.

Si tienes datos de series temporales limpios con valores para cada fecha, puede estar bien utilizar `slide_dbl()`, como se demuestra aquí en la página de [series temporales y detección de brotes](#timeseries_moving).

Sin embargo, en muchas circunstancias de epidemiología aplicada puede haber fechas ausentes en los datos, donde no hay eventos registrados. En estos casos, es mejor utilizar las versiones "index" de las funciones **slider**. 


### Datos indexados {.unnumbered}  

A continuación, mostramos un ejemplo utilizando `slide_index_dbl()` en la lista de casos. Digamos que nuestro objetivo es calcular una incidencia acumulada de 7 días - la suma de casos utilizando una ventana móvil de 7 días. Si estás buscando un ejemplo de media móvil, mira la sección de abajo sobre [balanceo agrupado](#roll_slider_group).

Para empezar, se crean los datos `daily_counts` para reflejar los recuentos diarios de casos de `linelist`, calculados con `count()` de **dplyr**.

```{r}
# make dataset of daily counts
daily_counts <- linelist %>% 
  count(date_hospitalisation, name = "new_cases")
```


Aquí está el dataframe `daily_counts` - hay `nrow(daily_counts)` filas, cada día está representado por una fila, pero especialmente al principio de la epidemia *algunos días no están presentes (no hubo casos admitidos en esos días)*.


```{r, echo=F}
DT::datatable(daily_counts, rownames = FALSE, options = list(pageLength = 6, scrollX=T) )
```



Es crucial reconocer que una función estándar de balanceo (como `slide_dbl()` utilizaría una ventana de 7 *filas*, no de 7 *días*. Por lo tanto, si hay fechas ausentes, ¡algunas ventanas se extenderán realmente más de 7 días naturales!

Se puede conseguir una ventana móvil "inteligente" con `slide_index_dbl()`. El "índex" significa que la función utiliza una *columna independiente* como "index" para la ventana móvil. La ventana no se basa simplemente en las filas del dataframe.

Si la columna índex es una fecha, tienes la posibilidad añadida de especificar la extensión de la ventana a `.before = ` y/o `.after = ` en unidades de `days()` o `months()` de **lubridate**. Si haces estas cosas, la función incluirá los días ausentes en las ventanas como si estuvieran allí (como valores `NA`).

Mostremos una comparación. A continuación, calculamos la incidencia móvil de casos de 7 días con ventanas regulares e indexadas.


```{r}
rolling <- daily_counts %>% 
  mutate(                                # create new columns
    # Using slide_dbl()
    ###################
    reg_7day = slide_dbl(
      new_cases,                         # calculate on new_cases
      .f = ~sum(.x, na.rm = T),          # function is sum() with missing values removed
      .before = 6),                      # window is the ROW and 6 prior ROWS
    
    # Using slide_index_dbl()
    #########################
    indexed_7day = slide_index_dbl(
        new_cases,                       # calculate on new_cases
        .i = date_hospitalisation,       # indexed with date_onset 
        .f = ~sum(.x, na.rm = TRUE),     # function is sum() with missing values removed
        .before = days(6))               # window is the DAY and 6 prior DAYS
    )

```

Fíjate cómo en la columna normal de las 7 primeras filas el recuento aumenta constantemente a *pesar de que las filas no tienen 7 días de diferencia*. La columna adyacente "indexada" tiene en cuenta estos días naturales ausentes, por lo que la suma de 7 días son mucho menores, al menos en este periodo de la epidemia en el que los casos están más alejados.

```{r, echo=F}
DT::datatable(rolling, rownames = FALSE, options = list(pageLength = 12, scrollX=T) )
```



Ahora puede trazar estos datos utilizando `ggplot()`: 

```{r}
ggplot(data = rolling)+
  geom_line(mapping = aes(x = date_hospitalisation, y = indexed_7day), size = 1)
```




<!-- ### Rolling by month {.unnumbered}   -->

<!-- If you want to calculate statistics by month (e.g. sum, mean, max) you can do this with **dplyr** as described in the [Grouping data] page. Simply create a "month" column, group the data, and run your calculations with `summarise()`.   -->

<!-- If however, you want to calculate rolling statistics over several months (e.g a 2-month rolling window), you can use the `slide_period()` function from **slider**.   -->

<!-- ```{r} -->
<!-- monthly_mean = function(data){ -->
<!--   summarise(data, mean = mean(new_cases, na.rm=T)) -->
<!-- } -->

<!-- linelist %>%  -->
<!--   count(date_hospitalisation, name = "new_cases") %>%  -->
<!--   mutate( -->
<!--     slide_period_dfr( -->
<!--       new_cases,  -->
<!--       .i = date_hospitalisation, -->
<!--       .period = "month", -->
<!--       .f = monthly_mean))  #~mean(.x, na.rm=T))) -->

<!--       #values_col = new_cases, -->
<!--       #index_col = date_hospitalisation -->
<!--     )) -->



<!-- ``` -->


### Balanceando por grupo {#roll_slider_group .unnumbered}  

Si agrupas los datos antes de utilizar una función **slider**, las ventanas deslizantes se aplicarán por grupo. Ten cuidado de organizar las filas en el orden deseado *por grupo*.

Cada vez que se inicia un nuevo grupo, la ventana deslizante se reinicia. Por lo tanto, un matiz a tener en cuenta es que si tus datos están agrupados *y* has establecido `.complete = TRUE`, tendrás valores vacíos en cada transición entre grupos. A medida que la función se desplaza hacia abajo a través de las filas, cada transición en la columna de agrupación reiniciará la acumulación del tamaño mínimo de la ventana para permitir un cálculo.

Consulta la página del manual sobre [Agrupar datos](#grouping-data) para obtener detalles sobre la agrupación de datos.

A continuación, contamos los casos del listado por fecha *y* por hospital. Luego ordenamos las filas en orden ascendente, primero ordenando por hospital y luego dentro de éste por fecha. A continuación establecemos `group_by()`. Entonces podemos crear nuestra nueva media móvil.


```{r}
grouped_roll <- linelist %>%

  count(hospital, date_hospitalisation, name = "new_cases") %>% 

  arrange(hospital, date_hospitalisation) %>%   # arrange rows by hospital and then by date
  
  group_by(hospital) %>%              # group by hospital 
    
  mutate(                             # rolling average  
    mean_7day_hosp = slide_index_dbl(
      .x = new_cases,                 # the count of cases per hospital-day
      .i = date_hospitalisation,      # index on date of admission
      .f = mean,                      # use mean()                   
      .before = days(6)               # use the day and the 6 days prior
      )
  )

```

Aquí está el nuevo conjunto de datos:  

```{r, echo=F}
DT::datatable(grouped_roll, rownames = FALSE, options = list(pageLength = 12, scrollX=T) )
```


Ahora podemos trazar las medias móviles, mostrando los datos por grupo especificando `~ hospital` a `facet_wrap()` en `ggplot()`. Para divertirnos, trazamos dos geometrías: una `geom_col()` que muestra los recuentos de casos diarios y una `geom_line()` que muestra la media móvil de 7 días.


```{r, warning=F, message=F}
ggplot(data = grouped_roll)+
  geom_col(                       # plot daly case counts as grey bars
    mapping = aes(
      x = date_hospitalisation,
      y = new_cases),
    fill = "grey",
    width = 1)+
  geom_line(                      # plot rolling average as line colored by hospital
    mapping = aes(
      x = date_hospitalisation,
      y = mean_7day_hosp,
      color = hospital),
    size = 1)+
  facet_wrap(~hospital, ncol = 2)+ # create mini-plots per hospital
  theme_classic()+                 # simplify background  
  theme(legend.position = "none")+ # remove legend
  labs(                            # add plot labels
    title = "7-day rolling average of daily case incidence",
    x = "Date of admission",
    y = "Case incidence")
```


<span style="color: red;">***PELIGRO:*** Si obtienes un error que dice *"slide() was deprecated in tsibble 0.9.0 and is now defunct. Please use slider::slide() instead."*, significa que la función `slide()` del paquete **tsibble** está enmascarando la función `slide()` del paquete **slider**. Soluciona esto especificando el paquete en el comando, como `slider::slide_dbl()`.</span>




<!-- You can group the data prior to using a **slider** function. For example, if you want to calculate the same 7-day rolling sum as above, but by hospital. above rolling mean delay from symptom onset to hospital admission (column `days_onset_hosp`).   -->

<!-- You can group the data by the month of symptom onset using **lubridate**'s `floor_date()` as described in the [Grouping data] page. Then, use `slide_index_dbl()` as before but set your window extent using `months()` (also from **lubridate**).  -->

<!-- f you want a rolling average by *months*, you can use **lubridate** to group the data by month, and then apply `slide_index_dbl()` as below shown for a three-month rolling average:   -->

<!-- ```{r} -->
<!-- months_delay <- linelist %>% -->
<!--   arrange(date_onset) %>%    # drop rows missing date of onset -->
<!--   group_by(hospital) %>%  -->
<!--   #group_by(month_onset = floor_date(date_onset, "month")) %>% # create and group by month of onset  -->
<!--   mutate( -->
<!--     delay_7d = slide_index_dbl( -->
<!--       days_onset_hosp,                  # calculate avg based on value in new_cases column -->
<!--       .i = date_onset,                 # index column is date_onset, so non-present dates are included in 7day window  -->
<!--       .f = ~mean(.x, na.rm = TRUE),     # function is mean() with missing values removed -->
<!--       .before = days(7)), -->

<!--     delay_month = slide_index_dbl( -->
<!--       days_onset_hosp,                  # calculate avg based on value in new_cases column -->
<!--       .i = date_onset,                 # index column is date_onset, so non-present dates are included in 7day window  -->
<!--       .f = ~mean(.x, na.rm = TRUE),     # function is mean() with missing values removed -->
<!--       .before = months(1)))               # window is the month and the prior month -->


<!-- # window is the month and the prior month -->

<!-- ``` -->

<!-- ```{r} -->
<!-- ggplot(data = months_delay, mapping = aes(x = month_onset))+ -->
<!--   geom_line(mapping = aes(y = )) -->

<!-- ``` -->






<!-- ======================================================= -->
## Calcular con **tidyquant** dentro de `ggplot()`{#calculate-with-tidyquant-within-ggplot}

El paquete **tidyquant** ofrece otro enfoque para calcular las medias móviles, esta vez *dentro* del comando `ggplot()`.

Bajo `linelist`, los datos se cuentan por fecha de inicio, y esto se traza como una línea descolorida (`alpha` < 1). Encima hay una línea creada con `geom_ma()` del paquete **tidyquant**, con una ventana de 7 días (`n = 7`) con el color y el grosor especificados.

Por defecto `geom_ma()` utiliza una media móvil simple (`ma_fun = "SMA"`), pero se pueden especificar otros tipos, como:

* "EMA" - media móvil exponencial (más peso a las observaciones recientes)
* "WMA" - media móvil ponderada (los `wts` se utilizan para ponderar las observaciones en la media móvil)
* Otros se pueden encontrar en la documentación de la función

```{r}
linelist %>% 
  count(date_onset) %>%                 # count cases per day
  drop_na(date_onset) %>%               # remove cases missing onset date
  ggplot(aes(x = date_onset, y = n))+   # start ggplot
    geom_line(                          # plot raw values
      size = 1,
      alpha = 0.2                       # semi-transparent line
      )+             
    tidyquant::geom_ma(                 # plot moving average
      n = 7,           
      size = 1,
      color = "blue")+ 
  theme_minimal()                       # simple background
```

Consulta esta [viñeta](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ04-charting-with-tidyquant.html) para obtener más detalles sobre las opciones disponibles en **tidyquant**.


<!-- ## Rolling regression  -->

<!-- ```{r} -->
<!-- a <- linelist %>% -->
<!--   separate(time_admission, into = c("hour", "minute"), sep = ":") %>%  -->
<!--   count(days_onset_hosp, hour) %>%  -->
<!--   mutate(reg_admit_hour = slide(., ~lm(days_onset_hosp ~ hour), .before = 3, .complete = T)) %>%  -->
<!--   mutate(coeff = reg_admit_hour[[1]]) -->

<!-- ggplot()+ -->
<!--   geom_point(aes(x = hour, y = days_onset_hosp)) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- linelist %>%  -->
<!--   mutate( -->

<!--   ) -->

<!-- ``` -->


<!-- ======================================================= -->
## Recursos {#resources-14}

Consulta la útil [viñeta en línea del paquete  **slider**](https://cran.r-project.org/web/packages/slider/vignettes/slider.html) 

La [página github del](https://github.com/DavisVaughan/slider) **slider**

Una [viñeta](https://davisvaughan.github.io/slider/articles/slider.html) **slider**

[viñeta tidyquant](https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ04-charting-with-tidyquant.html)

Si tu caso de uso requiere que te "saltes" los fines de semana e incluso los días festivos, puede que te guste el paquete **almanac**. 

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/moving_average.Rmd-->


# Series temporales y detección de brotes {#time-series-and-outbreak-detection}

<!-- ======================================================= -->


## Resumen {#overview-2}

Esta página muestra el uso de varios paquetes para el análisis de series temporales. Principalmente se basa en paquetes de la familia [**tidyverts**](https://tidyverts.org/), pero también utilizará el paquete de RECON [**trending**](https://github.com/reconhub/trending) para ajustar modelos más apropiados para la epidemiología de enfermedades infecciosas.

Ten en cuenta que en el siguiente ejemplo utilizamos unos datos del paquete **surveillance** sobre Campylobacter en Alemania (véase el capítulo [Descargando el manual y los datoss](#download-handbook-and-data) del manual para más detalles). Sin embargo, si deseas ejecutar el mismo código en unos datos con múltiples países u otros estratos, hay una plantilla de código de ejemplo para esto en el [repo de github de r4epis](https://github.com/R4EPI/epitsa).

Los temas que se tratan son:

1.  Datos de series temporales
2.  Análisis descriptivo
3.  Ajuste de regresiones
4.  Relación de dos series temporales
5.  Detección de brotes
6.  Series temporales interrumpidas


<!-- ======================================================= -->
## Preparación {#preparation-14}

### Paquetes {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También se pueden cargar paquetes con `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R. 

```{r load_packages}

pacman::p_load(rio,          # File import
               here,         # File locator
               tidyverse,    # data management + ggplot2 graphics
               tsibble,      # handle time series datasets 
               slider,       # for calculating moving averages
               imputeTS,     # for filling in missing values
               feasts,       # for time series decomposition and autocorrelation
               forecast,     # fit sin and cosin terms to data (note: must load after feasts)
               trending,     # fit and assess models 
               tmaptools,    # for getting geocoordinates (lon/lat) based on place names
               ecmwfr,       # for interacting with copernicus sateliate CDS API
               stars,        # for reading in .nc (climate data) files
               units,        # for defining units of measurement (climate data)
               yardstick,    # for looking at model accuracy
               surveillance  # for aberration detection
               )


``` 

### Cargar datos {.unnumbered}

Puedes descargar todos los datos utilizados en este manual mediante las instrucciones de la página de [descargando el manual y los datos](#download-handbook-and-data).

Los datos de ejemplo utilizado en esta sección son los recuentos semanales de casos de campylobacter notificados en Alemania entre 2001 y 2011. [Puedes clicar aquí para descargar este archivo de datos (.xlsx).](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/time_series/campylobacter_germany.xlsx)

Este conjunto de datos es una versión reducida de los datos disponibles en el paquete  [**surveillance**](https://cran.r-project.org/web/packages/surveillance/). (para más detalles, carga el paquete surveillance y consulta `?campyDE`)

Importa estos datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - Mira la página de [importación y exportación](#import-and-export) para más detalles).

```{r read_data_hide, echo=F}
# import the counts into R
counts <- rio::import(here::here("data", "time_series", "campylobacter_germany.xlsx"))
```

```{r read_data_show, eval=F}
# import the counts into R
counts <- rio::import("campylobacter_germany.xlsx")
```

A continuación se muestran las 10 primeras filas de los recuentos.

```{r inspect_data, message=FALSE, echo=F}
# display the counts data as a table
DT::datatable(head(counts, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Limpiar datos {.unnumbered}

El código siguiente se asegura de que la columna de la fecha tenga el formato adecuado. Para esta sección utilizaremos el paquete **tsibble** y la función `yearweek` se utilizará para crear una variable de semana de calendario. Hay otras maneras de hacer esto (ver la página de [Trabajar con fechas](#working-with-dates) para más detalles), sin embargo para las series temporales es mejor mantenerse dentro de un marco (**tsibble**).

```{r clean_data}

## ensure the date column is in the appropriate format
counts$date <- as.Date(counts$date)

## create a calendar week variable 
## fitting ISO definitons of weeks starting on a monday
counts <- counts %>% 
     mutate(epiweek = yearweek(date, week_start = 1))

```

### Descargar datos climáticos {.unnumbered} 

En la sección de [relación de dos series temporales](#relation-of-two-time-series), compararemos los recuentos de casos de campylobacter con los datos climáticos.

Los datos climáticos de cualquier parte del mundo pueden descargarse del satélite Copérnico de la UE. No se trata de mediciones exactas, sino que se basan en un modelo (similar a la interpolación), pero la ventaja es la cobertura horaria global, así como las previsiones.

Puedes descargar cada uno de estos archivos de datos climáticos en la página [descargando el manual y los datos](#download-handbook-and-data).

Para propósitos de demostración aquí, mostraremos el código R para usar el paquete **ecmwfr** para extraer estos datos del almacén de datos climáticos de Copernicus. Es necesario crear una cuenta gratuita para que esto funcione. El sitio web del paquete tiene una [guía](https://github.com/bluegreen-labs/ecmwfr#use-copernicus-climate-data-store-cds) útil de cómo hacerlo. A continuación se muestra un código de ejemplo de cómo hacer esto, una vez que tienes las claves de la API adecuada. Tienes que sustituir las X de abajo por los ID de tu cuenta. Tendrás que descargar un año de datos a la vez, de lo contrario el servidor se queda sin tiempo.

Si no estás seguro de las coordenadas de un lugar del que quieres descargar datos, puedes utilizar el paquete **tmaptools** para obtener las coordenadas de OpenStreetMaps. Una opción alternativa es el paquete [**photon**](https://github.com/rCarto/photon), aunque todavía no se ha publicado en CRAN; lo bueno de **photon** es que proporciona más datos contextuales para cuando hay varias coincidencias en la búsqueda.

```{r weather_data, eval = FALSE}

## retrieve location coordinates
coords <- geocode_OSM("Germany", geometry = "point")

## pull together long/lats in format for ERA-5 querying (bounding box) 
## (as just want a single point can repeat coords)
request_coords <- str_glue_data(coords$coords, "{y}/{x}/{y}/{x}")


## Pulling data modelled from copernicus satellite (ERA-5 reanalysis)
## https://cds.climate.copernicus.eu/cdsapp#!/software/app-era5-explorer?tab=app
## https://github.com/bluegreen-labs/ecmwfr

## set up key for weather data 
wf_set_key(user = "XXXXX",
           key = "XXXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXX",
           service = "cds") 

## run for each year of interest (otherwise server times out)
for (i in 2002:2011) {
  
  ## pull together a query 
  ## see here for how to do: https://bluegreen-labs.github.io/ecmwfr/articles/cds_vignette.html#the-request-syntax
  ## change request to a list using addin button above (python to list)
  ## Target is the name of the output file!!
  request <- request <- list(
    product_type = "reanalysis",
    format = "netcdf",
    variable = c("2m_temperature", "total_precipitation"),
    year = c(i),
    month = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12"),
    day = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12",
            "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24",
            "25", "26", "27", "28", "29", "30", "31"),
    time = c("00:00", "01:00", "02:00", "03:00", "04:00", "05:00", "06:00", "07:00",
             "08:00", "09:00", "10:00", "11:00", "12:00", "13:00", "14:00", "15:00",
             "16:00", "17:00", "18:00", "19:00", "20:00", "21:00", "22:00", "23:00"),
    area = request_coords,
    dataset_short_name = "reanalysis-era5-single-levels",
    target = paste0("germany_weather", i, ".nc")
  )
  
  ## download the file and store it in the current working directory
  file <- wf_request(user     = "XXXXX",  # user ID (for authentication)
                     request  = request,  # the request
                     transfer = TRUE,     # download the file
                     path     = here::here("data", "Weather")) ## path to save the data
  }

```

### Cargar datos climáticos {.unnumbered}

Tanto si has descargado los datos climáticos a través de nuestro manual, como si has utilizado el código anterior, ahora deberías tener 10 años de archivos de datos climáticos ".nc" almacenados en la misma carpeta de tu ordenador.

Utiliza el siguiente código para importar estos archivos en R con el paquete **stars**.

```{r read_climate, warning = FALSE, message = FALSE}

## define path to weather folder 
file_paths <- list.files(
  here::here("data", "time_series", "weather"), # replace with your own file path 
  full.names = TRUE)

## only keep those with the current name of interest 
file_paths <- file_paths[str_detect(file_paths, "germany")]

## read in all the files as a stars object 
data <- stars::read_stars(file_paths)
```

Una vez importados estos archivos como datos del objeto, los convertiremos en un dataframe.

```{r}
## change to a data frame 
temp_data <- as_tibble(data) %>% 
  ## add in variables and correct units
  mutate(
    ## create an calendar week variable 
    epiweek = tsibble::yearweek(time), 
    ## create a date variable (start of calendar week)
    date = as.Date(epiweek),
    ## change temperature from kelvin to celsius
    t2m = set_units(t2m, celsius), 
    ## change precipitation from metres to millimetres 
    tp  = set_units(tp, mm)) %>% 
  ## group by week (keep the date too though)
  group_by(epiweek, date) %>% 
  ## get the average per week
  summarise(t2m = as.numeric(mean(t2m)), 
            tp = as.numeric(mean(tp)))

```




<!-- ======================================================= -->
## Datos de series temporales {#time-series-data}

Existen varios paquetes para estructurar y manejar los datos de las series temporales. Como ya hemos dicho, nos centraremos en la familia de paquetes **tidyverts** y, por tanto, utilizaremos el paquete **tsibble** para definir nuestro objeto de serie temporal. Tener unos datos definidos como objeto de serie temporal significa que es mucho más fácil estructurar nuestro análisis.

Para ello utilizamos la función `tsibble()` y especificamos el "índex", es decir, la variable que especifica la unidad de tiempo de interés. En nuestro caso se trata de la variable `epiweek`.

Si tuviéramos unos datos con recuentos semanales por provincia, por ejemplo, también podríamos especificar la variable de agrupación utilizando el argumento `key = `. Esto nos permitiría hacer un análisis para cada grupo.


```{r ts_object}

## define time series object 
counts <- tsibble(counts, index = epiweek)

```

Si observamos el tipo `class(counts)`, veremos que, además de ser un dataframe ordenado ("tbl_df", "tbl", "data.frame"), tiene las propiedades adicionales de un dataframe de series temporales ("tbl_ts").

Se puede echar un vistazo rápido a los datos utilizando **ggplot2**. En el gráfico vemos que hay un claro patrón estacional y que no hay pérdidas. Sin embargo, parece haber un problema con la notificación al principio de cada año; los casos descienden en la última semana del año y luego aumentan en la primera semana del año siguiente.

```{r basic_plot}

## plot a line graph of cases by week
ggplot(counts, aes(x = epiweek, y = case)) + 
     geom_line()

```


<span style="color: red;">***PELIGRO:*** La mayoría de los conjuntos de datos no están tan limpios como este ejemplo. Tendrás que comprobar si hay duplicados y faltas como se indica a continuación.</span>

<!-- ======================================================= -->
### Duplicados {.unnumbered}

**tsibble** no permite observaciones duplicadas. Así que cada fila deberá ser única, o única dentro del grupo (variable `key`). El paquete tiene algunas funciones que ayudan a identificar los duplicados. Entre ellas se encuentran `are_duplicated()`, que proporciona un vector TRUE/FALSE para saber si la fila es un duplicado, y `duplicates()`, que proporciona un dataframe de las filas duplicadas.

Consulta la página sobre [De-duplicación](#de-duplication) para obtener más detalles sobre cómo seleccionar las filas que desees.

```{r duplicates, eval = FALSE}

## get a vector of TRUE/FALSE whether rows are duplicates
are_duplicated(counts, index = epiweek) 

## get a data frame of any duplicated rows 
duplicates(counts, index = epiweek) 

```

<!-- ======================================================= -->
### Valores faltantes {.unnumbered}

En nuestra breve inspección anterior hemos visto que no hay faltas, pero también hemos visto que parece haber un problema de retraso en la notificación en torno al año nuevo. Una forma de abordar este problema podría ser establecer estos valores como faltantes y luego imputar los valores. La forma más sencilla de imputación de series temporales consiste en trazar una línea recta entre el último valor no faltante y el siguiente valor no faltante. Para ello, utilizaremos la función `na_interpolation()` del paquete **imputeTS**.

Consulta la página de [datos faltantes](#missing-data) para conocer otras opciones de imputación.

Otra alternativa sería calcular una media móvil para intentar suavizar estos aparentes problemas de información (véase la siguiente sección y la página sobre [medias móviles](#moving-averages).

```{r missings}

## create a variable with missings instead of weeks with reporting issues
counts <- counts %>% 
     mutate(case_miss = if_else(
          ## if epiweek contains 52, 53, 1 or 2
          str_detect(epiweek, "W51|W52|W53|W01|W02"), 
          ## then set to missing 
          NA_real_, 
          ## otherwise keep the value in case
          case
     ))

## alternatively interpolate missings by linear trend 
## between two nearest adjacent points
counts <- counts %>% 
  mutate(case_int = imputeTS::na_interpolation(case_miss)
         )

## to check what values have been imputed compared to the original
ggplot_na_imputations(counts$case_miss, counts$case_int) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()

```




<!-- ======================================================= -->
## Análisis descriptivo {#descriptive-analysis}



<!-- ======================================================= -->
### Medias móviles {#timeseries_moving .unnumbered}


Si los datos tienen mucho ruido (los recuentos suben y bajan), puede ser útil calcular una media móvil. En el ejemplo siguiente, para cada semana se calcula la media de casos de las cuatro semanas anteriores. Esto suaviza los datos para hacerlos más interpretables. En nuestro caso esto no aporta mucho, así que nos quedaremos con los datos interpolados para el análisis posterior. Véase la página de [medias móviles](#moving-averages) para más detalles.

```{r moving_averages}

## create a moving average variable (deals with missings)
counts <- counts %>% 
     ## create the ma_4w variable 
     ## slide over each row of the case variable
     mutate(ma_4wk = slider::slide_dbl(case, 
                               ## for each row calculate the name
                               ~ mean(.x, na.rm = TRUE),
                               ## use the four previous weeks
                               .before = 4))

## make a quick visualisation of the difference 
ggplot(counts, aes(x = epiweek)) + 
     geom_line(aes(y = case)) + 
     geom_line(aes(y = ma_4wk), colour = "red")

```


<!-- ======================================================= -->
### Periodicidad {.unnumbered}

A continuación definimos una función personalizada para crear un periodograma. Consulta la página [Escribir funciones](#writing-functions-1) para obtener información sobre cómo escribir funciones en R.

En primer lugar, se define la función. Sus argumentos incluyen unos datos con las columnas `counts`, `start_week = ` que es la primera semana de los datos, un número para indicar cuántos períodos por año (por ejemplo, 52, 12) y, por último, el estilo de salida (véanse los detalles en el código siguiente).


```{r periodogram}
## Function arguments
#####################
## x is a dataset
## counts is variable with count data or rates within x 
## start_week is the first week in your dataset
## period is how many units in a year 
## output is whether you want return spectral periodogram or the peak weeks
  ## "periodogram" or "weeks"

# Define function
periodogram <- function(x, 
                        counts, 
                        start_week = c(2002, 1), 
                        period = 52, 
                        output = "weeks") {
  

    ## make sure is not a tsibble, filter to project and only keep columns of interest
    prepare_data <- dplyr::as_tibble(x)
    
    # prepare_data <- prepare_data[prepare_data[[strata]] == j, ]
    prepare_data <- dplyr::select(prepare_data, {{counts}})
    
    ## create an intermediate "zoo" time series to be able to use with spec.pgram
    zoo_cases <- zoo::zooreg(prepare_data, 
                             start = start_week, frequency = period)
    
    ## get a spectral periodogram not using fast fourier transform 
    periodo <- spec.pgram(zoo_cases, fast = FALSE, plot = FALSE)
    
    ## return the peak weeks 
    periodo_weeks <- 1 / periodo$freq[order(-periodo$spec)] * period
    
    if (output == "weeks") {
      periodo_weeks
    } else {
      periodo
    }
    
}

## get spectral periodogram for extracting weeks with the highest frequencies 
## (checking of seasonality) 
periodo <- periodogram(counts, 
                       case_int, 
                       start_week = c(2002, 1),
                       output = "periodogram")

## pull spectrum and frequence in to a dataframe for plotting
periodo <- data.frame(periodo$freq, periodo$spec)

## plot a periodogram showing the most frequently occuring periodicity 
ggplot(data = periodo, 
                aes(x = 1/(periodo.freq/52),  y = log(periodo.spec))) + 
  geom_line() + 
  labs(x = "Period (Weeks)", y = "Log(density)")


## get a vector weeks in ascending order 
peak_weeks <- periodogram(counts, 
                          case_int, 
                          start_week = c(2002, 1), 
                          output = "weeks")

```

<span style="color: black;">***NOTA:***  Es posible utilizar las semanas anteriores para añadirlas a los términos del seno y del coseno, sin embargo, utilizaremos una función para generar estos términos (véase la sección de regresión más adelante) </span>

<!-- ======================================================= -->
### Descomposición {.unnumbered}

La descomposición clásica se utiliza para desglosar una serie temporal en varias partes, que en conjunto conforman el patrón que se observa. Estas diferentes partes son:

* La tendencia-ciclo (la dirección a largo plazo de los datos)
* La estacionalidad (patrones repetitivos)
* El azar (lo que queda después de quitar la tendencia y la estacionalidad)


```{r decomposition, warning=F, message=F}

## decompose the counts dataset 
counts %>% 
  # using an additive classical decomposition model
  model(classical_decomposition(case_int, type = "additive")) %>% 
  ## extract the important information from the model
  components() %>% 
  ## generate a plot 
  autoplot()

```

<!-- ======================================================= -->
### Autocorrelación {.unnumbered}

La autocorrelación informa de la relación entre los recuentos de cada semana y las semanas anteriores (denominadas retrasos o retardos).

Utilizando la función `ACF()`, podemos producir un gráfico que nos muestre un número de líneas para la relación en diferentes retrasos. Cuando el retardo es 0 (x = 0), esta línea sería siempre 1, ya que muestra la relación entre una observación y ella misma (no se muestra aquí). La primera línea mostrada aquí (x = 1) muestra la relación entre cada observación y la observación anterior (retardo de 1), la segunda muestra la relación entre cada observación y la observación anterior (retardo de 2) y así sucesivamente hasta el retardo de 52 que muestra la relación entre cada observación y la observación de 1 año (52 semanas antes).

El uso de la función `PACF()` (para la autocorrelación parcial) muestra el mismo tipo de relación pero ajustada para todas las demás semanas intermedias. Esto es menos informativo para determinar la periodicidad.


```{r autocorrelation}

## using the counts dataset
counts %>% 
  ## calculate autocorrelation using a full years worth of lags
  ACF(case_int, lag_max = 52) %>% 
  ## show a plot
  autoplot()

## using the counts data set 
counts %>% 
  ## calculate the partial autocorrelation using a full years worth of lags
  PACF(case_int, lag_max = 52) %>% 
  ## show a plot
  autoplot()

```

Puedes probar formalmente la hipótesis nula de independencia en una serie temporal (es decir, que no está autocorrelacionada) utilizando la prueba de Ljung-Box (en el paquete **stats**). Un valor-p significativo sugiere que hay autocorrelación en los datos.

```{r ljung_box}

## test for independance 
Box.test(counts$case_int, type = "Ljung-Box")

```


<!-- ======================================================= -->
## Ajuste de regresiones {#fitting-regressions}

Es posible ajustar un gran número de regresiones diferentes a una serie temporal, sin embargo, aquí mostraremos cómo ajustar una regresión binomial negativa, ya que suele ser la más apropiada para los datos de recuento en las enfermedades infecciosas.

<!-- ======================================================= -->
### Términos de Fourier {.unnumbered}

Los términos de Fourier son el equivalente a las curvas seno y coseno. La diferencia es que éstos se ajustan basándose en la búsqueda de la combinación de curvas más adecuada para explicar los datos.

Si sólo se ajusta un término de fourier, esto equivaldría a ajustar un seno y un coseno para el desfase más frecuente que se ve en el periodograma (en nuestro caso, 52 semanas). Utilizamos la función `fourier()` del paquete **forecast**.

En el código de abajo asignamos usando el `$`, ya que `fourier()` devuelve dos columnas (una para seno y otra para el coseno) y así se añaden al conjunto de datos como una lista, llamada "fourier" - pero esta lista se puede usar como una variable normal en la regresión.

```{r fourier}

## add in fourier terms using the epiweek and case_int variabless
counts$fourier <- select(counts, epiweek, case_int) %>% 
  fourier(K = 1)
```

<!-- ======================================================= -->
### Binomial negativa {.unnumbered}

Es posible ajustar las regresiones utilizando las funciones básicas de **stats** o **MASS** (por ejemplo, `lm()`, `glm()` y `glm.nb()`). Sin embargo, utilizaremos las del paquete **trending**, ya que esto permite calcular intervalos de confianza y predicción adecuados (que de otro modo no están disponibles). La sintaxis es la misma, y se especifica una variable de resultado, luego una tilde (`~)` y luego se añaden las diversas variables de exposición de interés separadas por un signo más (`+`).

La otra diferencia es que primero definimos el modelo y luego lo ajustamos a los datos (`fit()`). Esto es útil porque permite comparar varios modelos diferentes con la misma sintaxis.

<span style="color: darkgreen;">***SUGERENCIA:*** Si deseas utilizar tasas, en lugar de recuentos, puedes incluir la variable de población como un término de compensación logarítmica, añadiendo `offset(log(population)`. Entonces tendría que establecerse que la población es 1, antes de usar `predict()` para producir una tasa. </span>

<span style="color: darkgreen;">***SUGERENCIA:*** Para ajustar modelos más complejos, como ARIMA o prophet, consulta el paquete [**fable**](https://fable.tidyverts.org/index.html)</span>

```{r nb_reg, warning = FALSE}

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the fourier terms to account for seasonality
    fourier)

## fit your model using the counts dataset
fitted_model <- trending::fit(model, data.frame(counts))

## calculate confidence intervals and prediction intervals 
observed <- predict(fitted_model, simulate_pi = FALSE)

## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "Red") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()


```

<!-- ======================================================= -->
### Residuos {.unnumbered}

Para ver si nuestro modelo se ajusta a los datos observados, tenemos que observar los residuos. Los residuos son la diferencia entre los recuentos observados y los recuentos estimados a partir del modelo. Podríamos calcularlo simplemente utilizando `case_int - estimate`, pero la función `residuals()` lo extrae directamente de la regresión por nosotros.

Lo que vemos a continuación es que no estamos explicando toda la variación que podríamos con el modelo. Es posible que debamos ajustar más términos de Fourier y abordar la amplitud. Sin embargo, para este ejemplo lo dejaremos como está. Los gráficos muestran que nuestro modelo es peor en los picos y en los valles (cuando los recuentos son los más altos y los más bajos) y que es más probable que subestime los recuentos observados.

```{r, warning=F, message=F}

## calculate the residuals 
observed <- observed %>% 
  mutate(resid = residuals(fitted_model$fitted_model, type = "response"))

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
observed %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() + 
  labs(x = "epiweek", y = "Residuals")

## is there autocorelation in the residuals (is there a pattern to the error?)  
observed %>% 
  as_tsibble(index = epiweek) %>% 
  ACF(resid, lag_max = 52) %>% 
  autoplot()

## are residuals normally distributed (are under or over estimating?)  
observed %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count") 
  
## compare observed counts to their residuals 
  ## should also be no pattern 
observed %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

## formally test autocorrelation of the residuals
## H0 is that residuals are from a white-noise series (i.e. random)
## test for independence 
## if p value significant then non-random
Box.test(observed$resid, type = "Ljung-Box")

```

<!-- ======================================================= -->
## Relación de dos series temporales {#relation-of-two-time-series}

En este caso, analizamos el uso de los datos meteorológicos (concretamente la temperatura) para explicar los recuentos de casos de campylobacter.

<!-- ======================================================= -->
### Fusión de conjuntos de datos {.unnumbered}

Podemos unir nuestros conjuntos de datos utilizando la variable semana (epiweek). Para obtener más información sobre la fusión, consulta la página del manual sobre [unir datos](#joining-data)

```{r join}

## left join so that we only have the rows already existing in counts
## drop the date variable from temp_data (otherwise is duplicated)
counts <- left_join(counts, 
                    select(temp_data, -date),
                    by = "epiweek")

```

<!-- ======================================================= -->
### Análisis descriptivo {.unnumbered}

En primer lugar, traza los datos para ver si hay alguna relación evidente. El siguiente gráfico muestra que hay una clara relación en la estacionalidad de las dos variables, y que la temperatura puede alcanzar su punto máximo unas semanas antes que el número de casos. Para más información sobre pivotar datos, consulta la sección del manual sobre [pivotar datos](#pivoting-data). 

```{r basic_plot_bivar}

counts %>% 
  ## keep the variables we are interested 
  select(epiweek, case_int, t2m) %>% 
  ## change your data in to long format
  pivot_longer(
    ## use epiweek as your key
    !epiweek,
    ## move column names to the new "measure" column
    names_to = "measure", 
    ## move cell values to the new "values" column
    values_to = "value") %>% 
  ## create a plot with the dataset above
  ## plot epiweek on the x axis and values (counts/celsius) on the y 
  ggplot(aes(x = epiweek, y = value)) + 
    ## create a separate plot for temperate and case counts 
    ## let them set their own y-axes
    facet_grid(measure ~ ., scales = "free_y") +
    ## plot both as a line
    geom_line()

```

<!-- ======================================================= -->
### Retrasos y correlación cruzada {.unnumbered}

Para comprobar formalmente qué semanas están más relacionadas entre los casos y la temperatura. Podemos utilizar la función de correlación cruzada (`CCF()`) del paquete de **feats**. También se podría visualizar (en lugar de utilizar `arrange`) utilizando la función `autoplot()`.

```{r cross_correlation, warning=FALSE}

counts %>% 
  ## calculate cross-correlation between interpolated counts and temperature
  CCF(case_int, t2m,
      ## set the maximum lag to be 52 weeks
      lag_max = 52, 
      ## return the correlation coefficient 
      type = "correlation") %>% 
  ## arange in decending order of the correlation coefficient 
  ## show the most associated lags
  arrange(-ccf) %>% 
  ## only show the top ten 
  slice_head(n = 10)

```

Vemos que un desfase de 4 semanas es el más correlacionado, por lo que creamos una variable de temperatura retardada para incluirla en nuestra regresión.

<span style="color: red;">***PELIGRO:*** Ten en cuenta que las primeras cuatro semanas de nuestros datos en la variable de temperatura retardada faltan (`NA`) - ya que no hay cuatro semanas anteriores para obtener datos. Para utilizar este conjunto de datos con la función  `predict()` de **trending**, necesitamos utilizar el argumento `simulate_pi = FALSE` dentro de `predict()` más abajo. Si quisiéramos utilizar la opción de simulación, entonces tenemos que eliminar estas pérdidas y almacenarlas como un nuevo conjunto de datos añadiendo `drop_na(t2m_lag4)` al fragmento de código que aparece a continuación.</span>  
 

```{r lag_tempvar}

counts <- counts %>% 
  ## create a new variable for temperature lagged by four weeks
  mutate(t2m_lag4 = lag(t2m, n = 4))

```


<!-- ======================================================= -->
### Binomial negativa con dos variables {.unnumbered}

Ajustamos una regresión binomial negativa como se hizo anteriormente. Esta vez añadimos la variable de temperatura con un retraso de cuatro semanas.

<span style="color: orange;">***PRECAUCIóN:*** Observa el uso de `simulate_pi = FALSE ` dentro del argumento `predict()`. Esto se debe a que el comportamiento por defecto de **trending** es utilizar el paquete **ciTools** para estimar un intervalo de predicción. Esto no funciona si hay recuentos `NA`, y también produce intervalos más granulares. Véase `?trending::predict.trending_model_fit` para más detalles. </span>  

```{r nb_reg_bivar, warning = FALSE}

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the fourier terms to account for seasonality
    fourier + 
    ## use the temperature lagged by four weeks 
    t2m_lag4
    )

## fit your model using the counts dataset
fitted_model <- trending::fit(model, data.frame(counts))

## calculate confidence intervals and prediction intervals 
observed <- predict(fitted_model, simulate_pi = FALSE)

```


Para investigar los términos individuales, podemos sacar la regresión binomial negativa original del formato de **trending** utilizando `get_model()` y pasarla a la función `tidy()` del paquete **broom** para recuperar las estimaciones exponenciadas y los intervalos de confianza asociados.

Lo que esto nos muestra es que la temperatura retardada, tras controlar la tendencia y la estacionalidad, es similar a los recuentos de casos (estimación ~ 1) y está significativamente asociada. Esto sugiere que podría ser una buena variable para predecir el número de casos futuros (ya que las previsiones climáticas están disponibles).

```{r results_nb_reg_bivar}

fitted_model %>% 
  ## extract original negative binomial regression
  get_model() %>% 
  ## get a tidy dataframe of results
  tidy(exponentiate = TRUE, 
       conf.int = TRUE)
```

Una rápida inspección visual del modelo muestra que se podría hacer un mejor trabajo de estimación de los recuentos de casos observados.

```{r plot_nb_reg_bivar, warning=F, message=F}

## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "Red") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()


```


#### Residuos {.unnumbered}

Volvemos a investigar los residuos para ver si nuestro modelo se ajusta a los datos observados. Los resultados y la interpretación aquí son similares a los de la regresión anterior, por lo que puede ser más factible quedarse con el modelo más simple sin temperatura.

```{r}

## calculate the residuals 
observed <- observed %>% 
  mutate(resid = case_int - estimate)

## are the residuals fairly constant over time (if not: outbreaks? change in practice?)
observed %>%
  ggplot(aes(x = epiweek, y = resid)) +
  geom_line() +
  geom_point() + 
  labs(x = "epiweek", y = "Residuals")

## is there autocorelation in the residuals (is there a pattern to the error?)  
observed %>% 
  as_tsibble(index = epiweek) %>% 
  ACF(resid, lag_max = 52) %>% 
  autoplot()

## are residuals normally distributed (are under or over estimating?)  
observed %>%
  ggplot(aes(x = resid)) +
  geom_histogram(binwidth = 100) +
  geom_rug() +
  labs(y = "count") 
  
## compare observed counts to their residuals 
  ## should also be no pattern 
observed %>%
  ggplot(aes(x = estimate, y = resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

## formally test autocorrelation of the residuals
## H0 is that residuals are from a white-noise series (i.e. random)
## test for independence 
## if p value significant then non-random
Box.test(observed$resid, type = "Ljung-Box")

```

<!-- ======================================================= -->
## Detección de brotes { #outbreak-detection}

Aquí mostraremos dos métodos (similares) de detección de brotes. El primero se basa en las secciones anteriores. Utilizamos el paquete **trending** para ajustar las regresiones a los años anteriores, y luego predecir lo que esperamos ver en el año siguiente. Si los recuentos observados están por encima de lo que esperamos, esto podría sugerir que hay un brote. El segundo método se basa en principios similares, pero utiliza el paquete **surveillance**, que tiene varios algoritmos diferentes para la detección de aberraciones.

<span style="color: orange;">***ATENCIÓN:*** Normalmente, estás interesado en el año actual (donde sólo se conocen los recuentos hasta la semana actual). Así que en este ejemplo pretendemos estar en la semana 39 de 2011.</span>

<!-- ======================================================= -->
### Paquete **trending** {.unnumbered}

Para este método definimos una línea base (que normalmente debería ser de unos 5 años de datos). Ajustamos una regresión a los datos de referencia y la utilizamos para predecir las estimaciones del año siguiente.

<!-- ======================================================= -->
#### Fecha de corte { -}

Es más fácil definir las fechas en un lugar y luego utilizarlas en el resto del código.

Aquí definimos una fecha de inicio (cuando comenzaron nuestras observaciones) y una fecha de corte (el final de nuestro período de referencia - y cuando comienza el período que queremos predecir). ~También definimos cuántas semanas hay en nuestro año de interés (el que vamos a predecir)~. También definimos cuántas semanas hay entre nuestra fecha límite de referencia y la fecha final para la que nos interesa predecir.


<span style="color: black;">***NOTA:*** En este ejemplo pretendemos estar actualmente a finales de septiembre de 2011 ("2011 W39").</span>  

```{r cut_off}

## define start date (when observations began)
start_date <- min(counts$epiweek)

## define a cut-off week (end of baseline, start of prediction period)
cut_off <- yearweek("2010-12-31")

## define the last date interested in (i.e. end of prediction)
end_date <- yearweek("2011-12-31")

## find how many weeks in period (year) of interest
num_weeks <- as.numeric(end_date - cut_off)

```


<!-- ======================================================= -->
#### Añadir filas {.unnumbered}

Para poder pronosticar en un formato tidyverse, necesitamos tener el número correcto de filas en nuestro conjunto de datos, es decir, una fila por cada semana hasta la `end_date` (fecha de corte) definida anteriormente. El código siguiente permite añadir estas filas por una variable de agrupación - por ejemplo, si tuviéramos varios países en unos datos, podríamos agrupar por país y luego añadir filas apropiadas para cada uno. La función `group_by_key()` de **tsibble** nos permite hacer esta agrupación y luego pasar los datos agrupados a las funciones de **dplyr**, `group_modify()` y `add_row()`. Luego especificamos la secuencia de semanas entre una después de la semana máxima disponible actualmente en los datos y la semana final.

```{r add_rows}

## add in missing weeks till end of year 
counts <- counts %>%
  ## group by the region
  group_by_key() %>%
  ## for each group add rows from the highest epiweek to the end of year
  group_modify(~add_row(.,
                        epiweek = seq(max(.$epiweek) + 1, 
                                      end_date,
                                      by = 1)))

```



<!-- ======================================================= -->
#### Términos de Fourier {.unnumbered}

Tenemos que redefinir nuestros términos de fourier, ya que queremos ajustarlos sólo a la fecha de referencia y luego predecir (extrapolar) esos términos para el año siguiente. Para ello tenemos que combinar dos listas de salida de la función `fourier()` juntas; la primera es para los datos de referencia, y la segunda predice para el año de interés (definiendo el argumento `h`).

*N.b.* para enlazar filas tenemos que usar `rbind()` (en lugar de `bind_rows` de tidyverse) ya que las columnas de fourier son una lista (por lo que no se nombran individualmente).

```{r fourier_terms_pred}


## define fourier terms (sincos) 
counts <- counts %>% 
  mutate(
    ## combine fourier terms for weeks prior to  and after 2010 cut-off date
    ## (nb. 2011 fourier terms are predicted)
    fourier = rbind(
      ## get fourier terms for previous years
      fourier(
        ## only keep the rows before 2011
        filter(counts, 
               epiweek <= cut_off), 
        ## include one set of sin cos terms 
        K = 1
        ), 
      ## predict the fourier terms for 2011 (using baseline data)
      fourier(
        ## only keep the rows before 2011
        filter(counts, 
               epiweek <= cut_off),
        ## include one set of sin cos terms 
        K = 1, 
        ## predict 52 weeks ahead
        h = num_weeks
        )
      )
    )

```

<!-- ======================================================= -->
#### Dividir los datos y ajustar la regresión {.unnumbered}

Ahora tenemos que dividir nuestro conjunto de datos en el período de referencia y el período de predicción. Esto se hace utilizando la función `group_split()` de **dplyr** después de `group_by()`, y creará una lista con dos dataframes, uno para antes de tu corte y otro para después.

A continuación, utilizamos la función `pluck()` del paquete **purrr** para extraer los datos del listado (lo que equivale a utilizar corchetes, por ejemplo, `dat[1]])`, y podemos ajustar nuestro modelo a los datos de referencia, y luego utilizar la función `predict()` para nuestros datos de interés después del corte.

Consulta la página sobre [Iteración, bucles y listas](#iteration-loops-and-lists) para saber más sobre **purrr**. 

<span style="color: orange;">***ATENCIÓN:*** Observa el uso de `simulate_pi = FALSE` dentro del argumento  `predict()`. Esto se debe a que el comportamiento por defecto de **trending** es utilizar el paquete **ciTools** para estimar un intervalo de predicción. Esto no funciona si hay recuentos NA, y también produce intervalos más granulares. Véase `?trending::predict.trending_model_fit` para más detalles. </span>  

```{r forecast_regression, warning = FALSE}
# split data for fitting and prediction
dat <- counts %>% 
  group_by(epiweek <= cut_off) %>%
  group_split()

## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the furier terms to account for seasonality
    fourier
)

# define which data to use for fitting and which for predicting
fitting_data <- pluck(dat, 2)
pred_data <- pluck(dat, 1) %>% 
  select(case_int, epiweek, fourier)

# fit model 
fitted_model <- trending::fit(model, data.frame(fitting_data))

# get confint and estimates for fitted data
observed <- fitted_model %>% 
  predict(simulate_pi = FALSE)

# forecast with data want to predict with 
forecasts <- fitted_model %>% 
  predict(data.frame(pred_data), simulate_pi = FALSE)

## combine baseline and predicted datasets
observed <- bind_rows(observed, forecasts)

```

Como anteriormente, podemos visualizar nuestro modelo con **ggplot**. Resaltamos las alertas con puntos rojos para los recuentos observados por encima del intervalo de predicción del 95%. Esta vez también añadimos una línea vertical para etiquetar cuándo empieza la predicción.


```{r forecast_plot}

## plot your regression 
ggplot(data = observed, aes(x = epiweek)) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate),
            col = "grey") + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add in a line for your observed case counts
  geom_line(aes(y = case_int), 
            col = "black") + 
  ## plot in points for the observed counts above expected
  geom_point(
    data = filter(observed, case_int > upper_pi), 
    aes(y = case_int), 
    colour = "red", 
    size = 2) + 
  ## add vertical line and label to show where forecasting started
  geom_vline(
           xintercept = as.Date(cut_off), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Forecast", 
           x = cut_off, 
           y = max(observed$upper_pi) - 250, 
           angle = 90, 
           vjust = 1
           ) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()
```



<!-- ======================================================= -->
#### Validación de la predicción {.unnumbered}

Más allá de la inspección de los residuos, es importante investigar lo bueno que es tu modelo para predecir casos en el futuro. Esto te da una idea de la fiabilidad de tus umbrales de alerta.

La forma tradicional de validar es ver lo bien que se puede predecir el último año anterior al actual (porque aún no se conocen los recuentos del "año actual"). Por ejemplo, en nuestro conjunto de datos, utilizaríamos los datos de 2002 a 2009 para predecir 2010, y luego veríamos la precisión de esas predicciones. A continuación, volveríamos a ajustar el modelo para incluir los datos de 2010 y los utilizaríamos para predecir los recuentos de 2011.

Como puede verse en la siguiente figura de *Hyndman et al* en ["Forecasting principles and practice"](https://otexts.com/fpp3/).

![](`r "https://otexts.com/fpp3/fpp_files/figure-html/traintest-1.png"`)
*figura reproducida con permiso de los autores*

La desventaja de esto es que no estás usando todos los datos disponibles, y no es el modelo final que estás usando para la predicción.

Una alternativa es utilizar un método llamado validación cruzada. En este caso, se pasan todos los datos disponibles para ajustar múltiples modelos de predicción a un año vista. Se utilizan cada vez más datos en cada modelo, como se ve en la siguiente figura del mismo [texto de *Hyndman et al*]([(](https://otexts.com/fpp3/)https://otexts.com/fpp3/). Por ejemplo, el primer modelo utiliza 2002 para predecir 2003, el segundo utiliza 2002 y 2003 para predecir 2004, y así sucesivamente.
![](`r "https://otexts.com/fpp2/fpp_files/figure-html/cv1-1.png"`)
*figura reproducida con permiso de los autores*

A continuación, utilizamos la función `map()` del paquete **purrr** para recorrer cada conjunto de datos. Luego, ponemos las estimaciones conjunto de datos y las fusionamos con los recuentos de casos originales, para utilizar el paquete **yardstick** para calcular las medidas de precisión. Calculamos cuatro medidas que incluyen: Error medio cuadrático (RMSE), Error medio absoluto (MAE), Error medio absoluto a escala (MASE), Error medio porcentual absoluto (MAPE).

<span style="color: orange;">***ATENCIÓN:*** Observa el uso de `simulate_pi = FALSE` dentro del argumento  `predict()`. Esto se debe a que el comportamiento por defecto de la **tendencia** es utilizar el paquete **ciTools** para estimar un intervalo de predicción. Esto no funciona si hay recuentos NA, y también produce intervalos más granulares. Véase `?trending::predict.trending_model_fit` para más detalles.</span>  

```{r cross_validation, warning = FALSE}

## Cross validation: predicting week(s) ahead based on sliding window

## expand your data by rolling over in 52 week windows (before + after) 
## to predict 52 week ahead
## (creates longer and longer chains of observations - keeps older data)

## define window want to roll over
roll_window <- 52

## define weeks ahead want to predict 
weeks_ahead <- 52

## create a data set of repeating, increasingly long data
## label each data set with a unique id
## only use cases before year of interest (i.e. 2011)
case_roll <- counts %>% 
  filter(epiweek < cut_off) %>% 
  ## only keep the week and case counts variables
  select(epiweek, case_int) %>% 
    ## drop the last x observations 
    ## depending on how many weeks ahead forecasting 
    ## (otherwise will be an actual forecast to "unknown")
    slice(1:(n() - weeks_ahead)) %>%
    as_tsibble(index = epiweek) %>% 
    ## roll over each week in x after windows to create grouping ID 
    ## depending on what rolling window specify
    stretch_tsibble(.init = roll_window, .step = 1) %>% 
  ## drop the first couple - as have no "before" cases
  filter(.id > roll_window)


## for each of the unique data sets run the code below
forecasts <- purrr::map(unique(case_roll$.id), 
                        function(i) {
  
  ## only keep the current fold being fit 
  mini_data <- filter(case_roll, .id == i) %>% 
    as_tibble()
  
  ## create an empty data set for forecasting on 
  forecast_data <- tibble(
    epiweek = seq(max(mini_data$epiweek) + 1,
                  max(mini_data$epiweek) + weeks_ahead,
                  by = 1),
    case_int = rep.int(NA, weeks_ahead),
    .id = rep.int(i, weeks_ahead)
  )
  
  ## add the forecast data to the original 
  mini_data <- bind_rows(mini_data, forecast_data)
  
  ## define the cut off based on latest non missing count data 
  cv_cut_off <- mini_data %>% 
    ## only keep non-missing rows
    drop_na(case_int) %>% 
    ## get the latest week
    summarise(max(epiweek)) %>% 
    ## extract so is not in a dataframe
    pull()
  
  ## make mini_data back in to a tsibble
  mini_data <- tsibble(mini_data, index = epiweek)
  
  ## define fourier terms (sincos) 
  mini_data <- mini_data %>% 
    mutate(
    ## combine fourier terms for weeks prior to  and after cut-off date
    fourier = rbind(
      ## get fourier terms for previous years
      forecast::fourier(
        ## only keep the rows before cut-off
        filter(mini_data, 
               epiweek <= cv_cut_off), 
        ## include one set of sin cos terms 
        K = 1
        ), 
      ## predict the fourier terms for following year (using baseline data)
      fourier(
        ## only keep the rows before cut-off
        filter(mini_data, 
               epiweek <= cv_cut_off),
        ## include one set of sin cos terms 
        K = 1, 
        ## predict 52 weeks ahead
        h = weeks_ahead
        )
      )
    )
  
  
  # split data for fitting and prediction
  dat <- mini_data %>% 
    group_by(epiweek <= cv_cut_off) %>%
    group_split()

  ## define the model you want to fit (negative binomial) 
  model <- glm_nb_model(
    ## set number of cases as outcome of interest
    case_int ~
      ## use epiweek to account for the trend
      epiweek +
      ## use the furier terms to account for seasonality
      fourier
  )

  # define which data to use for fitting and which for predicting
  fitting_data <- pluck(dat, 2)
  pred_data <- pluck(dat, 1)
  
  # fit model 
  fitted_model <- trending::fit(model, fitting_data)
  
  # forecast with data want to predict with 
  forecasts <- fitted_model %>% 
    predict(data.frame(pred_data), simulate_pi = FALSE) %>% 
    ## only keep the week and the forecast estimate
    select(epiweek, estimate)
    
  }
  )

## make the list in to a data frame with all the forecasts
forecasts <- bind_rows(forecasts)

## join the forecasts with the observed
forecasts <- left_join(forecasts, 
                       select(counts, epiweek, case_int),
                       by = "epiweek")

## using {yardstick} compute metrics
  ## RMSE: Root mean squared error
  ## MAE:  Mean absolute error	
  ## MASE: Mean absolute scaled error
  ## MAPE: Mean absolute percent error
model_metrics <- bind_rows(
  ## in your forcasted dataset compare the observed to the predicted
  rmse(forecasts, case_int, estimate), 
  mae( forecasts, case_int, estimate),
  mase(forecasts, case_int, estimate),
  mape(forecasts, case_int, estimate),
  ) %>% 
  ## only keep the metric type and its output
  select(Metric  = .metric, 
         Measure = .estimate) %>% 
  ## make in to wide format so can bind rows after
  pivot_wider(names_from = Metric, values_from = Measure)

## return model metrics 
model_metrics

```


<!-- ======================================================= -->
### paquete **surveillance** {.unnumbered}

En esta sección utilizamos el paquete **surveillance** para crear umbrales de alerta basados en algoritmos de detección de brotes. Hay varios métodos diferentes disponibles en el paquete, aunque aquí nos centraremos en dos opciones. Para más detalles, consulta estos documentos sobre la [aplicación](https://cran.r-project.org/web/packages/surveillance/vignettes/monitoringCounts.pdf) y la [teoría](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf) de los algoritmos utilizados.

La primera opción utiliza el método Farrington mejorado. Este método ajusta un `glm binomial` negativo (incluyendo la tendencia) y pondera a la baja los brotes pasados (valores atípicos) para crear un nivel de umbral.

La segunda opción utiliza el método `glrnb`. Esto también se ajusta a un `glm binomial` negativo, pero incluye la tendencia y los términos de fourier (por lo que se favorece aquí). La regresión se utiliza para calcular la "media de control" (~valores ajustados), y a continuación se utiliza un estadístico de relación de verosimilitud generalizada para evaluar si hay un cambio en la media de cada semana. Ten en cuenta que el umbral de cada semana tiene en cuenta las semanas anteriores, por lo que si hay un cambio sostenido se activará una alarma. (También hay que tener en cuenta que después de cada alarma el algoritmo se reinicia)

Para trabajar con el paquete **surveillance**, primero tenemos que definir un objeto "surveillance time series" (utilizando la función `sts()`) para que encaje en el marco de trabajo.

```{r surveillance_obj}

## define surveillance time series object
## nb. you can include a denominator with the population object (see ?sts)
counts_sts <- sts(observed = counts$case_int[!is.na(counts$case_int)],
                  start = c(
                    ## subset to only keep the year from start_date 
                    as.numeric(str_sub(start_date, 1, 4)), 
                    ## subset to only keep the week from start_date
                    as.numeric(str_sub(start_date, 7, 8))), 
                  ## define the type of data (in this case weekly)
                  freq = 52)

## define the week range that you want to include (ie. prediction period)
## nb. the sts object only counts observations without assigning a week or 
## year identifier to them - so we use our data to define the appropriate observations
weekrange <- cut_off - start_date

```

<!-- ======================================================= -->
#### Método Farrington {.unnumbered}

A continuación, definimos cada uno de nuestros parámetros para el método Farrington en una `list`. A continuación, ejecutamos el algoritmo utilizando `farringtonFlexible()` y luego podemos extraer el umbral de una alerta utilizando `farringtonmethod@upperbound` para incluirlo en nuestro conjunto de datos. También es posible extraer un TRUE/FALSE para cada semana si se activó una alerta (estaba por encima del umbral) utilizando `farringtonmethod@alarm`.

```{r farrington}

## define control
ctrl <- list(
  ## define what time period that want threshold for (i.e. 2011)
  range = which(counts_sts@epoch > weekrange),
  b = 9, ## how many years backwards for baseline
  w = 2, ## rolling window size in weeks
  weightsThreshold = 2.58, ## reweighting past outbreaks (improved noufaily method - original suggests 1)
  ## pastWeeksNotIncluded = 3, ## use all weeks available (noufaily suggests drop 26)
  trend = TRUE,
  pThresholdTrend = 1, ## 0.05 normally, however 1 is advised in the improved method (i.e. always keep)
  thresholdMethod = "nbPlugin",
  populationOffset = TRUE
  )

## apply farrington flexible method
farringtonmethod <- farringtonFlexible(counts_sts, ctrl)

## create a new variable in the original dataset called threshold
## containing the upper bound from farrington 
## nb. this is only for the weeks in 2011 (so need to subset rows)
counts[which(counts$epiweek >= cut_off & 
               !is.na(counts$case_int)),
              "threshold"] <- farringtonmethod@upperbound
```

A continuación, podemos visualizar los resultados en **ggplot** como se hizo anteriormente.

```{r plot_farrington, warning=F, message=F}

ggplot(counts, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in upper bound of aberration algorithm
  geom_line(aes(y = threshold, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic() + 
  ## remove title of legend 
  theme(legend.title = element_blank())

```

<!-- ======================================================= -->
#### Método GLRNB {.unnumbered}

Del mismo modo, para el método GLRNB definimos cada uno de nuestros parámetros en una `list`, luego ajustamos el algoritmo y extraemos los límites superiores.

<span style="color: orange;">***ATENCIÓN:*** Este método utiliza la "fuerza bruta" (similar al bootstrapping) para calcular los umbrales, por lo que puede llevar mucho tiempo.</span>

Consulta la [viñeta GLRNB](https://cran.r-project.org/web/packages/surveillance/vignettes/glrnb.pdf) para más detalles.

```{r glrnb, warning = FALSE, message = FALSE}

## define control options
ctrl <- list(
  ## define what time period that want threshold for (i.e. 2011)
  range = which(counts_sts@epoch > weekrange),
  mu0 = list(S = 1,    ## number of fourier terms (harmonics) to include
  trend = TRUE,   ## whether to include trend or not
  refit = FALSE), ## whether to refit model after each alarm
  ## cARL = threshold for GLR statistic (arbitrary)
     ## 3 ~ middle ground for minimising false positives
     ## 1 fits to the 99%PI of glm.nb - with changes after peaks (threshold lowered for alert)
   c.ARL = 2,
   # theta = log(1.5), ## equates to a 50% increase in cases in an outbreak
   ret = "cases"     ## return threshold upperbound as case counts
  )

## apply the glrnb method
glrnbmethod <- glrnb(counts_sts, control = ctrl, verbose = FALSE)

## create a new variable in the original dataset called threshold
## containing the upper bound from glrnb 
## nb. this is only for the weeks in 2011 (so need to subset rows)
counts[which(counts$epiweek >= cut_off & 
               !is.na(counts$case_int)),
              "threshold_glrnb"] <- glrnbmethod@upperbound

```

Visualiza los resultados como antes.

```{r plot_glrnb, message=F, warning=F}

ggplot(counts, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in upper bound of aberration algorithm
  geom_line(aes(y = threshold_glrnb, colour = "Alert threshold"), 
            linetype = "dashed", 
            size = 1.5) +
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Alert threshold" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic() + 
  ## remove title of legend 
  theme(legend.title = element_blank())

```

<!-- ======================================================= -->
## Series temporales interrumpidas { #interrupted-timeseries}

Las series temporales interrumpidas (también llamadas análisis de regresión segmentada o de intervención), se utilizan a menudo para evaluar el impacto de las vacunas en la incidencia de la enfermedad. Pero puede utilizarse para evaluar el impacto de una amplia gama de intervenciones o introducciones. Por ejemplo, cambios en los procedimientos hospitalarios o la introducción de una nueva cepa de enfermedad en una población. 

En este ejemplo, supondremos que se introdujo una nueva cepa de Campylobacter en Alemania a finales de 2008, y veremos si eso afecta al número de casos. Volveremos a utilizar la regresión binomial negativa. Esta vez, la regresión se dividirá en dos partes, una antes de la intervención (o introducción de la nueva cepa en este caso) y otra después (los períodos anterior y posterior). Esto nos permite calcular una tasa de incidencia comparando los dos periodos de tiempo. Explicar la ecuación podría aclararlo (si no es así, ignórala).

La regresión binomial negativa puede definirse como sigue:

$$\log(Y_t)= β_0 + β_1 \times t+ β_2 \times δ(t-t_0) + β_3\times(t-t_0 )^+ + log(pop_t) + e_t$$

Donde:
$(Y_t$) es el número de casos observados en el momento $(t$)
$(pop_t$) es el tamaño de la población en 100.000s en el momento $(t$) (no se utiliza aquí) 
$(t_0$) es el último año del preperíodo (incluyendo el tiempo de transición si lo hay) 
$(δ(x$) es la función indicadora (es 0 si x≤0 y 1 si x$>0) 
$((x)$^+$) es el operador de corte (es x si x$>0 y 0 en caso contrario) 
$(e_t$) denota el residuo 

Se pueden añadir los términos adicionales tendencia y estación según sea necesario.

$β_2 \times δ(t-t_0) + β_3\times(t-t_0 )^+$ es la parte lineal generalizada del periodo posterior y es cero en el periodo anterior. Esto significa que las estimaciones $β_2$ y $β_3$ son los efectos de la intervención.

Aquí tenemos que volver a calcular los términos de Fourier sin previsión, ya que utilizaremos todos los datos de que disponemos (es decir, a posteriori). Además, tenemos que calcular los términos adicionales necesarios para la regresión.

```{r define_terms_interrupted}

## add in fourier terms using the epiweek and case_int variabless
counts$fourier <- select(counts, epiweek, case_int) %>% 
  as_tsibble(index = epiweek) %>% 
  fourier(K = 1)

## define intervention week 
intervention_week <- yearweek("2008-12-31")

## define variables for regression 
counts <- counts %>% 
  mutate(
    ## corresponds to t in the formula
      ## count of weeks (could probably also just use straight epiweeks var)
    # linear = row_number(epiweek), 
    ## corresponds to delta(t-t0) in the formula
      ## pre or post intervention period
    intervention = as.numeric(epiweek >= intervention_week), 
    ## corresponds to (t-t0)^+ in the formula
      ## count of weeks post intervention
      ## (choose the larger number between 0 and whatever comes from calculation)
    time_post = pmax(0, epiweek - intervention_week + 1))

```

A continuación, utilizamos estos términos para ajustar una regresión binomial negativa y elaboramos una tabla con el porcentaje de cambio. Lo que muestra este ejemplo es que no hubo ningún cambio significativo.

<span style="color: orange;">***ATENCIÓN:*** Observa el uso de `simulate_pi = FALSE` dentro del argumento de `predict()`. Esto se debe a que el comportamiento por defecto de la **tendencia** es utilizar el paquete **ciTools** para estimar un intervalo de predicción. Esto no funciona si hay recuentos `NA`, y también produce intervalos más granulares. Véase `?trending::predict.trending_model_fit` para más detalles.</span>  

```{r interrupted_regression, warning = FALSE}


## define the model you want to fit (negative binomial) 
model <- glm_nb_model(
  ## set number of cases as outcome of interest
  case_int ~
    ## use epiweek to account for the trend
    epiweek +
    ## use the furier terms to account for seasonality
    fourier + 
    ## add in whether in the pre- or post-period 
    intervention + 
    ## add in the time post intervention 
    time_post
    )

## fit your model using the counts dataset
fitted_model <- trending::fit(model, counts)

## calculate confidence intervals and prediction intervals 
observed <- predict(fitted_model, simulate_pi = FALSE)



## show estimates and percentage change in a table
fitted_model %>% 
  ## extract original negative binomial regression
  get_model() %>% 
  ## get a tidy dataframe of results
  tidy(exponentiate = TRUE, 
       conf.int = TRUE) %>% 
  ## only keep the intervention value 
  filter(term == "intervention") %>% 
  ## change the IRR to percentage change for estimate and CIs 
  mutate(
    ## for each of the columns of interest - create a new column
    across(
      all_of(c("estimate", "conf.low", "conf.high")), 
      ## apply the formula to calculate percentage change
            .f = function(i) 100 * (i - 1), 
      ## add a suffix to new column names with "_perc"
      .names = "{.col}_perc")
    ) %>% 
  ## only keep (and rename) certain columns 
  select("IRR" = estimate, 
         "95%CI low" = conf.low, 
         "95%CI high" = conf.high,
         "Percentage change" = estimate_perc, 
         "95%CI low (perc)" = conf.low_perc, 
         "95%CI high (perc)" = conf.high_perc,
         "p-value" = p.value)
```

Como en el caso anterior, podemos visualizar los resultados de la regresión.

```{r plot_interrupted}

ggplot(observed, aes(x = epiweek)) + 
  ## add in observed case counts as a line
  geom_line(aes(y = case_int, colour = "Observed")) + 
  ## add in a line for the model estimate
  geom_line(aes(y = estimate, col = "Estimate")) + 
  ## add in a band for the prediction intervals 
  geom_ribbon(aes(ymin = lower_pi, 
                  ymax = upper_pi), 
              alpha = 0.25) + 
  ## add vertical line and label to show where forecasting started
  geom_vline(
           xintercept = as.Date(intervention_week), 
           linetype = "dashed") + 
  annotate(geom = "text", 
           label = "Intervention", 
           x = intervention_week, 
           y = max(observed$upper_pi), 
           angle = 90, 
           vjust = 1
           ) + 
  ## define colours
  scale_colour_manual(values = c("Observed" = "black", 
                                 "Estimate" = "red")) + 
  ## make a traditional plot (with black axes and white background)
  theme_classic()

```


<!-- ======================================================= -->
## Recursos {#resources-16}

[Forecasting: principles and practice. Libro de texto](https://otexts.com/fpp3/)

[Estudios de casos de análisis de series temporales de EPIET](https://github.com/EPIET/TimeSeriesAnalysis)

[Curso de Penn State](https://online.stat.psu.edu/stat510/lesson/1) 

[Manuscrito del paquete Surveillance](https://www.jstatsoft.org/article/view/v070i10)





```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/time_series.Rmd-->


# Modelización de epidemias {#epidemic-modeling}

<!-- ======================================================= -->
## Resumen {#overview-3}

Existe un conjunto creciente de herramientas para la modelización de epidemias que nos permite realizar análisis bastante complejos con un esfuerzo mínimo. En esta sección se ofrece una visión general de cómo utilizar estas herramientas para:

* estimar el número de reproducción efectivo R<sub>t</sub> y las estadísticas relacionadas, como el tiempo de duplicación

* elaborar proyecciones a corto plazo de la incidencia futura

*No* pretende ser una visión general de las metodologías y los métodos estadísticos en los que se basan estas herramientas, así que consulta la sección de Recursos para ver los enlaces a algunos documentos que cubren esto. Asegúrese de que conoce los métodos antes de utilizar estas herramientas, ya que así podrá interpretar con precisión sus resultados.

A continuación se muestra un ejemplo de uno de los resultados que produciremos en esta sección.

```{r out.width=c('100%', '100%'), fig.show='hold', echo=F, fig.width = 12, fig.height = 9, message=F, warning=F}

## install and load packages
pacman::p_load(tidyverse, EpiNow2, EpiEstim, here, incidence2, epicontacts, rio, projections)

## load linelist
linelist <- import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)

## ## estimate gamma generation time
## generation_time <- bootstrapped_dist_fit(
##   get_pairwise(epic, "date_infection"),
##   dist = "gamma",
##   max_value = 20,
##   bootstraps = 1
## )

## ## export for caching
## export(
##   generation_time,
##   here("data/cache/epidemic_models/generation_time.rds")
## )

## import cached generation time
generation_time <- import(here("data/cache/epidemic_models/generation_time.rds"))

## ## estimate incubation period
## incubation_period <- bootstrapped_dist_fit(
##   linelist$date_onset - linelist$date_infection,
##   dist = "lognormal",
##   max_value = 100,
##   bootstraps = 1
## )

## ## export for caching
## export(
##   incubation_period,
##   here("data/cache/epidemic_models/incubation_period.rds")
## )

## import cached incubation period
incubation_period <- import(here("data/cache/epidemic_models/incubation_period.rds"))

## get incidence from onset date
cases <- linelist %>%
  group_by(date = date_onset) %>%
  summarise(confirm = n())

## ## run epinow
## epinow_res <- epinow(
##   reported_cases = cases,
##   generation_time = generation_time,
##   delays = delay_opts(incubation_period),
##   target_folder = here("data/cache/epidemic_models"),
##   return_output = TRUE,
##   output = "samples",
##   verbose = TRUE,
##   stan = stan_opts(samples = 750, chains = 4),
##   horizon = 21
## )

## ## export for caching
## export(
##   epinow_res,
##   here("data/cache/epidemic_models/epinow_res.rds")
## )

## import cached epinow results
epinow_res <- import(here("data/cache/epidemic_models/epinow_res.rds"))

## plot summary figure
plot(epinow_res)

```

<!-- ======================================================= -->
## Preparación {#preparation-15}

Utilizaremos dos métodos y paquetes diferentes para la estimación de Rt, a saber, **EpiNow** y **EpiEstim**, así como el paquete **projections** para la previsión de la incidencia de casos.

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

	
```{r epidemic_models_packages, }
pacman::p_load(
   rio,          # File import
   here,         # File locator
   tidyverse,    # Data management + ggplot2 graphics
   epicontacts,  # Analysing transmission networks
   EpiNow2,      # Rt estimation
   EpiEstim,     # Rt estimation
   projections,  # Incidence projections
   incidence2,   # Handling incidence data
   epitrix,      # Useful epi functions
   distcrete     # Discrete delay distributions
)
```
	
Utilizaremos la lista de casos limpia para todos los análisis de esta sección. Si quieres seguir el proceso, [clica para descargar `linelist` "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Consulta la página de [descargando el manual y los datos](#download-handbook-and-data) para descargar todos los datos de ejemplo utilizados en este manual.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r eval=F}
# import the cleaned linelist
linelist <- import("linelist_cleaned.rds")
```


<!-- ======================================================= -->
## Estimación de R<sub>t</sub> {#estimating-rt}

### EpiNow2 vs. EpiEstim {.unnumbered}

El número de reproducción R es una medida de la transmisibilidad de una enfermedad y se define como el número esperado de casos secundarios por cada caso infectado. En una población totalmente susceptible, este valor representa el número básico de reproducción R0. Sin embargo, como el número de individuos susceptibles en una población cambia en el transcurso de un brote o pandemia, y como se aplican diversas medidas de respuesta, la medida de transmisibilidad más utilizada es el número de reproducción efectivo R<sub>t</sub>; éste se define como el número esperado de casos secundarios por caso infectado en un tiempo *t* determinado.

El paquete **EpiNow2** proporciona el marco más sofisticado para estimar R<sub>t</sub>. Tiene dos ventajas clave sobre el otro paquete comúnmente utilizado, **EpiEstim**:

* Tiene en cuenta los retrasos en la notificación y, por lo tanto, puede estimar la R<sub>t</sub> incluso cuando los datos recientes son incompletos.

* Estima la R<sub>t</sub> en función de las *fechas de infección* y no de las fechas de inicio de la notificación, lo que significa que el efecto de una intervención se reflejará inmediatamente en un cambio en la R<sub>t</sub>, en lugar de con un retraso.

Sin embargo, también tiene dos desventajas fundamentales:

* Requiere conocer la distribución del tiempo de generación (es decir, la distribución de los retrasos entre la infección de un caso primario y uno secundario), la distribución del periodo de incubación (es decir, la distribución de los retrasos entre la infección y el inicio de los síntomas) y cualquier otra distribución de los retrasos que sea relevante para sus datos (por ejemplo, si tiene fechas de notificación, necesita la distribución de los retrasos desde el inicio de los síntomas hasta la notificación). Aunque esto permitirá una estimación más precisa de R<sub>t</sub>, **EpiEstim** sólo requiere la distribución de intervalos en serie (es decir, la distribución de retrasos entre el inicio de los síntomas de un caso primario y uno secundario), que puede ser la única distribución disponible para usted.

* **EpiNow2** es significativamente más lento que **EpiEstim**, anecdóticamente por un factor de 100-1000. Por ejemplo, la estimación de R<sub>t</sub> para el brote de la muestra considerada en esta sección tarda unas cuatro horas (esto se ejecutó para un gran número de iteraciones para asegurar una alta precisión y probablemente podría reducirse si fuera necesario, sin embargo los puntos son que el algoritmo es lento en general). Esto puede ser inviable si se actualizan regularmente las estimaciones de R<sub>t</sub>.

Por tanto, el paquete que elijas utilizar dependerá de los datos, el tiempo y los recursos informáticos de que disponga.

### EpiNow2 {.unnumbered}

#### Estimación de las distribuciones de los retrasos {.unnumbered}

Las distribuciones de retraso necesarias para ejecutar **EpiNow2** dependen de los datos que tengas. Esencialmente, necesita poder describir el retraso desde la fecha de la infección hasta la fecha del evento que quieres usar para estimar R<sub>t</sub>. Si estás usando fechas de inicio, esto sería simplemente la distribución del periodo de incubación. Si se utilizan las fechas de notificación, se requiere el retraso desde la infección hasta la notificación. Como es poco probable que esta distribución se conozca directamente, **EpiNow2** permite encadenar varias distribuciones de retraso; en este caso, el retraso desde la infección hasta el inicio de los síntomas (por ejemplo, el periodo de incubación, que probablemente se conoce) y desde el inicio de los síntomas hasta la notificación (que a menudo se puede estimar a partir de los datos).

Como tenemos las fechas de inicio de todos nuestros casos en nuestro `linelist` de ejemplo, sólo necesitaremos la distribución del periodo de incubación para relacionar nuestros datos (por ejemplo, las fechas de inicio de los síntomas) con la fecha de la infección. Podemos estimar esta distribución a partir de los datos o utilizar valores de la literatura.

Una estimación bibliográfica del periodo de incubación del ébola (tomada de [este documento](https://www.nejm.org/doi/full/10.1056/nejmoa1411100)) con una media de 9,1, una desviación estándar de 7,3 y un valor máximo de 30 se especificaría como sigue:

```{r epidemic_models_incubation_literature, eval=F}
incubation_period_lit <- list(
  mean = log(9.1),
  mean_sd = log(0.1),
  sd = log(7.3),
  sd_sd = log(0.1),
  max = 30
)
```
Ten en cuenta que **EpiNow2** requiere que estas distribuciones de retardo se proporcionen en una escala **logarítmica**, de ahí la llamada `log` alrededor de cada valor (excepto el parámetro `max` que, confusamente, tiene que proporcionarse en una escala natural). Los parámetros `mean_sd` y `sd_sd` definen la desviación estándar de las estimaciones de la media y la desviación estándar. Como no se conocen en este caso, elegimos el valor bastante arbitrario de 0,1.

En este análisis, en cambio, estimamos la distribución del periodo de incubación a partir del propio listado utilizando la función `bootstrapped_dist_fit`, que ajustará una distribución lognormal a los retrasos observados entre la infección y el inicio en `linelist`.

```{r epidemic_models_incubation_estimate, eval=F}
## estimate incubation period
incubation_period <- bootstrapped_dist_fit(
  linelist$date_onset - linelist$date_infection,
  dist = "lognormal",
  max_value = 100,
  bootstraps = 1
)
```

La otra distribución que necesitamos es el tiempo de generación. Como tenemos datos sobre los tiempos de infección **y** los enlaces de transmisión, podemos estimar esta distribución a partir de `linelist` calculando el retraso entre los tiempos de infección de los pares infector-infectado. Para ello, utilizamos la práctica función `get_pairwise` del paquete **epicontacts**, que nos permite calcular las diferencias por pares de las propiedades de `linelist` entre los pares de transmisión. Primero creamos un objeto epicontacts (ver la página de [cadenas de transmisión](#transmission-chains) para más detalles):

```{r epidemic_models_epicontacts, eval=F}
## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

A continuación, ajustamos la diferencia de tiempos de infección entre pares de transmisión, calculada mediante `get_pairwise`, a una distribución gamma:

```{r epidemic_models_generation_estimate, eval=F}
## estimate gamma generation time
generation_time <- bootstrapped_dist_fit(
  get_pairwise(epic, "date_infection"),
  dist = "gamma",
  max_value = 20,
  bootstraps = 1
)
```

#### Ejecución de **EpiNow2** {.unnumbered}

Ahora sólo tenemos que calcular la incidencia diaria de `linelist`, lo que podemos hacer fácilmente con las funciones `group_by()` y `n()` de **dplyr**. Ten en cuenta que **EpiNow2** requiere que los nombres de las columnas sean  `date` y `confirm`.


```{r epidemic_models_cases, eval=F}
## get incidence from onset dates
cases <- linelist %>%
  group_by(date = date_onset) %>%
  summarise(confirm = n())
```

We can then estimate R<sub>t</sub> using the `epinow` function. Some notes on
the inputs:

* We can provide any number of 'chained' delay distributions to the `delays`
  argument; we would simply insert them alongside the `incubation_period` object
  within the `delay_opts` function.
* `return_output` ensures the output is returned within R and not just saved to
  a file.
* `verbose` specifies that we want a readout of the progress.
* `horizon` indicates how many days we want to project future incidence for.
* We pass additional options to the `stan` argument to specify how long
  we want to run the inference for. Increasing `samples` and `chains` will give
  you a more accurate estimate that better characterises uncertainty, however
  will take longer to run.

Podemos entonces estimar R<sub>t</sub> utilizando la función `epinow`. Algunas notas sobre
las entradas:

* Podemos proporcionar cualquier número de distribuciones de retraso "encadenadas" al argumento `delays`:
  simplemente las insertaríamos junto al objeto `incubation_period` dentro de la función `delay_opts`.  
* El objeto `return_output` asegura que la salida se devuelve dentro de R y no solo se guarda en un archivo.  
* `verbose` especifica que queremos una lectura del progreso.
* `horizonte` indica para cuántos días queremos proyectar la incidencia futura.
* Pasamos opciones adicionales al argumento `stan` para especificar durante cuánto tiempo
  queremos ejecutar la inferencia. Aumentando  `samples` y `chains`  obtendremos
  una estimación más precisa que caracteriza mejor la incertidumbre, sin embargo
  tardará más en ejecutarse.  
  

```{r epidemic_models_run_epinow, eval=F}
## run epinow
epinow_res <- epinow(
  reported_cases = cases,
  generation_time = generation_time,
  delays = delay_opts(incubation_period),
  return_output = TRUE,
  verbose = TRUE,
  horizon = 21,
  stan = stan_opts(samples = 750, chains = 4)
)
```

#### Análisis de los resultados {.unnumbered}

Una vez que el código ha terminado de ejecutarse, podemos trazar un resumen muy fácilmente, como se indica a continuación. Desplaza la imagen para ver la extensión completa.


```{r out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F }
## plot summary figure
plot(epinow_res)
```

También podemos consultar varias estadísticas resumidas:

```{r epidemic_models_epinow_summary,}
## summary table
epinow_res$summary
```

Para otros análisis y trazados personalizados, puedes acceder a las estimaciones diarias resumidas a través de `$estimates$summarised`. Convertiremos esto desde `data.table` por defecto a un `tibble` para facilitar su uso con **dplyr**.

```{r epidemic_models_to_tibble, eval=F}
## extract summary and convert to tibble
estimates <- as_tibble(epinow_res$estimates$summarised)
estimates
```

```{r epidemic_models_tibble_show,  echo = F}
## show outputs
estimates <- as_tibble(epinow_res$estimates$summarised)
DT::datatable(
  estimates,
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap'
)
```

A modo de ejemplo, hagamos un gráfico del tiempo de duplicación y R<sub>t</sub>. Sólo nos fijaremos en los primeros meses del brote, cuando R<sub>t</sub> es muy superior a uno, para evitar trazar tiempos de duplicación extremadamente altos.

Utilizamos la fórmula  `log(2)/growth_rate` para calcular el tiempo de duplicación a partir de la tasa de crecimiento estimada.

```{r epidemic_models_plot_epinow_cusotom, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## make wide df for median plotting
df_wide <- estimates %>%
  filter(
    variable %in% c("growth_rate", "R"),
    date < as.Date("2014-09-01")
  ) %>%
  ## convert growth rates to doubling times
  mutate(
    across(
      c(median, lower_90:upper_90),
      ~ case_when(
        variable == "growth_rate" ~ log(2)/.x,
        TRUE ~ .x
      )
    ),
    ## rename variable to reflect transformation
    variable = replace(variable, variable == "growth_rate", "doubling_time")
  )

## make long df for quantile plotting
df_long <- df_wide %>%
  ## here we match matching quantiles (e.g. lower_90 to upper_90)
  pivot_longer(
    lower_90:upper_90,
    names_to = c(".value", "quantile"),
    names_pattern = "(.+)_(.+)"
  )

## make plot
ggplot() +
  geom_ribbon(
    data = df_long,
    aes(x = date, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = date, y = median)
  ) +
  ## use label_parsed to allow subscript label
  facet_wrap(
    ~ variable,
    ncol = 1,
    scales = "free_y",
    labeller = as_labeller(c(R = "R[t]", doubling_time = "Doubling~time"), label_parsed),
    strip.position = 'left'
  ) +
  ## manually define quantile transparency
  scale_alpha_manual(
    values = c(`20` = 0.7, `50` = 0.4, `90` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = NULL,
    alpha = "Credibel\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.background = element_blank(),
    strip.placement = 'outside'
  )

```

<!-- ======================================================= -->
### EpiEstim {.unnumbered}

Para ejecutar **EpiEstim**, necesitamos proporcionar datos sobre la incidencia diaria y especificar el intervalo de serie (es decir, la distribución de los retrasos entre el inicio de los síntomas de los casos primarios y secundarios).

Los datos de incidencia pueden proporcionarse a **EpiEstim** como un vector, un dataframe o un objeto `incidence` del paquete **incidence** original. Incluso se puede distinguir entre infecciones importadas y adquiridas localmente; consulta la documentación en `?estimate_R` para más detalles.

Crearemos la entrada utilizando **incidence2**. Consulta la página sobre [curvas epidémicas](#epidemic-curves) para ver más ejemplos con el paquete **incidence2**. Dado que ha habido actualizaciones en el paquete **incidence2** que no se alinean completamente con la entrada esperada de `estimate_R()`, hay algunos pasos adicionales menores necesarios. El objeto `incidence` consiste en un tibble con fechas y sus respectivos recuentos de casos. Usamos `complete()` de **tidyr** para asegurarnos que se incluyen todas las fechas (incluso las que no tienen casos), y luego `rename()` las columnas para alinearlas con lo que espera `estimate_R()` en un paso posterior.  

```{r epidemic_models_epiestim_incidence,}
## get incidence from onset date
cases <- incidence2::incidence(linelist, date_index = date_onset) %>% # get case counts by day
  tidyr::complete(date_index = seq.Date(                              # ensure all dates are represented
    from = min(date_index, na.rm = T),
    to = max(date_index, na.rm=T),
    by = "day"),
    fill = list(count = 0)) %>%                                       # convert NA counts to 0
  rename(I = count,                                                   # rename to names expected by estimateR
         dates = date_index)
```

El paquete proporciona varias opciones para especificar el intervalo en serie, cuyos detalles se proporcionan en la documentación en `?estimate_R`. Aquí cubriremos dos de ellas.

#### Utilizando estimaciones de intervalos de serie de la literatura {.unnumbered}

Utilizando la opción `method = "parametric_si"`, podemos especificar manualmente la media y la desviación estándar del intervalo en serie en un objeto `config` creado con la función `make_config`. Utilizamos una media y una desviación estándar de 12,0 y 5,2, respectivamente, definidas en [este documento](https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-014-0196-0):

```{r epidemic_models_epiestim_config,}
## make config
config_lit <- make_config(
  mean_si = 12.0,
  std_si = 5.2
)
```

Entonces podemos estimar R<sub>t</sub> con la función `estimate_R`:

```{r epidemic_models_epiestim_lit,  warning = FALSE}
epiestim_res_lit <- estimate_R(
  incid = cases,
  method = "parametric_si",
  config = config_lit
)
```

y trazar un resumen de los resultados:

```{r epidemic_models_epiestim_lit_plot,  warning = FALSE}
plot(epiestim_res_lit)
```

#### Utilización de estimaciones de intervalos de serie a partir de los datos {.unnumbered}

Como tenemos datos sobre las fechas de inicio de los síntomas *y* los vínculos de transmisión, también podemos estimar el intervalo de serie a partir de `linelist` calculando el retraso entre las fechas de inicio de los pares infector-infectado. Como hicimos en la sección **EpiNow2**, utilizaremos la función `get_pairwise` del paquete **epicontacts**, que nos permite calcular las diferencias por pares de las propiedades de `linelist` entre los pares de transmisión. Primero creamos un objeto epicontacts (ver la página de [cadenas de transmisión](#transmission-chains) para más detalles):

```{r epidemic_models_epicontacts_epiestim, eval=F}
## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

A continuación, ajustamos la diferencia de fechas de inicio entre los pares de transmisión, calculada mediante `get_pairwise`, a una distribución gamma. Utilizamos el práctico `fit_disc_gamma` del paquete **epitrix** para este procedimiento de ajuste, ya que necesitamos una distribución *discreta*.

```{r epidemic_models_incubation_estimate_epiestim,  warning = FALSE}
## estimate gamma serial interval
serial_interval <- fit_disc_gamma(get_pairwise(epic, "date_onset"))
```

A continuación, pasamos esta información al objeto `config`, ejecutamos de nuevo **EpiEstim** y trazamos los resultados:

```{r epidemic_models_epiestim_emp,  warning = FALSE}
## make config
config_emp <- make_config(
  mean_si = serial_interval$mu,
  std_si = serial_interval$sd
)

## run epiestim
epiestim_res_emp <- estimate_R(
  incid = cases,
  method = "parametric_si",
  config = config_emp
)

## plot outputs
plot(epiestim_res_emp)
```

#### Especificación de las ventanas de tiempo de estimación {.unnumbered}

Estas opciones por defecto proporcionarán una estimación deslizante semanal y podrían actuar como una advertencia de que está estimando R<sub>t</sub> demasiado pronto en el brote para una estimación precisa. Puedes cambiar esto estableciendo una fecha de inicio posterior para la estimación, como se muestra a continuación. Lamentablemente, **EpiEstim** sólo proporciona una forma muy tosca de especificar estos tiempos de estimación, ya que tiene que proporcionar un vector de **enteros** que se refieran a las fechas de inicio y fin de cada ventana temporal.

```{r epidemic_models_epiestim_config_late,}

## define a vector of dates starting on June 1st
start_dates <- seq.Date(
  as.Date("2014-06-01"),
  max(cases$dates) - 7,
  by = 1
) %>%
  ## subtract the starting date to convert to numeric
  `-`(min(cases$dates)) %>%
  ## convert to integer
  as.integer()

## add six days for a one week sliding window
end_dates <- start_dates + 6
  
## make config
config_partial <- make_config(
  mean_si = 12.0,
  std_si = 5.2,
  t_start = start_dates,
  t_end = end_dates
)
```
Ahora volvemos a ejecutar **EpiEstim** y podemos ver que las estimaciones sólo comienzan a partir de junio:

```{r epidemic_models_epiestim_config_late_run,}

## run epiestim
epiestim_res_partial <- estimate_R(
  incid = cases,
  method = "parametric_si",
  config = config_partial
)

## plot outputs
plot(epiestim_res_partial)

```

#### Análisis de los resultados {.unnumbered}

Se puede acceder a los principales resultados a través de `$R`. Como ejemplo, crearemos un gráfico de R<sub>t</sub> y una medida de "potencial de transmisión" dada por el producto de R<sub>t</sub> y el número de casos notificados en ese día; esto representa el número esperado de casos en la siguiente generación de infección.

```{r epidemic_models_epiestim_plot_full, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## make wide dataframe for median
df_wide <- epiestim_res_lit$R %>%
  rename_all(clean_labels) %>%
  rename(
    lower_95_r = quantile_0_025_r,
    lower_90_r = quantile_0_05_r,
    lower_50_r = quantile_0_25_r,
    upper_50_r = quantile_0_75_r,
    upper_90_r = quantile_0_95_r,
    upper_95_r = quantile_0_975_r,
    ) %>%
  mutate(
    ## extract the median date from t_start and t_end
    dates = epiestim_res_emp$dates[round(map2_dbl(t_start, t_end, median))],
    var = "R[t]"
  ) %>%
  ## merge in daily incidence data
  left_join(cases, "dates") %>%
  ## calculate risk across all r estimates
  mutate(
    across(
      lower_95_r:upper_95_r,
      ~ .x*I,
      .names = "{str_replace(.col, '_r', '_risk')}"
    )
  ) %>%
  ## seperate r estimates and risk estimates
  pivot_longer(
    contains("median"),
    names_to = c(".value", "variable"),
    names_pattern = "(.+)_(.+)"
  ) %>%
  ## assign factor levels
  mutate(variable = factor(variable, c("risk", "r")))

## make long dataframe from quantiles
df_long <- df_wide %>%
  select(-variable, -median) %>%
  ## seperate r/risk estimates and quantile levels
  pivot_longer(
    contains(c("lower", "upper")),
    names_to = c(".value", "quantile", "variable"),
    names_pattern = "(.+)_(.+)_(.+)"
  ) %>%
  mutate(variable = factor(variable, c("risk", "r")))

## make plot
ggplot() +
  geom_ribbon(
    data = df_long,
    aes(x = dates, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = dates, y = median),
    alpha = 0.2
  ) +
  ## use label_parsed to allow subscript label
  facet_wrap(
    ~ variable,
    ncol = 1,
    scales = "free_y",
    labeller = as_labeller(c(r = "R[t]", risk = "Transmission~potential"), label_parsed),
    strip.position = 'left'
  ) +
  ## manually define quantile transparency
  scale_alpha_manual(
    values = c(`50` = 0.7, `90` = 0.4, `95` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = NULL,
    alpha = "Credible\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    strip.background = element_blank(),
    strip.placement = 'outside'
  )
  
```

<!-- ======================================================= -->
## Proyección de la incidencia {#projecting-incidence}

### EpiNow2 {.unnumbered}

Además de la estimación de R<sub>t</sub>, **EpiNow2** también admite la previsión de R<sub>t</sub> y las proyecciones del número de casos mediante la integración con el paquete **EpiSoon** por debajo. Todo lo que hay que hacer es especificar el argumento de `horizon` en la llamada a la función `epinow`, indicando cuántos días se quiere proyectar en el futuro; véase **EpiNow2** en la sección "Estimación de R<sub>t</sub>" para obtener detalles sobre cómo poner en marcha **EpiNow2**. En esta sección, sólo vamos a trazar los resultados de ese análisis, almacenados en el objeto `epinow_res`.

```{r epidemic_models_episoon, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## define minimum date for plot
min_date <- as.Date("2015-03-01")

## extract summarised estimates
estimates <-  as_tibble(epinow_res$estimates$summarised)

## extract raw data on case incidence
observations <- as_tibble(epinow_res$estimates$observations) %>%
  filter(date > min_date)

## extract forecasted estimates of case numbers
df_wide <- estimates %>%
  filter(
    variable == "reported_cases",
    type == "forecast",
    date > min_date
  )

## convert to even longer format for quantile plotting
df_long <- df_wide %>%
  ## here we match matching quantiles (e.g. lower_90 to upper_90)
  pivot_longer(
    lower_90:upper_90,
    names_to = c(".value", "quantile"),
    names_pattern = "(.+)_(.+)"
  )

## make plot
ggplot() +
  geom_histogram(
    data = observations,
    aes(x = date, y = confirm),
    stat = 'identity',
    binwidth = 1
  ) +
  geom_ribbon(
    data = df_long,
    aes(x = date, ymin = lower, ymax = upper, alpha = quantile),
    color = NA
  ) +
  geom_line(
    data = df_wide,
    aes(x = date, y = median)
  ) +
  geom_vline(xintercept = min(df_long$date), linetype = 2) +
  ## manually define quantile transparency
  scale_alpha_manual(
    values = c(`20` = 0.7, `50` = 0.4, `90` = 0.2),
    labels = function(x) paste0(x, "%")
  ) +
  labs(
    x = NULL,
    y = "Daily reported cases",
    alpha = "Credible\ninterval"
  ) +
  scale_x_date(
    date_breaks = "1 month",
    date_labels = "%b %d\n%Y"
  ) +
  theme_minimal(base_size = 14)

```

### Proyecciones {.unnumbered}

El paquete **projections** desarrollado por RECON hace que sea muy fácil hacer previsiones de incidencia a corto plazo, requiriendo sólo el conocimiento del número de reproducción efectivo R<sub>t</sub> y el intervalo serial. Aquí cubriremos cómo utilizar las estimaciones del intervalo de serie de la literatura y cómo utilizar nuestras propias estimaciones de `linelist`.

#### Utilizando estimaciones de intervalos de serie de la literatura {.unnumbered}

Las **proyecciones** requieren una distribución de intervalos seriales discretizados del tipo `distcrete` del paquete **distcrete**. Utilizaremos una distribución gamma con una media de 12,0 y una desviación estándar de 5,2 definida en [este documento](https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-014-0196-0). Para convertir estos valores en los parámetros de forma y escala necesarios para una distribución gamma, utilizaremos la función `gamma_mucv2shapescale` del paquete **epitrix**.

```{r epidemic_models_projections_distcrete,}

## get shape and scale parameters from the mean mu and the coefficient of
## variation (e.g. the ratio of the standard deviation to the mean)
shapescale <- epitrix::gamma_mucv2shapescale(mu = 12.0, cv = 5.2/12)

## make distcrete object
serial_interval_lit <- distcrete::distcrete(
  name = "gamma",
  interval = 1,
  shape = shapescale$shape,
  scale = shapescale$scale
)

```

Aquí tenemos una comprobación rápida para asegurarnos que el intervalo de la serie parece correcto. Accedemos a la densidad de la distribución gamma que acabamos de definir mediante `$d`, lo que equivale a llamar a `dgamma`:

```{r epidemic_models_projections_distcrete_plot,}

## check to make sure the serial interval looks correct
qplot(
  x = 0:50, y = serial_interval_lit$d(0:50), geom = "area",
  xlab = "Serial interval", ylab = "Density"
)

```

#### Utilización de estimaciones de intervalos de serie a partir de los datos {.unnumbered}

Como tenemos datos sobre las fechas de inicio de los síntomas *y* los vínculos de transmisión, también podemos estimar el intervalo de serie a partir de `linelist` calculando el retraso entre las fechas de inicio de los pares infector-infectado. Como hicimos en la sección **EpiNow2**, utilizaremos la función `get_pairwise` del paquete epicontacts, que nos permite calcular las diferencias por pares de las propiedades de `linelist` entre los pares de transmisión. Primero creamos un objeto epicontacts (ver la página de [cadenas de transmisión](#transmission-chains) para más detalles):

```{r epidemic_models_epicontacts_projections, eval=F}
## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id
  ) %>%
  drop_na()

## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)
```

A continuación, ajustamos la diferencia de fechas de inicio entre los pares de transmisión, calculada mediante `get_pairwise`, a una distribución gamma. Utilizamos el práctico `fit_disc_gamma` del paquete **epitrix** para este procedimiento de ajuste, ya que necesitamos una distribución *discreta*.

```{r epidemic_models_incubation_estimate_projections,  warning = FALSE}
## estimate gamma serial interval
serial_interval <- fit_disc_gamma(get_pairwise(epic, "date_onset"))

## inspect estimate
serial_interval[c("mu", "sd")]
```

#### Proyección de la incidencia {.unnumbered}

Para proyectar la incidencia futura, todavía tenemos que proporcionar la incidencia histórica en forma de un objeto de `incidence`, así como una muestra de valores de R<sub>t</sub> plausibles. Generaremos estos valores utilizando las estimaciones de R<sub>t</sub> generadas por **EpiEstim** en la sección anterior (en "Estimación de R<sub>t</sub>") y almacenadas en el objeto `epiestim_res_emp`. En el código siguiente, extraemos las estimaciones de la media y la desviación estándar de R<sub>t</sub> para la última ventana temporal del brote (utilizando la función `tail` para acceder al último elemento de un vector), y simulamos 1000 valores a partir de una distribución gamma utilizando `rgamma`. También puedes proporcionar un vector propio de valores de R<sub>t</sub> que desees utilizar para las proyecciones a futuro.

```{r epidemic_models_projection_setup,  warning = FALSE}

## create incidence object from dates of onset
inc <- incidence::incidence(linelist$date_onset)

## extract plausible r values from most recent estimate
mean_r <- tail(epiestim_res_emp$R$`Mean(R)`, 1)
sd_r <- tail(epiestim_res_emp$R$`Std(R)`, 1)
shapescale <- gamma_mucv2shapescale(mu = mean_r, cv = sd_r/mean_r)
plausible_r <- rgamma(1000, shape = shapescale$shape, scale = shapescale$scale)

## check distribution
qplot(x = plausible_r, geom = "histogram", xlab = expression(R[t]), ylab = "Counts")

```

A continuación, utilizamos la función `project()` para realizar la previsión real. Especificamos para cuántos días queremos proyectar mediante los argumentos `n_days`, y especificamos el número de simulaciones utilizando el argumento `n_sim`.

```{r epidemic_models_make_projection,}

## make projection
proj <- project(
  x = inc,
  R = plausible_r,
  si = serial_interval$distribution,
  n_days = 21,
  n_sim = 1000
)

```

A continuación, podemos trazar fácilmente la incidencia y las proyecciones utilizando las funciones `plot()` y `add_projections()`. Podemos fácilmente subconjuntar el objeto de incidencia para mostrar sólo los casos más recientes utilizando el operador de corchetes.

```{r epidemic_models_plot_projection, out.width=c('100%', '100%'), fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}

## plot incidence and projections
plot(inc[inc$dates > as.Date("2015-03-01")]) %>%
  add_projections(proj)

```

También puedes extraer fácilmente las estimaciones brutas del número de casos diarios convirtiendo la salida en un dataframe.

```{r epidemic_models_projection_df, eval=F, warning = FALSE}
## convert to data frame for raw data
proj_df <- as.data.frame(proj)
proj_df
```

```{r epidemic_models_projection_dt,  echo = F}

## convert to data frame for raw data
proj_df <- as.data.frame(proj)

## data table output
DT::datatable(
  proj_df[1:11],
  rownames = FALSE,
  filter = "top",
  options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap'
)

```


<!-- ======================================================= -->
## Recursos {#resources-16}

* [Aquí está el documento](https://www.sciencedirect.com/science/article/pii/S1755436519300350) que describe la metodología implementada en **EpiEstim**.

* [Aquí está el documento](https://wellcomeopenresearch.org/articles/5-112/v1) que describe la metodología implementada en **EpiNow2**.

* [Aquí hay un documento](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008409) que describe varias consideraciones metodológicas y prácticas para estimar el R<sub>t</sub>.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/epidemic_models.Rmd-->


# Rastreo de contactos {#contact-tracing-1}

Esta página muestra el análisis descriptivo de los datos de rastreo de contactos, abordando algunas consideraciones clave y enfoques exclusivos de este tipo de datos.

Esta página hace referencia a muchas de las competencias básicas de gestión y visualización de datos de R tratadas en otras páginas (por ejemplo, limpieza de datos, pivoteo, tablas, análisis de series temporales), pero destacaremos ejemplos específicos del rastreo de contactos que han sido útiles para la toma de decisiones operativas. Por ejemplo, esto incluye la visualización de los datos de seguimiento del rastreo de contactos a lo largo del tiempo o a través de áreas geográficas, o la producción de tablas limpias de Indicadores Clave de Rendimiento (KPI) para los supervisores del rastreo de contactos.

Para la demostración utilizaremos datos de rastreo de contactos de la plataforma [Go.Data](https://www.who.int/tools/godata). Los principios que aquí se exponen son válidos para los datos de rastreo de contactos de otras plataformas, sólo que puede ser necesario realizar diferentes pasos de preprocesamiento de datos en función de la estructura de los mismos.

Puedes leer más sobre el proyecto Go.Data en el [sitio de documentación de Github](https://worldhealthorganization.github.io/godata/) o en su [Comunidad de Prácticas](https://community-godata.who.int/).

## Preparation {#preparation-16}


### Cargar paquetes {.unnumbered}  

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r, message = F}
pacman::p_load(
  rio,          # importing data  
  here,         # relative file pathways  
  janitor,      # data cleaning and tables
  lubridate,    # working with dates
  epikit,       # age_categories() function
  apyramid,     # age pyramids
  tidyverse,    # data manipulation and visualization
  RColorBrewer, # color palettes
  formattable,  # fancy tables
  kableExtra    # table formatting
)
```


### Importar datos {.unnumbered}

Importaremos conjuntos de datos de muestra de contactos y de su "seguimiento". Estos datos se han recuperado y desanidado de la API Go.Data y se han almacenado como archivos ".rds".

Puedes descargar todos los datos de ejemplo de este manual en la página de [descarga de manuales y datos](#download-handbook-and-data).

Si deseas descargar los datos de seguimiento de contactos de ejemplo específicos de esta página, utiliza los tres enlaces de descarga que aparecen a continuación:

[Clica para descargar los datos de casos de la investigación (archivo .rds) ](https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/cases_clean.rds?raw=true)

[Clica para descargar los datos del registro de contactos (archivo .rds) ](https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/contacts_clean.rds?raw=true)

[Clica para descargar los datos de seguimiento de los contactos (archivo .rds) ](https://github.com/WorldHealthOrganization/godata/blob/master/analytics/r-reporting/data/followups_clean.rds?raw=true)

<!-- ```{r out.width = "100%", fig.align = "center", echo=F} -->
<!-- knitr::include_graphics(here::here("images", "godata_api_github.png")) -->
<!-- ``` -->

En su formato original los archivos descargables, reflejan los datos proporcionados por la API de Go.Data (puedes aprender sobre las [API aquí](#import_api)). A modo de ejemplo, aquí limpiaremos los datos para que sean más fáciles de leer en esta página. Si estás utilizando una instancia de Go.Data, puedes ver las instrucciones completas sobre cómo recuperar sus datos [aquí](https://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting).

A continuación, los conjuntos de datos se importan utilizando la función `import()` del paquete **rio**. Consulta la página sobre [importación y exportación](#import-and-export) para conocer las distintas formas de importar datos. Utilizamos `here()` para especificar la ruta del archivo - debes escribir la ruta del archivo específica de tu ordenador. A continuación, utilizamos `select()` para seleccionar sólo ciertas columnas de los datos, para simplificar la demostración.

#### Datos de casos {.unnumbered}  

Estos datos son una tabla de los casos, y la información sobre ellos.

```{r}
cases <- import(here("data", "godata", "cases_clean.rds")) %>% 
  select(case_id, firstName, lastName, gender, age, age_class,
         occupation, classification, was_contact, hospitalization_typeid)
```

Aquí están los casos `nrow(cases)`: 

```{r, message=FALSE, echo=F}
DT::datatable(cases, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Datos de contactos {.unnumbered}  

Estos datos son una tabla de todos los contactos e información sobre ellos. De nuevo, proporciona tu propia ruta de acceso al archivo. Después de la importación, realizamos algunos pasos preliminares de limpieza de datos que incluyen:

* Establecer age_class como factor e invertir el orden de los niveles para que las edades más jóvenes sean las primeras
* Seleccionar sólo una columna determinada, renombrando una de ellas
* Asignar artificialmente a "Djembe" las filas a las que les falta el nivel 2 de administración, para mejorar la claridad de algunas visualizaciones de ejemplo

```{r}
contacts <- import(here("data", "godata", "contacts_clean.rds")) %>% 
  mutate(age_class = forcats::fct_rev(age_class)) %>% 
  select(contact_id, contact_status, firstName, lastName, gender, age,
         age_class, occupation, date_of_reporting, date_of_data_entry,
         date_of_last_exposure = date_of_last_contact,
         date_of_followup_start, date_of_followup_end, risk_level, was_case, admin_2_name) %>% 
  mutate(admin_2_name = replace_na(admin_2_name, "Djembe"))
```

Aquí están las filas de los datos de contactos (`nrow(contacts)`):

```{r, message=FALSE, echo=F}
DT::datatable(contacts, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Datos de seguimiento {.unnumbered}  

Estos datos son registros de las interacciones de "seguimiento" con los contactos. Se supone que cada contacto tiene un encuentro diario durante los 14 días siguientes a su exposición.

Importamos y realizamos algunos pasos de limpieza. Seleccionamos ciertas columnas y también convertimos una columna de caracteres a todos los valores en minúsculas.


```{r}
followups <- rio::import(here::here("data", "godata", "followups_clean.rds")) %>% 
  select(contact_id, followup_status, followup_number,
         date_of_followup, admin_2_name, admin_1_name) %>% 
  mutate(followup_status = str_to_lower(followup_status))
```

Aquí están las primeras 50 filas de `followups` (cada fila es una interacción de seguimiento, con el estado del resultado en la columna `followup_status`):

```{r, message=FALSE, echo=F}
DT::datatable(head(followups, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Datos de las relaciones {.unnumbered}  

Aquí importamos datos que muestran la relación entre casos y contactos. Seleccionamos cierta columna para mostrarlos.

```{r}
relationships <- rio::import(here::here("data", "godata", "relationships_clean.rds")) %>% 
  select(source_visualid, source_gender, source_age, date_of_last_contact,
         date_of_data_entry, target_visualid, target_gender,
         target_age, exposure_type)
```

A continuación se muestran las primeras 50 filas de los datos de relaciones (`relationships`), cuyos registros son todas las relaciones entre casos y contactos.

```{r, message=FALSE, echo=F}
DT::datatable(head(relationships, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




## Análisis descriptivo {#descriptive-analyses}

Puedes utilizar las técnicas tratadas en otras páginas de este manual para realizar análisis descriptivos de los casos, contactos y sus relaciones. A continuación se ofrecen algunos ejemplos. 


### Datos demográficos {.unnumbered}  

Como se muestra en la página dedicada a las [pirámides demográficas](#demographic-pyramids-and-likert-scales), se puede visualizar la distribución por edades y por sexos (aquí utilizamos el paquete **apyramid**). 

#### Edad y sexo de los contactos {.unnumbered}  

La pirámide que se muestra a continuación compara la distribución de la edad de los contactos, por género. Observa que los contactos a los que les falta la edad se incluyen en su propia barra en la parte superior. Puedes cambiar este comportamiento por defecto, pero entonces considera listar el número que falta en una leyenda.

```{r, warning=F, message=F}
apyramid::age_pyramid(
  data = contacts,                                   # use contacts dataset
  age_group = "age_class",                           # categorical age column
  split_by = "gender") +                             # gender for halfs of pyramid
  labs(
    fill = "Gender",                                 # title of legend
    title = "Age/Sex Pyramid of COVID-19 contacts")+ # title of the plot
  theme_minimal()                                    # simple background
```

Con la estructura de datos Go.Data, los datos `relationships` contienen las edades tanto de los casos como de los contactos, por lo que podrías utilizar ese conjunto de datos y crear una pirámide de edades que muestre las diferencias entre estos dos grupos de personas. El dataframe `relationships` será mutado para transformar las columnas numéricas de edad en categorías (véase la página de [limpieza de datos y funciones básicas](#cleaning-data-and-core-functions)). También pivotamos el dataframe a largo para facilitar el trazado con **ggplot2** (ver [Pivotar datos](#pivoting-data)).

```{r}
relation_age <- relationships %>% 
  select(source_age, target_age) %>% 
  transmute(                              # transmute is like mutate() but removes all other columns not mentioned
    source_age_class = epikit::age_categories(source_age, breakers = seq(0, 80, 5)),
    target_age_class = epikit::age_categories(target_age, breakers = seq(0, 80, 5)),
    ) %>% 
  pivot_longer(cols = contains("class"), names_to = "category", values_to = "age_class")  # pivot longer


relation_age
```

Ahora podemos representar este conjunto de datos transformado con `age_pyramid()` como antes, pero sustituyendo `gender` con la  `category` (contacto, o caso).

```{r, warning=F, message=F}
apyramid::age_pyramid(
  data = relation_age,                               # use modified relationship dataset
  age_group = "age_class",                           # categorical age column
  split_by = "category") +                           # by cases and contacts
  scale_fill_manual(
    values = c("orange", "purple"),                  # to specify colors AND labels
    labels = c("Case", "Contact"))+
  labs(
    fill = "Legend",                                           # title of legend
    title = "Age/Sex Pyramid of COVID-19 contacts and cases")+ # title of the plot
  theme_minimal()                                              # simple background
```

También podemos ver otras características como el desglose profesional (por ejemplo, en forma de gráfico circular).

```{r, warning=F, message=F}
# Clean dataset and get counts by occupation
occ_plot_data <- cases %>% 
  mutate(occupation = forcats::fct_explicit_na(occupation),  # make NA missing values a category
         occupation = forcats::fct_infreq(occupation)) %>%   # order factor levels in order of frequency
  count(occupation)                                          # get counts by occupation
  
# Make pie chart
ggplot(data = occ_plot_data, mapping = aes(x = "", y = n, fill = occupation))+
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  labs(
    fill = "Occupation",
    title = "Known occupations of COVID-19 cases")+
  theme_minimal() +                    
  theme(axis.line = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank())
```


### Contactos por caso {.unnumbered}  

El número de contactos por caso puede ser una métrica importante para evaluar la calidad de la enumeración de los contactos y la conformidad de la población con la respuesta de salud pública.

Dependiendo de la estructura de datos, esto puede evaluarse con un juego de datos que contenga todos los casos y contactos. En el conjunto de datos de Go.Data, los vínculos entre los casos ("fuentes") y los contactos ("objetivos") se almacenan en  `relationships`.

En este conjunto de datos, cada fila es un contacto, y el caso de origen aparece en la fila. No hay contactos que tengan relaciones con múltiples casos, pero si esto existiese, puede ser necesario tenerlos en cuenta antes de representarlo (¡y explorarlos también!).

Comenzamos contando el número de filas (contactos) por caso de origen. Esto se guarda como un dataframe.

```{r}
contacts_per_case <- relationships %>% 
  count(source_visualid)

contacts_per_case
```

Utilizamos `geom_histogram()` para trazar estos datos como un histograma. 

```{r, warning=F, message=F}
ggplot(data = contacts_per_case)+        # begin with count data frame created above
  geom_histogram(mapping = aes(x = n))+  # print histogram of number of contacts per case
  scale_y_continuous(expand = c(0,0))+   # remove excess space below 0 on y-axis
  theme_light()+                         # simplify background
  labs(
    title = "Number of contacts per case",
    y = "Cases",
    x = "Contacts per case"
  )
  

```



## Seguimiento de contactos  {#contact-follow-up}

Los datos de rastreo de contactos suelen contener datos de "seguimiento", que registran los resultados de los controles diarios de los síntomas de las personas en cuarentena. El análisis de estos datos puede servir de base para la estrategia de respuesta e identificar a los contactos con riesgo de pérdida de seguimiento o con riesgo de desarrollar la enfermedad.


### Limpieza de datos {.unnumbered}  

Estos datos pueden existir en una variedad de formatos. Pueden existir como una hoja de Excel de formato "ancho" con una fila por contacto y una columna por "día" de seguimiento. Consulta [Pivotar datos](#pivoting-data) para ver las descripciones de los datos "largos" y "anchos" y cómo pivotar los datos anchos o largos.

En nuestro ejemplo de Go.Data, estos datos se almacenan en el dataframe  `followups`, que tiene un formato "largo" con una fila por interacción de seguimiento. Las primeras 50 filas tienen este aspecto:

```{r, message=FALSE, echo=FALSE}
# display the first 50 rows of contact linelist data as a table
DT::datatable(head(followups, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


<span style="color: orange;">***PRECAUCIÓN:*** Ten cuidado con los duplicados al tratar los datos de seguimiento, ya que podría haber varios seguimientos erróneos en el mismo día para un contacto determinado. Tal vez parezca un error, pero refleja la realidad: por ejemplo, un rastreador de contactos podría enviar un formulario de seguimiento a primera hora del día cuando no pudo contactar con el contacto, y enviar un segundo formulario cuando se le pudo contactar más tarde. Dependerá del contexto operativo la forma en que desees gestionar los duplicados, pero asegúrate de documentar claramente tu enfoque. </span>

*Veamos* cuántos casos de filas "duplicadas" tenemos:

```{r}
followups %>% 
  count(contact_id, date_of_followup) %>%   # get unique contact_days
  filter(n > 1)                             # view records where count is more than 1  
```

En nuestros datos de ejemplo, los únicos registros a los que se aplica esto son los que carecen de ID. Podemos eliminarlos. Pero, a efectos de demostración, mostraremos los pasos para la eliminación de la duplicación de modo que sólo haya un registro de seguimiento por persona y por día. Para más detalles, consulta la página de [De-duplicación](#de-duplication). Asumiremos que el registro de encuentro más reciente es el correcto. También aprovechamos la oportunidad para limpiar la columna `followup_number` (el "día" de seguimiento que debe ir de 1 a 14).

```{r, warning=F, message=F}
followups_clean <- followups %>%
  
  # De-duplicate
  group_by(contact_id, date_of_followup) %>%        # group rows per contact-day
  arrange(contact_id, desc(date_of_followup)) %>%   # arrange rows, per contact-day, by date of follow-up (most recent at top)
  slice_head() %>%                                  # keep only the first row per unique contact id  
  ungroup() %>% 
  
  # Other cleaning
  mutate(followup_number = replace(followup_number, followup_number > 14, NA)) %>% # clean erroneous data
  drop_na(contact_id)                               # remove rows with missing contact_id
```

Para cada encuentro de seguimiento, tenemos un estado de seguimiento (como si el encuentro se produjo y, si es así, el contacto tuvo síntomas o no). Para ver todos los valores podemos ejecutar un `tabyl()` rápido (de **janitor**) o `table()` (de R **base**) (ver [Tablas descriptivas](#descriptive-tables)) por `followup_status` para ver la frecuencia de cada uno de los resultados.

En este conjunto de datos, "seen_not_ok" significa "visto con síntomas", y "seen_ok" significa "visto sin síntomas".

```{r}
followups_clean %>% 
  tabyl(followup_status)
```


### Gráfica en el tiempo {.unnumbered}  

Como los datos de las fechas son continuos, utilizaremos un histograma para representarlos con `date_of_followup` asignado al eje-x. Podemos conseguir un histograma "apilado" especificando un argumento `fill = ` dentro de `aes()`, que asignamos a la columna `followup_status`. En consecuencia, se puede establecer el título de la leyenda utilizando el argumento `fill = ` de `labs()`.

Podemos ver que los contactos se identificaron en oleadas (presumiblemente correspondientes a las oleadas epidémicas de casos), y que la finalización del seguimiento no parece haber mejorado a lo largo de la epidemia. 

```{r, warning=F, message=F}
ggplot(data = followups_clean)+
  geom_histogram(mapping = aes(x = date_of_followup, fill = followup_status)) +
  scale_fill_discrete(drop = FALSE)+   # show all factor levels (followup_status) in the legend, even those not used
  theme_classic() +
  labs(
    x = "",
    y = "Number of contacts",
    title = "Daily Contact Followup Status",
    fill = "Followup Status",
    subtitle = str_glue("Data as of {max(followups$date_of_followup, na.rm=T)}"))   # dynamic subtitle
  
```


<span style="color: orange;">***PRECAUCIÓN:*** Si estás preparando muchos gráficos (por ejemplo, para múltiples jurisdicciones) querrás que las leyendas aparezcan de forma idéntica incluso con diferentes niveles de finalización o composición de los datos. Puede haber gráficos para los cuales no todos los estados de seguimiento están presentes, pero todavía quieres que esas categorías aparezcan en las leyendas. En ggplot (como arriba), puedes especificar el argumento `drop = FALSE` de `scale_fill_discrete()`. En las tablas, utiliza `tabyl()` que muestra los recuentos de todos los niveles de los factores, o si utilizas `count()` de **dplyr** añade el argumento `.drop = FALSE` para incluir los recuentos de todos los niveles de los factores.</span>  


### Seguimiento individual diario  {.unnumbered}  

Si tu brote es lo suficientemente pequeño, es posible que quieras mirar cada contacto individualmente y ver su estado a lo largo del seguimiento. Afortunadamente, este conjunto de datos de seguimiento ya contiene una columna con el "número" de día de seguimiento (1-14). Si no existe en tus datos, puedes crearla calculando la diferencia entre la fecha de encuentro y la fecha en la que el seguimiento debía comenzar para el contacto.

Un mecanismo de visualización conveniente (si el número de casos no es demasiado grande) puede ser un gráfico de calor, hecho con `geom_tile()`. Mira más detalles en la página [Gráficos de calor](#heat-plots). 

```{r, warning=F, message=F}
ggplot(data = followups_clean)+
  geom_tile(mapping = aes(x = followup_number, y = contact_id, fill = followup_status),
            color = "grey")+       # grey gridlines
  scale_fill_manual( values = c("yellow", "grey", "orange", "darkred", "darkgreen"))+
  theme_minimal()+
  scale_x_continuous(breaks = seq(from = 1, to = 14, by = 1))
```


### Analizar por grupos {.unnumbered}  

Tal vez estos datos de seguimiento se consulten diaria o semanalmente para la toma de decisiones operativas. Es posible que desees desgloses más significativos por zona geográfica o por equipo de seguimiento de contactos. Podemos hacerlo ajustando las columnas proporcionadas a `group_by()`.

```{r, warning=F, message=F}

plot_by_region <- followups_clean %>%                                        # begin with follow-up dataset
  count(admin_1_name, admin_2_name, followup_status) %>%   # get counts by unique region-status (creates column 'n' with counts)
  
  # begin ggplot()
  ggplot(                                         # begin ggplot
    mapping = aes(x = reorder(admin_2_name, n),     # reorder admin factor levels by the numeric values in column 'n'
                  y = n,                            # heights of bar from column 'n'
                  fill = followup_status,           # color stacked bars by their status
                  label = n))+                      # to pass to geom_label()              
  geom_col()+                                     # stacked bars, mapping inherited from above 
  geom_text(                                      # add text, mapping inherited from above
    size = 3,                                         
    position = position_stack(vjust = 0.5), 
    color = "white",           
    check_overlap = TRUE,
    fontface = "bold")+
  coord_flip()+
  labs(
    x = "",
    y = "Number of contacts",
    title = "Contact Followup Status, by Region",
    fill = "Followup Status",
    subtitle = str_glue("Data as of {max(followups_clean$date_of_followup, na.rm=T)}")) +
  theme_classic()+                                                                      # Simplify background
  facet_wrap(~admin_1_name, strip.position = "right", scales = "free_y", ncol = 1)      # introduce facets 

plot_by_region
```

<!-- If this was disaggregated by contact tracer, perhaps we would want to add a threshold line to display total # contacts that normally one person or area/team can handle, and how the current workload compares. We just do this by using `geom_hline()` function. -->

<!-- ```{r, warning=F, message=F} -->

<!-- plot_by_region +  -->
<!--      geom_hline(aes(yintercept=25), color="#C70039", linetype = "dashed") # fictitious threshold at 25 contacts -->

<!-- ``` -->



## Tablas KPI {#kpi-tables} 

Hay una serie de Indicadores Clave de Rendimiento (KPI) que pueden calcularse y seguirse a distintos niveles de desagregación y a lo largo de diferentes períodos de tiempo para supervisar el rendimiento del rastreo de contactos. Una vez que se tienen los cálculos y el formato básico de la tabla, es bastante fácil cambiar los diferentes KPI.

Existen numerosas fuentes de KPI de rastreo de contactos, como ésta de [ResolveToSaveLives.org](https://contacttracingplaybook.resolvetosavelives.org/checklists/metrics). La mayor parte del trabajo consistirá en recorrer la estructura de datos y pensar en todos los criterios de inclusión/exclusión. A continuación mostramos algunos ejemplos, utilizando la estructura de metadatos de Go.Data:


Categoría         | Indicador                | Numerador Go.Data         | Denominador Go.Data
------------------|--------------------------|---------------------------|--------------------
Indicador de proceso - Velocidad de rastreo de contactos| % de casos entrevistados y aislados en las 24 horas siguientes a la notificación del caso |COUNT OF `case_id` WHERE (`date_of_reporting` - `date_of_data_entry`) < 1 day AND (`isolation_startdate` - `date_of_data_entry`) < 1 day|COUNT OF  `case_id`
Indicador de proceso - Velocidad de rastreo de contactos|% de contactos notificados y puestos en cuarentena en las 24 horas siguientes a la solicitud |COUNT OF `contact_id` WHERE `followup_status` == "SEEN_NOT_OK" OR "SEEN_OK" AND `date_of_followup` -  `date_of_reporting` < 1 day|COUNT OF `contact_id`
Indicador de proceso - Completitud de las pruebas|% de nuevos casos sintomáticos examinados y entrevistados en los 3 días siguientes al inicio de los síntomas |COUNT OF `case_id` WHERE (`date_of_reporting` - `date_of_onset`) < =3 days|COUNT OF  `case_id`
Indicador de resultado - Global|% de nuevos casos entre la lista de contactos existente|COUNT OF `case_id` WHERE `was_contact` == "TRUE"|COUNT OF  `case_id`

A continuación veremos un ejercicio de ejemplo para crear una bonita tabla visual para mostrar el seguimiento de los contactos en las áreas de administración. Al final, lo haremos apto para la presentación con el paquete **formattable** (pero podrías usar otros paquetes como **flextable** - ver [Tablas para presentaciones](#tables-for-presentation)).

La forma de crear una tabla como ésta dependerá de la estructura de los datos de seguimiento de contactos. Utiliza la página de [tablas descriptivas](#descriptive-tables) para aprender a resumir los datos utilizando las funciones de **dplyr**.

Crearemos una tabla que será dinámica y cambiará a medida que cambien los datos. Para que los resultados sean interesantes, estableceremos una `report_date` que nos permita simular la ejecución de la tabla en un día determinado (elegimos el 10 de junio de 2020). Los datos se filtran por esa fecha.  

```{r, warning=F, message=F}
# Set "Report date" to simulate running the report with data "as of" this date
report_date <- as.Date("2020-06-10")

# Create follow-up data to reflect the report date.
table_data <- followups_clean %>% 
  filter(date_of_followup <= report_date)
```


Ahora, basándonos en nuestra estructura de datos, haremos lo siguiente:

1)  Comienza con los datos de `followups` y resúmelos para contener, para cada contacto único:
  * La fecha del último registro (sin importar el estado del encuentro)
  * La fecha del último encuentro en el que el contacto fue "visto"
  * El estado del encuentro en ese último encuentro "visto" (por ejemplo, con síntomas, sin síntomas)
2)  Uniremos estos datos a los de los contactos, que contienen otra información como el estado general del contacto, la fecha de la última exposición a un caso, etc. También calcularemos las métricas de interés para cada contacto, como los días desde la última exposición
3)  Agrupamos los datos de contacto mejorados por región geográfica (`admin_2_name) y calculamos las estadísticas resumidas por región
4)  Por último, damos un buen formato a la tabla para su presentación


Primero resumimos los datos de seguimiento para obtener la información de interés:

```{r, warning=F, message=F}
followup_info <- table_data %>% 
  group_by(contact_id) %>% 
  summarise(
    date_last_record   = max(date_of_followup, na.rm=T),
    date_last_seen     = max(date_of_followup[followup_status %in% c("seen_ok", "seen_not_ok")], na.rm=T),
    status_last_record = followup_status[which(date_of_followup == date_last_record)]) %>% 
  ungroup()
```

Así es como se ven estos datos:

```{r, echo=F}
DT::datatable(followup_info, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```


Ahora añadiremos esta información a los datos de `contacts` y calcularemos algunas columnas adicionales. 

```{r}
contacts_info <- followup_info %>% 
  right_join(contacts, by = "contact_id") %>% 
  mutate(
    database_date       = max(date_last_record, na.rm=T),
    days_since_seen     = database_date - date_last_seen,
    days_since_exposure = database_date - date_of_last_exposure
    )
```

Así es como se ven estos datos. Observa la columna `contacts` a la derecha, y la nueva columna calculada en el extremo derecho.  

```{r, echo=F}
DT::datatable(contacts_info, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```


A continuación, resumimos los datos de los contactos por región, para conseguir un dataframe conciso de columnas de estadísticas resumidas.    

```{r}
contacts_table <- contacts_info %>% 
  
  group_by(`Admin 2` = admin_2_name) %>%
  
  summarise(
    `Registered contacts` = n(),
    `Active contacts`     = sum(contact_status == "UNDER_FOLLOW_UP", na.rm=T),
    `In first week`       = sum(days_since_exposure < 8, na.rm=T),
    `In second week`      = sum(days_since_exposure >= 8 & days_since_exposure < 15, na.rm=T),
    `Became case`         = sum(contact_status == "BECAME_CASE", na.rm=T),
    `Lost to follow up`   = sum(days_since_seen >= 3, na.rm=T),
    `Never seen`          = sum(is.na(date_last_seen)),
    `Followed up - signs` = sum(status_last_record == "Seen_not_ok" & date_last_record == database_date, na.rm=T),
    `Followed up - no signs` = sum(status_last_record == "Seen_ok" & date_last_record == database_date, na.rm=T),
    `Not Followed up`     = sum(
      (status_last_record == "NOT_ATTEMPTED" | status_last_record == "NOT_PERFORMED") &
        date_last_record == database_date, na.rm=T)) %>% 
    
  arrange(desc(`Registered contacts`))

```


```{r, echo=F}
DT::datatable(contacts_table, rownames = FALSE, options = list(pageLength = 12, scrollX=T), class = 'white-space: nowrap' )
```

Y ahora aplicamos el estilo de los paquetes **formattable** y **knitr**, incluyendo una nota a pie de página que muestra la fecha "a partir de". 


```{r}
contacts_table %>%
  mutate(
    `Admin 2` = formatter("span", style = ~ formattable::style(
      color = ifelse(`Admin 2` == NA, "red", "grey"),
      font.weight = "bold",font.style = "italic"))(`Admin 2`),
    `Followed up - signs`= color_tile("white", "orange")(`Followed up - signs`),
    `Followed up - no signs`= color_tile("white", "#A0E2BD")(`Followed up - no signs`),
    `Became case`= color_tile("white", "grey")(`Became case`),
    `Lost to follow up`= color_tile("white", "grey")(`Lost to follow up`), 
    `Never seen`= color_tile("white", "red")(`Never seen`),
    `Active contacts` = color_tile("white", "#81A4CE")(`Active contacts`)
  ) %>%
  kable("html", escape = F, align =c("l","c","c","c","c","c","c","c","c","c","c")) %>%
  kable_styling("hover", full_width = FALSE) %>%
  add_header_above(c(" " = 3, 
                     "Of contacts currently under follow up" = 5,
                     "Status of last visit" = 3)) %>% 
  kableExtra::footnote(general = str_glue("Data are current to {format(report_date, '%b %d %Y')}"))

```


## Matrices de transmisión  {#transmission-matrices}

Como se discutió en la página de [Gráficos de calor](#heat-plots), puedes crear una matriz de "quién infectó a quién" utilizando `geom_tile()`.

Cuando se crean nuevos contactos, Go.Data almacena esta información de relación en el punto final de la API `relationships`; y podemos ver las primeras 50 filas de este conjunto de datos a continuación. Esto significa que podemos crear un gráfico de calor con relativamente pocos pasos, dado que cada contacto ya está unido a su caso de origen.

```{r, warning=F, message=F, echo=F}
# display the first 50 rows of relationships data as a table
DT::datatable(head(relationships, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Al igual que en el caso de la pirámide de edad que compara casos y contactos, podemos seleccionar las pocas variables que necesitamos y crear columnas con agrupaciones categóricas de edad tanto para las fuentes (casos) como para los objetivos (contactos).

```{r}
heatmap_ages <- relationships %>% 
  select(source_age, target_age) %>% 
  mutate(                              # transmute is like mutate() but removes all other columns
    source_age_class = epikit::age_categories(source_age, breakers = seq(0, 80, 5)),
    target_age_class = epikit::age_categories(target_age, breakers = seq(0, 80, 5))) 
```

Como se ha descrito anteriormente, creamos una tabulación cruzada;

```{r, warning=F, message=FALSE}

cross_tab <- table(
  source_cases = heatmap_ages$source_age_class,
  target_cases = heatmap_ages$target_age_class)

cross_tab
```

convertimos en formato largo con proporciones;

```{r, warning=FALSE, message=FALSE}

long_prop <- data.frame(prop.table(cross_tab))

```

y creamos un mapa de calor para la edad.


```{r, warning=F, message=F}

ggplot(data = long_prop)+       # use long data, with proportions as Freq
  geom_tile(                    # visualize it in tiles
    aes(
      x = target_cases,         # x-axis is case age
      y = source_cases,     # y-axis is infector age
      fill = Freq))+            # color of the tile is the Freq column in the data
  scale_fill_gradient(          # adjust the fill color of the tiles
    low = "blue",
    high = "orange")+
  theme(axis.text.x = element_text(angle = 90))+
  labs(                         # labels
    x = "Target case age",
    y = "Source case age",
    title = "Who infected whom",
    subtitle = "Frequency matrix of transmission events",
    fill = "Proportion of all\ntranmsission events"     # legend title
  )

```


## Recursos  {#resources-18}

https://github.com/WorldHealthOrganization/godata/tree/master/analytics/r-reporting

https://worldhealthorganization.github.io/godata/

https://community-godata.who.int/
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/contact_tracing.Rmd-->


# Análisis de encuestas {#survey-analysis}


<!-- ======================================================= -->
## Resumen {#overview-4}

Esta página muestra el uso de varios paquetes para el análisis de encuestas.

La mayoría de los paquetes R de encuestas se basan en el [paquete **survey**](https://cran.r-project.org/web/packages/survey/index.html) para realizar análisis ponderados. Utilizaremos **survey**, así como [**srvyr**](https://cran.r-project.org/web/packages/srvyr/index.html) (una envoltura para **survey** que permite la codificación al estilo tidyverse) y [**gtsummary**](https://cran.r-project.org/web/packages/gtsummary/index.html) (una envoltura para **survey** que permite obtener tablas listas para su publicación). Aunque el paquete original **survey** no permite la codificación al estilo tidyverse, tiene la ventaja añadida de permitir modelos lineales generalizados ponderados por la encuesta (que se añadirán a esta página más adelante). También demostraremos el uso de una función del paquete [**sitrep**](https://github.com/R4EPI/sitrep) para crear ponderaciones de muestreo (*n.b.* este paquete no está todavía en CRAN, pero se puede instalar desde github).

La mayor parte de esta página se basa en el trabajo realizado para el [proyecto "R4Epis"](https://r4epis.netlify.app/); para ver el código detallado y las plantillas R-markdown del mismo, consulta la [página github de "R4Epis"](https://github.com/R4EPI/sitrep). Parte del código basado en el paquete de encuestas se basa en las primeras versiones de los [estudios de caso de EPIET](https://github.com/EPIET/RapidAssessmentSurveys).

Actualmente, esta página no aborda el cálculo del tamaño de la muestra ni el muestreo. Para una calculadora del tamaño  muestral fácil de usar, consulta [OpenEpi](https://www.openepi.com/Menu/OE_Menu.htm). La página de [conceptos básicos de los SIG](#gis-basics) del manual tendrá eventualmente una sección sobre muestreo aleatorio espacial, y esta página tendrá eventualmente una sección sobre marcos de muestreo así como cálculos del tamaño de la muestra.



1.  Datos de encuestas
2.  Tiempo de observación
3.  Ponderación
4.  Objetos de diseño de la encuesta
5.  Análisis descriptivo
6.  Proporciones ponderadas
7.  Tasas ponderadas


<!-- ======================================================= -->
## Preparación {#preparation-17}

### Paquetes {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También se pueden cargar paquetes con `library()` de R **base .** Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.
Aquí también mostramos el uso de la función `p_load_gh()` de **pacman** para instalar y cargar un paquete de github que aún no ha sido publicado en CRAN.

```{r}

## load packages from CRAN
pacman::p_load(rio,          # File import
               here,         # File locator
               tidyverse,    # data management + ggplot2 graphics
               tsibble,      # handle time series datasets
               survey,       # for survey functions
               srvyr,        # dplyr wrapper for survey package
               gtsummary,    # wrapper for survey package to produce tables
               apyramid,     # a package dedicated to creating age pyramids
               patchwork,    # for combining ggplots
               ggforce       # for alluvial/sankey plots
              
               ) 


``` 

```{r, include = FALSE}

pacman::p_load_gh(
  
  "r4epi/epikit") # jfmont

```


```{r}

## load packages from github

pacman::p_load_gh(
  
  "r4epi/sitrep" # for observation time / weighting functions

)
```



### Carga de datos {.unnumbered}

El conjunto de datos de ejemplo utilizado en esta sección:

-   datos de encuesta de mortalidad ficticia.
-   recuentos de población ficticios para la zona de la encuesta.
-   diccionario de datos para los datos de la encuesta de mortalidad ficticia.

Se basa en la encuesta pre-aprobada por la junta de revisión ética de MSF OCA. Los datos ficticios se produjeron como parte del [proyecto "R4Epis"](https://r4epis.netlify.app/). Todo ello se basa en los datos recopilados mediante [KoboToolbox,](https://www.kobotoolbox.org/) un software de recopilación de datos basado en [Open Data Kit](https://opendatakit.org/).

Kobo permite exportar tanto los datos recogidos como el diccionario de datos para ese conjunto de datos. Recomendamos encarecidamente hacer esto, ya que simplifica la limpieza de los datos y es útil para buscar variables/preguntas.


<span style="color: darkgreen;">***CONSEJO:*** El diccionario de datos de Kobo tiene nombres de variables en la columna "name" de la hoja de la encuesta. Los valores posibles para cada variable se especifican en la hoja de opciones. En la hoja de opciones, "name" tiene el valor acortado y las columnas "label::english" y "label::french" tienen las versiones largas correspondientes. Si utilizas la función `msf_dict_survey()` del paquete **epidict** para importar un archivo excel del diccionario Kobo, éste se reformulará para que pueda utilizarse fácilmente para recodificar. </span>

<span style="color: orange;">***PRECAUCIÓN:*** El conjunto de datos de ejemplo no es lo mismo que una exportación (ya que en Kobo se exportan los diferentes niveles del cuestionario de forma individual) - Mira la sección de datos de la encuesta más abajo para fusionar los diferentes niveles.</span>


Los datos se importan mediante la función `import()` del paquete **rio**. Consulta la página sobre [importación y exportación](#import-and-export) para conocer las distintas formas de importar datos.

```{r echo = FALSE}
# import the survey into R
survey_data <- rio::import(here::here("data", "surveys", "survey_data.xlsx"))

# import the dictionary into R
survey_dict <- rio::import(here::here("data", "surveys", "survey_dict.xlsx")) 

# import the population in to R 
population <- rio::import(here::here("data", "surveys", "population.xlsx"))
```

```{r eval = FALSE}
# import the survey data
survey_data <- rio::import("survey_data.xlsx")

# import the dictionary into R
survey_dict <- rio::import("survey_dict.xlsx") 
```

Más abajo se muestran las primeras 10 filas de la encuesta

```{r, message = FALSE, echo = FALSE}
# display the survey data as a table
DT::datatable(head(survey_data, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

También queremos importar los datos de la población de muestreo para poder elaborar las ponderaciones adecuadas. Estos datos pueden estar en diferentes formatos, sin embargo sugerimos tenerlos como se ve a continuación (esto puede ser simplemente escrito en un Excel).


```{r read_data_pop_show, eval = FALSE}
# import the population data
population <- rio::import("population.xlsx")
```

A continuación se muestran las 10 primeras filas de la encuesta.

```{r message=FALSE, echo=F}
# display the survey data as a table
DT::datatable(head(population, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

En el caso de las encuestas por conglomerados, es posible que desees añadir ponderaciones de la encuesta a nivel de conglomerado. Puedes introducir estos datos como se indica más arriba. Alternativamente, si sólo hay unos pocos recuentos, éstos podrían introducirse como se indica a continuación en un tibble. En cualquier caso, tendrá que tener una columna con un identificador de conglomerado que coincida con los datos de tu encuesta, y otra columna con el número de hogares en cada conglomerado.

```{r cluster_counts}

## define the number of households in each cluster
cluster_counts <- tibble(cluster = c("village_1", "village_2", "village_3", "village_4", 
                                     "village_5", "village_6", "village_7", "village_8",
                                     "village_9", "village_10"), 
                         households = c(700, 400, 600, 500, 300, 
                                        800, 700, 400, 500, 500))

```

### Limpieza de datos {.unnumbered}

A continuación se asegura que la columna de fechas tenga el formato adecuado. Hay varias otras maneras de hacer esto (ver la página [Trabajar con fechas](#working-with-dates) para más detalles), sin embargo, usar el diccionario para definir las fechas es rápido y fácil.

También creamos una variable de grupo de edad utilizando la función `age_categories()` de **epikit** - véase la sección del manual de [limpieza de datos](#cleaning-data-and-core-functions.html#num_cats) para más detalles. Además, creamos una variable de carácter que define en qué distrito se encuentran las distintas agrupaciones.

Por último, recodificamos todas las variables sí/no en variables VERDADERO/FALSO, ya que de lo contrario no pueden ser utilizadas por las funciones de proporción de **survey**.

```{r cleaning}

## select the date variable names from the dictionary 
DATEVARS <- survey_dict %>% 
  filter(type == "date") %>% 
  filter(name %in% names(survey_data)) %>% 
  ## filter to match the column names of your data
  pull(name) # select date vars
  
## change to dates 
survey_data <- survey_data %>%
  mutate(across(all_of(DATEVARS), as.Date))


## add those with only age in months to the year variable (divide by twelve)
survey_data <- survey_data %>% 
  mutate(age_years = if_else(is.na(age_years), 
                             age_months / 12, 
                             age_years))

## define age group variable
survey_data <- survey_data %>% 
     mutate(age_group = age_categories(age_years, 
                                    breakers = c(0, 3, 15, 30, 45)
                                    ))


## create a character variable based off groups of a different variable 
survey_data <- survey_data %>% 
  mutate(health_district = case_when(
    cluster_number %in% c(1:5) ~ "district_a", 
    TRUE ~ "district_b"
  ))


## select the yes/no variable names from the dictionary 
YNVARS <- survey_dict %>% 
  filter(type == "yn") %>% 
  filter(name %in% names(survey_data)) %>% 
  ## filter to match the column names of your data
  pull(name) # select yn vars
  
## change to dates 
survey_data <- survey_data %>%
  mutate(across(all_of(YNVARS), 
                str_detect, 
                pattern = "yes"))

```



<!-- ======================================================= -->
## Datos de encuestas {#survey-data}

Existen numerosos diseños de muestreo que pueden utilizarse para las encuestas. Aquí mostraremos el código para: 
- Estratificado 
- Conglomerado 
- Estratificado y conglomerado

Como se ha descrito anteriormente (dependiendo de cómo se diseñe el cuestionario) los datos de cada nivel se exportarían como unos datos separados desde Kobo. En nuestro ejemplo hay un nivel para los hogares y un nivel para los individuos dentro de esos hogares.

Estos dos niveles están vinculados por un identificador único. Para unos datos de Kobo, esta variable es "_index" en el nivel del hogar, que coincide con "_parent_index" en el nivel individual. Esto creará nuevas filas para el hogar con cada individuo que coincida, véase la sección del manual sobre [unir datos](#joining-data) para más detalles.

```{r merge_data_levels, eval = FALSE}

## join the individual and household data to form a complete data set
survey_data <- left_join(survey_data_hh, 
                         survey_data_indiv,
                         by = c("_index" = "_parent_index"))


## create a unique identifier by combining indeces of the two levels 
survey_data <- survey_data %>% 
     mutate(uid = str_glue("{index}_{index_y}"))

```

<!-- ======================================================= -->
## Tiempo de observación {#observation-time}

En el caso de las encuestas de mortalidad, queremos saber cuánto tiempo ha estado presente cada individuo en el lugar para poder calcular una tasa de mortalidad adecuada para nuestro periodo de interés. Esto no es relevante para todas las encuestas, pero en particular para las encuestas de mortalidad es importante, ya que se realizan con frecuencia entre poblaciones móviles o desplazadas.

Para ello, primero definimos nuestro periodo de interés, también conocido como periodo de recuerdo (es decir, el tiempo sobre el que se pide a los participantes que informen al responder a las preguntas). A continuación, podemos utilizar este periodo para establecer las fechas inadecuadas como ausentes, es decir, si las muertes se notifican fuera del periodo de interés.

```{r recall_period}

## set the start/end of recall period
## can be changed to date variables from dataset 
## (e.g. arrival date & date questionnaire)
survey_data <- survey_data %>% 
  mutate(recall_start = as.Date("2018-01-01"), 
         recall_end   = as.Date("2018-05-01")
  )


# set inappropriate dates to NA based on rules 
## e.g. arrivals before start, departures departures after end
survey_data <- survey_data %>%
      mutate(
           arrived_date = if_else(arrived_date < recall_start, 
                                 as.Date(NA),
                                  arrived_date),
           birthday_date = if_else(birthday_date < recall_start,
                                  as.Date(NA),
                                  birthday_date),
           left_date = if_else(left_date > recall_end,
                              as.Date(NA),
                               left_date),
           death_date = if_else(death_date > recall_end,
                               as.Date(NA),
                               death_date)
           )

```


Entonces podemos utilizar nuestras variables de fecha para definir las fechas de inicio y fin de cada individuo. Podemos utilizar la función `find_start_date()` de **sitrep** para afinar la elección de las fechas y luego utilizarla para calcular la diferencia entre días (persona-tiempo).

Fecha de inicio: 
Evento de llegada más temprano dentro del período de recogida O bien el inicio del período de recogida (definidas de antemano), o una fecha posterior al inicio de la recogida, si procede (por ejemplo, llegadas o nacimientos)

Fecha de finalización: 
Evento de salida más temprano dentro del periodo de recogida O bien el final del periodo de recogida, o una fecha anterior al final de la recogida si procede (por ejemplo, salidas, fallecimientos)

```{r observation_time}

## create new variables for start and end dates/causes
survey_data <- survey_data %>% 
     ## choose earliest date entered in survey
     ## from births, household arrivals, and camp arrivals 
     find_start_date("birthday_date",
                  "arrived_date",
                  period_start = "recall_start",
                  period_end   = "recall_end",
                  datecol      = "startdate",
                  datereason   = "startcause" 
                 ) %>%
     ## choose earliest date entered in survey
     ## from camp departures, death and end of the study
     find_end_date("left_date",
                "death_date",
                period_start = "recall_start",
                period_end   = "recall_end",
                datecol      = "enddate",
                datereason   = "endcause" 
               )


## label those that were present at the start/end (except births/deaths)
survey_data <- survey_data %>% 
     mutate(
       ## fill in start date to be the beginning of recall period (for those empty) 
       startdate = if_else(is.na(startdate), recall_start, startdate), 
       ## set the start cause to present at start if equal to recall period 
       ## unless it is equal to the birth date 
       startcause = if_else(startdate == recall_start & startcause != "birthday_date",
                              "Present at start", startcause), 
       ## fill in end date to be end of recall period (for those empty) 
       enddate = if_else(is.na(enddate), recall_end, enddate), 
       ## set the end cause to present at end if equall to recall end 
       ## unless it is equal to the death date
       endcause = if_else(enddate == recall_end & endcause != "death_date", 
                            "Present at end", endcause))


## Define observation time in days
survey_data <- survey_data %>% 
  mutate(obstime = as.numeric(enddate - startdate))

```


<!-- ======================================================= -->
## Ponderación {#weighting}

Es importante que elimines las observaciones erróneas antes de añadir los pesos de la encuesta. Por ejemplo, si hay observaciones con tiempo de observación negativo, tendrás que comprobarlas (puedes hacerlo con la función `assert_positive_timespan()` de **sitrep**. Otra cosa es si quieres eliminar las filas vacías (por ejemplo, con `drop_na(uid)`) o eliminar los duplicados (véase la sección del manual sobre [De-duplicación](#de-duplication) para más detalles). También hay que eliminar las que no tienen consentimiento.

En este ejemplo, filtramos los casos que queremos eliminar y los almacenamos en un dataframe separado, de forma que podamos describir los que fueron excluidos de la encuesta. A continuación, utilizamos la función `anti_join()` de **dplyr** para eliminar estos casos descartados de los datos de nuestra encuesta.

<span style="color: red;">***PELIGRO:*** No puede haber valores faltantes en la variable de peso, ni en ninguna de las variables relevantes para el diseño de la encuesta (por ejemplo, edad, sexo, estratos o variables de agrupación).</span>  

```{r remove_unused_data}

## store the cases that you drop so you can describe them (e.g. non-consenting 
## or wrong village/cluster)
dropped <- survey_data %>% 
  filter(!consent | is.na(startdate) | is.na(enddate) | village_name == "other")

## use the dropped cases to remove the unused rows from the survey data set  
survey_data <- anti_join(survey_data, dropped, by = names(dropped))

```

Como se ha mencionado anteriormente, demostramos cómo añadir ponderaciones para tres diseños de estudio diferentes (estratificado, conglomerado y conglomerado estratificado). Estos requieren información sobre la población de origen y/o los conglomerados encuestados. Utilizaremos el código de conglomerado estratificado para este ejemplo, pero utiliza el que sea más apropiado para tu diseño de estudio.

```{r survey_weights}

# stratified ------------------------------------------------------------------
# create a variable called "surv_weight_strata"
# contains weights for each individual - by age group, sex and health district
survey_data <- add_weights_strata(x = survey_data,
                                         p = population,
                                         surv_weight = "surv_weight_strata",
                                         surv_weight_ID = "surv_weight_ID_strata",
                                         age_group, sex, health_district)

## cluster ---------------------------------------------------------------------

# get the number of people of individuals interviewed per household
# adds a variable with counts of the household (parent) index variable
survey_data <- survey_data %>%
  add_count(index, name = "interviewed")


## create cluster weights
survey_data <- add_weights_cluster(x = survey_data,
                                          cl = cluster_counts,
                                          eligible = member_number,
                                          interviewed = interviewed,
                                          cluster_x = village_name,
                                          cluster_cl = cluster,
                                          household_x = index,
                                          household_cl = households,
                                          surv_weight = "surv_weight_cluster",
                                          surv_weight_ID = "surv_weight_ID_cluster",
                                          ignore_cluster = FALSE,
                                          ignore_household = FALSE)


# stratified and cluster ------------------------------------------------------
# create a survey weight for cluster and strata
survey_data <- survey_data %>%
  mutate(surv_weight_cluster_strata = surv_weight_strata * surv_weight_cluster)

```


<!-- ======================================================= -->
## Objetos de diseño de la encuesta {#survey-design-objects}

Crea un objeto de encuesta de acuerdo con el diseño de tu estudio. Se utiliza de la misma manera que los dataframes para calcular las proporciones ponderadas, etc. Asegúrate que todas las variables necesarias están creadas antes de esto.

Hay cuatro opciones, comenta las que no utilizas: 
- Aleatorio simple 
- Estratificado 
- Conglomerado 
- Conglomerado estratificado

Para esta plantilla, supondremos que agrupamos las encuestas en dos estratos distintos (distritos sanitarios A y B). Por lo tanto, para obtener las estimaciones globales necesitamos haber combinado las ponderaciones de los grupos y de los estratos.

Como se ha mencionado anteriormente, hay dos paquetes disponibles para hacer esto. El clásico es **survey** y luego hay un paquete envolvente llamado **srvyr** que hace objetos y funciones amigables con tidyverse. Mostraremos ambos, pero ten en cuenta que la mayor parte del código de este capítulo utilizará objetos basados en **srvyr**. La única excepción es que el paquete **gtsummary** sólo acepta objetos de **survey**.

### Paquete **survey** {.unnumbered}

El paquete **survey** utiliza efectivamente la codificación de R **base**, por lo que no es posible utilizar pipes (`%>%`) u otra sintaxis de **dplyr**. Con el paquete de **survey** utilizamos la función `svydesign()` para definir un objeto de encuesta con clusters, pesos y estratos adecuados.

<span style="color: black;">***NOTA:*** necesitamos utilizar la tilde (`~`) delante de las variables, esto es porque el paquete utiliza la sintaxis de R **base** de asignación de variables basadas en fórmulas.</span>

```{r survey_design}

# simple random ---------------------------------------------------------------
base_survey_design_simple <- svydesign(ids = ~1, # 1 for no cluster ids
                   weights = NULL,               # No weight added
                   strata = NULL,                # sampling was simple (no strata)
                   data = survey_data            # have to specify the dataset
                  )

## stratified ------------------------------------------------------------------
base_survey_design_strata <- svydesign(ids = ~1,  # 1 for no cluster ids
                   weights = ~surv_weight_strata, # weight variable created above
                   strata = ~health_district,     # sampling was stratified by district
                   data = survey_data             # have to specify the dataset
                  )

# cluster ---------------------------------------------------------------------
base_survey_design_cluster <- svydesign(ids = ~village_name, # cluster ids
                   weights = ~surv_weight_cluster, # weight variable created above
                   strata = NULL,                 # sampling was simple (no strata)
                   data = survey_data              # have to specify the dataset
                  )

# stratified cluster ----------------------------------------------------------
base_survey_design <- svydesign(ids = ~village_name,      # cluster ids
                   weights = ~surv_weight_cluster_strata, # weight variable created above
                   strata = ~health_district,             # sampling was stratified by district
                   data = survey_data                     # have to specify the dataset
                  )
```



### Paquete **Srvyr** {.unnumbered}

Con el paquete **srvyr** podemos utilizar la función `as_survey_design()`, que tiene los mismos argumentos que la anterior pero permite los pipes (`%>%`), por lo que no es necesario utilizar la tilde (`~`).

```{r survey_design_srvyr}
## simple random ---------------------------------------------------------------
survey_design_simple <- survey_data %>% 
  as_survey_design(ids = 1, # 1 for no cluster ids 
                   weights = NULL, # No weight added
                   strata = NULL # sampling was simple (no strata)
                  )
## stratified ------------------------------------------------------------------
survey_design_strata <- survey_data %>%
  as_survey_design(ids = 1, # 1 for no cluster ids
                   weights = surv_weight_strata, # weight variable created above
                   strata = health_district # sampling was stratified by district
                  )
## cluster ---------------------------------------------------------------------
survey_design_cluster <- survey_data %>%
  as_survey_design(ids = village_name, # cluster ids
                   weights = surv_weight_cluster, # weight variable created above
                   strata = NULL # sampling was simple (no strata)
                  )

## stratified cluster ----------------------------------------------------------
survey_design <- survey_data %>%
  as_survey_design(ids = village_name, # cluster ids
                   weights = surv_weight_cluster_strata, # weight variable created above
                   strata = health_district # sampling was stratified by district
                  )
```

<!-- ======================================================= -->
## Análisis descriptivo {#descriptive-analysis-2}

El análisis descriptivo básico y la visualización se tratan extensamente en otros capítulos del manual, por lo que no nos detendremos en ellos aquí. Para más detalles, consulta los capítulos sobre [tablas descriptivas](#descriptive-tables), [pruebas estadísticas](#simple-statistical-tests), [tablas para  presentaciones](#tables-for-presentation), [conceptos básicos de ggplot](#ggplot-basics) e [informes con R markdown](#reports-with-r-markdown).

En este apartado nos centraremos en cómo investigar el sesgo de la muestra y visualizarlo. También veremos cómo visualizar el flujo de la población en un entorno de encuesta utilizando diagramas aluviales/sankey.

En general, debes considerar incluir los siguientes análisis descriptivos:

- Número final de agrupaciones, hogares e individuos incluidos
- Número de personas excluidas y motivos de la exclusión
- Mediana (rango) del número de hogares por grupo y de individuos por hogar


### Sesgo de muestreo  {.unnumbered}

Compara las proporciones de cada grupo de edad entre tu muestra y la población de origen. Esto es importante para poder resaltar el posible sesgo de muestreo. También puedes repetir esta operación para ver las distribuciones por sexo.

Ten en cuenta que estos valores-p son sólo indicativos, y que una discusión descriptiva (o la visualización con las pirámides de edad que aparecen a continuación) de las distribuciones en tu muestra de estudio en comparación con la población de origen es más importante que la prueba binomial en sí. Esto se debe a que el aumento del tamaño de la muestra suele dar lugar a diferencias que pueden ser irrelevantes después de ponderar los datos.

```{r descriptive_sampling_bias, warning = FALSE}

## counts and props of the study population
ag <- survey_data %>% 
  group_by(age_group) %>% 
  drop_na(age_group) %>% 
  tally() %>% 
  mutate(proportion = n / sum(n), 
         n_total = sum(n))

## counts and props of the source population
propcount <- population %>% 
  group_by(age_group) %>%
    tally(population) %>%
    mutate(proportion = n / sum(n))

## bind together the columns of two tables, group by age, and perform a 
## binomial test to see if n/total is significantly different from population
## proportion.
  ## suffix here adds to text to the end of columns in each of the two datasets
left_join(ag, propcount, by = "age_group", suffix = c("", "_pop")) %>%
  group_by(age_group) %>%
  ## broom::tidy(binom.test()) makes a data frame out of the binomial test and
  ## will add the variables p.value, parameter, conf.low, conf.high, method, and
  ## alternative. We will only use p.value here. You can include other
  ## columns if you want to report confidence intervals
  mutate(binom = list(broom::tidy(binom.test(n, n_total, proportion_pop)))) %>%
  unnest(cols = c(binom)) %>% # important for expanding the binom.test data frame
  mutate(proportion_pop = proportion_pop * 100) %>%
  ## Adjusting the p-values to correct for false positives 
  ## (because testing multiple age groups). This will only make 
  ## a difference if you have many age categories
  mutate(p.value = p.adjust(p.value, method = "holm")) %>%
                      
  ## Only show p-values over 0.001 (those under report as <0.001)
  mutate(p.value = ifelse(p.value < 0.001, 
                          "<0.001", 
                          as.character(round(p.value, 3)))) %>% 
  
  ## rename the columns appropriately
  select(
    "Age group" = age_group,
    "Study population (n)" = n,
    "Study population (%)" = proportion,
    "Source population (n)" = n_pop,
    "Source population (%)" = proportion_pop,
    "P-value" = p.value
  )
```



### Pirámides demográficas {.unnumbered}

Las pirámides demográficas (o de edad y sexo) son una forma sencilla de visualizar la distribución de la población de la encuesta. También vale la pena considerar la creación de [tablas descriptivas](#descriptive-tables) de edad y sexo por estratos de la encuesta. Demostraremos el uso del paquete **apyramid**, ya que permite las proporciones ponderadas utilizando nuestro objeto de diseño de la encuesta creado anteriormente. Otras opciones para crear [pirámides demográficas](#demographic-pyramids-and-likert-scales) se tratan ampliamente en ese capítulo del manual. También utilizaremos una función envolvente de **sitrep** llamada `age_pyramid()` que ahorra algunas líneas de codificación para producir un gráfico con proporciones.

Al igual que con el test binomial formal de la diferencia, vista anteriormente en la sección de sesgo de muestreo, aquí estamos interesados en visualizar si nuestra población muestreada es sustancialmente diferente de la población de origen y si la ponderación corrige esta diferencia. Para ello, utilizaremos el paquete **patchwork** para mostrar nuestras visualizaciones **ggplot** una al lado de la otra; para más detalles, consulta la sección sobre la combinación de gráficos en el capítulo de [consejos de ggplot](#combine-plots) del manual. Visualizaremos nuestra población de origen, nuestra población de encuesta no ponderada y nuestra población de encuesta ponderada. También puedes considerar la posibilidad de visualizar por cada estrato de tu encuesta - en nuestro ejemplo aquí sería utilizando el argumento `stack_by = "health_district"` (ver `?plot_age_pyramid` para más detalles).

<span style="color: black;">***NOTA:*** Los ejes-x e y están invertidos en las pirámides</span>

```{r weighted_age_pyramid, warning = FALSE, message = FALSE, fig.show = "hold", fig.width = 15}

## define x-axis limits and labels ---------------------------------------------
## (update these numbers to be the values for your graph)
max_prop <- 35      # choose the highest proportion you want to show 
step <- 5           # choose the space you want beween labels 

## this part defines vector using the above numbers with axis breaks
breaks <- c(
    seq(max_prop/100 * -1, 0 - step/100, step/100), 
    0, 
    seq(0 + step / 100, max_prop/100, step/100)
    )

## this part defines vector using the above numbers with axis limits
limits <- c(max_prop/100 * -1, max_prop/100)

## this part defines vector using the above numbers with axis labels
labels <-  c(
      seq(max_prop, step, -step), 
      0, 
      seq(step, max_prop, step)
    )


## create plots individually  --------------------------------------------------

## plot the source population 
## nb: this needs to be collapsed for the overall population (i.e. removing health districts)
source_population <- population %>%
  ## ensure that age and sex are factors
  mutate(age_group = factor(age_group, 
                            levels = c("0-2", 
                                       "3-14", 
                                       "15-29",
                                       "30-44", 
                                       "45+")), 
         sex = factor(sex)) %>% 
  group_by(age_group, sex) %>% 
  ## add the counts for each health district together 
  summarise(population = sum(population)) %>% 
  ## remove the grouping so can calculate overall proportion
  ungroup() %>% 
  mutate(proportion = population / sum(population)) %>% 
  ## plot pyramid 
  age_pyramid(
            age_group = age_group, 
            split_by = sex, 
            count = proportion, 
            proportional = TRUE) +
  ## only show the y axis label (otherwise repeated in all three plots)
  labs(title = "Source population", 
       y = "", 
       x = "Age group (years)") + 
  ## make the x axis the same for all plots 
  scale_y_continuous(breaks = breaks, 
    limits = limits, 
    labels = labels)
  
  
## plot the unweighted sample population 
sample_population <- age_pyramid(survey_data, 
                 age_group = "age_group", 
                 split_by = "sex",
                 proportion = TRUE) + 
  ## only show the x axis label (otherwise repeated in all three plots)
  labs(title = "Unweighted sample population", 
       y = "Proportion (%)", 
       x = "") + 
  ## make the x axis the same for all plots 
  scale_y_continuous(breaks = breaks, 
    limits = limits, 
    labels = labels)


## plot the weighted sample population 
weighted_population <- survey_design %>% 
  ## make sure the variables are factors
  mutate(age_group = factor(age_group), 
         sex = factor(sex)) %>%
  age_pyramid( 
    age_group = "age_group",
    split_by = "sex", 
    proportion = TRUE) +
  ## only show the x axis label (otherwise repeated in all three plots)
  labs(title = "Weighted sample population", 
       y = "", 
       x = "")  + 
  ## make the x axis the same for all plots 
  scale_y_continuous(breaks = breaks, 
    limits = limits, 
    labels = labels)

## combine all three plots  ----------------------------------------------------
## combine three plots next to eachother using + 
source_population + sample_population + weighted_population + 
  ## only show one legend and define theme 
  ## note the use of & for combining theme with plot_layout()
  plot_layout(guides = "collect") & 
  theme(legend.position = "bottom",                    # move legend to bottom
        legend.title = element_blank(),                # remove title
        text = element_text(size = 18),                # change text size
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1) # turn x-axis text
       )
```


### Diagrama aluvial/sankey {.unnumbered}

Visualizar los puntos de partida y los resultados de los individuos puede ser muy útil para obtener una visión general. Su aplicación es bastante obvia en el caso de las poblaciones móviles, pero hay muchas otras aplicaciones, como las cohortes o cualquier otra situación en la que haya transiciones de estados para los individuos. Estos diagramas tienen varios nombres diferentes, como diagramas aluviales, de sankey y paralelos; los detalles se encuentran en el capítulo del manual sobre [diagramas y gráficos](#alluvialsankey-diagrams).


```{r visualise_population_flow}
## summarize data
flow_table <- survey_data %>%
  count(startcause, endcause, sex) %>%  # get counts 
  gather_set_data(x = c("startcause", "endcause"))     # change format for plotting


## plot your dataset 
  ## on the x axis is the start and end causes
  ## gather_set_data generates an ID for each possible combination
  ## splitting by y gives the possible start/end combos
  ## value as n gives it as counts (could also be changed to proportion)
ggplot(flow_table, aes(x, id = id, split = y, value = n)) +
  ## colour lines by sex 
  geom_parallel_sets(aes(fill = sex), alpha = 0.5, axis.width = 0.2) +
  ## fill in the label boxes grey
  geom_parallel_sets_axes(axis.width = 0.15, fill = "grey80", color = "grey80") +
  ## change text colour and angle (needs to be adjusted)
  geom_parallel_sets_labels(color = "black", angle = 0, size = 5) +
  ## remove axis labels
  theme_void()+
  ## move legend to bottom
  theme(legend.position = "bottom")              
```


<!-- ======================================================= -->
## Proporciones ponderadas {#weighted-proportions}

Esta sección detallará cómo producir tablas para recuentos y proporciones ponderadas, con los intervalos de confianza asociados y el efecto del diseño. Hay cuatro opciones diferentes que utilizan funciones de los siguientes paquetes: **survey**, **srvyr**, **sitrep** y **gtsummary**. Para una codificación mínima que produzca una tabla de estilo epidemiológico estándar, recomendaríamos la función sitrep - que es una envoltura para el código **srvyr**; Ten en cuenta, sin embargo, que esto no está todavía en CRAN y puede cambiar en el futuro. Por lo demás, es probable que el código de **survey** sea el más estable a largo plazo, mientras que **srvyr** se adaptará mejor a los flujos de trabajo de tidyverse. Aunque las funciones de **gtsummary** tienen mucho potencial, parecen ser experimentales e incompletas en el momento de escribir este artículo.


### Paquete **survey** {.unnumbered}


Podemos utilizar la función `svyciprop()` de **survey** para obtener las proporciones ponderadas y los correspondientes intervalos de confianza del 95%. Se puede extraer un efecto de diseño apropiado utilizando la función `svymean()` en lugar de `svyprop()`. Cabe señalar que `svyprop()` sólo parece aceptar variables entre 0 y 1 (o TRUE/FALSE), por lo que las variables categóricas no funcionarán.

<span style="color: black;">***NOTA:*** Las funciones de **survey** también aceptan objetos de diseño **srvyr**, pero aquí hemos utilizado el objeto de diseño de **survey** sólo por coherencia</span>


```{r survey_props}

## produce weighted counts 
svytable(~died, base_survey_design)

## produce weighted proportions
svyciprop(~died, base_survey_design, na.rm = T)

## get the design effect 
svymean(~died, base_survey_design, na.rm = T, deff = T) %>% 
  deff()

```

Podemos combinar las funciones de **survey** mostradas arriba en una función que definimos nosotros mismos a continuación, llamada `svy_prop`; y podemos entonces usar esa función junto con `map()` del paquete purrr para iterar sobre varias variables y crear una tabla. Consulta el capítulo de [iteración](#iteration-loops-and-lists) del manual para obtener más detalles sobre **purrr**. 

```{r survey_prop_fun}
# Define function to calculate weighted counts, proportions, CI and design effect
# x is the variable in quotation marks 
# design is your survey design object

svy_prop <- function(design, x) {
  
  ## put the variable of interest in a formula 
  form <- as.formula(paste0( "~" , x))
  ## only keep the TRUE column of counts from svytable
  weighted_counts <- svytable(form, design)[[2]]
  ## calculate proportions (multiply by 100 to get percentages)
  weighted_props <- svyciprop(form, design, na.rm = TRUE) * 100
  ## extract the confidence intervals and multiply to get percentages
  weighted_confint <- confint(weighted_props) * 100
  ## use svymean to calculate design effect and only keep the TRUE column
  design_eff <- deff(svymean(form, design, na.rm = TRUE, deff = TRUE))[[TRUE]]
  
  ## combine in to one data frame
  full_table <- cbind(
    "Variable"        = x,
    "Count"           = weighted_counts,
    "Proportion"      = weighted_props,
    weighted_confint, 
    "Design effect"   = design_eff
    )
  
  ## return table as a dataframe
  full_table <- data.frame(full_table, 
             ## remove the variable names from rows (is a separate column now)
             row.names = NULL)
  
  ## change numerics back to numeric
  full_table[ , 2:6] <- as.numeric(full_table[, 2:6])
  
  ## return dataframe
  full_table
}

## iterate over several variables to create a table 
purrr::map(
  ## define variables of interest
  c("left", "died", "arrived"), 
  ## state function using and arguments for that function (design)
  svy_prop, design = base_survey_design) %>% 
  ## collapse list in to a single data frame
  bind_rows() %>% 
  ## round 
  mutate(across(where(is.numeric), round, digits = 1))

```



### Paquete **Srvyr** {.unnumbered}

Con **srvyr** podemos utilizar la sintaxis de **dplyr** para crear una tabla. Observa que se utiliza la función `survey_mean()` y se especifica el argumento de la proporción, y también que se utiliza la misma función para calcular el efecto del diseño. Esto se debe a que **srvyr** envuelve las dos funciones del paquete **survey**, `svyciprop()` y `svymean()`, que se utilizan en la sección anterior.

<span style="color: black;">***NOTA:*** Tampoco parece posible obtener proporciones a partir de variables categóricas utilizando **srvyr**, si lo necesitas, consulta la sección siguiente utilizando **sitrep **</span>

```{r srvyr_prop}

## use the srvyr design object
survey_design %>% 
  summarise(
    ## produce the weighted counts 
    counts = survey_total(died), 
    ## produce weighted proportions and confidence intervals 
    ## multiply by 100 to get a percentage 
    props = survey_mean(died, 
                        proportion = TRUE, 
                        vartype = "ci") * 100, 
    ## produce the design effect 
    deff = survey_mean(died, deff = TRUE)) %>% 
  ## only keep the rows of interest
  ## (drop standard errors and repeat proportion calculation)
  select(counts, props, props_low, props_upp, deff_deff)

```

Aquí también podríamos escribir una función para iterar sobre múltiples variables utilizando el paquete purrr. Consulta el capítulo de [iteración](#iteration-loops-and-lists) del manual para obtener más detalles sobre **purrr**.

```{r srvyr_prop_fun}

# Define function to calculate weighted counts, proportions, CI and design effect
# design is your survey design object
# x is the variable in quotation marks 


srvyr_prop <- function(design, x) {
  
  summarise(
    ## using the survey design object
    design, 
    ## produce the weighted counts 
    counts = survey_total(.data[[x]]), 
    ## produce weighted proportions and confidence intervals 
    ## multiply by 100 to get a percentage 
    props = survey_mean(.data[[x]], 
                        proportion = TRUE, 
                        vartype = "ci") * 100, 
    ## produce the design effect 
    deff = survey_mean(.data[[x]], deff = TRUE)) %>% 
  ## add in the variable name
  mutate(variable = x) %>% 
  ## only keep the rows of interest
  ## (drop standard errors and repeat proportion calculation)
  select(variable, counts, props, props_low, props_upp, deff_deff)
  
}
  

## iterate over several variables to create a table 
purrr::map(
  ## define variables of interest
  c("left", "died", "arrived"), 
  ## state function using and arguments for that function (design)
  ~srvyr_prop(.x, design = survey_design)) %>% 
  ## collapse list in to a single data frame
  bind_rows()
  

```



### Paquete **Sitrep** {.unnumbered}

La función `tab_survey()` de **sitrep** es una envoltura para **srvyr**, que permite crear tablas ponderadas con una codificación mínima. También permite calcular proporciones ponderadas para variables categóricas.

```{r sitrep_props}

## using the survey design object
survey_design %>% 
  ## pass the names of variables of interest unquoted
  tab_survey(arrived, left, died, education_level,
             deff = TRUE,   # calculate the design effect
             pretty = TRUE  # merge the proportion and 95%CI
             )

```



### Paquete **Gtsummary** {.unnumbered}

Con **gtsummary** no parece haber todavía funciones incorporadas para añadir intervalos de confianza o efecto de diseño. Aquí mostramos cómo definir una función para añadir intervalos de confianza y luego añadir intervalos de confianza a una tabla gtsummary creada con la función `tbl_svysummary()`.


```{r gtsummary_table}


confidence_intervals <- function(data, variable, by, ...) {
  
  ## extract the confidence intervals and multiply to get percentages
  props <- svyciprop(as.formula(paste0( "~" , variable)),
              data, na.rm = TRUE)
  
  ## extract the confidence intervals 
  as.numeric(confint(props) * 100) %>% ## make numeric and multiply for percentage
    round(., digits = 1) %>%           ## round to one digit
    c(.) %>%                           ## extract the numbers from matrix
    paste0(., collapse = "-")          ## combine to single character
}

## using the survey package design object
tbl_svysummary(base_survey_design, 
               include = c(arrived, left, died),   ## define variables want to include
               statistic = list(everything() ~ c("{n} ({p}%)"))) %>% ## define stats of interest
  add_n() %>%  ## add the weighted total 
  add_stat(fns = everything() ~ confidence_intervals) %>% ## add CIs
  ## modify the column headers
  modify_header(
    list(
      n ~ "**Weighted total (N)**",
      stat_0 ~ "**Weighted Count**",
      add_stat_1 ~ "**95%CI**"
    )
    )

```



<!-- ======================================================= -->
## Razones ponderadas {#weighted-ratios}

Del mismo modo, para los ratios ponderados (como los ratios de mortalidad) puedes utilizar el paquete **survey** o **srvyr**. También se pueden escribir funciones (similares a las anteriores) para iterar sobre varias variables. También se podría crear una función para **gtsummary** como la anterior, pero actualmente no tiene una funcionalidad incorporada.


### Paquete **survey** {.unnumbered}

```{r survey_ratio}

ratio <- svyratio(~died, 
         denominator = ~obstime, 
         design = base_survey_design)

ci <- confint(ratio)

cbind(
  ratio$ratio * 10000, 
  ci * 10000
)

```


### Paquete **Srvyr** {.unnumbered}

```{r srvyr_ratio}

survey_design %>% 
  ## survey ratio used to account for observation time 
  summarise(
    mortality = survey_ratio(
      as.numeric(died) * 10000, 
      obstime, 
      vartype = "ci")
    )

```




<!-- ======================================================= -->
## Recursos {#resources-18}

[Página de estadísticas de la UCLA](https://stats.idre.ucla.edu/r/seminars/survey-data-analysis-with-r/)

[Analizar datos de encuestas gratis](http://asdfree.com/)

[srvyr packge](http://gdfe.co/srvyr/)

[paquete gtsummary](http://www.danieldsjoberg.com/gtsummary/reference/index.html)

[Estudios de caso de la encuesta EPIET](https://github.com/EPIET/RapidAssessmentSurveys)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/survey_analysis.Rmd-->


<!-- ======================================================= -->
<!-- ======================================================= -->
<!-- ======================================================= -->
# Análisis de supervivencia {#survival-analysis}


```{r out.width = c('75%'), fig.align='center', fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "survival_analysis.png"))
```

<!-- ======================================================= -->

## Resumen {#overview-5}

El *análisis de supervivencia* se centra en la descripción, para un individuo o grupo de individuos determinado, de un acontecimiento puntual denominado **_evento_** (aparición de una enfermedad, curación de una enfermedad, muerte, recaída tras la respuesta al tratamiento...) que se produce tras un periodo de tiempo denominado **_tiempo del evento_** (o  **_tiempo de seguimiento_** (tiempo de seguimiento en los estudios basados en cohortes/poblaciones) durante el cual se observa a los individuos. Para determinar el tiempo de fracaso, es necesario definir un tiempo de origen (que puede ser la fecha de inclusión, la fecha de diagnóstico...).

El objetivo de la inferencia para el análisis de supervivencia es entonces el tiempo entre un origen y un evento. En la investigación médica actual, se utiliza ampliamente en los estudios clínicos para evaluar el efecto de un tratamiento, por ejemplo, o en la epidemiología del cáncer para evaluar una gran variedad de medidas de supervivencia del cáncer.

Suele expresarse mediante la ***probabilidad de supervivencia*** (survival probability), que es la probabilidad de que el suceso de interés no haya ocurrido en una duración t.

***Censura***: La censura se produce cuando al final del seguimiento, algunos de los individuos no han tenido el evento de interés, y por lo tanto su verdadero tiempo hasta el evento es desconocido. Aquí nos centraremos principalmente en la censura derecha, pero para más detalles sobre la censura y el análisis de supervivencia en general, puedes consultar las referencias.


```{r echo=F, eval=F, out.width = "80%", out.height="80%", fig.align = "center"}
 
#Add a figure from the following chunks for the last version of the page
#do not forget to save the output figure in "images"
# knitr::include_graphics(here::here("images", "survanalysis.png"))

```  

<!-- ======================================================= -->
## Preparación {#preparation-18}

### Cargar paquetes {.unnumbered}  

Para realizar análisis de supervivencia en R, uno de los paquetes más utilizados es el de **survival**. Primero lo instalamos y luego lo cargamos, así como los demás paquetes que se utilizarán en esta sección:

En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r, echo=F, message=FALSE, warning=FALSE}

# install/load the different packages needed for this page
pacman::p_load(
  survival,      # survival analysis 
  survminer,     # survival analysis
  rio,           # importing data  
  here,          # relative file pathways  
  janitor,       # tabulations
  SemiCompRisks, # dataset examples and advanced tools for working with Semi-Competing Risks data
  tidyverse,     # data manipulation and visualization
  Epi,           # stat analyses in Epi
  survival,      # survival analysis
  survminer      # survival analysis: advanced KM curves
)


```


Esta página explora los análisis de supervivencia sobre el archivo `linelist` utilizado en la mayoría de las páginas anteriores y sobre el que aplicamos algunos cambios para tener unos datos de supervivencia adecuados.


### Importar los datos {.unnumbered}  

Importamos los datos de casos de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica para descargar linelist "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa los datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - Mira la página de [importación y exportación](#import-and-export) para más detalles).

```{r echo=F}
# import linelist
linelist_case_data <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r eval=F}
# import linelist
linelist_case_data <- rio::import("linelist_cleaned.rds")
```

### Gestión y transformación de datos {.unnumbered}

En resumen, los datos de supervivencia pueden describirse con las tres características siguientes:

1)  la variable dependiente o respuesta es el tiempo de espera hasta la ocurrencia de un evento bien definido,
2)  observaciones censuradas, en el sentido de que para algunas unidades el evento de interés no ha ocurrido en el momento en que se analizan los datos, y
3.  existen predictores o variables explicativas cuyo efecto sobre el tiempo de espera queremos evaluar o controlar.

Así, crearemos las diferentes variables necesarias para respetar esa estructura y ejecutaremos el análisis de supervivencia.

Definiremos:

- un nuevo dataframe `linelist_surv` para este análisis
- nuestro evento de interés como "death" (por lo tanto, nuestra probabilidad de supervivencia será la probabilidad de estar vivo después de un cierto tiempo después del momento de origen),
- el tiempo de seguimiento (`futime`) como el tiempo transcurrido entre el momento del inicio y el momento del desenlace *en días*,
- pacientes censurados son aquellos que se recuperaron o para los que no se conoce el resultado final, es decir, no se observó el evento "muerte" (`evento=0`).


<span style="color: orange;">***PRECAUCIÓN:*** Dado que en un estudio de cohortes real, la información sobre el momento de origen y el final del seguimiento se conoce dado que los individuos son observados, eliminaremos las observaciones en las que se desconozca la fecha de inicio o la fecha de desenlace. También se eliminarán los casos en los que la fecha de inicio sea posterior a la fecha de desenlace, ya que se consideran erróneos.</span>

<span style="color: darkgreen;">***CONSEJO:*** Dado que el filtrado a mayor (>) o menor (<) de una fecha puede eliminar las filas con valores faltantes, la aplicación del filtro en las fechas incorrectas también eliminará las filas con fechas faltantes.</span>

A continuación, utilizamos `case_when()` para crear una `columna age_cat_small` en la que sólo hay 3 categorías de edad.

```{r }
#create a new data called linelist_surv from the linelist_case_data

linelist_surv <-  linelist_case_data %>% 
     
  dplyr::filter(
       # remove observations with wrong or missing dates of onset or date of outcome
       date_outcome > date_onset) %>% 
  
  dplyr::mutate(
       # create the event var which is 1 if the patient died and 0 if he was right censored
       event = ifelse(is.na(outcome) | outcome == "Recover", 0, 1), 
    
       # create the var on the follow-up time in days
       futime = as.double(date_outcome - date_onset), 
    
       # create a new age category variable with only 3 strata levels
       age_cat_small = dplyr::case_when( 
            age_years < 5  ~ "0-4",
            age_years >= 5 & age_years < 20 ~ "5-19",
            age_years >= 20   ~ "20+"),
       
       # previous step created age_cat_small var as character.
       # now convert it to factor and specify the levels.
       # Note that the NA values remain NA's and are not put in a level "unknown" for example,
       # since in the next analyses they have to be removed.
       age_cat_small = fct_relevel(age_cat_small, "0-4", "5-19", "20+")
       )
```


<span style="color: darkgreen;">***CONSEJO:*** Podemos verificar las nuevas columnas que hemos creado haciendo un resumen sobre  `futime` y una tabulación cruzada entre `event` y `outcome` a partir del cual se ha creado. Además de esta verificación, es un buen hábito comunicar la mediana del tiempo de seguimiento al interpretar los resultados del análisis de supervivencia.</span>

```{r }

summary(linelist_surv$futime)

# cross tabulate the new event var and the outcome var from which it was created
# to make sure the code did what it was intended to
linelist_surv %>% 
  tabyl(outcome, event)
```

Ahora cruzamos la nueva var de `age_cat_small` y la antigua columna `age_cat` para asegurarnos de que las asignaciones son correctas  

```{r}
linelist_surv %>% 
  tabyl(age_cat_small, age_cat)
```

Ahora revisamos las 10 primeras observaciones de los datos de `linelist_surv` mirando las variables específicas (incluyendo las de nueva creación). 


```{r}
linelist_surv %>% 
  select(case_id, age_cat_small, date_onset, date_outcome, outcome, event, futime) %>% 
  head(10)
```

También podemos cruzar las columnas `age_cat_small` y `gender` para tener más detalles sobre la distribución de esta nueva columna por género. Utilizamos `tabyl()` y las funciones de *adorno* de **janitor** como se describe en la página de [tablas descriptivas](#descriptive-tables).

<!-- For this we use the `stat.table()` function of the **Epi** package. -->

```{r}

linelist_surv %>% 
  tabyl(gender, age_cat_small, show_na = F) %>% 
  adorn_totals(where = "both") %>% 
  adorn_percentages() %>% 
  adorn_pct_formatting() %>% 
  adorn_ns(position = "front")

```

<!-- Epi::stat.table(  -->
<!--   #give variables for the cross tabulation -->
<!--   list( -->
<!--     gender,  -->
<!--     age_cat_small -->
<!--     ), -->

<!--   #precise the function you want to call (mean,count..) -->
<!--   list(  -->
<!--     count(), -->
<!--     percent(age_cat_small) -->
<!--     ),  -->

<!--   #add margins -->
<!--   margins=T,  -->

<!--   #data used -->
<!--   data = linelist_surv  -->
<!--   ) -->

<!-- ``` -->


<!-- ======================================================= -->
## Fundamentos del análisis de supervivencia {#basics-of-survival-analysis}


### Construir un objeto de tipo surv-type {.unnumbered}

Primero utilizaremos `Surv()` de **survival** para construir un objeto de supervivencia a partir de las columnas de tiempo de seguimiento y evento.

El resultado de este paso es producir un objeto de tipo *Surv* que condensa la información de tiempo y si fue observado el evento de interés (muerte). Este objeto se utilizará en última instancia en el lado derecho de las fórmulas posteriores del modelo ([véase la documentación](https://cran.r-project.org/web/packages/survival/vignettes/survival.pdf)). 


```{r survobj }
# Use Suv() syntax for right-censored data
survobj <- Surv(time = linelist_surv$futime,
                event = linelist_surv$event)
```

<!-- ```{r} -->
<!-- survobj <- with(linelist_surv, -->

<!--                 survival::Surv(futime, event) -->

<!--                 ) -->
<!-- ``` -->


Puedes revisar las primeras 10 filas de los datos de `linelist_surv`, viendo sólo algunas columnas importantes.  

```{r}
linelist_surv %>% 
  select(case_id, date_onset, date_outcome, futime, outcome, event) %>% 
  head(10)
```

Y aquí están los primeros 10 elementos de `survobj`. Se imprime esencialmente como un vector de tiempo de seguimiento, con "+" a la derecha para representar si una observación fue censurada. Mira cómo los números se alinean arriba y abajo.  

```{r}
#print the 50 first elements of the vector to see how it presents
head(survobj, 10)
```


### Realización de los primeros análisis {.unnumbered}

A continuación, iniciamos nuestro análisis utilizando la función `survfit()` para producir un *objeto survfit*, que se ajusta a los cálculos por defecto para las estimaciones de ***Kaplan Meier*** (KM) de la curva de supervivencia global (marginal), que son de hecho una función escalonada con saltos en los tiempos de los eventos observados. El *objeto survfit* final contiene una o más curvas de supervivencia y se crea utilizando el objeto *Surv* como variable de respuesta en la fórmula del modelo.

<span style="color: black;">***NOTA:*** La estimación de Kaplan-Meier es una estimación no paramétrica de máxima verosimilitud (MLE) de la función de supervivencia. (ver recursos para más información).</span>

El resumen de este *objeto survfit* dará lo que se llama una *tabla de vida*. Para cada paso de tiempo de seguimiento (time) en el que ocurrió un evento (en orden ascendente):

* el número de personas que estaban en riesgo de desarrollar el evento (personas que aún no tenían el evento ni estaban censuradas: (`n.risk`)
* los que sí desarrollaron el evento (`n.event`)
* y de lo anterior: la probabilidad de *no* desarrollar el evento (probabilidad de no morir, o de sobrevivir más allá de ese tiempo específico)
* por último, se obtienen y muestran el error estándar y el intervalo de confianza de esa probabilidad

Ajustamos las estimaciones de KM mediante la fórmula en la que el objeto Surv "survobj" anterior es la variable de respuesta. "~ 1" precisa que ejecutamos el modelo para la supervivencia global.

```{r fit}
# fit the KM estimates using a formula where the Surv object "survobj" is the response variable.
# "~ 1" signifies that we run the model for the overall survival  
linelistsurv_fit <-  survival::survfit(survobj ~ 1)

#print its summary for more details
summary(linelistsurv_fit)

```


Al utilizar `summary()` podemos añadir la opción `times` y especificar ciertos tiempos en los que queremos ver la información de supervivencia

```{r print_spec_times}

#print its summary at specific times
summary(linelistsurv_fit, times = c(5,10,20,30,60))

```


También podemos utilizar la función `print()`. El argumento `print.rmean = TRUE` se utiliza para obtener el tiempo medio de supervivencia y su error estándar (se).

***NOTA:*** El tiempo medio de supervivencia restringido (RMST) es una medida de supervivencia específica cada vez más utilizada en el análisis de supervivencia del cáncer y que suele definirse como el área bajo la curva de supervivencia, dado que observamos a los pacientes hasta el tiempo restringido T (más detalles en la sección Recursos).


```{r, mean_survtime}
# print linelistsurv_fit object with mean survival time and its se. 
print(linelistsurv_fit, print.rmean = TRUE)

```


<span style="color: darkgreen;">***CONSEJO:*** Podemos crear el *objeto surv* directamente en la función `survfit()` y ahorrarnos una línea de código. Esto se verá como: `linelistsurv_quick <- survfit(Surv(futime, event) ~ 1, data=linelist_surv)`.</span>


### Riesgo acumulado {.unnumbered}  

Además de la función `summary()`, también podemos utilizar la función `str()` que da más detalles sobre la estructura del objeto `survfit()`. Se trata de una lista de 16 elementos.

Entre estos elementos hay uno importante: el `cumhaz`, que es un vector numérico. Se puede trazar para mostrar el ***riesgo acumulado,*** siendo el ***riesgo*** la ***tasa instantánea de ocurrencia del evento*** (ver referencias).

```{r fit_struct}

str(linelistsurv_fit)

```

<!-- ======================================================= -->
### Representar curvas de Kaplan-Meir  {.unnumbered}

Una vez ajustadas las estimaciones de KM, podemos visualizar la probabilidad de estar vivo a lo largo de un tiempo determinado utilizando la función básica `plot()` que dibuja la "curva de Kaplan-Meier". En otras palabras, la curva de abajo es una ilustración convencional de la experiencia de supervivencia en todo el grupo de pacientes.

Podemos verificar rápidamente el tiempo de seguimiento mínimo y máximo en la curva.

Una forma fácil de interpretarlo es decir que en el momento cero, todos los participantes están vivos y la probabilidad de supervivencia es entonces del 100%. Esta probabilidad disminuye con el tiempo a medida que los pacientes mueren. La proporción de participantes que sobreviven más allá de los 60 días de seguimiento se sitúa en torno al 40%.

```{r }

plot(linelistsurv_fit, 
     xlab = "Days of follow-up",    # x-axis label
     ylab="Survival Probability",   # y-axis label
     main= "Overall survival curve" # figure title
     )

```

El intervalo de confianza de las estimaciones de supervivencia de KM también se representa por defecto y puede descartarse añadiendo la opción `conf.int = FALSE` al comando `plot()`.

Dado que el evento de interés es "death", dibujar una curva que describa los complementos de las proporciones de supervivencia llevará a dibujar las proporciones de mortalidad acumulada. Esto puede hacerse con `lines()`, que añade información a un gráfico existente.  


```{r}

# original plot
plot(
  linelistsurv_fit,
  xlab = "Days of follow-up",       
  ylab = "Survival Probability",       
  mark.time = TRUE,              # mark events on the curve: a "+" is printed at every event
  conf.int = FALSE,              # do not plot the confidence interval
  main = "Overall survival curve and cumulative mortality"
  )

# draw an additional curve to the previous plot
lines(
  linelistsurv_fit,
  lty = 3,             # use different line type for clarity
  fun = "event",       # draw the cumulative events instead of the survival 
  mark.time = FALSE,
  conf.int = FALSE
  )

# add a legend to the plot
legend(
  "topright",                               # position of legend
  legend = c("Survival", "Cum. Mortality"), # legend text 
  lty = c(1, 3),                            # line types to use in the legend
  cex = .85,                                # parametes that defines size of legend text
  bty = "n"                                 # no box type to be drawn for the legend
  )

```

<!-- ======================================================= -->
## Comparación de las curvas de supervivencia {#comparison-of-survival-curves}

Para comparar la supervivencia dentro de los diferentes grupos de nuestros participantes o pacientes observados, es posible que tengamos que observar primero sus respectivas curvas de supervivencia y luego realizar pruebas para evaluar la diferencia entre grupos independientes. Esta comparación puede referirse a grupos basados en el género, la edad, el tratamiento, la comorbilidad...

### Test Log rank {.unnumbered}

El test Log rank (de rango logarítmico) es una prueba popular que compara toda la experiencia de supervivencia entre dos o más grupos *independientes* y puede considerarse como una prueba de si las curvas de supervivencia son idénticas (se superponen) o no (hipótesis nula de no diferencia de supervivencia entre los grupos). La función `survdiff()` del paquete **survival** permite ejecutar el test Log rank cuando especificamos `rho = 0` (que es el valor predeterminado). Los resultados de la prueba dan un estadístico chi-cuadrado junto con un valor-p, ya que el estadístico log rank se distribuye aproximadamente como un test estadístico de chi-cuadrado.

En primer lugar, tratamos de comparar las curvas de supervivencia por grupos de género. Para ello, primero intentamos visualizarlo (comprobar si las dos curvas de supervivencia se superponen). Se creará un nuevo *objeto survfit* con una fórmula ligeramente diferente. Luego se creará el *objeto survdiff*.

Al suministrar ` ~ gender` como lado derecho de la fórmula, ya no trazamos la supervivencia global sino por género.


```{r comp_surv, warning=FALSE}

# create the new survfit object based on gender
linelistsurv_fit_sex <-  survfit(Surv(futime, event) ~ gender, data = linelist_surv)
```

Ahora podemos trazar las curvas de supervivencia por género. Observa el *orden* de los niveles de los estratos en la columna de género antes de definir los colores y la leyenda.

```{r}
# set colors
col_sex <- c("lightgreen", "darkgreen")

# create plot
plot(
  linelistsurv_fit_sex,
  col = col_sex,
  xlab = "Days of follow-up",
  ylab = "Survival Probability")

# add legend
legend(
  "topright",
  legend = c("Female","Male"),
  col = col_sex,
  lty = 1,
  cex = .9,
  bty = "n")
```

Y ahora podemos calcular la prueba de la diferencia entre las curvas de supervivencia utilizando `survdiff()`

```{r}
#compute the test of the difference between the survival curves
survival::survdiff(
  Surv(futime, event) ~ gender, 
  data = linelist_surv
  )

```

Vemos que la curva de supervivencia de las mujeres y la de los hombres se superponen y la prueba de rango logarítmico no da pruebas de una diferencia de supervivencia entre mujeres y hombres.

Algunos otros paquetes de R permiten ilustrar curvas de supervivencia para diferentes grupos y probar la diferencia de una sola vez. Utilizando la función `ggsurvplot()` del paquete **survminer**, también podemos incluir en nuestra curva las tablas de riesgo impresas para cada grupo, así como el valor p del test log-rank.

<span style="color: orange;">***PRECAUCIÓN:*** las funciones de **survminer** requieren que especifiques el objeto de supervivencia *y* que vuelvas a especificar los datos utilizados para ajustar el objeto de supervivencia. Recuerda hacer esto para evitar mensajes de error no específicos. </span>

```{r, warning=F, message=F}

survminer::ggsurvplot(
    linelistsurv_fit_sex, 
    data = linelist_surv,          # again specify the data used to fit linelistsurv_fit_sex 
    conf.int = FALSE,              # do not show confidence interval of KM estimates
    surv.scale = "percent",        # present probabilities in the y axis in %
    break.time.by = 10,            # present the time axis with an increment of 10 days
    xlab = "Follow-up days",
    ylab = "Survival Probability",
    pval = T,                      # print p-value of Log-rank test 
    pval.coord = c(40,.91),        # print p-value at these plot coordinates
    risk.table = T,                # print the risk table at bottom 
    legend.title = "Gender",       # legend characteristics
    legend.labs = c("Female","Male"),
    font.legend = 10, 
    palette = "Dark2",             # specify color palette 
    surv.median.line = "hv",       # draw horizontal and vertical lines to the median survivals
    ggtheme = theme_light()        # simplify plot background
)

```


También podemos comprobar si hay diferencias en la supervivencia según la fuente de infección (fuente de contaminación).

En este caso, la prueba de rango logarítmico da pruebas suficientes de una diferencia en las probabilidades de supervivencia a `alfa= 0,005`. Las probabilidades de supervivencia de los pacientes que se infectaron en los funerales son mayores que las de los pacientes que se infectaron en otros lugares, lo que sugiere un beneficio para la supervivencia.

```{r}

linelistsurv_fit_source <-  survfit(
  Surv(futime, event) ~ source,
  data = linelist_surv
  )

# plot
ggsurvplot( 
  linelistsurv_fit_source,
  data = linelist_surv,
  size = 1, linetype = "strata",   # line types
  conf.int = T,
  surv.scale = "percent",  
  break.time.by = 10, 
  xlab = "Follow-up days",
  ylab= "Survival Probability",
  pval = T,
  pval.coord = c(40,.91),
  risk.table = T,
  legend.title = "Source of \ninfection",
  legend.labs = c("Funeral", "Other"),
  font.legend = 10,
  palette = c("#E7B800","#3E606F"),
  surv.median.line = "hv", 
  ggtheme = theme_light()
)

```

<!-- ======================================================= -->
## Análisis de regresión de Cox {#cox-regression-analysis}

La regresión de riesgos proporcionales de Cox es una de las técnicas de regresión más populares para el análisis de supervivencia. También se pueden utilizar otros modelos, ya que el modelo de Cox requiere *supuestos importantes* que deben verificarse para un uso adecuado, como el supuesto de riesgos proporcionales: véanse las referencias.

En un modelo de regresión de riesgos proporcionales de Cox, la medida del efecto es la tasa de ***riesgo (***HR), que es el riesgo de fracaso (o el riesgo de muerte en nuestro ejemplo), dado que el participante ha sobrevivido hasta un momento específico. Normalmente, nos interesa comparar grupos *independientes* con respecto a sus riesgos, y utilizamos una razón de riesgo, que es análoga a una razón de probabilidades en el entorno del análisis de regresión logística múltiple. La función `cox.ph()` del paquete de supervivencia se utiliza para ajustar el modelo. La función `cox.zph()` del paquete **survival** puede utilizarse para probar la suposición de riesgos proporcionales para un ajuste del modelo de regresión de Cox.

<span style="color: black;">***NOTA:*** Una probabilidad debe estar en el rango de 0 a 1. Sin embargo, el peligro representa el número esperado de eventos por una unidad de tiempo.

* Si la razón de riesgo (RR)de un predictor es cercana a 1, entonces ese predictor no afecta a la supervivencia,
* Si la RR es inferior a 1, entonces el predictor es protector (es decir, está asociado a una mejor supervivencia),
* y si la RR es mayor que 1, entonces el predictor se asocia a un mayor riesgo (o a una menor supervivencia).</span> 

### Ajuste de un modelo de Cox {.unnumbered}

Primero podemos ajustar un modelo para evaluar el efecto de la edad y el sexo en la supervivencia. Con sólo imprimir el modelo, tenemos la información sobre:

  + los coeficientes de regresión estimados `coef` que cuantifican la asociación entre los predictores y el resultado,
  + su exponencial (para su interpretación, `exp(coef)`) que produce la *razón de riesgo*,
  + su error estándar `se(coef)`,
  + la puntuación z: cuántos errores estándar se aleja el coeficiente estimado de 0,
  + y el valor- p: la probabilidad de que el coeficiente estimado sea 0.

La función `summary()` aplicada al objeto del modelo de Cox ofrece más información, como el intervalo de confianza de la RR estimada y las diferentes puntuaciones de la prueba.

El efecto de la primera covariable, `gender`, se presenta en la primera fila. Se imprime `genderm` (masculino), lo que implica que el primer nivel de estrato ("f"), es decir, el grupo femenino, es el grupo de referencia para el género. Por lo tanto, la interpretación del parámetro de la prueba es la de los hombres en comparación con las mujeres. El valor p indica que no hay pruebas suficientes de un efecto del género sobre el peligro esperado o de una asociación entre el género y la mortalidad por todas las causas.

La misma falta de pruebas se observa en relación con el grupo de edad.

```{r coxmodel_agesex}

#fitting the cox model
linelistsurv_cox_sexage <-  survival::coxph(
              Surv(futime, event) ~ gender + age_cat_small, 
              data = linelist_surv
              )


#printing the model fitted
linelistsurv_cox_sexage


#summary of the model
summary(linelistsurv_cox_sexage)

```


Fue interesante ejecutar el modelo y observar los resultados, pero un primer vistazo para verificar si se respetan los supuestos de riesgos proporcionales podría ayudar a ahorrar tiempo.

```{r test_assumption}

test_ph_sexage <- survival::cox.zph(linelistsurv_cox_sexage)
test_ph_sexage

```


<span style="color: black;">***NOTA:*** Se puede especificar un segundo argumento llamado *método* cuando se calcula el modelo de Cox, que determina cómo se manejan los empates. El *valor por defecto* es "efron", y las otras opciones son "breslow" y "exact".</span>

En otro modelo añadimos más factores de riesgo, como el origen de la infección y el número de días entre la fecha de inicio y el ingreso. Esta vez, primero verificamos la hipótesis de riesgos proporcionales antes de seguir adelante.

En este modelo, hemos incluido un predictor continuo (`days_onset_hosp`). En este caso, interpretamos las estimaciones de los parámetros como el aumento del logaritmo esperado del riesgo relativo por cada aumento de una unidad en el predictor, manteniendo los demás predictores constantes. Primero verificamos el supuesto de riesgos proporcionales.

```{r coxmodel_fit_ph,  message=FALSE}

#fit the model
linelistsurv_cox <-  coxph(
                        Surv(futime, event) ~ gender + age_years+ source + days_onset_hosp,
                        data = linelist_surv
                        )


#test the proportional hazard model
linelistsurv_ph_test <- cox.zph(linelistsurv_cox)
linelistsurv_ph_test
```

La verificación gráfica de esta suposición puede realizarse con la función `ggcoxzph()` del paquete **survminer**.

```{r}
survminer::ggcoxzph(linelistsurv_ph_test)

```


Los resultados del modelo indican que existe una asociación negativa entre la duración del inicio del ingreso y la mortalidad por todas las causas. El riesgo esperado es 0,9 veces menor en una persona que ingresa un día más tarde que otra, manteniendo el género constante. O, en una explicación más directa, un aumento de una unidad en la duración del inicio al ingreso se asocia con una disminución del 10,7% (`coef *100`) en el riesgo de muerte.

Los resultados muestran también una asociación positiva entre la fuente de infección y la mortalidad por todas las causas. Es decir, hay un mayor riesgo de muerte (1,21 veces) para los pacientes que tuvieron una fuente de infección distinta de los funerales.


```{r coxmodel_summary,  message=FALSE}

#print the summary of the model
summary(linelistsurv_cox)

```


Podemos comprobar esta relación con una tabla:


```{r}
linelist_case_data %>% 
  tabyl(days_onset_hosp, outcome) %>% 
  adorn_percentages() %>%  
  adorn_pct_formatting()

```


Habría que considerar e investigar por qué existe esta asociación en los datos. Una posible explicación podría ser que los pacientes que viven lo suficiente como para ser ingresados más tarde tenían una enfermedad menos grave para empezar. Otra explicación, quizá más probable, es que, dado que utilizamos unos datos falsos simulados, este patrón no refleja la realidad.


<!-- ======================================================= -->

### Forest plots {.unnumbered}

A continuación, podemos visualizar los resultados del modelo cox utilizando los prácticos gráficos de bosque con la función `ggforest()` del **paquete survminer**.

```{r forestp}

ggforest(linelistsurv_cox, data = linelist_surv)

```

<!-- ======================================================= -->
## Covariables tiempo-dependientes en modelos de supervivencia {#time-dependent-covariates-in-survival-models}

Algunas de las siguientes secciones han sido adaptadas con permiso de la excelente [introducción al análisis de supervivencia en R](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html) por [la Dra. Emily Zabor](https://www.emilyzabor.com/)

En la última sección hemos tratado el uso de la regresión de Cox para examinar las asociaciones entre las covariables de interés y los resultados de supervivencia, pero estos análisis dependen de que la covariable se mida en la línea de base, es decir, antes de que comience el tiempo de seguimiento del evento.

¿Qué ocurre si tienes interés en una covariable que se mide **después** del tiempo de seguimiento? O, ¿qué pasa si tienes una covariable que puede cambiar con el tiempo?

Por ejemplo, tal vez estés trabajando con datos clínicos en los que se repiten medidas de valores de laboratorio del hospital que pueden cambiar con el tiempo. Este es un ejemplo de una **covariable dependiente del tiempo**. Para abordar esto se necesita una configuración especial, pero afortunadamente el modelo de Cox es muy flexible y este tipo de datos también puede ser modelado con herramientas del paquete **survival**.

### Configuración de covariables dependientes del tiempo {.unnumbered} 

El análisis de covariables dependientes del tiempo en R requiere la configuración de unos datos especial. Si tienes interés, mira el documento del autor del paquete **survival** [Using Time Dependent Covariates and Time Dependent Coefficients in the Cox Model](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf).

Para ello, utilizaremos un nuevo conjunto de datos del paquete **SemiCompRisks** denominado `BMT`, que incluye datos de 137 pacientes de trasplante de médula ósea. Las variables en las que nos centraremos son:

* `T1` - tiempo (en días) hasta la muerte o el último seguimiento
* `delta1` - indicador de muerte; 1-Muerto, 0-Vivo
* `TA` - tiempo (en días) hasta la enfermedad aguda de injerto contra huésped
* `deltaA` - indicador de la enfermedad aguda de injerto contra huésped;
  * 1 - Desarrolló la enfermedad aguda de injerto contra huésped
  * 0 - Nunca desarrolló la enfermedad aguda de injerto contra huésped

Cargaremos este conjunto de datos del paquete **survival** utilizando el comando DE R **base** `data()`, que puede utilizarse para cargar datos que ya están incluidos en un paquete de R que se ha cargado. El dataframe `BMT` aparecerá en tu entorno de R.

```{r}
data(BMT, package = "SemiCompRisks")
```

#### Añadir un identificador único de paciente {.unnumbered}  

No hay una columna de identificación única en los datos de `BMT`, que es necesaria para crear el tipo de conjunto de datos que queremos. Así que utilizamos la función `rowid_to_column()` del paquete **tibble** de **tidyverse** para crear una nueva columna de identificación llamada `my_id` (añade una columna al principio del dataframe con identificadores de fila secuenciales, empezando por el 1). Llamamos al dataframe `bmt`.

```{r}
bmt <- rowid_to_column(BMT, "my_id")
```

El conjunto de datos tiene ahora este aspecto:

```{r message=FALSE, echo=F}
DT::datatable(bmt, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Ampliar las filas de pacientes {.unnumbered}  

A continuación, utilizaremos la función `tmerge()` con las funciones de ayuda `event()` y `tdc()` para crear el conjunto de datos reestructurado. Nuestro objetivo es reestructurar el conjunto de datos para crear una fila separada para cada paciente por cada intervalo de tiempo en el que tengan un valor diferente de `deltaA`. En este caso, cada paciente puede tener como máximo dos filas dependiendo de si desarrollaron la enfermedad aguda de injerto contra huésped durante el periodo de recogida de datos. Llamaremos a nuestro nuevo indicador para el desarrollo de la enfermedad aguda de injerto contra huésped `agvhd`.

* `tmerge()` crea unos datos largos con múltiples intervalos de tiempo para los diferentes valores de las covariables de cada paciente
* `event()` crea el nuevo indicador de eventos para que vaya con los intervalos de tiempo recién creados
* `tdc()` crea la columna de covarianza dependiente del tiempo, `agvhd`, para que vaya con los intervalos de tiempo recién creados

```{r}
td_dat <- 
  tmerge(
    data1 = bmt %>% select(my_id, T1, delta1), 
    data2 = bmt %>% select(my_id, T1, delta1, TA, deltaA), 
    id = my_id, 
    death = event(T1, delta1),
    agvhd = tdc(TA)
    )
```

Para ver qué hace esto, veamos los datos de los 5 primeros pacientes individuales.

Las variables de interés en los datos originales tenían este aspecto:

```{r}
bmt %>% 
  select(my_id, T1, delta1, TA, deltaA) %>% 
  filter(my_id %in% seq(1, 5))
```

El nuevo conjunto de datos para estos mismos pacientes tiene el siguiente aspecto:

```{r}
td_dat %>% 
  filter(my_id %in% seq(1, 5))
```

Ahora algunos de nuestros pacientes tienen dos filas en el conjunto de datos correspondientes a intervalos en los que tienen un valor diferente de nuestra nueva variable, agvhd. Por ejemplo, el paciente 1 tiene ahora dos filas con un valor de agvhd de cero desde el tiempo 0 hasta el tiempo 67, y un valor de 1 desde el tiempo 67 hasta el tiempo 2081.

### Regresión de Cox con covariables dependientes del tiempo {.unnumbered} 

Ahora que hemos remodelado nuestros datos y añadido la nueva variable `aghvd` dependiente del tiempo, vamos a ajustar un modelo de regresión cox simple de una sola variable. Podemos utilizar la misma función `coxph()` que antes, sólo tenemos que cambiar nuestra función `Surv()` para especificar tanto el tiempo de inicio como el de finalización de cada intervalo utilizando los argumentos `time1 = ` y `time2 = `.


```{r}
bmt_td_model = coxph(
  Surv(time = tstart, time2 = tstop, event = death) ~ agvhd, 
  data = td_dat
  )

summary(bmt_td_model)
```

De nuevo, visualizaremos los resultados de nuestro modelo de Cox utilizando la función `ggforest()` del paquete **urvminer**.:

```{r}

ggforest(bmt_td_model, data = td_dat)

```

Como se puede ver en el gráfico de forest, el intervalo de confianza y el valor-p, no parece haber una fuerte asociación entre la muerte y la enfermedad aguda de injerto contra huésped en el contexto de nuestro modelo simple.

<!-- ======================================================= -->
## Recursos {#resources-20}

[Análisis de supervivencia Parte I: Conceptos básicos y primeros análisis](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2394262/)

[Análisis de supervivencia en R](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html)

[Análisis de supervivencia en la investigación de enfermedades infecciosas: Descripción de eventos en el tiempo](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2954271/)

[Capítulo sobre modelos de supervivencia avanzados Princeton](https://data.princeton.edu/wws509/notes/c7.pdf)

[Uso de covariables y coeficientes dependientes del tiempo en el modelo de Cox](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf)

[Hoja de trucos de análisis de supervivencia con R](https://publicifsv.sund.ku.dk/~ts/survival/survival-cheat.pdf)

[Hoja de trucos de Survminer](https://paulvanderlaken.files.wordpress.com/2017/08/survminer_cheatsheet.pdf)

[Documento sobre diferentes medidas de supervivencia para datos de registros de cáncer con Rcode proporcionado como material suplementario](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6322561/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/survival_analysis.Rmd-->

# Conceptos básicos de los SIG {#gis-basics}

```{r, out.width=c('100%', '100%'), echo=F}
knitr::include_graphics(here::here("images", "gis_head_image.png"))
```

<!-- ======================================================= -->

## Resumen {#overview-6}

Los aspectos espaciales de tus datos pueden proporcionar mucha información sobre la situación del brote, y responder a preguntas como:

-   ¿Dónde están los focos actuales de la enfermedad?
-   ¿Cómo han cambiado los puntos conflictivos con el tiempo?
-   ¿Cómo es el acceso a las instalaciones sanitarias? ¿Se necesitan mejoras?

El enfoque actual de esta página de los SIG (Sistemas de información geográfico; GIS, por sus siglas en inglés) es abordar las necesidades de la epidemiología aplicada en su respuesta a los brotes. Exploraremos los métodos básicos de visualización de datos espaciales utilizando los paquetes **tmap** y **ggplot2**. También recorreremos algunos de los métodos básicos de gestión y consulta de datos espaciales con el paquete **sf**. Por último, abordaremos brevemente conceptos de *estadística espacial* como las relaciones espaciales, la autocorrelación espacial y la regresión espacial utilizando el paquete **spdep**.

## Términos clave {#key-terms-1}

A continuación presentamos algunos términos clave. Para una introducción completa a los SIG y al análisis espacial, te sugerimos que revises uno de los tutoriales o cursos más largos que aparecen en la sección de [Recursos](#resources-21).

**Sistemas de Información Geográfica (SIG)** - Un SIG es un marco o entorno de trabajo para recopilar, gestionar, analizar y visualizar datos espaciales.

### Software SIG {.unnumbered}

Algunos de los programas de SIG más conocidos permiten la interacción "señalar y clicar" para el desarrollo de mapas y el análisis espacial. Estas herramientas tienen ventajas como no tener que aprender código y la facilidad de seleccionar y colocar manualmente los iconos y características en un mapa. He aquí dos de los más populares:

**ArcGIS** - Un software comercial de SIG desarrollado por la empresa ESRI, que es muy popular pero bastante caro.

**QGIS** - Un software de SIG gratuito de código abierto que puede hacer casi todo lo que ArcGIS puede hacer. Puedes [descargar QGIS aquí](https://qgis.org/en/site/forusers/download.html)

El uso de R como SIG puede parecer más intimidante al principio porque, en lugar de "señalar y clicar", tiene una "interfaz de línea de comandos" (hay que codificar para adquirir el resultado deseado). Sin embargo, esto es una gran ventaja si se necesita producir mapas repetidamente o crear un análisis que sea reproducible.

### Datos espaciales {.unnumbered}

Las dos formas principales de datos espaciales utilizadas en los SIG son los datos vectoriales y los ráster:

**Datos vectoriales** - Es el formato más común de datos espaciales utilizado en los SIG. Los datos vectoriales se componen de características geométricas de vértices y trayectorias. Los datos espaciales vectoriales pueden dividirse a su vez en tres tipos ampliamente utilizados:

-   *Puntos* - Un punto consiste en un par de coordenadas (x,y) que representan una ubicación específica en un sistema de coordenadas. Los puntos son la forma más básica de datos espaciales, y pueden utilizarse para denotar un caso (por ejemplo, el domicilio de un paciente) o una ubicación (por ejemplo, un hospital) en un mapa.

-   *Líneas* - Una línea está compuesta por dos puntos conectados. Las líneas tienen una longitud y pueden utilizarse para indicar cosas como carreteras o ríos.

-   *Polígonos* - Un polígono está compuesto por al menos tres líneas conectadas por puntos. Los polígonos tiene una longitud (es decir, el perímetro del área) así como una medida de área. Los polígonos pueden utilizarse para señalar una zona (por ejemplo, un pueblo) o una estructura (por ejemplo, la superficie real de un hospital).

**Datos ráster** - Es un formato alternativo para los datos espaciales. Los datos ráster son una matriz de celdas (por ejemplo, píxeles) en la que cada celda contiene información como la altura, la temperatura, la pendiente, la cubierta forestal, etc. Suelen ser fotografías aéreas, imágenes de satélite, etc. Las imágenes ráster también pueden utilizarse como "mapas base" debajo de los datos vectoriales.

### Visualización de datos espaciales {.unnumbered}

Para representar visualmente los datos espaciales en un mapa, el software SIG requiere que se proporcione suficiente información sobre dónde deben estar las diferentes características y la relación de unas con otras. Si se utilizan datos vectoriales, como ocurre en la mayoría de los casos, esta información suele almacenarse en un archivo *shapefile*:

**Shapefiles** - Un shapefile es un formato de datos común para almacenar datos espaciales "vectoriales" consistentes en líneas, puntos o polígonos. Un shapefile es en realidad un grupo de al menos tres archivos - .shp, .shx y .dbf. Estos archivos  deben estar en un determinado directorio (una misma carpeta) para que el shapefile se pueda leer. Estos archivos asociados pueden comprimirse en un archivo ZIP para enviarlos por correo electrónico o descargarlos de un sitio web.

El shapefile contendrá información sobre las características con las que estemos tratando, así como su ubicación en la superficie de la Tierra. Esto es importante porque, aunque la Tierra es un globo terráqueo, los mapas suelen ser bidimensionales; las decisiones sobre cómo "aplanar" los datos espaciales pueden tener un gran impacto en el aspecto y la interpretación del mapa resultante.

**Sistemas de referencia de coordenadas (CRS-SRC)** - Un SRC es un sistema basado en coordenadas que se utiliza para localizar accidentes geográficos en la superficie de la Tierra. Tiene unos cuantos componentes clave:

-   *Sistema de coordenadas* - Hay muchos sistemas de coordenadas diferentes, así que asegúrate de saber en qué sistema están tus coordenadas. Los grados de latitud/longitud son muy comunes, pero también puede que tu información esté guardada como coordenadas [UTM](https://www.maptools.com/tutorials/utm/quick_guide).

-   *Units* - Asegúrate de saber cuáles son las unidades del sistema de coordenadas (por ejemplo, grados decimales, metros).

-   *Datum* - Es una versión particular modelada de la Tierra. Los datum ya estan establecidos y han sido revisados a lo largo de los años, así que asegúrate de que las capas de tu mapa utilizan el mismo datum.

-   *Proyección* - Es la referencia a la ecuación matemática que se utilizó para proyectar la tierra (realmente redonda) sobre una superficie plana (mapa).

Recuerda que puedes resumir los datos espaciales sin utilizar las herramientas cartográficas que se muestran a continuación. A veces basta con una simple tabla por zonas geográficas (por ejemplo, distrito, país, etc.).

## Introducción a los SIG {#getting-started-with-gis}

Hay un par de elementos clave que deberás tener y en los que deberás pensar para hacer un mapa. Entre ellos están:

-   **Datos**: pueden estar con un formato de datos espaciales (como los shapefiles, como se ha indicado anteriormente) o pueden no estar en un formato espacial (por ejemplo, sólo como un csv).

-   Si tus datos no están en formato espacial, también necesitarás unos datos **de referencia**. Los datos de referencia consisten en la representación espacial de los datos y sus **atributos** relacionados, que incluirían el material que contiene la información de ubicación y dirección de características específicas.

-   Si estás trabajando con límites geográficos predefinidos (por ejemplo, regiones administrativas), probablemente puedas encontrar shapefiles de referencia que te puedes descargar de forma gratuita desde una agencia gubernamental u organización de intercambio de datos. En caso de duda, un buen punto de partida es buscar en Google "[regions] shapefile"

-   Si tus datos están guardados como direcciones, en vez de como latitud/longitud, puede que tengas que utilizar un **motor de geocodificación** que te permita hacer la "traducción" de una dirección específica a a datos en formato espacial.

-   Piensa **cómo quieres presentar** la información de los datos a tu audiencia. Hay muchos tipos diferentes de mapas, y es importante pensar qué tipo de mapa se ajusta mejor a tus necesidades.

### Tipos de mapas para visualizar tus datos {.unnumbered}

**Mapa de coropletas**: Es un tipo de mapa temático en el que se utiliza colores, sombreados o patrones para representar regiones geográficas en relación con el valor de un atributo. Por ejemplo, un valor mayor podría indicarse con un color más oscuro que un valor menor. Este tipo de mapa es particularmente útil cuando se visualiza una variable y cómo cambia a través de regiones o áreas geopolíticas definidas.

```{r, out.width = '50%', fig.align = "center", fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "gis_choropleth.png"))
```

**Mapa de densidad de calor de casos**: Es un tipo de mapa temático en el que se utiliza colores para representar la intensidad de un valor, pero que no utiliza regiones definidas ni límites geopolíticos para agrupar los datos. Este tipo de mapa se suele utilizar para mostrar "puntos conflictivos" o zonas con una alta densidad o concentración de puntos.

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "gis_heatmap.png"))
```

**Mapa de densidad de puntos**: Es un tipo de mapa temático que utiliza puntos para representar los valores de los datos. Este tipo de mapa es el más adecuado para visualizar cómo están dispersos los datos en el mapa y buscar clústers visualmente.

```{r, fig.align = "center", echo=F}
# dot density img here
```

Mapa de **símbolos proporcionales (mapa de símbolos graduados)**: es un mapa temático similar a un mapa de coropletas, pero en lugar de utilizar el color para indicar el valor de un atributo, utiliza un símbolo (normalmente un círculo) cuyo tamaño esta en relación con el valor. Por ejemplo, un valor mayor podría indicarse con un símbolo de tamaño mayor que un valor menor. Este tipo de mapa se utiliza mejor cuando se quiere visualizar el tamaño o la cantidad de los datos en distintas regiones geográficas.

```{r, fig.align = "center", echo=F}
# proportional symbols img here
```

También puedes combinar varios tipos de visualizaciones diferentes para mostrar patrones geográficos complejos. Por ejemplo, los casos (puntos) del siguiente mapa están coloreados según su centro sanitario más cercano (véase la leyenda). Los círculos grandes de color negro muestran las *zonas de captación de los centros sanitarios* de un determinado radio, y los puntos/círculos rojos brillantes son los que no tienen ninguna zona de captación de centros sanitarios dentro de ese radio determinado:

```{r, fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "gis_hf_catchment.png"))
```

Nota: El enfoque principal de esta página del SIG se centra en el contexto de respuesta a brotes en el terreno. Por lo tanto, el contenido de esta página cubrirá la manipulación, visualización y análisis básicos de datos espaciales.

<!-- ======================================================= -->

## Preparación {#preparation-19}

### Cargar paquetes {.unnumbered}

Este trozo de código muestra cómo puedes cargar de los paquetes necesarios para el análisis espacial que vamos a realizar. En este manual destacamos la función `p_load()` del paquete **pacman**: esta instala el paquete (si aún no está instalado) *y* lo carga para su uso. También puedes cargar los paquetes uno por uno  con la funcion `library()` de R **base.**, si ya los tienes instalados. Consulta la página sobre los [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r}
pacman::p_load(
  rio,           # to import data
  here,          # to locate files
  tidyverse,     # to clean, handle, and plot the data (includes ggplot2 package)
  sf,            # to manage spatial data using a Simple Feature format
  tmap,          # to produce simple maps, works for both interactive and static maps
  janitor,       # to clean column names
  OpenStreetMap, # to add OSM basemap in ggplot map
  spdep          # spatial statistics
  ) 
                  
```

En este enlace de CRAN ["Spatial Task View"](https://cran.r-project.org/web/views/Spatial.html) puedes ver un resumen de todos los paquetes de R que pueden trabajar con datos espaciales. 

### Ejemplos con una base de datos {.unnumbered}

A muestra de ejemplo, trabajaremos con una muestra aleatoria de 1000 casos del brote de ébola simulado en el dataframe `linelist` (computacionalmente, una base de datos pequeña hace que sea más fácil trabajar con este ejemplo). Si quieres seguir el proceso, [clica para descargar linelist "limpio"](https://github.com/appliedepi/epirhandbook_eng/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds).

Los resultados que tengas al correr el código de este ejercicio puede que sean un poco diferentes de los que se aquí mostramos. Esto es porque vamos a estar trabajando con una muestra aleatoria de los casos.

Importa los datos con la función `import()` del paquete **rio** (este paquete puede manejar muchos tipos de archivos, como .xlsx, .csv, .rds - véase la página de [importación y exportación](#import-and-export) para más detalles).

```{r, echo=F}
# import clean case linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))  
```

```{r, eval=F}
# import clean case linelist
linelist <- import("linelist_cleaned.rds")  
```

Para seleccionar la muestra aleatoria de 1000 filas utiliza `sample()` de R **base**.

```{r}
# generate 1000 random row numbers, from the number of rows in linelist
sample_rows <- sample(nrow(linelist), 1000)

# subset linelist to keep only the sample rows, and all columns
linelist <- linelist[sample_rows,]
```

Ahora queremos convertir este `linelist`, que es de tipo "dataframe", en un objeto de tipo "sf" (spatial features, por sus siglas en inglés). Dado que linelist tiene dos columnas "lon" y "lat" que representan la longitud y latitud de la residencia de cada caso, esto será fácil.

Utilizamos la función `st_as_sf()` del paquete **sf** (spatial features) para crear el nuevo objeto que llamamos `linelist_sf`. Este nuevo objeto tiene el mismo aspecto que la linelist, pero las columnas lon y lat han sido designadas como columnas de coordenadas, y se ha asignado un sistema de referencia de coordenadas (CRS) para poder visualizar los puntos. Este CRS es el númermo 4326 que corresponde a coordenadas especificas basadas en el [Sistema Geodésico Mundial 1984 (WGS84)](https://gisgeography.com/wgs84-world-geodetic-system/) - que es el estándar para las coordenadas GPS.

```{r}
# Create sf object
linelist_sf <- linelist %>%
     sf::st_as_sf(coords = c("lon", "lat"), crs = 4326)
```

Este es el aspecto del dataframe original `linelist`. En esta demostración, sólo utilizaremos la columna `date_onset` y `geometry` (que se construyó a partir de los campos de longitud y latitud anteriores y es la última columna del dataframe).

```{r}
DT::datatable(head(linelist_sf, 10), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Archivos shapefiles de límites administrativos {.unnumbered}

**Sierra Leona: Archivos shapefiles de los límites administrativos**

Primero, descárgate todos los límites administrativos de Sierra Leona del sitio web de [Humanitarian Data Exchange (HDX)](https://data.humdata.org/dataset/sierra-leone-all-ad-min-level-boundaries). Como alternativa, también te puedes descargar estos archivos y todos los demás datos de ejemplo que usamos para este manual a través de nuestro paquete R, como se explica en la página [descargando el manual y los datos](#download-handbook-and-data).

Ahora vamos a hacer lo siguiente para guardar el shapefile del nivel administrativo 3 en R:

1.  Importar el shapefile
2.  Limpiar los nombres de las columnas
3.  Filtrar las filas para mantener sólo las áreas de interés

Para importar un shapefile utilizamos la función `read_sf()` del paquete **sf**, y usamos la función `here()` para que R encuentre el archivo. En nuestro caso, el archivo se encuentra dentro de nuestro proyecto R en las subcarpetas "data", "gis" y "shp", con nombre de archivo "sle_adm3.shp" (para más información ver las páginas sobre [Importación y exportación](#import-and-export) y [Proyectos en R](#r-projects)). En tu caso, tendrás que indicar la localización específica donde tienes tus archivos en tu ordenador.

```{r, echo=F}
sle_adm3_raw <- sf::read_sf(here("data", "gis", "shp", "sle_adm3.shp"))
```

A continuación utilizamos `clean_names()` del paquete **janitor** para estandarizar los nombres de las columnas del shapefile. También utilizamos `filter()` para mantener sólo las filas con admin2name de "Western Area Urban" o "Western Area Rural".


```{r, include=FALSE}
# ADM3 level clean
sle_adm3 <- sle_adm3_raw %>%
  janitor::clean_names() %>% # standardize column names #jfmont
  filter(admin2name %in% c("Western Area Urban", "Western Area Rural")) # filter to keep certain areas
```


```{r, eval = FALSE}
# ADM3 level clean
sle_adm3 <- sle_adm3_raw %>%
  janitor::clean_names() %>% # standardize column names 
  filter(admin2name %in% c("Western Area Urban", "Western Area Rural")) # filter to keep certain areas
```

A continuación puedes ver el aspecto del shapefile después de la importación y limpieza. *Desplázate a la derecha* para ver cómo hay columnas con el nivel de administración 0 (país), el nivel de administración 1, el nivel de administración 2 y, finalmente, el nivel de administración 3. Cada nivel tiene un nombre y un identificador único "pcode". El pcode se expande con cada nivel de administración creciente, por ejemplo, SL (Sierra Leona) -> SL04 (Occidental) -> SL0410 (Zona Occidental Rural) -> SL040101 (Koya Rural).

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(sle_adm3, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Datos de población {.unnumbered}

**Sierra Leona: Población por ADM3**

Al igual que hemos hecho antes, te puedes descargar estos datos 
de [HDX](https://data.humdata.org/dataset/sierra-leone-population)) o a través de nuestro paquete R **epirhandbook** como se explica [en esta página](#download-handbook-and-data). Utilizamos `import()` para cargar el archivo .csv. Al igual que antes, pasamos el archivo importado a `clean_names()` para estandarizar la sintaxis de los nombres de las columnas.

```{r, include = FALSE}

library("janitor") #jfmont

```



```{r, include=FALSE} 
# Population by ADM3
sle_adm3_pop <- import(here("data", "gis", "population", "sle_admpop_adm3_2020.csv")) %>%
  janitor::clean_names() #jfmont
```

```{r, eval=FALSE} 
# Population by ADM3
sle_adm3_pop <- import(here("data", "gis", "population", "sle_admpop_adm3_2020.csv")) %>%
  clean_names() 
```


Este es el aspecto del archivo de población. *Desplázate a la derecha* para ver cómo cada jurisdicción tiene columnas con la población `male`, `female`, y la población `total` y el desglose de la población en columnas por grupos de edad.

```{r message=FALSE, echo=F}
# display the population as a table
DT::datatable(head(sle_adm3_pop, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Instalaciones sanitarias {.unnumbered}

**Sierra Leone: Health facility data from OpenStreetMap**

Una vez más, hemos descargado la localización de los centros sanitarios desde el HDX [aquí](https://data.humdata.org/dataset/hotosm_sierra_leone_health_facilities) o mediante las instrucciones de la página [descarga de manuales y datos](#download-handbook-and-data).

Importamos el shapefile de puntos de las instalaciones con `read_sf()`, limpiamos de nuevo los nombres de las columnas y filtramos para mantener sólo los puntos etiquetados como "hospital", "clinic" o "doctors".

```{r, include = FALSE}

library("janitor")

```


```{r, include=FALSE}
# OSM health facility shapefile
sle_hf <- sf::read_sf(here("data", "gis", "shp", "sle_hf.shp")) %>% 
  janitor::clean_names() %>% #jfmont
  filter(amenity %in% c("hospital", "clinic", "doctors"))
```

```{r, eval=FALSE}
# OSM health facility shapefile
sle_hf <- sf::read_sf(here("data", "gis", "shp", "sle_hf.shp")) %>% 
  clean_names() %>% 
  filter(amenity %in% c("hospital", "clinic", "doctors"))
```


Aquí está el dataframe resultante -- *desplázate a la derecha* para ver el nombre de la instalación y las coordenadas geométricas (`geometry`).

```{r message=FALSE, echo=F}
# display the population as a table
DT::datatable(head(sle_hf, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

<!-- ======================================================= -->

## Trazado de coordenadas {#plotting-coordinates}

La forma más sencilla de representar las coordenadas X-Y (longitud/latitud, puntos) de los casos es dibujarlas como puntos directamente desde el objeto `linelist_sf` que ya creamos en la sección de preparación.

El paquete **tmap** nos permite visualizar este tipo de informacion tanto de modo estático (modo "plot") como de modo interactivo (modo "view") con sólo unas pocas líneas de código. La sintaxis de **tmap** es similar a la de **ggplot2**, de forma que los comandos se añaden unos a otros con `+`. Lee más detalles en esta [viñeta](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html).

1.  Lo primero es establecer qué modo **tmap** queremos. En este caso utilizaremos el modo "plot", que produce salidas estáticas.

```{r, warning = F, message=F}
tmap_mode("plot") # choose either "view" or "plot"
```

Abajo puedes ver que, por ahora, sólo estamos trazando los puntos. Utilizamos el `tm_shape()` con el objeto `linelist_sf`. A continuación, añadimos la informacion de los puntos mediante `tm_dots()`, especificando el tamaño y el color deseados. Recuerda que el `linelist_sf` es un objeto sf, con lo que ya tenemos designadas las dos columnas que contienen las coordenadas lat/long y el sistema de referencia de coordenadas (CRS):

```{r, warning = F, message=F}
# Just the cases (points)
tm_shape(linelist_sf) + tm_dots(size=0.08, col='blue')
```

Por sí solos, los puntos no nos dicen mucho. Así que también tenemos que dibujar los límites administrativos:

Para esto, vamos a usar `tm_shape()` ([documentación](https://www.rdocumentation.org/packages/tmap/versions/3.3/topics/tm_shape)), pero en vez de proporcionar el shapefile de los puntos de los casos, proporcionamos el shapefile de los límites administrativos (polígonos).

Con el argumento `bbox =`(bbox significa "bounding box") podemos especificar los límites de las coordenadas, lo cual nos ayuda a hacer zoom a una zona específica de interés. Primero mostramos la visualización del mapa sin `bbox`, y luego con él.

```{r, out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}
# Just the administrative boundaries (polygons)
tm_shape(sle_adm3) +               # admin boundaries shapefile
  tm_polygons(col = "#F7F7F7")+    # show polygons in light grey
  tm_borders(col = "#000000",      # show borders with color and line weight
             lwd = 2) +
  tm_text("admin3name")            # column text to display for each polygon


# Same as above, but with zoom from bounding box
tm_shape(sle_adm3,
         bbox = c(-13.3, 8.43,    # corner
                  -13.2, 8.5)) +  # corner
  tm_polygons(col = "#F7F7F7") +
  tm_borders(col = "#000000", lwd = 2) +
  tm_text("admin3name")

```

Con esto ya podemos trazar los puntos y los polígonos juntos:

```{r, warning=F, message=FALSE}
# All together
tm_shape(sle_adm3, bbox = c(-13.3, 8.43, -13.2, 8.5)) +     #
  tm_polygons(col = "#F7F7F7") +
  tm_borders(col = "#000000", lwd = 2) +
  tm_text("admin3name")+
tm_shape(linelist_sf) +
  tm_dots(size=0.08, col='blue', alpha = 0.5) +
  tm_layout(title = "Distribution of Ebola cases")   # give title to map

```

Para leer una buena comparación de las opciones de mapeo en R, consulta esta [entrada del blog](https://rstudio-pubs-static.s3.amazonaws.com/324400_69a673183ba449e9af4011b1eeb456b9.html).

<!-- ======================================================= -->

## Uniones espaciales {#spatial-joins}

Es posible que estés familiarizado/a con la *unión* de datos entre bases de datos. En la página [unión de datos](#joining-data) de este manual se trata varios métodos para unir bases de datos. Una unión espacial tiene un propósito similar, pero aprovecha las relaciones espaciales entre bases de datos. En lugar de confiar en los valores comunes de las columnas para hacer coincidir correctamente las observaciones, puede utilizar distintos tipos de relaciones espaciales, como por ejemplo que una característica esté *contenida dentro de* otra, que *sea el "vecino más cercano"* de otra característica, o que esté dentro de un *buffer* de determinado radio de otra.

El paquete **sf** ofrece varios métodos para las uniones espaciales. Puedes explorar la documentación del método `st_join()` y otros tipos de uniones espaciales en esta [referencia](https://r-spatial.github.io/sf/reference/geos_binary_pred.html).

### Puntos en el polígono {.unnumbered}

**Asignación espacial de unidades administrativas a los casos**

Aquí se plantea un dilema interesante: la lista de casos no contiene ninguna información sobre las unidades administrativas de los mismos. Aunque lo ideal es recoger dicha información durante la fase inicial de recogida de datos, también podemos asignar unidades administrativas a los casos individuales basándonos en sus relaciones espaciales (es decir, el punto se cruza con un polígono).

A continuación, haremos una intersección espacial de las ubicaciones de nuestros casos (puntos) con los límites de la ADM3 (polígonos):

1.  Comenzamos con la linelist (casos/puntos)
2.  Continuamos con la unión espacial a los límites administrativos, estableciendo el tipo de unión en "st_intersects"
3.  Utilizamos `select()` para mantener sólo algunas de las nuevas columnas de los límites administrativos (este paso lo realizamos un poco más abajo)

```{r, warning=F, message=F}
linelist_adm <- linelist_sf %>%
  
  # join the administrative boundary file to the linelist, based on spatial intersection
  sf::st_join(sle_adm3, join = st_intersects)
```

¡Todas las columnas de `sle_adms` se han añadido a linelist! Cada caso tiene ahora columnas que detallan los niveles administrativos a los que pertenece. En este ejemplo, sólo queremos mantener dos de las nuevas columnas (las de nivel administrativo 3), así que haremos `select()` de los nombres de las columnas antiguas de la linelist, y sólo las dos adicionales de interés:

```{r, warning=F, message=F}
linelist_adm <- linelist_sf %>%
  
  # join the administrative boundary file to the linelist, based on spatial intersection
  sf::st_join(sle_adm3, join = st_intersects) %>% 
  
  # Keep the old column names and two new admin ones of interest
  select(names(linelist_sf), admin3name, admin3pcod)
```

Aquí os mostramos los diez primeros casos para que veais que se ha añadido la información correspondiente de sus jurisdicciones a nivel de administración 3 (ADM3), basándose en la región del polígono donde se cruza el punto.

```{r, warning=F, message=F}
# Now you will see the ADM3 names attached to each case
linelist_adm %>% select(case_id, admin3name, admin3pcod)
```

Ahora podemos describir nuestros casos por unidad administrativa, algo que no podíamos hacer antes de la unión espacial.

```{r, warning=F, message=F}
# Make new dataframe containing counts of cases by administrative unit
case_adm3 <- linelist_adm %>%          # begin with linelist with new admin cols
  as_tibble() %>%                      # convert to tibble for better display
  group_by(admin3pcod, admin3name) %>% # group by admin unit, both by name and pcode 
  summarise(cases = n()) %>%           # summarize and count rows
  arrange(desc(cases))                     # arrange in descending order

case_adm3
```

También podemos crear un gráfico de barras con el número de casos por unidad administrativa.

En este ejemplo, utilizamos `ggplot()` con `linelist_adm` para poder aplicar funciones que manejan factores, como `fct_infreq()` que ordena las barras por frecuencia (véase la página sobre [Factores](#factors) para ver algunos consejos).

```{r, warning=F, message=F}
ggplot(
    data = linelist_adm,                       # begin with linelist containing admin unit info
    mapping = aes(
      x = fct_rev(fct_infreq(admin3name))))+ # x-axis is admin units, ordered by frequency (reversed)
  geom_bar()+                                # create bars, height is number of rows
  coord_flip()+                              # flip X and Y axes for easier reading of adm units
  theme_classic()+                           # simplify background
  labs(                                      # titles and labels
    x = "Admin level 3",
    y = "Number of cases",
    title = "Number of cases, by adminstative unit",
    caption = "As determined by a spatial join, from 1000 randomly sampled cases from linelist"
  )
```

<!-- ======================================================= -->

### Vecino más cercano {.unnumbered}

**Encontrar el centro sanitario más cercano**

Podría ser útil saber dónde se encuentran los centros sanitarios (clínicas/hospitales) en relación con los focos de la enfermedad.

Podemos utilizar el método de unión *st_nearest_feature* de la función `st_join()` (paquete **sf**) para visualizar el centro sanitario más cercano a cada uno de los casos.

1.  Comenzamos con el shapefile linelist `linelist_sf`
2.  Unimos espacialmente con `sle_hf`, que es la ubicación de los centros sanitarios y las clínicas (puntos), estableciendo el tipo de unión en "st_nearest_feature"

```{r, warning=F, message=F}
# Closest health facility to each case
linelist_sf_hf <- linelist_sf %>%                  # begin with linelist shapefile  
  st_join(sle_hf, join = st_nearest_feature) %>%   # data from nearest clinic joined to case data 
  select(case_id, osm_id, name, amenity) %>%       # keep columns of interest, including id, name, type, and geometry of healthcare facility
  rename("nearest_clinic" = "name")                # re-name for clarity
```

Podemos ver a continuación (primeras 50 filas) que cada caso tiene ahora datos sobre la clínica/hospital más cercano

```{r message=FALSE, echo=F}
DT::datatable(head(linelist_sf_hf, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Podemos ver que "Den Clinic" es el centro sanitario más cercano para aproximadamente el 30% de los casos.

```{r}
# Count cases by health facility
hf_catchment <- linelist_sf_hf %>%   # begin with linelist including nearest clinic data
  as.data.frame() %>%                # convert from shapefile to dataframe
  count(nearest_clinic,              # count rows by "name" (of clinic)
        name = "case_n") %>%         # assign new counts column as "case_n"
  arrange(desc(case_n))              # arrange in descending order

hf_catchment                         # print to console
```

Para visualizar los resultados, podemos utilizar **tmap** - esta vez en modo interactivo para facilitar la visualización

```{r, warning=F, message=F}
tmap_mode("view")   # set tmap mode to interactive  

# plot the cases and clinic points 
tm_shape(linelist_sf_hf) +            # plot cases
  tm_dots(size=0.08,                  # cases colored by nearest clinic
          col='nearest_clinic') +    
tm_shape(sle_hf) +                    # plot clinic facilities in large black dots
  tm_dots(size=0.3, col='black', alpha = 0.4) +      
  tm_text("name") +                   # overlay with name of facility
tm_view(set.view = c(-13.2284, 8.4699, 13), # adjust zoom (center coords, zoom)
        set.zoom.limits = c(13,14))+
tm_layout(title = "Cases, colored by nearest clinic")
```

### Buffers {.unnumbered}

También podemos explorar cuántos casos se encuentran a menos de 2,5 km (~30 minutos) de distancia a pie del centro sanitario más cercano. Esto se conoce como buffer o zona o ámbito de influencia.

*Nota: Para un cálculo más preciso de la distancia, es mejor reproyectar el objeto sf al respectivo sistema de proyección cartográfica local, como UTM (Tierra proyectada sobre una superficie plana). En este ejemplo, para simplificar, nos ceñiremos al sistema de coordenadas geográficas del Sistema Geodésico Mundial (WGS84) (la Tierra representada en una superficie esférica/redonda, por lo que las unidades están en grados decimales). Utilizaremos una conversión general de: 1 grado decimal = ~111km.*

Puedes ver más información sobre proyecciones cartográficas y sistemas de coordenadas en este [artículo de esri](https://www.esri.com/arcgis-blog/products/arcgis-pro/mapping/gcs_vs_pcs/). Este [blog](http://www.geo.hunter.cuny.edu/~jochen/gtech201/lectures/lec6concepts/map%20coordinate%20systems/how%20to%20choose%20a%20projection.htm) habla de los diferentes tipos de proyecciones cartográficas y de cómo se puede elegir una proyección adecuada en función del área de interés y del contexto de su mapa/análisis.

**En primer lugar**, se crea un buffer circular con un radio de ~2,5km alrededor de cada centro sanitario. Esto se hace con la función `st_buffer()` de **tmap**. Como la unidad del mapa está en grados decimales lat/long, tenemos que proporcionar el valor de `dist` en grados decimales, es decir, tenemos que proporcionar un valor de "0,02" (2,5/111 = 0.02 grados decimales, correspondiente a ~2,5km). Si el sistema de coordenadas del mapa está en metros, el número debe proporcionarse en metros.

```{r, warning=F, message=F}
sle_hf_2k <- sle_hf %>%
  st_buffer(dist=0.02)       # decimal degrees translating to approximately 2.5km 
```

A continuación, trazamos las zonas de influencia propiamente dichas, con ese buffer:

```{r, warning=F, message=F}
tmap_mode("plot")
# Create circular buffers
tm_shape(sle_hf_2k) +
  tm_borders(col = "black", lwd = 2)+
tm_shape(sle_hf) +                    # plot clinic facilities in large red dots
  tm_dots(size=0.3, col='black')      
```

**En segundo lugar**, intersectamos estos buffers con los casos (puntos) utilizando `st_join()` y el tipo de unión *st_intersects*. Es decir, ponemos juntos los datos de los buffers y los de los puntos con los que se cruzan.

```{r, warning=F, message=F}
# Intersect the cases with the buffers
linelist_sf_hf_2k <- linelist_sf_hf %>%
  st_join(sle_hf_2k, join = st_intersects, left = TRUE) %>%
  filter(osm_id.x==osm_id.y | is.na(osm_id.y)) %>%
  select(case_id, osm_id.x, nearest_clinic, amenity.x, osm_id.y)
```

Ahora podemos contabilizar cuántos de nuestros casos no estan dentro de ningun buffer (sus puntos no se *cruzan* con ningún buffer): `nrow(linelist_sf_hf_2k[is.na(linelist_sf_hf_2k$osm_id.y),])`. Con este código vemos que falta información para este valor (estamos buscando `NA`), y por tanto, inferimos que estos casos viven a más de 30 minutos a pie del centro sanitario más cercano.

```{r}
# Cases which did not get intersected with any of the health facility buffers
linelist_sf_hf_2k %>% 
  filter(is.na(osm_id.y)) %>%
  nrow()
```

Podemos visualizar los resultados de forma que los casos que no se cruzan con ningún buffer aparezcan en rojo.

```{r, out.width = '100%', warning=F, message=F}
tmap_mode("view")

# First display the cases in points
tm_shape(linelist_sf_hf) +
  tm_dots(size=0.08, col='nearest_clinic') +

# plot clinic facilities in large black dots
tm_shape(sle_hf) +                    
  tm_dots(size=0.3, col='black')+   

# Then overlay the health facility buffers in polylines
tm_shape(sle_hf_2k) +
  tm_borders(col = "black", lwd = 2) +

# Highlight cases that are not part of any health facility buffers
# in red dots  
tm_shape(linelist_sf_hf_2k %>%  filter(is.na(osm_id.y))) +
  tm_dots(size=0.1, col='red') +
tm_view(set.view = c(-13.2284,8.4699, 13), set.zoom.limits = c(13,14))+

# add title  
tm_layout(title = "Cases by clinic catchment area")

```

### Otras uniones espaciales {.unnumbered}

Los siguientes son valores alternativos para el argunmento `join` (puedes encontrar más información en [documentation](https://r-spatial.github.io/sf/reference/st_join.html))

-   st_contains_properly\
-   st_contains\
-   st_covered_by\
-   st_covers\
-   st_crosses\
-   st_disjoint\
-   st_equals_exact\
-   st_equals\
-   st_is_within_distance\
-   st_nearest_feature\
-   st_overlaps\
-   st_touches\
-   st_within

## Mapas de coropletas {#choropleth-maps}

Los mapas de coropletas pueden ser útiles para visualizar los datos por áreas predefinidas, como por ejemplo unidades administrativas o áreas de salud. Cuando se responde a un brote esto puede ser muy útil para dirigir los recursos a zonas específicas con altas tasas de incidencia, por ejemplo.

Ahora que tenemos los nombres de las unidades administrativas asignados a todos los casos (véase la sección sobre uniones espaciales, más arriba), podemos empezar a mapear el número de casos por zonas (mapa de coropletas).

Como también tenemos datos de población por ADM3, podemos añadir esta información a la tabla *case_adm3* creada anteriormente.

Comenzamos con el dataframe creado en el paso anterior `case_adm3`, que es una tabla resumen de cada unidad administrativa y su número de casos.

1)  Los datos de la población `sle_adm3_pop` se unen utilizando un `left_join()` de **dplyr** utilizando la columna `admin3pcod` del dataframe `case_adm3`, y la columna `adm_pcode` en el dataframe `sle_adm3_pop`, que tienen la misma información. Véase la página sobre la [unión de datos](#joining-data).
2)  `select()` se aplica al nuevo dataframe, para mantener sólo las columnas que nos interesan - `total` es la población total.
3)  Los casos por cada 10.000 habitantes se calculan como una nueva columna con `mutate()`.

```{r}
# Add population data and calculate cases per 10K population
case_adm3 <- case_adm3 %>% 
     left_join(sle_adm3_pop,                             # add columns from pop dataset
               by = c("admin3pcod" = "adm3_pcode")) %>%  # join based on common values across these two columns
     select(names(case_adm3), total) %>%                 # keep only important columns, including total population
     mutate(case_10kpop = round(cases/total * 10000, 3)) # make new column with case rate per 10000, rounded to 3 decimals

case_adm3                                                # print to console for viewing
```

Para poder mapear estos datos, tenemos que unir esta tabla con el shapefile de polígonos ADM3:

```{r, warning=F, message=F}
case_adm3_sf <- case_adm3 %>%                 # begin with cases & rate by admin unit
  left_join(sle_adm3, by="admin3pcod") %>%    # join to shapefile data by common column
  select(objectid, admin3pcod,                # keep only certain columns of interest
         admin3name = admin3name.x,           # clean name of one column
         admin2name, admin1name,
         cases, total, case_10kpop,
         geometry) %>%                        # keep geometry so polygons can be plotted
  drop_na(objectid)%>%                        # drop any empty rows
  st_as_sf()                                  # convert to shapefile
  

```

Para mapear los resultados en un mapa estático:

```{r, message=F, warning=F}

# tmap mode
tmap_mode("plot")               # view static map

# plot polygons
tm_shape(case_adm3_sf) + 
        tm_polygons("cases") +  # color by number of cases column
        tm_text("admin3name")   # name display
```

También podemos mapear las tasas de incidencia:

```{r, warning=F, message=F}
# Cases per 10K population
tmap_mode("plot")             # static viewing mode

# plot
tm_shape(case_adm3_sf) +                # plot polygons
  tm_polygons("case_10kpop",            # color by column containing case rate
              breaks=c(0, 10, 50, 100), # define break points for colors
              palette = "Purples"       # use a purple color palette
              ) +
  tm_text("admin3name")                 # display text

```

## Mapeo con ggplot2 {#mapping-with-ggplot2}

Si ya conoces el uso de **ggplot2**, puedes utilizar ese paquete para crear mapas estáticos de tus datos. La función `geom_sf()` dibujará diferentes objetos en función de las características de los datos. Por ejemplo, puedes utilizar `geom_sf()` en un `ggplot()` utilizando datos `sf` con geometría de polígonos para crear un mapa de coropletas.

Para ilustrar cómo funciona esto, podemos empezar con el archivo shape de polígonos ADM3 que hemos utilizado antes. Recordemos que se trata de regiones de nivel administrativo 3 en Sierra Leona:

```{r}
sle_adm3
```

Podemos utilizar la función `left_join()` de **dplyr** para añadir al objeto shapefile los datos que queremos mapear. En este caso, vamos a utilizar el dataframe `case_adm3` que creamos anteriormente para resumir los recuentos de casos por región administrativa. También podemos utilizar este mismo enfoque para mapear cualquier dato almacenado en un dataframe.

```{r}
sle_adm3_dat <- sle_adm3 %>% 
  inner_join(case_adm3, by = "admin3pcod") # inner join = retain only if in both data objects

select(sle_adm3_dat, admin3name.x, cases) # print selected variables to console
```

Para hacer un gráfico de columnas de los recuentos de casos por región utilizando **ggplot2,** podemos llamar a `geom_col()` de la siguiente manera:

```{r, fig.align = "center"}
ggplot(data=sle_adm3_dat) +
  geom_col(aes(x=fct_reorder(admin3name.x, cases, .desc=T),   # reorder x axis by descending 'cases'
               y=cases)) +                                  # y axis is number of cases by region
  theme_bw() +
  labs(                                                     # set figure text
    title="Number of cases, by administrative unit",
    x="Admin level 3",
    y="Number of cases"
  ) + 
  guides(x=guide_axis(angle=45))                            # angle x-axis labels 45 degrees to fit better

```

Si queremos utilizar **ggplot2** para hacer un mapa de coropletas de los recuentos de casos, podemos utilizar una sintaxis similar para llamar a la función `geom_sf()`:

```{r, fig.align = "center"}
ggplot(data=sle_adm3_dat) + 
  geom_sf(aes(fill=cases))    # set fill to vary by case count variable

```

A continuación, podemos personalizar la apariencia de nuestro mapa utilizando una sintaxis que sea consistente en **ggplot2**, por ejemplo:

```{r, fig.align = "center"}
ggplot(data=sle_adm3_dat) +                           
  geom_sf(aes(fill=cases)) +						
  scale_fill_continuous(high="#54278f", low="#f2f0f7") +    # change color gradient
  theme_bw() +
  labs(title = "Number of cases, by administrative unit",   # set figure text
       subtitle = "Admin level 3"
  )
```

Para las personas que se sientan cómodas trabajando con **ggplot2**, `geom_sf()` ofrece una implementación simple y directa que es adecuada para las visualizaciones básicas de mapas. Para saber más, mira la [viñeta de geom_sf()](https://ggplot2.tidyverse.org/reference/ggsf.html) o el [libro de ggplot2](https://ggplot2-book.org/maps.html).

<!-- ======================================================= -->

## Mapas base {#basemaps}

### OpenStreetMap {.unnumbered}

A continuación describimos cómo conseguir un mapa base para realizar un mapa con **ggplot2** utilizando las características de OpenStreetMap. Existen métodos alternativos que incluyen el uso de **ggmap** que requiere el registro gratuito con Google ([detalles](https://www.earthdatascience.org/courses/earth-analytics/lidar-raster-data-r/ggmap-basemap/)).

[[**OpenStreetMap**]{.ul}](https://en.wikipedia.org/wiki/OpenStreetMap) es un proyecto de colaboración para crear un mapa editable y gratuito del mundo. Los datos de geolocalización subyacentes (por ejemplo, ubicaciones de ciudades, carreteras, características naturales, aeropuertos, escuelas, hospitales, caminos, etc.) se consideran el resultado principal del proyecto.

Primero cargamos el paquete **OpenStreetMap**, del que obtendremos nuestro mapa base.

A continuación, creamos el objeto `map`, que definimos mediante la función `openmap()` del paquete **OpenStreetMap** ([documentación](https://www.rdocumentation.org/packages/OpenStreetMap/versions/0.3.4/topics/openmap)). Proporcionamos lo siguiente:

-   `upperLeft` y `lowerRight`: Estas son dos pares de coordenadas que especifican los límites del marco del mapa base.

    -   En este caso hemos puesto los máximos y mínimos de las filas del listado, para que el mapa responda dinámicamente a los datos

-   `zoom =` (si es nulo se determina automáticamente)

-   `type =` qué tipo de mapa base - aquí hemos enumerado varias posibilidades y el código utiliza actualmente la primera ([1]) "osm"

-   `mergeTiles` = elegimos TRUE para que las capas se fusionen en uno solo

```{r, message=FALSE, warning=FALSE}
# load package
pacman::p_load(OpenStreetMap)

# Fit basemap by range of lat/long coordinates. Choose tile type
map <- openmap(
  upperLeft = c(max(linelist$lat, na.rm=T), max(linelist$lon, na.rm=T)),   # limits of basemap tile
  lowerRight = c(min(linelist$lat, na.rm=T), min(linelist$lon, na.rm=T)),
  zoom = NULL,
  type = c("osm", "stamen-toner", "stamen-terrain", "stamen-watercolor", "esri","esri-topo")[1])
```

Si trazamos este mapa ahora mismo, usando `autoplot.OpenStreetMap()` del paquete **OpenStreetMap**, verás que las unidades en los ejes no son coordenadas de latitud/longitud. Se está utilizando un sistema de coordenadas diferente. Para mostrar correctamente las residencias de los casos (que se almacenan en lat/long), se debe cambiar esto.

```{r, warning=F, message=F}
autoplot.OpenStreetMap(map)
```

Vamos a convertir el mapa a latitud/longitud con la función `openproj()` del paquete **OpenStreetMap**. Proporcionamos el mapa base `map` y también el Sistema de Referencia de Coordenadas (CRS) que queremos. Lo hacemos proporcionando la cadena de caracteres "proj.4" para la proyección WGS 1984, pero también se puede proporcionar el CRS de otras maneras. (ver [esta página](https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/understand-epsg-wkt-and-other-crs-definition-file-types/) para entender mejor qué es una cadena proj.4)

```{r, warning=F, message=F}
# Projection WGS84
map_latlon <- openproj(map, projection = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
```

Ahora cuando creamos el gráfico vemos que a lo largo de los ejes están las coordenadas de latitud y longitud. El sistema de coordenadas ha sido convertido. Ahora nuestros casos se trazarán correctamente si se superponen:

```{r, warning=F, message=F}
# Plot map. Must use "autoplot" in order to work with ggplot
autoplot.OpenStreetMap(map_latlon)
```

Consulta estos dos tutoriales [aquí](http://data-analytics.net/cep/Schedule_files/geospatial.html) y [aquí](https://www.rdocumentation.org/packages/OpenStreetMap/versions/0.3.4/topics/autoplot.OpenStreetMap) para obtener más información sobre este tema.

## Mapas de calor de densidad contorneada {#contoured-density-heatmaps}

A continuación describimos cómo conseguir un mapa de calor de densidad contorneada de casos, sobre un mapa base, comenzando con un listado (una fila por caso).

1)  Crear un mapa base a partir de OpenStreetMap, como se ha descrito anteriormente.
2)  Trazar los casos de `linelist` utilizando las columnas de latitud y longitud.
3)  Convertir los puntos en un mapa de calor de densidad con `stat_density_2d()` de **ggplot2**,

Cuando tenemos un mapa base con coordenadas de latitud y longitud, podemos trazar nuestros casos encima utilizando las coordenadas de latitud y longitud de su residencia.

Partiendo de la función `autoplot.OpenStreetMap()` para crear el mapa base, se pueden añadir las funciones de **ggplot2**, como se muestra con `geom_point()` a continuación:

```{r, warning=F, message=F}
# Plot map. Must be autoplotted to work with ggplot
autoplot.OpenStreetMap(map_latlon)+                 # begin with the basemap
  geom_point(                                       # add xy points from linelist lon and lat columns 
    data = linelist,                                
    aes(x = lon, y = lat),
    size = 1, 
    alpha = 0.5,
    show.legend = FALSE) +                          # drop legend entirely
  labs(x = "Longitude",                             # titles & labels
       y = "Latitude",
       title = "Cumulative cases")

```

El mapa anterior puede ser difícil de interpretar, especialmente con los puntos superpuestos. Para mejorar esto, vamos a trazar un mapa de densidad en 2d utilizando la función **ggplot2** `stat_density_2d()`. Sguemos utilizando las coordenadas lat/lon del listado, pero ahora estamos realizando una estimación de la densidad del núcleo en 2D y los resultados se muestran con líneas de contorno - como un mapa topográfico. Puedes leer esta [documentación](https://ggplot2.tidyverse.org/reference/geom_density_2d.html) completa para saber más.

```{r, warning=F, message=F}
# begin with the basemap
autoplot.OpenStreetMap(map_latlon)+
  
  # add the density plot
  ggplot2::stat_density_2d(
        data = linelist,
        aes(
          x = lon,
          y = lat,
          fill = ..level..,
          alpha = ..level..),
        bins = 10,
        geom = "polygon",
        contour_var = "count",
        show.legend = F) +                          
  
  # specify color scale
  scale_fill_gradient(low = "black", high = "red")+
  
  # labels 
  labs(x = "Longitude",
       y = "Latitude",
       title = "Distribution of cumulative cases")

```

<!-- ======================================================= -->

### Mapa de calor de series temporales {.unnumbered}

El mapa de calor de densidad anterior muestra los *casos acumulados*. Podemos examinar el brote a lo largo del tiempo y del espacio haciendo un facetado del mapa de calor basado en el *mes de inicio de los síntomas*, si tenemos esta información en el linelist.

Comenzamos con `linelist`, creando una nueva columna con el Año y el Mes de inicio. La función `format()` de R **base** cambia la forma en que se muestra una fecha. En este caso queremos "AAAA-MM".

```{r, warning=F, message=F}
# Extract month of onset
linelist <- linelist %>% 
  mutate(date_onset_ym = format(date_onset, "%Y-%m"))

# Examine the values 
table(linelist$date_onset_ym, useNA = "always")
```

Ahora, simplemente introducimos el facetado a través de **ggplot2** en el mapa de calor de densidad. Se aplica `facet_wrap()`, utilizando la nueva columna como filas. Fijamos el número de columnas de facetas en 4 paraque se vea mejor.

```{r, warning=F, message=F}
# packages
pacman::p_load(OpenStreetMap, tidyverse)

# begin with the basemap
autoplot.OpenStreetMap(map_latlon)+
  
  # add the density plot
  ggplot2::stat_density_2d(
        data = linelist,
        aes(
          x = lon,
          y = lat,
          fill = ..level..,
          alpha = ..level..),
        bins = 10,
        geom = "polygon",
        contour_var = "count",
        show.legend = F) +                          
  
  # specify color scale
  scale_fill_gradient(low = "black", high = "red")+
  
  # labels 
  labs(x = "Longitude",
       y = "Latitude",
       title = "Distribution of cumulative cases over time")+
  
  # facet the plot by month-year of onset
  facet_wrap(~ date_onset_ym, ncol = 4)               

```

<!-- SPATIAL STATISTICS SECTION IS UNDER DEVELOPMENT -->

## Estadísticas espaciales {#spatial-statistics}

La mayor parte de nuestra discusión hasta ahora se ha centrado en la visualización de datos espaciales. En algunos casos, también puede interesarte utilizar *estadísticas espaciales* para cuantificar las relaciones espaciales de los atributos de tus datos. En esta sección se ofrece una breve visión general de algunos conceptos claves de la estadística espacial y se sugiere algunos recursos que te resultarán útiles si deseas realizar análisis espaciales más exhaustivos.

### Relaciones espaciales {.unnumbered}

Antes de poder calcular cualquier estadística espacial, tenemos que especificar las relaciones entre las características de nuestros datos. Hay muchas formas de conceptualizar las relaciones espaciales, pero un modelo sencillo y comúnmente aplicable es el de la *adyacencia*, es decir, que esperamos una relación geográfica entre las zonas que comparten una frontera o son "vecinas" unas de otras.

Podemos cuantificar las relaciones de adyacencia entre los polígonos de las regiones administrativas en los datos `sle_adm3` que hemos estado utilizando con el paquete **spdep**. Especificaremos la contigüidad *queen*, que significa que las regiones serán vecinas si comparten al menos un punto a lo largo de sus fronteras. La alternativa sería la contigüidad *rook*, que requiere que las regiones compartan un borde - en nuestro caso, con polígonos irregulares, la distinción es trivial, pero en algunos casos la elección entre *queen* y *rook* puede ser influyente.

```{r}
sle_nb <- spdep::poly2nb(sle_adm3_dat, queen=T) # create neighbors 
sle_adjmat <- spdep::nb2mat(sle_nb)    # create matrix summarizing neighbor relationships
sle_listw <- spdep::nb2listw(sle_nb)   # create listw (list of weights) object -- we will need this later

sle_nb
round(sle_adjmat, digits = 2)
```

La matriz de arriba muestra las relaciones entre las 9 regiones de nuestros datos `sle_adm3`. Una puntuación de 0 indica que dos regiones no son vecinas, mientras que cualquier valor distinto de 0 indica una relación de vecindad. Los valores de la matriz se han escalado para que cada región tenga un peso total de 1 en la fila.

La mejor manera de visualizar estas relaciones de vecindad es dibujarlas:

```{r, fig.align='center', results='hide'}
plot(sle_adm3_dat$geometry) +                                           # plot region boundaries
  spdep::plot.nb(sle_nb,as(sle_adm3_dat, 'Spatial'), col='grey', add=T) # add neighbor relationships
```

Hemos utilizado un enfoque de adyacencia para identificar los polígonos vecinos; los vecinos que identificamos también se denominan a veces **vecinos por contigüidad**. Pero ésta es sólo una forma de elegir qué regiones se espera que tengan una relación geográfica. Los enfoques alternativos más comunes para identificar las relaciones geográficas generan vecinos basados en la **distancia**; brevemente, estos son:

-   **K-vecinos más cercanos** - Basándose en la distancia entre los centroides (el centro ponderado geográficamente de cada región poligonal), selecciona las *n* regiones más cercanas como vecinas. También se puede especificar un umbral de proximidad de distancia máxima. En **spdep,** puedes utilizar `knearneigh()` ([documentación](https://r-spatial.github.io/spdep/reference/knearneigh.html)).

-   **Vecinos de umbral de distancia** - Selecciona todos los vecinos dentro de un umbral de distancia. En **spdep**, estas relaciones de vecindad pueden ser identificadas usando `dnearneigh()` ([documentación](https://www.rdocumentation.org/packages/spdep/versions/1.1-7/topics/dnearneigh)).

### Autocorrelación espacial {.unnumbered}

La tan citada primera ley de la geografía de Tobler afirma que "todo está relacionado con todo lo demás, pero las cosas cercanas están más relacionadas que las lejanas". En epidemiología, esto suele significar que el riesgo de un determinado resultado sanitario en una región determinada es más similar al de sus regiones vecinas que al de las lejanas. Este concepto se ha formalizado como **autocorrelación espacial**: la propiedad estadística de que las características geográficas con valores similares se agrupan en el espacio. Las medidas estadísticas de autocorrelación espacial pueden utilizarse para *cuantificar el alcance de la agrupación espacial* de tus datos, *localizar dónde se produce la agrupación* e *identificar patrones compartidos de autocorrelación espacial* entre distintas variables de los datos. Esta sección ofrece una visión general de algunas medidas comunes de autocorrelación espacial y cómo calcularlas en R.

I de **Moran** - Se trata de una estadística de resumen global de la correlación entre el valor de una variable en una región y los valores de la misma variable en las regiones vecinas. La estadística I de Moran suele oscilar entre -1 y 1. Un valor de 0 indica que no hay ningún patrón de correlación espacial, mientras que los valores más cercanos a 1 o -1 indican una mayor autocorrelación espacial (valores similares cercanos) o dispersión espacial (valores disímiles cercanos), respectivamente.

Como ejemplo, calcularemos la estadística I de Moran para cuantificar la autocorrelación espacial en los casos de Ébola que hemos mapeado antes (recordemos que se trata de un subconjunto de casos de la epidemia simulada del  dataframe `linelist`). El paquete **spdep** tiene una función, `moran.test`, que puede hacer este cálculo por nosotros:

```{r}
moran_i <-spdep::moran.test(sle_adm3_dat$cases,    # numeric vector with variable of interest
                            listw=sle_listw)       # listw object summarizing neighbor relationships

moran_i                                            # print results of Moran's I test
```

El resultado de la función `moran.test()` nos muestra una estadística I de Moran de `round(moran_i$estimate[1],2)`. Esto indica la presencia de autocorrelación espacial en nuestros datos; en concreto, sugiere que es probable que las regiones con un número similar de casos de Ébola estén próximas entre sí. El valor p proporcionado por `moran.test()` se genera mediante la comparación con la expectativa bajo la hipótesis nula de ausencia de autocorrelación espacial, y puede utilizarse si se necesita informar de los resultados de una prueba de hipótesis formal.

I de **Moran local** - Podemos descomponer la estadística I de Moran (global) calculada anteriormente para identificar la autocorrelación espacial *localizada*; es decir, para identificar grupos específicos en nuestros datos. Esta estadística, que a veces se denomina **indicador local de asociación espacial (LISA)**, resume el grado de autocorrelación espacial alrededor de cada región individual. Puede ser útil para encontrar puntos "calientes" y "fríos" en el mapa.

Para mostrar un ejemplo, podemos calcular y mapear la I de Moran local para los recuentos de casos de Ébola utilizados anteriormente, con la función `local_moran()` de **spdep**:

```{r, fig.align='center'}
# calculate local Moran's I
local_moran <- spdep::localmoran(                  
  sle_adm3_dat$cases,                              # variable of interest
  listw=sle_listw                                  # listw object with neighbor weights
)

# join results to sf data
sle_adm3_dat<- cbind(sle_adm3_dat, local_moran)    

# plot map
ggplot(data=sle_adm3_dat) +
  geom_sf(aes(fill=Ii)) +
  theme_bw() +
  scale_fill_gradient2(low="#2c7bb6", mid="#ffffbf", high="#d7191c",
                       name="Local Moran's I") +
  labs(title="Local Moran's I statistic for Ebola cases",
       subtitle="Admin level 3 regions, Sierra Leone")

```

**Getis-Ord Gi**
* - Esta es otra estadística que se utiliza comúnmente para el análisis de puntos calientes; en gran parte, la popularidad de esta estadística se relaciona con su uso en la herramienta de análisis de puntos calientes en ArcGIS. Se basa en la suposición de que, normalmente, la diferencia del valor de una variable entre regiones vecinas debería seguir una distribución normal. Utiliza un enfoque de puntuación z para identificar las regiones que tienen valores significativamente más altos (punto caliente) o significativamente más bajos (punto frío) de una variable específica, en comparación con sus vecinos.

Podemos calcular y asignar la estadística Gi* utilizando la función `localG()` de **spdep**:

```{r}
# Perform local G analysis
getis_ord <- spdep::localG(
  sle_adm3_dat$cases,
  sle_listw
)

# join results to sf data
sle_adm3_dat$getis_ord <- getis_ord

# plot map
ggplot(data=sle_adm3_dat) +
  geom_sf(aes(fill=getis_ord)) +
  theme_bw() +
  scale_fill_gradient2(low="#2c7bb6", mid="#ffffbf", high="#d7191c",
                       name="Gi*") +
  labs(title="Getis-Ord Gi* statistic for Ebola cases",
       subtitle="Admin level 3 regions, Sierra Leone")

```

Como puedes ver, el mapa de Getis-Ord Gi* tiene un aspecto ligeramente diferente del mapa de Moran local elaborado anteriormente. Esto refleja que el método utilizado para calcular estas dos estadísticas es ligeramente diferente. Cuál de ellas debes utilizar depende de tu caso de uso específico y de la pregunta de investigación de interés.

**Prueba L de Lee** - Es una prueba estadística de correlación espacial bivariada. Permite comprobar si el patrón espacial de una determinada variable *x* es similar al patrón espacial de otra variable, *y*, que se supone que está relacionada espacialmente con *x*.

Para dar un ejemplo, vamos a probar si el patrón espacial de los casos de Ébola de la epidemia simulada está correlacionado con el patrón espacial de la población. Para empezar, necesitamos tener una variable `population` en nuestros datos `sle_adm3`. Podemos utilizar la variable `total` del dataframe `sle_adm3_pop` que hemos cargado anteriormente.

```{r}
sle_adm3_dat <- sle_adm3_dat %>% 
  rename(population = total)                          # rename 'total' to 'population'
```

Podemos visualizar rápidamente los patrones espaciales de las dos variables una al lado de la otra, para ver si se parecen:

```{r, fig.align='center', warning=F, message=F}
tmap_mode("plot")

cases_map <- tm_shape(sle_adm3_dat) + tm_polygons("cases") + tm_layout(main.title="Cases")
pop_map <- tm_shape(sle_adm3_dat) + tm_polygons("population") + tm_layout(main.title="Population")

tmap_arrange(cases_map, pop_map, ncol=2)   # arrange into 2x1 facets
```

Visualmente, los patrones parecen no parecen muy similares. Podemos utilizar la función `lee.test()` de **spdep** para comprobar estadísticamente si el patrón de autocorrelación espacial de las dos variables está relacionado. La estadística L será cercana a 0 si no hay correlación entre los patrones, cercana a 1 si hay una fuerte correlación positiva (es decir, los patrones son similares), y cercana a -1 si hay una fuerte correlación negativa (es decir, los patrones son inversos).

```{r, warning=F, message=F}
lee_test <- spdep::lee.test(
  x=sle_adm3_dat$cases,          # variable 1 to compare
  y=sle_adm3_dat$population,     # variable 2 to compare
  listw=sle_listw                # listw object with neighbor weights
)

lee_test
```

El resultado anterior muestra que la estadística L de Lee para nuestras dos variables fue `round(lee_test$estimate[1],2)`, lo que indica una débil correlación negativa. Esto confirma nuestra evaluación visual de que el patrón de los casos y la población no están relacionados entre sí, y proporciona pruebas de que el patrón espacial de los casos no es estrictamente un resultado de la densidad de población en las zonas de alto riesgo.

La estadística L de Lee puede ser útil para hacer este tipo de inferencias sobre la relación entre variables distribuidas espacialmente; sin embargo, para describir la naturaleza de la relación entre dos variables con más detalle, o ajustar por confusión, tenemos que aplicar técnicas de *regresión espacial*. Describeremos brevemente algunas de estas en la siguiente sección.

### Regresión espacial {.unnumbered}

Es posible que quieras hacer inferencias estadísticas sobre las relaciones entre las variables de tus datos espaciales. En estos casos, es útil considerar las técnicas de *regresión espacial*, es decir, los enfoques de regresión que consideran explícitamente la organización espacial de las unidades en los datos. Algunas de las razones por las que puedes necesitar considerar modelos de regresión espacial, en lugar de modelos de regresión estándar como los GLM, incluyen:

-   Los modelos de regresión estándar asumen que los residuos son independientes entre sí. En presencia de una  *autocorrelación espacial* fuerte, es probable que los residuos de un modelo de regresión estándar también estén autocorrelacionados espacialmente, violando así este supuesto. Esto puede dar lugar a problemas de interpretación de los resultados del modelo, en cuyo caso sería preferible un modelo espacial.

-   Los modelos de regresión también suelen suponer que el efecto de una variable *x* es constante en todas las observaciones. En el caso de la *heterogeneidad espacial*, los efectos que deseamos estimar pueden variar a lo largo del espacio, y podemos estar interesados en cuantificar esas diferencias. En este caso, los modelos de regresión espacial ofrecen más flexibilidad para estimar e interpretar los efectos.

Los detalles de los enfoques de regresión espacial están fuera del alcance de este manual. En su lugar, esta sección ofrece una visión general de los modelos de regresión espacial más comunes y sus usos, y te remite a referencias que pueden ser útiles por si se deseas profundizar en este ámbito.

**Modelos de error espacial** - Estos modelos suponen que los términos de error entre unidades espaciales están correlacionados, en cuyo caso los datos violarían los supuestos de un modelo OLS estándar. Los modelos de error espacial también se denominan a veces **modelos autorregresivos simultáneos (SAR)**. Pueden ajustarse utilizando la función `errorsarlm()` del paquete **spatialreg** (funciones de regresión espacial que solían formar parte de **spdep**).

**Modelos de desfase espacial** - Estos modelos suponen que la variable dependiente de una región *i* está influida no sólo por el valor de las variables independientes en *i*, sino también por los valores de esas variables en las regiones vecinas a *i.* Al igual que los modelos de error espacial, los modelos de desfase espacial también se describen a veces como **modelos autorregresivos simultáneos (SAR)**. Pueden ajustarse utilizando la función `lagsarlm()` del paquete **spatialreg**.

El paquete **spdep** contiene varias pruebas de diagnóstico útiles para decidir entre los modelos OLS estándar, de desfase espacial y de error espacial. Estas pruebas, denominadas *diagnósticos del multiplicador de Lagrange*, pueden utilizarse para identificar el tipo de dependencia espacial en sus datos y elegir el modelo más apropiado. La función `lm.LMtests()` puede utilizarse para calcular todos los diagnósticos del multiplicador de Lagrange. Anselin (1988) también proporciona una útil herramienta de diagrama de flujo para decidir qué modelo de regresión espacial utilizar basándose en los resultados de las pruebas del multiplicador de Lagrange:

```{r, fig.align='center', echo=F}
knitr::include_graphics(here::here("images", "gis_lmflowchart.jpg"))
```

**Modelos jerárquicos bayesianos**: Los enfoques bayesianos se utilizan habitualmente para algunas aplicaciones del análisis espacial, sobre todo para el [mapeo de enfermedades](https://pubmed.ncbi.nlm.nih.gov/15690999/). Se prefieren en los casos en los que los datos de los casos están escasamente distribuidos (por ejemplo, en el caso de un resultado raro) o son estadísticamente "ruidosos", ya que pueden utilizarse para generar estimaciones "suavizadas" del riesgo de enfermedad al tener en cuenta el proceso espacial latente subyacente. Esto puede mejorar la calidad de las estimaciones. También permiten que el investigador especifique previamente (mediante la elección de "priors" (valores pre-establecidos)) los patrones complejos de correlación espacial que pueden existir en los datos, los cuales pueden tomar en cuenta la variación espacialmente dependiente e independiente en las variables independientes y dependientes. En R, los modelos jerárquicos bayesianos pueden ajustarse utilizando el paquete **CARbayes** (véase [la viñeta](https://cran.r-project.org/web/packages/CARBayes/vignettes/CARBayes.pdf)) o R-INLA (véase [este sitio web](https://www.r-inla.org/home) y el [libro de texto](https://becarioprecario.bitbucket.io/inla-gitbook/)). A través de R también puedes usar software externo que realice estimaciones bayesianas, como JAGS o WinBUGS.

<!-- ======================================================= -->

## Recursos {#resources-21}

-   Funciones simples de R y [viñeta del paquete sf](https://cran.r-project.org/web/packages/sf/vignettes/sf1.html)

-   [Viñeta del paquete tmap](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html)

-   ggmap: [Visualización espacial con ggplot2](https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf)

-   [Introducción a la elaboración de mapas con R, visión general de los diferentes paquetes](https://bookdown.org/nicohahn/making_maps_with_r5/docs/introduction.html)

-   Datos espaciales en R [(curso EarthLab)](https://www.earthdatascience.org/courses/earth-analytics/spatial-data-r/)

-   [Libro de texto](https://link.springer.com/book/10.1007/978-1-4614-7618-4) Applied Spatial Data Analysis in R

-   **SpatialEpiApp** - una [aplicación Shiny que se puede descargar como un paquete de R](https://github.com/Paula-Moraga/SpatialEpiApp), lo que le permite proporcionar sus propios datos y llevar a cabo la cartografía, el análisis de conglomerados y las estadísticas espaciales.

-   [Taller de](http://www.econ.uiuc.edu/~lab/workshop/Spatial_in_R.html) introducción a la econometría espacial en R
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/gis.Rmd-->

# (PART) Visualización de datos {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_data_viz.Rmd-->


# Tablas para presentaciones {#tables-for-presentation}


```{r echo=FALSE, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}

linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) 

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make HTML tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

table <- linelist %>% 
  # filter
  ########
  #filter(!is.na(outcome) & hospital != "Missing") %>%  # Remove cases with missing outcome or hospital
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known) %>%                                 # Arrange rows from lowest to highest (Total row at bottom)

  # formatting
  ############
  flextable() %>% 
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  #bg(., j=c(1:8), i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") %>% 
  bg(j = 7, i = ~ Pct_Death >= 55, part = "body", bg = "red") %>% 
  colformat_num(., j = c(4,7), digits = 1) %>%
  bold(i = 1, bold = TRUE, part = "header") %>% 
  bold(i = 7, bold = TRUE, part = "body")

table
```


Esta página muestra cómo convertir dataframes con datos agrupados en tablas preparadas para su presentación con el paquete **flextable**. Estas tablas pueden insertarse en diapositivas de PowerPoint, páginas HTML, documentos PDF o Word, etc.

Comprende que *antes de* utilizar **flextable**, debes crear la tabla resumen como un dataframe. Utiliza los métodos de las páginas [Tablas descriptivas](#descriptive-tables) y [Pivotar de datos](#pivoting-data), como tabulaciones, tabulaciones cruzadas, pivoteo y cálculo de estadísticas descriptivas. El dataframe resultante puede pasarse a **flextable** para ponerle el formato.

Hay muchos otros paquetes de R que se pueden utilizar para elaborar tablas para su presentación - hemos elegido destacar **flextable** en esta página. Un ejemplo que utiliza el paquete **knitr** y su función `kable()` se puede encontrar en la página [rastreo de contactos](#contact-tracing-1). Asimismo, el paquete **DT** se destaca en la página [Dashboards con Shiny](#dashboards-with-shiny). Otros como **GT** y **huxtable** se mencionan en la página de [Paquetes recomendados](#suggested-packages-1).



<!-- ======================================================= -->
## Preparación {#preparation-20}

### Cargar paquetes {.unnumbered} 

Instala y carga **flextable**. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar paquetes con `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R. 

```{r}
pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make HTML tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

```

### Importar datos {.unnumbered}  

Para empezar, importamos los datos limpios de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica aquí para descargar linelist "limpio" ](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds)(como archivo .rds). Importa los datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - Mira la página de [importación y exportación](#import-and-export) para más detalles).


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas de `linelist`.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Preparar la tabla {.unnumbered}  

*Antes de* empezar a utilizar **flextable** tendrás que *crear* tu tabla como un dataframe. Consulta la página sobre [Tablas descriptivas](#descriptive-tables) y [Pivotar datos](#pivoting-data) para aprender a crear un dataframe utilizando paquetes como **janitor** y **dplyr**. Debes organizar el contenido en filas y columnas tal y como quieres que se muestre. Luego, el dataframe se pasará a **flextable** para mostrarlo con colores, encabezados, fuentes, etc.

A continuación se muestra un ejemplo de la página de [tablas descriptivas](#descriptive-tables) para convertir la lista de casos en un dataframe que resume los resultados de los pacientes y los valores de TC por hospital, con una fila de totales en la parte inferior. El resultado se guarda como `table`. 

```{r message=FALSE, warning=FALSE}
table <- linelist %>% 
  
  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Agrupar datos
  summarise(                                           # Creaar columnas nuevas con indicadores de interés
    N = n(),                                            # Número de filas por grupos de hospital-resultado     
    ct_value = median(ct_blood, na.rm=T)) %>%           # Valor de la mediana CT por grupo
  
  # add totals
  ############
  bind_rows(                                           # Une la tabla anterior con esta mini-tabla de totales
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Agrupados sólo por resultado, no por hospital    
      summarise(
        N = n(),                                       # Número de filas del conjunto de datos     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Mediana CT del conjunto de datos 
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivotar de largo a ancho
    values_from = c(ct_value, N),                       # Los nuevos valores están desde la columna ct a la count
    names_from = outcome) %>%                           # los nombres nuevos de columna son para el resultado 
  mutate(                                              # Añadir columnas nuevas
    N_Known = N_Death + N_Recover,                               # número con resultado conocidos
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # porcentaje de casos que fallecieron (con 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # porcentaje que se recuperaron (con 1 decimal)
  select(                                              # Re-ordenar columnas
    hospital, N_Known,                                   # Intro columnas
    N_Recover, Pct_Recover, ct_value_Recover,            # Columnas para recuerados
    N_Death, Pct_Death, ct_value_Death)  %>%             # Columnas para fallecidos
  arrange(N_Known)                                    # Ordenar las filas de menor a mayor (fila total al final)

table  # print

```




<!-- ======================================================= -->
## Flextable básica {#basic-flextable}

### Crear una flextble {.unnumbered}  

Para crear y gestionar los objetos de **flextable**, primero pasamos el dataframe por la función `flextable()`. Guardamos el resultado como `my_table`.

```{r}

my_table <- flextable(table) 
my_table

```

Después de hacer esto, podemos enlazar con pipe progresivamente el objeto `my_table` a través de más funciones de formato de **flextable**.

En esta página, para mayor claridad, guardaremos la tabla en pasos intermedios como `my_table`, añadiendo las funciones de **flextable** bit a bit. Si quieres ver *todo* el código de principio a fin escrito en un solo trozo, visita la sección [Todo el código junto](#tbl_pres_all) más abajo.

La sintaxis general de cada línea de código de **flextable** es la siguiente:

* `function(table, i = X, j = X, part = "X")`, donde:
  * La "función" puede ser una de muchas funciones diferentes, como `width()` para determinar el ancho de las columnas, `bg()` para establecer los colores de fondo, `align()` para establecer si el texto está alineado al centro/derecha/izquierda, etc.
  * `table = ` es el nombre del dataframe, aunque no es necesario indicarlo si el dataframe se introduce en la función.
  * `part = ` se refiere a la parte de la tabla a la que se aplica la función. Por ejemplo,  "header", "body" o "all".
  * `i= ` especifica la *fila* a la que se aplicará la función, donde 'X' es el número de fila. Si se trata de varias filas, por ejemplo de la primera a la tercera, se puede especificar:`i = c(1:3)`. Ten en cuenta que si se selecciona "body", la primera fila empieza por debajo de la sección de cabecera.
  * `j = ` especifica la *columna* a la que se aplicará la función, donde 'x' es el número o nombre de la columna. Si hay varias columnas, por ejemplo la quinta y la sexta, se puede especificar: `j = c(5,6)`.

Puedes encontrar la lista completa de funciones de formato de **flextable** [aquí](https://davidgohel.github.io/flextable/reference/index.html) o revisar la documentación escribiendo `?flextable`.


### Ancho de columna {.unnumbered}

Podemos utilizar la función `autofit()`, que estira la tabla de forma que cada celda sólo tiene una fila de texto. La función `qflextable()` es una abreviatura conveniente para `flextable()` y `autofit()`.

```{r}

my_table %>% autofit()

```

Sin embargo, esto podría no ser siempre apropiado, especialmente si hay valores muy largos dentro de las celdas, lo que significa que la tabla podría no caber en la página.

En cambio, podemos especificar el ancho con la función `width()`. Puede ser necesario jugar un poco para saber qué valor de anchura poner. En el ejemplo siguiente, especificamos diferentes anchos para la columna 1, la columna 2 y las columnas 4 a 8.

```{r}

my_table <- my_table %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1)

my_table
  
```

### Encabezados de columna {.unnumbered}

Queremos encabezados más claros para facilitar la interpretación del contenido de la tabla.

Para esta tabla, querremos añadir una segunda capa de cabecera para que las columnas que cubren los mismos subgrupos puedan agruparse. Lo hacemos con la función `add_header_row()` con `top = TRUE`. Proporcionamos el nuevo nombre de cada columna a `values =` , dejando los valores vacíos `""` para las columnas que sabemos que vamos a fusionar más tarde.

También renombramos los nombres de las cabeceras en la ahora segunda cabecera en un comando separado `set_header_labels()`.

Por último, para "combinar" ciertas cabeceras de columna en la cabecera superior utilizamos `merge_at()` para fusionar las cabeceras de columna en la fila de la cabecera superior.

```{r}
my_table <- my_table %>% 
  
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    
  set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header")     # Horizontally merge columns 6 to 8 in new header row

my_table  # print

```

### Bordes y fondos {.unnumbered}  

Puedes ajustar los bordes, las líneas internas, etc. con varias funciones de **flextable**. A menudo es más fácil empezar eliminando todos los bordes existentes con `border_remove()`.

A continuación, puedes aplicar los temas de borde por defecto pasando la tabla a `theme_box()`, `theme_booktabs()` o `theme_alafoli()`.

Puedes añadir líneas verticales y horizontales con una variedad de funciones. `hline()` y `vline()` añaden líneas a una fila o columna especificada, respectivamente. Dentro de cada una, debes especificar la `part = ` como "all", "body", o "header". Para las líneas verticales, especifica la columna `j = `, y para las líneas horizontales la fila a `i = `. Otras funciones como `vline_right()`, `vline_left()`, `hline_top()`, y `hline_bottom()` añaden líneas sólo a los lados.

En todas estas funciones, el propio estilo de línea debe especificarse a `border = ` y debe ser la salida de un comando separado utilizando la función `fp_border()` del paquete **officer**. Esta función te ayuda a definir el ancho y el color de la línea. Puedes definirlo sobre los comandos de la tabla, como se muestra a continuación.

```{r}
# define style for border line
border_style = officer::fp_border(color="black", width=1)

# add border lines to table
my_table <- my_table %>% 

  # Remove all existing borders
  border_remove() %>%  
  
  # add horizontal lines via a pre-determined theme setting
  theme_booktabs() %>% 
  
  # add vertical lines to separate Recovered and Died sections
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style)       # at column 5

my_table
```

### Fuente y alineación {.unnumbered}

Alineamos en el centro todas las columnas, excepto la más a la izquierda, con los nombres de los hospitales, utilizando la función `align()` de **flextable**.

```{r}
my_table <- my_table %>% 
   flextable::align(align = "center", j = c(2:8), part = "all") 
my_table
```

Además, podemos aumentar el tamaño de la fuente de la cabecera y cambiarla a negrita. También podemos cambiar la fila total a negrita. 

```{r}

my_table <-  my_table %>%  
  fontsize(i = 1, size = 12, part = "header") %>%   # adjust font size of header
  bold(i = 1, bold = TRUE, part = "header") %>%     # adjust bold face of header
  bold(i = 7, bold = TRUE, part = "body")           # adjust bold face of total row (row 7 of body)

my_table

```

Podemos asegurar que las columnas de proporción muestren sólo un decimal utilizando la función `colformat_num()`. Ten en cuenta que esto también podría haberse hecho en la fase de gestión de datos con la función `round()`.

```{r}
my_table <- colformat_num(my_table, j = c(4,7), digits = 1)
my_table
```

### Fusionar celdas {.unnumbered}  

Al igual que fusionamos celdas horizontalmente en la fila de la cabecera, también podemos fusionar celdas verticalmente utilizando `merge_at()` y especificando las filas (`i`) y la columna (`j`). Aquí fusionamos los valores "Hospital" y "Total cases with known outcome" verticalmente para darles más espacio.

```{r}
my_table <- my_table %>% 
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header")

my_table
```

### Color de fondo {.unnumbered}

Para distinguir el contenido de la tabla de las cabeceras, es posible que queramos añadir un formato adicional, por ejemplo, cambiando el color de fondo. En este ejemplo cambiamos el cuerpo de la tabla a gris.

```{r}
my_table <- my_table %>% 
    bg(part = "body", bg = "gray95")  

my_table 
```


<!-- ======================================================= -->
## Formato condicional {#conditional-formatting}

Podemos resaltar todos los valores de una columna que cumplan una determinada regla, por ejemplo, que más del 55% de los casos hayan muerto. Basta con poner el criterio en el argumento `i = ` o `j = `, precedido de una tilde `~`. Escribe la referencia a la columna en el dataframe, no a los valores del encabezamiento de la pantalla.

```{r}

my_table %>% 
  bg(j = 7, i = ~ Pct_Death >= 55, part = "body", bg = "red") 

```

O bien, podemos resaltar toda la fila que cumpla un determinado criterio, como un hospital de interés. Para ello, basta con eliminar la especificación de la columna (`j`) para que los criterios se apliquen a todas las columnas.


```{r}

my_table %>% 
  bg(., i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") 

```

## Todo el código junto {#tbl_pres_all}  

A continuación mostramos todo el código de las secciones anteriores juntas.

```{r}  

border_style = officer::fp_border(color="black", width=1)

pacman::p_load(
  rio,            # import/export
  here,           # file pathways
  flextable,      # make HTML tables 
  officer,        # helper functions for tables
  tidyverse)      # data management, summary, and visualization

table <- linelist %>% 

  # Get summary values per hospital-outcome group
  ###############################################
  group_by(hospital, outcome) %>%                      # Group data
  summarise(                                           # Create new summary columns of indicators of interest
    N = n(),                                            # Number of rows per hospital-outcome group     
    ct_value = median(ct_blood, na.rm=T)) %>%           # median CT value per group
  
  # add totals
  ############
  bind_rows(                                           # Bind the previous table with this mini-table of totals
    linelist %>% 
      filter(!is.na(outcome) & hospital != "Missing") %>%
      group_by(outcome) %>%                            # Grouped only by outcome, not by hospital    
      summarise(
        N = n(),                                       # Number of rows for whole dataset     
        ct_value = median(ct_blood, na.rm=T))) %>%     # Median CT for whole dataset
  
  # Pivot wider and format
  ########################
  mutate(hospital = replace_na(hospital, "Total")) %>% 
  pivot_wider(                                         # Pivot from long to wide
    values_from = c(ct_value, N),                       # new values are from ct and count columns
    names_from = outcome) %>%                           # new column names are from outcomes
  mutate(                                              # Add new columns
    N_Known = N_Death + N_Recover,                               # number with known outcome
    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)
    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %>% # percent who recovered (to 1 decimal)
  select(                                              # Re-order columns
    hospital, N_Known,                                   # Intro columns
    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns
    N_Death, Pct_Death, ct_value_Death)  %>%             # Death columns
  arrange(N_Known) %>%                                 # Arrange rows from lowest to highest (Total row at bottom)

  # formatting
  ############
  flextable() %>%              # table is piped in from above
  add_header_row(
    top = TRUE,                # New header goes on top of existing header row
    values = c("Hospital",     # Header values for each column below
               "Total cases with known outcome", 
               "Recovered",    # This will be the top-level header for this and two next columns
               "",
               "",
               "Died",         # This will be the top-level header for this and two next columns
               "",             # Leave blank, as it will be merged with "Died"
               "")) %>% 
    set_header_labels(         # Rename the columns in original header row
      hospital = "", 
      N_Known = "",                  
      N_Recover = "Total",
      Pct_Recover = "% of cases",
      ct_value_Recover = "Median CT values",
      N_Death = "Total",
      Pct_Death = "% of cases",
      ct_value_Death = "Median CT values")  %>% 
  merge_at(i = 1, j = 3:5, part = "header") %>% # Horizontally merge columns 3 to 5 in new header row
  merge_at(i = 1, j = 6:8, part = "header") %>%  
  border_remove() %>%  
  theme_booktabs() %>% 
  vline(part = "all", j = 2, border = border_style) %>%   # at column 2 
  vline(part = "all", j = 5, border = border_style) %>%   # at column 5
  merge_at(i = 1:2, j = 1, part = "header") %>% 
  merge_at(i = 1:2, j = 2, part = "header") %>% 
  width(j=1, width = 2.7) %>% 
  width(j=2, width = 1.5) %>% 
  width(j=c(4,5,7,8), width = 1) %>% 
  flextable::align(., align = "center", j = c(2:8), part = "all") %>% 
  bg(., part = "body", bg = "gray95")  %>% 
  bg(., j=c(1:8), i= ~ hospital == "Military Hospital", part = "body", bg = "#91c293") %>% 
  colformat_num(., j = c(4,7), digits = 1) %>%
  bold(i = 1, bold = TRUE, part = "header") %>% 
  bold(i = 7, bold = TRUE, part = "body")

table
```


<!-- ======================================================= -->
## Guardar tu tabla {#saving-your-table}

Hay diferentes maneras de integrar la tabla en tu salida.

### Guardar una tabla {.unnumbered}

Puedes exportar las tablas a Word, PowerPoint o HTML o como archivos de imagen (PNG). Para ello, utiliza una de las siguientes funciones:

* `save_as_docx()`  
* `save_as_pptx()`  
* `save_as_image()`  
* `save_as_html()`  

Por ejemplo, a continuación guardamos nuestra tabla como un documento de Word. Ten en cuenta la sintaxis del primer argumento - puedes proporcionar simplemente el nombre de tu objeto flextable, por ejemplo, `my_table`, o puedes darle un "nombre" como se muestra a continuación (el nombre es "my_table"). Si se especifica un nombre, éste aparecerá como el título de la tabla en Word. También mostramos el código para guardar como imagen PNG.

```{r message=FALSE, warning=FALSE, eval=F}
# Edit the 'my table' as needed for the title of table.  
save_as_docx("my table" = my_table, path = "file.docx")

save_as_image(my_table, path = "file.png")
```

Ten en cuenta que los paquetes `webshot` o `webshot2` son necesarios para guardar una flextable como imagen. Las imágenes pueden salir con fondos transparentes.

Si deseas ver una versión "en vivo" de la salida de **flextable** en el formato de documento previsto, utiliza `print()` y especifica uno de los siguientes para `preview = `. El documento se "abrirá" en tu ordenador en el programa de software especificado, pero no se guardará. Esto puede ser útil para comprobar si la tabla cabe en una página/diapositiva o para poder copiarla rápidamente en otro documento, puedes utilizar el método de impresión con el argumento vista previa establecido en "pptx" o "docx".

```{r, eval=F}
print(my_table, preview = "docx") # Word document example
print(my_table, preview = "pptx") # Powerpoint example
```

### Imprimir tabla en R markdown {.unnumbered}  

Esta tabla puede integrarse en un documento automatizado, una salida de R markdown, si el objeto tabla se llama dentro del chunk de R markdown. Esto significa que la tabla puede actualizarse como parte de un informe en el que los datos podrían cambiar, por lo que los números pueden actualizarse.

Mira los detalles en la página de [Informes con R Markdown](#reports-with-r-markdown) de este manual.

<!-- ======================================================= -->
## Recursos {#resources-22}

El libro completo de **flextable** está en: https://ardata-fr.github.io/flextable-book/  El sitio Github está [aquí](https://davidgohel.github.io/flextable/)
Un manual de todas las funciones de **flextable** puede encontrarse [aquí](https://davidgohel.github.io/flextable/reference/index.html)

Puedes acceder a una galería de bonitos ejemplos de **flextable**s con código [aquí](https://ardata-fr.github.io/flextable-gallery/gallery/)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/tables_presentation.Rmd-->


# Conceptos básicos de ggplot {#ggplot-basics}

```{r, out.width=c('100%', '100%'), fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "ggplot_basics_top.png"))
```


**ggplot2** es el paquete de R más popular para la visualización de datos. Su función `ggplot()` es el núcleo de este paquete, y todo este enfoque se conoce coloquialmente como *"ggplot"*, con las figuras resultantes a veces llamadas afectuosamente "ggplots". El "gg" en estos nombres se refiere a la "**g**ramática de los **g**ráficos" utilizada para construir las figuras. **ggplot2** se beneficia de una amplia variedad de paquetes de R complementarios que mejoran aún más su funcionalidad.

La sintaxis es significativamente diferente de los dibujos de `R` **base**, y tiene una curva de aprendizaje asociada. El uso de **ggplot2** generalmente requiere que el usuario formatee sus datos de una manera que sea altamente compatible con **tidyverse**, lo que en última instancia hace que el uso conjunto de estos paquetes sea muy eficaz.

En esta página cubriremos los fundamentos de la creación de gráficos con **ggplot2**. Consulta la página [Consejos de ggplot](#ggplot-tips) para sugerencias y técnicas avanzadas para lograr que sus gráficos se vean realmente bien.

Hay varios tutoriales extensos de **ggplot2** enlazados en la sección de recursos. También puede descargar esta [hoja de trucos de visualización de datos con ggplot](https://github.com/rstudio/cheatsheets/blob/main/data-visualization-2.1.pdf) desde el sitio web de RStudio. Si quieres inspirarte en formas de visualizar tus datos de forma creativa, te sugerimos que revises sitios web como la [galería de gráficos de R](https://www.r-graph-gallery.com/) y [Data-to-viz](https://www.data-to-viz.com/caveats.html).



<!-- ======================================================= -->
## Preparación {#preparation-21}

### Cargar paquetes {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También podes cargar los paquetes instalados con `library()` de R **base.** Consulta la página sobre los [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r}
pacman::p_load(
  tidyverse,      # incluye ggplot2 y otras herramientas de gestión de datos
  rio,            # importación/exportación
  here,           # rutas d elos archivos
  stringr         # trabajar con caracteres    
)
```

### Importar datos {.unnumbered} 

Importamos el conjunto de datos de casos de una epidemia de Ebola simulada. Si quieres seguir el proceso, [cliquea para descargar linelist "limpia"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Para importar sus datos utilizando la función `import()` del paquete **rio** (acepta muchos tipos de archivos como .xlsx, .rds, .csv - consulta la página de [importación y exportación](#import-and-export) para más detalles).

```{r,  echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist <- rio::import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas del listado. Nos centraremos en las variables continuas `age`, `wt_kg` (peso en kilos), `ct_blood` (valores de CT, cycle threshold, umbral de ciclo del test de PCR) y `days_onset_hosp` (diferencia entre la fecha de inicio de síntomas y la hospitalización). 

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### Limpieza general {.unnumbered}

Cuando se preparan los datos para trazarlos (graficarlos), lo mejor es hacer que los datos se adhieran a [los estándares de datos "ordenados" tanto como](https://es.r4ds.hadley.nz/datos-ordenados.html) sea posible. En las páginas de este manual, sobre [Limpieza de datos y funciones básicas](#cleaning-data-and-core-functions), se explica cómo conseguirlo.

Algunas formas sencillas de preparar nuestros datos para que sea mas fáciles trazarlos pueden incluir la mejora del contenido de los datos para su visualización, lo que no equivale necesariamente a una manipulación de los datos mas sencilla. Por ejemplo:

* Sustituye los valores `NA` de una columna de caracteres por la cadena de caracteres "Unknown" (Desconocido)
* Considera la posibilidad de convertir la columna en *de tipo factor* para que sus valores tengan niveles ordinales prescritos
* Limpia los valores de algunas columnas para cambiar texto "amigable con los datos" con barra baja, etc. a texto normal o a mayúsculas y minúsculas (ver [Caracteres y cadenas](#characters-and-strings))

He aquí algunos ejemplos de esto en acción:

```{r, }
# hace la versión de visualización de las columnas con nombres más amigables
linelist <- linelist %>%
  mutate(
    gender_disp = case_when(gender == "m" ~ "Male",        # m a Masculino 
                            gender == "f" ~ "Female",      # f a Femenino,
                            is.na(gender) ~ "Unknown"),    # NA a Desconocido
    
    outcome_disp = replace_na(outcome, "Unknown")          # sustituye el resultado NA por "unknown"
  )
```

### Pivotar a lo largo {.unnumbered}

Como una cuestión de estructura de datos, para **ggplot2** a menudo queremos pivotar nuestros datos en formatos *largos*. Lee más sobre esto en la página de [Pivoteo de datos](#pivoting-data).  


```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "pivoting", "pivot_longer_new.png"))
```


Por ejemplo, digamos que queremos trazar datos que están en un formato "a lo ancho", como por ejemplo para cada caso en `linelist` y sus síntomas. A continuación creamos una minilista llamada `symptoms_data` que contiene sólo las columnas `case_id` y symptoms.  

```{r}
symptoms_data <- linelist %>% 
  select(c(case_id, fever, chills, cough, aches, vomit))
```

Así es como se ven las primeras 50 filas de esta minilista - ¿ves cómo están formateadas "a lo ancho" con cada síntoma como una columna?

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(symptoms_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Si quisiéramos trazar el número de casos con síntomas específicos, estamos limitados por el hecho de que cada síntoma es una columna específica. Sin embargo, podemos *hacer pivotar* las columnas de síntomas a un formato más largo como este:

```{r, }
symptoms_data_long <- symptoms_data %>%    # comienza con una "mini" lista de líneas llamada symptoms_data
  
  pivot_longer(
    cols = -case_id,                       # pivotea todas las columnas excepto case_id (todas las de síntomas)
    names_to = "symptom_name",             # se asigna un nombre a la nueva columna que contiene los síntomas
    values_to = "symptom_is_present") %>%  # se asigna un nombre a la nueva columna que contiene los valores (yes/no)
  
  mutate(symptom_is_present = replace_na(symptom_is_present, "unknown")) # convierte NA en "unknown"

```


Aquí están las primeras 50 filas. Observa que cada caso tiene 5 filas - una para cada síntoma posible. Las nuevas columnas `symptom_name` y `symptom_is_present` son el resultado del pivote. Ten en cuenta que este formato puede no ser muy útil para otras operaciones, pero es útil para trazar.

```{r, message=FALSE, echo=F}
DT::datatable(head(symptoms_data_long, 50), rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```






<!-- ======================================================= -->
## Fundamentos de ggplot  {#basics-of-ggplot}

**"Gramática de los gráficos" - ggplot2**  

El trazado con **ggplot2** se basa en "añadir" capas de trazado y elementos de diseño unos sobre otros, añadiendo cada comando a los anteriores con un símbolo de suma (`+`). El resultado es un objeto de trazado multicapa que se puede guardar, modificar, imprimir, exportar, etc.

Los objetos ggplot pueden ser muy complejos, pero el orden básico de las capas suele ser el siguiente:

1.  Comienza con el comando `ggplot()` como punto de partida  - esto "abre" el ggplot y permite agregar las funciones subsecuentes con `+`. Normalmente, el conjunto de datos también se especifica en este comando
2.  Añadí capas "geom" - estas funciones visualizan los datos como *geometrías* (*formas*), por ejemplo, como un gráfico de barras, un gráfico de líneas, un gráfico de dispersión, un histograma (¡o una combinación!). Todas estas funciones comienzan con `geom_` como prefijo.
3.  Añadí elementos de diseño al gráfico, como etiquetas de ejes, título, fuentes, tamaños, esquemas de color, leyendas o rotación de ejes.

Un ejemplo sencillo del esqueleto del código es el siguiente. Explicaremos cada componente en las secciones siguientes.

```{r, eval=F}
# Traza los datos de las columnas de my_data como puntos rojos
ggplot(data = my_data)+                   # Usa el conjunto de datos my_data"
  geom_point(                             # añade una capa de puntos
    mapping = aes(x = col1, y = col2),    # "asigna" la columna de datos a los ejes
    color = "red")+                       # otras especificaciones para el geom
  labs()+                                 # aquí se añaden los títulos, las etiquetas de los ejes, etc.
  theme()                                 # aquí se ajusta el color, la fuente, el tamaño, etc. de los elementos de trazado no relacionados con los datos (ejes, título, etc.) 
```

 


## `ggplot()`  

El comando de apertura de cualquier gráfico ggplot2 es `ggplot()`. Este comando simplemente crea un lienzo en blanco sobre el que añadir capas. Se "abre" el camino para añadir más capas con un símbolo `+`.

Normalmente, el comando `ggplot()` incluye el argumento `data =` para el gráfico. Esto establece el conjunto de datos que se utilizará de manera predeterminada para las capas posteriores del gráfico.

Este comando terminará con un `+` después de su paréntesis de cierre. Esto deja el comando "abierto". El ggplot sólo se ejecutará/aparecerá cuando el comando completo incluya una capa final *sin* un `+` al final.

```{r, eval=F}
# Esto creará un lienzo en blanco
ggplot(data = linelist)
```


## Geoms  

Un lienzo en blanco no es suficiente: necesitamos crear geometrías (formas o tipos de gráfico) a partir de nuestros datos (por ejemplo, gráficos de barras, histogramas, gráficos de dispersión, gráficos de caja).

Esto se hace añadiendo capas "geoms" al comando inicial `ggplot()`. Hay muchas funciones de **ggplot2** que crean "geoms". Cada una de estas funciones comienza con "geom_", por lo que nos referiremos a ellas genéricamente como `geom_XXXX()`. Hay más de 40 geoms disponibles en **ggplot2** y muchos otros creados por fans. Míralos en la [galería de ggplot2](https://exts.ggplot2.tidyverse.org/gallery/). Algunos geoms de uso común se enumeran a continuación:

* Histogramas - `geom_histogram()`
* Gráficos de barras - `geom_bar()` o `geom_col()` (véase [la sección "Gráfico de barras"](#ggplot_basics_bars))
* Gráficos de caja - `geom_boxplot()`
* Puntos (por ejemplo, gráficos de dispersión) - `geom_point()`
* Gráficos de líneas - `geom_line()` o `geom_path()`
* Líneas de tendencia - `geom_smooth()`

En un gráfico se pueden exponer uno o varios geoms. Cada uno se añade a los comandos anteriores de **ggplot2** con un `+`, y se agregan secuencialmente de manera que los geoms posteriores se trazan encima de los anteriores.



## Asignación de datos al gráfico {#ggplot_basics_mapping}  

A la mayoría de las funciones geom hay que darle instrucciones sobre *qué elementos utilizar* para crear sus formas, por lo que hay que indicarles cómo se deben asignar las *columnas de los datos* a los distintos componentes del gráfico, como los ejes, los colores de las formas, los tamaños de las formas, etc. Para la mayoría de las funciones geom, los componentes *esenciales* que deben asignarse a las columnas de los datos son el eje-x y (si es necesario) el eje-y.

Este "mapeo" (o asignación) se produce con el argumento mapping `=`. Los mapeos que proporciones a **mapping** deben estar envueltos en la función `aes()`, por lo que hay que escribir algo como `mapping = aes(x = col1, y = col2)`, como se muestra a continuación.

A continuación, en el comando `ggplot()` los datos se identifican utilizando el termino `linelist` . En el argumento `mapping = aes()` la columna `age` se asigna al eje-x, y la columna `wt_kg` se asigna al eje-y.

Después de agregar un `+`, los comandos de trazado pueden continuar. Se crea una forma o tipo de gráfico con la función de "geom" denominada `geom_point()`. Este geom *hereda* los mapeos del comando `ggplot()` anterior - conoce las asignaciones eje-columna y procede a visualizar esas relaciones como *puntos* en el lienzo.

```{r, warning=F, message=F}
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+
  geom_point()
```

Otro ejemplo que presentamos a continuación demuestra el uso de los mismos datos pero con un mapeo ligeramente diferente y utilizando un geom diferente. Ahora utilizamos la función `geom_histogram()` que sólo requiere una columna mapeada en el eje-x, ya que el eje-y de conteo de casos ('count') se genera automáticamente.

```{r, warning=F, message=F}
ggplot(data = linelist, mapping = aes(x = age))+
  geom_histogram()
```


### Estética del gráfico {.unnumbered}  

En la terminología de ggplot, la "estética" de un gráfico tiene un significado específico. Se refiere a una propiedad visual de *los datos trazados*. Ten en cuenta que "estética" aquí se refiere a los *datos que se trazan en geoms / formas* - no a lo que aparece en la periferia, tales como títulos, etiquetas de los ejes, el color de fondo, como podría comúnmente asociarse con la palabra "estética". En ggplot esos detalles se llaman "temas" y se ajustan dentro de un comando denominado `theme()` (ver [esta sección](#ggplot_basics_themes)).

Por lo tanto, la *estética* de los objetos de ploteo puede ser colores, tamaños, transparencias, colocación, etc. *de los datos ploteados*. No todos los geoms tendrán las mismas opciones estéticas, pero muchas pueden ser utilizadas por la mayoría de los geoms. He aquí algunos ejemplos:

* `shape =` Representar un punto con `geom_point()` con forma de punto, estrella, triángulo o cuadrado...
* `fill =` El color interior (por ejemplo, de una barra o boxplot)
* `color =` El color de la línea exterior o borde de una barra, boxplot, etc., o el color del perimetro del punto si se utiliza `geom_point()`
* `size =` El tamaño (por ejemplo, grosor de línea, tamaño de punto)
* `alpha =` Transparencia (1 = opaco, 0 = invisible)
* `binwidth =` Ancho de los bins (o cubos) del histograma
* `width =` Ancho de las columnas del "diagrama de barras"
* `linetype =` Tipo de línea (por ejemplo, sólida, discontinua, punteada)

A esta estética de los objetos del gráfico se le pueden asignar valores de dos maneras:

1.  Se asigna un valor estático (por ejemplo, `color = "blue"`) que se aplica a todas las observaciones trazadas

2.  Se asigna a una columna de los datos (por ejemplo, `color = hospital`) de manera que la visualización de cada observación depende de su valor en esa columna

<!-- *These non-axis aesthetics can be assigned static values (e.g. `size = 1`) or can be mapped to a column (e.g. `size = age`).* If you want the aesthetic to be assigned a static value, the assignment is placed *outside* the `mapping = aes()`. If you want the aesthetic to be scaled/depend on the value in each row of data, the assignment is made *inside* the `mapping = aes()`.   -->

### Asignar un valor estático {.unnumbered}  

Si se desea que la estética del objeto de trazado sea estática, es decir, que sea la misma para cada observación de los datos, se escribe su asignación dentro del geom pero *fuera* del comando `mapping = aes()`. Estas asignaciones podrían escribirse como `size = 1` o `color = "blue"`. Aquí hay dos ejemplos:

* En el primer ejemplo, el `mapping = aes()` está en el comando `ggplot()` y los ejes se asignan a las columnas de edad (age) y peso (wt_kg) en los datos. La estética del gráfico `color =`, `size =`, y `alpha =` (transparencia) se asignan a valores estáticos. Aclaramos que la asignación de valores estéticos de naturaleza estática se hace en la función `geom_point()`, ya que se pueden añadir otros geoms después que tomarían valores estéticos diferentes

* En el segundo ejemplo, el histograma requiere sólo el eje-x mapeado a una columna. El `binwidth =` (el ancho de los cubos), el `color =` (el color del borde de los cubos), el `fill =` (color interno o color de relleno de los cubos), y el `alpha =` (la transparencia del color de los cubos) se establecen dentro del geom como valores estáticos.

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
# scatterplot
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  # establecer datos y ejes de mapeo
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)         # establecer la estética de los puntos estáticos

# histogram
ggplot(data = linelist, mapping = aes(x = age))+       # establecer datos y ejes
  geom_histogram(              # mostrar histograma
    binwidth = 7,                # anchura de los bins (cuadrados)
    color = "red",               # color de la línea del bin
    fill = "blue",               # color del interior del bin
    alpha = 0.1)                 # transparencia del bin
```


### Escalado a los valores de la columna {.unnumbered} 

Como alternativa al uso de estéticas de naturaleza estática, se pueden graficar objetos con tamaños proporcionales a sus valores como aparecen en su respectiva columna. Con este enfoque, la visualización de esta estética dependerá del valor de esa observación en la columna de datos correspondiente. Si los valores de la columna son continuos, la escala de visualización (en la leyenda) para esa estética será continua. Si los valores de la columna son discretos, la leyenda mostrará cada valor y los datos trazados aparecerán claramente "agrupados" (lea más en la sección de [agrupación](#ggplotgroups) de esta página).

Para conseguir esto, se asigna esa estética de gráfico a un *nombre de columna* o variable (sin utilizar comillas). Esto debe hacerse *dentro del comando `mapping = aes() `*(nota: hay varios lugares en el código donde puedes hacer estas asignaciones de mapeo, como se discute [a continuación](#ggplot_basics_map_loc)).

Presentamos dos ejemplos a continuación.

* En el primer ejemplo, la estética d `color =` (de cada punto) está mapeada a la columna `age` - ¡y ha aparecido una escala en una leyenda! Por ahora sólo hay que tener en cuenta que la escala existe - mostraremos cómo modificarla en secciones posteriores.
* En el segundo ejemplo, dos nuevas estéticas de trazado se asignan a columnas (`color =` y `size =`), mientras que las estéticas de trazado `shape =` y `alpha =` se asignan a valores estáticos fuera de cualquier función de `mapping = aes()`.

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
# scatterplot
ggplot(data = linelist,   # establecer los datos
       mapping = aes(     # asignar la estética a los valores de la columna
         x = age,           # asigna el eje-x a la edad             
         y = wt_kg,         # asignar el eje-y al peso
         color = age)     # asignar el color a la edad
       )+     
  geom_point()         # mostrar los datos como puntos 

# scatterplot
ggplot(data = linelist,   # establecer los datos
       mapping = aes(     # asignar la estética a los valores de la columna
         x = age,           # asigna el eje-x a la edad            
         y = wt_kg,         # asignar el eje-y al peso
         color = age,       # asignar el color a la edad
         size = age))+      # asignar el tamaño a la edad
  geom_point(             # mostrar los datos como puntos
    shape = "diamond",      # los puntos se muestran como diamantes
    alpha = 0.3)            # transparencia de los puntos al 30%


```



Nota: Los ejes siempre se asignan a las columnas de los datos o variables (no a los valores estáticos), y esto se hace siempre dentro de `mapping = aes()`.


Es importante mantener un seguimiento de las capas y las estéticas se hacen gráficos más complejos, por ejemplo, gráficos con múltiples geom. En el ejemplo siguiente, la estetica `size =` se asigna dos veces - una para `geom_point()` y otra para `geom_smooth()` - ambas veces como un valor estático. 

```{r, warning=F, message=F}
ggplot(data = linelist,
       mapping = aes(           # asignar la estética a las columnas
         x = age,
         y = wt_kg,
         color = age_years)
       ) + 
  geom_point(                   # añadir puntos para cada fila de datos
    size = 1,
    alpha = 0.5) +  
  geom_smooth(                  # añadir una línea de tendencia  
    method = "lm",             # con método lineal
    size = 2)                   # tamaño (ancho de la línea) de 2
```






### Dónde hacer las asignaciones {#ggplot_basics_map_loc .unnumbered}


La asignación de estéticas dentro de `mapping = aes()` puede hacerse en varios lugares en sus comandos e incluso puede escribirse más de una vez. Esto puede ser escrito en el comando `ggplot()` inicial, y/o en cada geom individual debajo. Los matices incluyen:

* Las asignaciones de estéticas realizadas en el comando `ggplot()` inicial se heredarán por defecto en cualquier geom a continuación, al igual que se heredan `x =` e `y =`

* Las asignaciones realizadas dentro de un geom se aplican sólo a ese geom

Del mismo modo, el comando `data =` especificado en el `ggplot()` inicial se aplicará por defecto a cualquier geom que se agregue a continuación, pero también se podrían especificar datos para cada geom (pero esto es más difícil).

Así, cada uno de los siguientes comandos creará el mismo gráfico:


```{r, eval=F, warning=F, message=F}
# Estos comandos producirán exactamente el mismo gráfico
ggplot(data = linelist, mapping = aes(x = age))+
  geom_histogram()

ggplot(data = linelist)+
  geom_histogram(mapping = aes(x = age))

ggplot()+
  geom_histogram(data = linelist, mapping = aes(x = age))
```




### Grupos {#ggplotgroups .unnumbered}  

Puedes agrupar fácilmente los datos y "graficar por grupo". De hecho, ¡ya lo has hecho!

Asigna la columna que quieres agrupar a la estética adecuada, dentro del comando `mapping = aes()`. Más arriba hemos mostrado esto usando valores continuos cuando asignamos el tamaño del punto usando `size =` a la columna `age`. Sin embargo, esto funciona de la misma manera con columnas o variables discretas/categóricas.

Por ejemplo, si quieres agrupar los puntos por género asignándole un color distinto a cada genero, deberás establecer `mapping = aes(color = gender)`. Automáticamente aparecerá una leyenda. Esta asignación puede hacerse dentro de `mapping = aes()` en el comando `ggplot()` inicial (y ser heredado por el geom), o podría asignarse dentro de `mapping = aes()` escrito dentro del comando de geom. Ambos enfoques se muestran a continuación:
  


```{r, warning=F, message=F}
ggplot(data = linelist,
       mapping = aes(x = age, y = wt_kg, color = gender))+
  geom_point(alpha = 0.5)
```


```{r, eval=F}
# Este código alternativo produce el mísmo gráfico
ggplot(data = linelist,
       mapping = aes(x = age, y = wt_kg))+
  geom_point(
    mapping = aes(color = gender),
    alpha = 0.5)

```


Tené en cuenta que dependiendo del tipo de geom, tendrás que utilizar diferentes argumentos para agrupar los datos. Para `geom_point()` lo más probable es que tengas que utilizar `color =`, `shape =` o `size =`. Mientras que para `geom_bar()` es más probable que utilices `fill =`. Esto dependerá del tipo de geom y de la estética del gráfico que deses usar para reflejar las agrupaciones.

Para tu información - la forma más básica de agrupar los datos es utilizando sólo el argumento `group =` dentro de `mapping = aes()`. Sin embargo, esto por sí mismo no cambiará los colores, el relleno o las formas. Tampoco creará una leyenda. Sin embargo, los datos están agrupados, por lo que las visualizaciones estadísticas pueden verse afectadas.

Para ajustar el orden de los grupos en un gráfico, consulta la página de [Consejos de ggplot](#ggplot-tips) o la página sobre [Factores](#factors). Hay muchos ejemplos de gráficos agrupados en las secciones siguientes sobre el trazado de datos continuos y categóricos. 



## Facetas / Múltiplos pequeños {#ggplot_basics_facet}  

Las facetas, o "pequeños gráficos múltiples", se utilizan para dividir un gráfico en una figura de varios paneles, con un panel ("faceta") representando un grupo de datos. El mismo tipo de gráfico se crea varias veces, cada vez utilizando un subgrupo del mismo conjunto de datos.

El facetado es una funcionalidad que viene con **ggplot2**, por lo que las leyendas y los ejes de los `paneles` facetados se alinean automáticamente. Hay otros paquetes que abordamos en la página de [Consejos de ggplot](#ggplot-tips) que se utilizan para combinar gráficos representando conjuntos de datos completamente diferentes (**cowplot** y **patchwork**) en una figura.

El facetado se realiza con una de las siguientes funciones de **ggplot2**:

1.  `facet_wrap()` Para mostrar un panel diferente para cada nivel de una *unica* variable. Un ejemplo de esto podría ser mostrar una curva de epidemia diferente para cada hospital de una región. Las facetas se ordenan alfabéticamente, a menos que la variable sea un factor con otro orden definido.
* Puedes invocar ciertas opciones para determinar la disposición de las facetas, por ejemplo, `nrow = 1` o `ncol = 1` para controlar el número de filas o columnas en las que se organizan los gráficos con facetas.

2.  `facet_grid()` Se utiliza cuando se quiere introducir una segunda variable en la disposición de las facetas. Aquí cada panel de una cuadrícula muestra la intersección entre los valores de *dos columnas*. Por ejemplo, las curvas epidémicas para cada combinación hospital-grupo de edad con los hospitales en la parte superior (columnas) y los grupos de edad en los lados (filas).
* `nrow` y `ncol` no son relevantes, ya que los subgrupos se presentan en una cuadrícula

Cada una de estas funciones acepta una sintaxis de fórmula para especificar la(s) columna(s) para el facetado. Ambas aceptan hasta dos columnas, una a cada lado de la tilde `~`.

* Para `facet_wrap()` lo más frecuente es escribir una sola columna precedida de una tilde `~` como `facet_wrap(~hospital)`. Sin embargo, puedes escribir dos columnas `facet_wrap(outcome~hospital)` - cada combinación única se mostrará en un panel separado, pero no se organizarán en una cuadrícula. Los encabezados mostrarán los términos combinados y éstos no tendrán una lógica específica para las columnas frente a las filas. Si quieres proporcionar una sóla variable de facetado, debes utilizar un punto `.` como marcador de posición en el otro lado de la fórmula - mira los ejemplos de código.

* Para `facet_grid()` también puedes especificar una o dos columnas en la fórmula (`rows` ~ `columns`). Si sólo quieres especificar una, puedes colocar un punto `.` al otro lado de la tilde como `facet_grid(. ~ hospital)` o `facet_grid(hospital ~ .)`.

Las facetas pueden contener rápidamente una cantidad abrumadora de información, por lo que conviene asegurarse de no tener demasiados niveles de cada variable por la que se elija hacer la faceta. He aquí algunos ejemplos rápidos con el conjunto de datos sobre la malaria (véase [Descargar el manual y los datos](#download-handbook-and-data)), que consiste en el recuento diario de casos de malaria en los centros, por grupos de edad.

A continuación importamos y hacemos algunas modificaciones rápidas para simplificar la tarea: 

```{r, , warning=F, message=F}
# Estos datos son recuentos diarios de casos de paludismo, por centro-día
malaria_data <- import(here("data", "malaria_facility_count_data.rds")) %>%  # importa
  select(-submitted_date, -Province, -newid)                                 # elimina columnas innecesarias

```

A continuación se muestran las primeras 50 filas de los datos sobre la malaria. Observa que hay una columna `malaria_tot`, pero también columnas para los recuentos por grupo de edad (que se utilizarán en el segundo ejemplo de `facet_grid()`).  

```{r, message=FALSE, echo=F}
DT::datatable(head(malaria_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### `facet_wrap()` {.unnumbered}

Por el momento, vamos a centrarnos en las columnas `malaria_tot` y `District`. Ignoremos por ahora las columnas de recuento por edad. Trazaremos las curvas epidémicas con `geom_col()`, que produce una columna para cada día a la altura del eje-y especificada en la columna `malaria_tot` (los datos ya son recuentos diarios, por lo que utilizamos `geom_col()` - véase más adelante la sección ["Diagrama de barras"](#ggplot_basics_bars)).

Cuando añadimos el comando `facet_wrap()`, especificamos una tilde y a continuación la columna sobre la que hacer la faceta (`District` en este caso). Podés colocar otra columna a la izquierda de la tilde, - esto creará una faceta para cada combinación - pero te recomendamos que lo hagas con `facet_grid()` en su lugar. En este caso, se crea una faceta para cada valor único de `District`.

```{r, warning=F, message=F}
# Un gráfico con facetas por distrito
ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_col(width = 1, fill = "darkred") +       # graficar los datos de recuento en forma de columnas
  theme_minimal()+                              # simplificar los paneles de fondo
  labs(                                         # añadir al gráfico etiquetas, título, etc.
    x = "Date of report",
    y = "Malaria cases",
    title = "Malaria cases by district") +
  facet_wrap(~District)                       # se crean las facetas
```

### `facet_grid()` {.unnumbered}  

Podemos utilizar un enfoque de `facet_grid()` para cruzar dos variables. Digamos que queremos cruzar `District` y edad. Bien, necesitamos hacer algunas transformaciones de datos en las columnas de edad para poner estos datos en el formato "largo" preferido por ggplot. Los grupos de edad tienen sus propias columnas - los queremos en una sola columna llamada `age_group` y otra llamada `num_cases`. Consulta la página sobre [Pivoteo de datos](#pivoting-data) para obtener más información sobre este proceso.  

```{r, message=F, warning=F}
malaria_age <- malaria_data %>%
  select(-malaria_tot) %>% 
  pivot_longer(
    cols = c(starts_with("malaria_rdt_")),  # elegir columnas para pivotar más largo
    names_to = "age_group",      # los nombres de las columnas se convierten en grupos de edad
    values_to = "num_cases"      # valores a una sola columna (num_cases)
  ) %>%
  mutate(
    age_group = str_replace(age_group, "malaria_rdt_", ""),
    age_group = forcats::fct_relevel(age_group, "5-14", after = 1))
```

Ahora las primeras 50 filas de datos tienen este aspecto: 

```{r, message=FALSE, echo=F}
DT::datatable(head(malaria_age, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Cuando se asignan las dos variables a `facet_grid()`, lo más fácil es utilizar la notación de fórmula (por ejemplo, `x ~  y`) donde x son filas e y son columnas. Aquí está el gráfico, utilizando `facet_grid()` que muestra los gráficos para cada combinación de las columnas `age_group` y `District`.

```{r, message=F, warning=F}
ggplot(malaria_age, aes(x = data_date, y = num_cases)) +
  geom_col(fill = "darkred", width = 1) +
  theme_minimal()+
  labs(
    x = "Date of report",
    y = "Malaria cases",
    title = "Casos de malaria por distrito y grupo de edad"
  ) +
  facet_grid(District ~ age_group)
```

### Ejes libres o fijos {.unnumbered}  

Las escalas de los ejes que se muestran en gráficos facetados son, por defecto, las mismas (fijas) en todas las facetas. Esto es útil para las comparaciones cruzadas, pero no siempre es apropiado.

Al utilizar `facet_wrap()` o `facet_grid()`, podemos añadir `scales = "free_y"` para "liberar" los ejes-y de los paneles para que se ajuste la escala adecuadamente en relación a su subconjunto de datos. Esto es particularmente útil si los recuentos reales son pequeños para una de las subcategorías y las tendencias son difíciles de ver. En lugar de "free_y" también podemos escribir "free_x" para hacer lo mismo con el eje-x (por ejemplo, para las fechas) o "free" para liberar ambos ejes. Ten en cuenta que en `facet_grid`, las escalas de y serán las mismas para las facetas en la misma fila, y las escalas de x serán las mismas para las facetas en la misma columna.

Cuando se utiliza `facet_grid` solamente, podemos añadir `space = "free_y"` o `space = "free_x"` para que la altura o el ancho de la faceta sea ponderada en relación a los valores de la figura en su interior. Esto sólo funciona si ya se ha asignado `scale = "free"` (y o x).

```{r, message=FALSE, warning=FALSE}

# Free y-axis
ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_col(width = 1, fill = "darkred") +       # graficar los datos de recuento en forma de columnas
  theme_minimal()+                              # simplificar los paneles de fondo
  labs(                                         # añadir al gráfico etiquetas, título, etc..
    x = "Date of report",
    y = "Malaria cases",
    title = "Malaria cases by district - 'free' x and y axes") +
  facet_wrap(~District, scales = "free")        # se crean las facetas
```


<!-- ```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')} -->
<!-- # A) Facet hospitalsation date by hospital, free y axis -->
<!-- ggplot(data = linelist %>% filter(hospital != "Missing"), # filter removes unknown hospital -->
<!--        aes(x = date_hospitalisation ))+ -->
<!--   geom_histogram(binwidth=7) + # Bindwidth = 7 days -->
<!--   labs(title = "A) Histogram with free y axis scales")+ -->
<!--   facet_grid(hospital~., # Facet with hospital as the row  -->
<!--              scales = "free_y") # Free the y scale of each facet -->

<!-- # B) Facet hospitalisation date by hospital, free y axis and vertical spacing -->
<!-- ggplot(data = linelist %>% filter(hospital != "Missing"), # filter removes unknown hospital -->
<!--        aes(x = date_hospitalisation ))+ -->
<!--   geom_histogram(binwidth=7) + # Bindwidth = 7 days -->
<!--   labs(title = "B) Histogram with free y axis scales and spacing")+ -->
<!--   facet_grid(hospital~., # Facet with hospital as the row  -->
<!--              scales = "free_y", # Free the y scale of each facet -->
<!--              space = "free_y") # Free the vertical spacing of each facet to optimise space -->

<!-- ``` -->

### Orden del nivel de los factores en las facetas {.unnumbered}  


Consulta esta [entrada](https://juliasilge.com/blog/reorder-within/) sobre cómo reordenar los niveles de los factores *dentro* de las facetas.


## Almacenamiento de gráficos {#storing-plots}

### Guardar los gráficos {.unnumbered}

Cuando se ejecuta un comando `ggplot()`, el gráfico se mostrará en el panel de Plots RStudio de manera predeterminada. Sin embargo, también podés guardar el gráfico como un objeto utilizando el operador de asignación `<-` y asignandole un nombre. Entonces el gráfico no se mostrará a menos que se ejecute el nombre del objeto mismo. También podés mostrarlo envolviendo el nombre del gráfico con `print()`, pero esto sólo es necesario en ciertas circunstancias, como cuando el gráfico se crea dentro de un *loop for* o bucle utilizado para imprimir múltiples gráficos a la vez (véase la página [Iteración, bucles y listas](#iteration-loops-and-lists) ).

```{r, warning=F, message=F}
# define plot
age_by_wt <- ggplot(data = linelist, mapping = aes(x = age_years, y = wt_kg, color = age_years))+
  geom_point(alpha = 0.1)

# print
age_by_wt    
```


### Modificación de gráficos guardados {.unnumbered} 

Una gran ventaja de **ggplot2** es que podés definir un gráfico (como se ve arriba), y luego añadirle capas empezando por su nombre sin necesidad de repetir todos los comandos que crearon el gráfico original.

Por ejemplo, si se desea modificar el gráfico `age_by_wt` que se definió anteriormente, para incluir una línea vertical a la edad de 50 años, sólo tendríamos que añadir un `+` y empezar a añadir capas adicionales al gráfico.

```{r, warning=F, message=F}
age_by_wt+
  geom_vline(xintercept = 50)
```


### Exportación de gráficos {.unnumbered}   

La exportación de ggplots es fácil con la función `ggsave()` de **ggplot2**. Puede funcionar de dos maneras, ya sea:

* Especifica el nombre del objeto del gráfico, a continuación, la ruta del archivo y el nombre del archivo incluyendo la extensión
     * Por ejemplo: `ggsave(my_plot, here("plots", "my_plot.png"))`
* Ejecuta el comando con sólo una ruta de archivo, para guardar el último gráfico que se imprimió en pantalla
     * Por ejemplo: `ggsave(here("plots", "my_plot.png"))`

Puedes exportar como png, pdf, jpeg, tiff, bmp, svg, o varios otros tipos de archivos, especificando la extensión del archivo en la ruta del mismo.

También puedes especificar los argumentos `width =`, `height =` y `units =` (ya sea "in", "cm" o "mm"). Asimismo podés especificar `dpi =` asignando un número para la resolución del trazado (por ejemplo, 300). Consulta los detalles de la función ejecutando `?ggsave` o leyendo la [documentación en línea](https://ggplot2.tidyverse.org/reference/ggsave.html).

Recuerda que podés utilizar la sintaxis `here()` para proporcionar la ruta de archivo deseada. Consulta la página de [importación y exportación](#import-and-export) para obtener más información.  


## Etiquetas {#labels} 

Seguramente querrás añadir o ajustar las etiquetas del gráfico. Esto se hace más fácilmente dentro de la función `labs()` que se añade al gráfico con `+` al igual que los geoms.

Dentro de `labs()` podes proporcionar cadenas de caracteres a estos argumentos:

* `x =` e `y =` El título del eje-x y del eje-y (etiquetas)
* `title =` El título del gráfico principal
* `subtitle =` El subtítulo del gráfico, en texto más pequeño debajo del título
* `caption` `=` El pie del gráfico, que aparecerá en la parte inferior derecha de manera predeterminada

Aquí está el mismo gráfico que hicimos antes, pero con etiquetas más bonitas:

```{r, warning=F, message=F}
age_by_wt <- ggplot(
  data = linelist,   # establecer los datos
  mapping = aes(     # asignar la estética a los valores de la columna
         x = age,           # asigna el eje-x a la edad             
         y = wt_kg,         # asignar el eje-y al peso
         color = age))+     # asignar el color a la edad
  geom_point()+           # mostrar los datos como puntos 
  labs(
    title = "Age and weight distribution",
    subtitle = "Fictional Ebola outbreak, 2014",
    x = "Age in years",
    y = "Weight in kilos",
    color = "Age",
    caption = stringr::str_glue("Data as of {max(linelist$date_hospitalisation, na.rm=T)}"))

age_by_wt
```

Observa cómo en la asignación del pie del gráfico hemos utilizado `str_glue()` del paquete **stringr** para integrar código R dinámico dentro del texto de la cadena. El pie del gráfico mostrará la fecha "Datos a partir de:" que refleja la fecha máxima de hospitalización en el listado de datos. Puedes leer más sobre esto en la página sobre [Caracteres y cadenas](#characters-and-strings).

Una nota sobre la especificación del título de la *leyenda*: No hay un argumento "título de la leyenda", ya que podrías tener múltiples escalas en tu leyenda. Dentro de `labs()`, podes escribir el argumento de la estética del gráfico utilizado para crear la leyenda, y proporcionar el título de esta manera. Por ejemplo, arriba asignamos `color = age` para crear la leyenda. Por lo tanto, proporcionamos `color` `=` a `labs()` y asignamos el título de la leyenda deseado ("Age" con A mayúscula). Si se crea la leyenda con `aes(fill = COLUMN)`, entonces en `labs()` se escribiría `fill =` para ajustar el título de esa leyenda. La sección sobre escalas de color en la página [Consejos de ggplot](#ggplot-tips) proporciona más detalles sobre la edición de leyendas, y un enfoque alternativo utilizando las funciones `scales_()`.



## Temas {#ggplot_basics_themes} 

Una de las mejores partes de **ggplot2** es el nivel de control que tienes sobre el gráfico - ¡puedes definir lo que quieras! Como se mencionó anteriormente, los aspectos de diseño del gráfico que *no están* relacionados con las formas/geometrías de los datos se ajustan dentro de la función `theme()`. Por ejemplo, el color de fondo del gráfico, la presencia/ausencia de líneas de cuadrícula, y la fuente/tamaño/color/alineación del texto (títulos, subtítulos, pie de gráfico, texto de los ejes...). Estos ajustes pueden realizarse de dos maneras:

* Añadiendo una función `theme_()` [*completa*](https://ggplot2.tidyverse.org/reference/ggtheme.html) para realizar ajustes de barrido -- estas funciones de tema completo incluyen `theme_classic()`, `theme_minimal()`, `theme_dark()`, `theme_light() theme_grey()`, `theme_bw()` entre otras

* Ajustando cada pequeño aspecto del gráfico individualmente dentro de `theme()`

### Temas completos {.unnumbered} 

Como son bastante sencillas, demostraremos las funciones del tema completo a continuación y no las describiremos más aquí. Ten en cuenta que cualquier microajuste con `theme()` debe hacerse *después de* utilizar un tema completo.

Escribílos con paréntesis vacíos.

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme classic")+
  theme_classic()

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme bw")+
  theme_bw()

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme minimal")+
  theme_minimal()

ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  
  geom_point(color = "darkgreen", size = 0.5, alpha = 0.2)+
  labs(title = "Theme gray")+
  theme_gray()
  


```

### Modificar el tema {.unnumbered}

La función `theme()` puede tomar un gran número de argumentos, cada uno de los cuales edita un aspecto específico del gráfico. No hay manera de que podamos cubrir todos los argumentos, pero describiremos el patrón general para ellos y te mostraremos cómo encontrar el nombre del argumento que necesitas. La sintaxis básica es esta:

1.  Dentro de `theme()` escribe el nombre del argumento del elemento del gráfico que queres editar, como `plot.title =`
2.  Proporciona una función `element_()` al argumento
     + Lo más habitual es utilizar `element_text()`, pero también `element_rect()` para los colores de fondo del lienzo, o `element_blank()` para eliminar los elementos del gráfico
4.  Dentro de la función `element_()`, escribí las asignaciones de argumentos para realizar los ajustes finos que desees

Esa descripción es bastante abstracta, así que aquí hay algunos ejemplos.

El siguiente gráfico parece tonto, pero sirve para mostrarte una variedad de formas en las que podés ajustar su gráfico.

* Comenzamos con el gráfico `age_by_wt` definido anteriormente y añadimos `theme_classic()`
* Para realizar ajustes más finos, añadimos `theme()` e incluimos un argumento por cada elemento del gráfico que queremos ajustar

Puede ser util organizar los argumentos en secciones lógicas. A continuación se describen algunos argumentos utilizados:

* `legend.position =` es el único argumento que acepta valores simples como “bottom”, “top”, “left”, y “right” (abajo, arriba, izquierda y derecha). Por lo general, los argumentos relacionados con el texto requieren que se coloquen los detalles *dentro de* `element_text()`.
* Tamaño del título se ajusta con `element_text(size = 30)`
* La alineación horizontal del pie del gráfico se logra con el argumento `element_text(hjust = 0)` (de derecha a izquierda)
* El subtítulo aparece en cursiva gracias al argumento `element_text(face = "italic")`

```{r, , warning=F, message=F}
age_by_wt + 
  theme_classic()+                                 # ajustes temáticos predefinidos
  theme(
    legend.position = "bottom",                    # mover la leyenda a la parte inferior
    
    plot.title = element_text(size = 30),          # tamaño del título a 30
    plot.caption = element_text(hjust = 0),        # alinear el título a la izquierda
    plot.subtitle = element_text(face = "italic"), # poner en cursiva el subtítulo
    
    axis.text.x = element_text(color = "red", size = 15, angle = 90), # ajustar sólo el texto del eje-x
    axis.text.y = element_text(size = 15),         # ajustar sólo el texto del eje-y
    
    axis.title = element_text(size = 20)           # ajusta los títulos de ambos ejes
    )     
```

Aquí hay algunos argumentos de `theme()` especialmente comunes. Reconocerás algunos patrones, como añadir `.x` o `.y` para aplicar el cambio sólo a un eje.


argumento de `theme()` argument    |Lo que ajusta
-----------------------------------|----------------------------------
`plot.title = element_text()`      |El título
`plot.subtitle = element_text()`   |El subtítulo
`plot.caption = element_text()`    | La leyenda (familia, cara, color, tamaño, ángulo, vjust (justificación vertical), hjust (justificación horizontal)...) 
`axis.title = element_text()`      |Títulos de los ejes (tanto x como y) (tamaño, cara, ángulo, color...)
`axis.title.x = element_text()`    |Título del eje-x solamente (usar `.y` para el eje-y solamente)
`axis.text = element_text()`       |Texto de los ejes (x e y)
`axis.text.x = element_text()`     |Texto del eje-x solamente (usar `.y` para el eje-y solamente) 
`axis.ticks = element_blank()`     |Eliminar las marcas del eje 
`axis.line = element_line()`       |Líneas del eje (color, tamaño, tipo de línea: sólida, punteada, etc.)
`strip.text = element_text()`      |Texto de la tira de facetas (color, cara, tamaño, ángulo...)
`strip.background = element_rect()`|Tira de facetas (relleno, color, tamaño...) 

Pero ¡hay tantos argumentos de tema! ¿Cómo podría recordarlos todos? No te preocupes, es imposible recordarlos todos. Por suerte, hay algunas herramientas que te ayudarán:

La documentación de **tidyverse** sobre la [modificación del tema](https://ggplot2.tidyverse.org/reference/theme.html), tiene una lista completa.

<span style="color: darkgreen;">**_CONSEJO:_** Ejecuta `theme_get()` de **ggplot2** para imprimir en pantalla una lista de los más de 90 argumentos de `theme()` en la consola.</span>  

<span style="color: darkgreen;">**_CONSEJO:_** Si alguna vez quieres eliminar un elemento de un gráfico, también puedes hacerlo a través de `theme()`. Basta con pasar `element_blank()` a un argumento para que desaparezca por completo. Para eliminar leyendas, puedes asignar `legend.position = "none".`</span>  




## Colores {#colors}  


Consulta esta [sección sobre las escalas de color de la página de consejos de ggplot](#ggplot_tips_colors).  



## Pipes en **ggplot2** {#piping-into-ggplot2}

Cuando se utilizan pipes para limpiar y transformar los datos, es fácil pasar los datos transformados a `ggplot()`.

Las pipes que pasan el conjunto de datos de función a función se convertiran a `+` una vez que se invoque a la función `ggplot()`. Ten en cuenta que, en este caso, no es necesario especificar el argumento `data =`, ya que éste se define automáticamente como el conjunto de datos canalizado.

Así es como podría verse:

```{r, warning=F, message=F}
linelist %>%                                                     # begin with linelist
  select(c(case_id, fever, chills, cough, aches, vomit)) %>%     # select columns
  pivot_longer(                                                  # pivot longer
    cols = -case_id,                                  
    names_to = "symptom_name",
    values_to = "symptom_is_present") %>%
  mutate(                                                        # replace missing values
    symptom_is_present = replace_na(symptom_is_present, "unknown")) %>% 
  
  ggplot(                                                        # begin ggplot!
    mapping = aes(x = symptom_name, fill = symptom_is_present))+
  geom_bar(position = "fill", col = "black") +                    
  theme_classic() +
  labs(
    x = "Symptom",
    y = "Symptom status (proportion)"
  )
```









## Trazado de datos continuos {#plot-continuous-data}

A lo largo de esta página, ya has visto muchos ejemplos de trazado de datos continuos. Aquí los consolidamos brevemente y presentamos algunas variaciones.\ Las visualizaciones que aquí se cubren incluyen:

* Gráficos para una variable continua:
  * **Histogram**, un gráfico clásico para presentar la distribución de una variable continua.
  * **Gráfico de caja** (también llamado de caja y bigotes), para mostrar los percentiles 25, 50 y 75, los extremos de la cola de la distribución y los valores atípicos ([limitaciones importantes](https://www.data-to-viz.com/caveat/boxplot.html)).
  * **Gráfico de fluctuación**, para mostrar todos los valores como puntos que se "fluctúan" para que se puedan ver (casi) todos, incluso cuando dos tienen el mismo valor.
  * **Gráfico del violín**, muestra la distribución de una variable continua en función del ancho simétrico del "violín".
  * Los **gráficos de Sina**, son una combinación de los gráficos de jitter y de violín, donde se muestran los puntos individuales pero con la forma simétrica de la distribución (a través del paquete **ggforce**).
  * **Gráfico de dispersión** para dos variables continuas.
  * **Gráficos de calor** para tres variables continuas (enlazado a la página de [gráficos de calor](#heat-plots))



### Histogramas {.unnumbered}

Los histogramas pueden parecerse a los gráficos de barras, pero son distintos porque miden la distribución de una variable *continua*. No hay espacios entre las "barras", y sólo se proporciona una columna a `geom_histogram()`.

A continuación se muestra el código para generar **histogramas**, que agrupan datos continuos en rangos y se muestran en barras adyacentes de altura variable. Esto se hace utilizando `geom_histogram()`. Consulta la [sección "Gráfico de barras"](#ggplot_basics_bars) de la página de fundamentos de ggplot para entender la diferencia entre `geom_histogram()`, `geom_bar()` y `geom_col()`.

Vamos a mostrar la distribución de las edades de los casos. Dentro de `mapping = aes()` especifique la columna de la que quiere ver la distribución. Puedes asignar esta columna al eje-x o al eje-y.

Las filas serán asignadas a "bins" basados en su edad numérica, y estos bins serán representados gráficamente por barras. Si se especifica un número de bins con la estética de gráfico `bins =`, los puntos de ruptura se espacian uniformemente entre los valores mínimos y máximos del histograma. Si no se especifica `bins =`, se adivinará un número apropiado de bins y aparecera este mensaje después del gráfico:  

```
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
``` 

Si no quieres especificar un número de bins a `bins =`, puedes especificar alternativamente `binwidth =` en las unidades del eje. Damos algunos ejemplos que muestran diferentes bins y anchos de bins:

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Regular histogram
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram()+
  labs(title = "A) Default histogram (30 bins)")

# B) More bins
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram(bins = 50)+
  labs(title = "B) Set to 50 bins")

# C) Fewer bins
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram(bins = 5)+
  labs(title = "C) Set to 5 bins")


# D) More bins
ggplot(data = linelist, aes(x = age))+  # provide x variable
  geom_histogram(binwidth = 1)+
  labs(title = "D) binwidth of 1")

```



Para obtener proporciones suavizadas, puede utilizar `geom_density()`:

```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# Frequency with proportion axis, smoothed
ggplot(data = linelist, mapping = aes(x = age)) +
  geom_density(size = 2, alpha = 0.2)+
  labs(title = "Proportional density")

# Stacked frequency with proportion axis, smoothed
ggplot(data = linelist, mapping = aes(x = age, fill = gender)) +
  geom_density(size = 2, alpha = 0.2, position = "stack")+
  labs(title = "'Stacked' proportional densities")
```


To get a "stacked" histogram (of a continuous column of data), you can do one of the following:  

1) Use `geom_histogram()` with the `fill = ` argument within `aes()` and assigned to the grouping column, or  
2) Use `geom_freqpoly()`, which is likely easier to read (you can still set `binwidth = `)  
3) To see proportions of all values, set the `y = after_stat(density)` (use this syntax exactly - not changed for your data). Note: these proportions will show *per group*.  

Cada uno se muestra a continuación (*nótese el uso de `color =` versus `fill =` en cada uno):

```{r, fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# "Stacked" histogram
ggplot(data = linelist, mapping = aes(x = age, fill = gender)) +
  geom_histogram(binwidth = 2)+
  labs(title = "'Stacked' histogram")

# Frequency 
ggplot(data = linelist, mapping = aes(x = age, color = gender)) +
  geom_freqpoly(binwidth = 2, size = 2)+
  labs(title = "Freqpoly")

# Frequency with proportion axis
ggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), color = gender)) +
  geom_freqpoly(binwidth = 5, size = 2)+
  labs(title = "Proportional freqpoly")

# Frequency with proportion axis, smoothed
ggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), fill = gender)) +
  geom_density(size = 2, alpha = 0.2)+
  labs(title = "Proportional, smoothed with geom_density()")
```

Si quieres divertirte un poco, prueba con `geom_density_ridges` del paquete **ggridges** [vignette aquí](https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html).

Lee más en detalle sobre los histogramas en la [página de **tidyverse** sobre geom_histogram()](https://ggplot2.tidyverse.org/reference/geom_histogram.html)  



### Gráficos de caja {.unnumbered}

Los gráficos de caja (Boxplot) son habituales, pero tienen limitaciones importantes. Pueden ocultar la distribución real - por ejemplo, una distribución bimodal. Consulta esta [galería de gráficos de R](https://www.r-graph-gallery.com/boxplot.html) y este [artículo sobre la visualización de datos](https://www.data-to-viz.com/caveat/boxplot.html) para obtener más detalles. Sin embargo, muestran muy bien el rango intercuartil y los valores atípicos, por lo que pueden superponerse a otros tipos de gráficos que muestran la distribución con más detalle.

A continuación te recordamos los distintos componentes de un boxplot: 

```{r, out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "boxplot.png"))
```

Cuando se utiliza `geom_boxplot()` para crear un gráfico de caja, generalmente se asigna sólo un eje (x o y) dentro de `aes()`. El eje especificado determina si los gráficos son horizontales o verticales.

En la mayoría de los geoms, se crea un gráfico por grupo asignando una estética como `color =` o `fill =` a una columna dentro de `aes()`. Sin embargo, en el caso de los gráficos de caja, esto se consigue asignando la columna de agrupación al eje no asignado (x o y). A continuación se muestra el código para un diagrama de caja de *todos los* valores de edad en el conjunto de datos, y un segundo trozo de código para mostrar un diagrama de caja para cada género (no ausente) en el conjunto de datos. Ten en cuenta que los valores `NA` (valores faltantes) aparecerán como un gráfico de caja separado a menos que se eliminen. En este ejemplo, también hemos asignado el `fill` de la columna `outcome` para que cada gráfico tenga un color diferente, pero esto no es necesario.

```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# A) Overall boxplot
ggplot(data = linelist)+  
  geom_boxplot(mapping = aes(y = age))+   # only y axis mapped (not x)
  labs(title = "A) Overall boxplot")

# B) Box plot by group
ggplot(data = linelist, mapping = aes(y = age, x = gender, fill = gender)) + 
  geom_boxplot()+                     
  theme(legend.position = "none")+   # remove legend (redundant)
  labs(title = "B) Boxplot by gender")      
```

Para ver el código para añadir un gráfico de caja a los bordes de un gráfico de dispersión (gráficos "marginales"), consulta la página [Consejos de ggplot](#ggplot-tips).





### Gráficos de violín, fluctuación y sina {.unnumbered}

A continuación se muestra el código para crear gráficos de `violín` (`geom_violin`) y `gráficos de fluctuación` (`geom_jitter`) para mostrar distribuciones. Se puede especificar que el relleno o el color también estén determinados por los datos, insertando estas opciones dentro de `aes()`.


```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}


# A) Jitter plot by group
ggplot(data = linelist %>% drop_na(outcome),      # remove missing values
       mapping = aes(y = age,                     # Continuous variable
           x = outcome,                           # Grouping variable
           color = outcome))+                     # Color variable
  geom_jitter()+                                  # Create the violin plot
  labs(title = "A) jitter plot by gender")     



# B) Violin plot by group
ggplot(data = linelist %>% drop_na(outcome),       # remove missing values
       mapping = aes(y = age,                      # Continuous variable
           x = outcome,                            # Grouping variable
           fill = outcome))+                       # fill variable (color)
  geom_violin()+                                   # create the violin plot
  labs(title = "B) violin plot by gender")    
```


Puedes combinar los dos tipos de gráfico usando la función `geom_sina()` del paquete `ggforce`. La sina traza los puntos de jitter en la forma del gráfico de violín. Cuando se superpone al gráfico de violín (ajustando las transparencias) puede ser más fácil de interpretar visualmente.

```{r, warning=F, message=F}

# A) Sina plot by group
ggplot(
  data = linelist %>% drop_na(outcome), 
  aes(y = age,           # numeric variable
      x = outcome)) +    # group variable
  geom_violin(
    aes(fill = outcome), # fill (color of violin background)
    color = "white",     # white outline
    alpha = 0.2)+        # transparency
### jcfernandezm
#    geom_sina(
#    size=1,                # Change the size of the jitter
#    aes(color = outcome))+ # color (color of dots)
  scale_fill_manual(       # Define fill for violin background by death/recover
    values = c("Death" = "#bf5300", 
              "Recover" = "#11118c")) + 
  scale_color_manual(      # Define colours for points by death/recover
    values = c("Death" = "#bf5300", 
              "Recover" = "#11118c")) + 
  theme_minimal() +                                # Remove the gray background
  theme(legend.position = "none") +                # Remove unnecessary legend
  labs(title = "B) violin and sina plot by gender, with extra formatting")      


```



### Dos variables continuas  {.unnumbered}

Siguiendo una sintaxis similar, `geom_point()` te permitirá trazar dos variables continuas en un **gráfico de dispersión**. Esto es útil para mostrar los valores reales en lugar de sus distribuciones. En (A) se muestra un gráfico de dispersión simple de la edad frente al peso. En (B) volvemos a utilizar `facet_grid()` para mostrar la relación entre dos variables continuas.


```{r fig.show='hold', message=FALSE, warning=FALSE, out.width=c('50%', '50%')}
# Basic scatter plot of weight and age
ggplot(data = linelist, 
       mapping = aes(y = wt_kg, x = age))+
  geom_point() +
  labs(title = "A) Scatter plot of weight and age")

# Scatter plot of weight and age by gender and Ebola outcome
ggplot(data = linelist %>% drop_na(gender, outcome), # filter retains non-missing gender/outcome
       mapping = aes(y = wt_kg, x = age))+
  geom_point() +
  labs(title = "B) Scatter plot of weight and age faceted by gender and outcome")+
  facet_grid(gender ~ outcome) 

```


### Tres variables continuas {.unnumbered}  

Puedes mostrar tres variables continuas utilizando el argumento `fill =` para crear *un gráfico de calor*. El color de cada "celda" reflejará el valor de la tercera columna continua de datos. Consulta la página [Consejos de ggplot](#ggplot-tips) y la página de [gráficos de calor](#heat-plots) para obtener más detalles y varios ejemplos.

Hay formas de hacer gráficos en 3D en R, pero para la epidemiología aplicada suelen ser difíciles de interpretar y, por tanto, menos útiles para la toma de decisiones.









## Trazar datos categóricos  {.unnumbered}

Los datos categóricos pueden ser valores de carácter, pueden ser lógicos (TRUE/FALSE), o factores (ver la página de [Factores](#factors)).

### Preparación {.unnumbered}

#### Estructura de datos {.unnumbered}  

Lo primero que hay que entender sobre tus datos categóricos es si existen como observaciones en bruto, como una lista de casos, o como un dataframe de resumen o agregado que contiene recuentos o proporciones. El estado de sus datos afectará a la función de trazado que utilice:

* Si tus datos son observaciones en bruto con una fila por observación, es probable que utilices `geom_bar()`
* Si sus datos ya están agregados en recuentos o proporciones, es probable que utilices `geom_col()`


#### Tipo de columna y ordenación de valores {.unnumbered}  

A continuación, examina el tipo o clase de las columnas (variables) que desea trazar. Veamos `hospital`, primero usando `class()` de R **base**, y luego con `tabyl()` de **janitor**.

```{r}
# View class of hospital column - we can see it is a character
class(linelist$hospital)

# Look at values and proportions within hospital column
###jcfernandezm
#linelist %>% 
#  tabyl(hospital)
```

Podemos ver que los valores de la variable hospital son caracteres, ya que son nombres de hospitales, y por defecto están ordenados alfabéticamente. Hay valores `otros` y `faltantes", que preferiríamos que fueran las últimas subcategorías al presentar los desgloses. Así que convertimos esta columna a clase factor y la reordenamos. Esto se trata con más detalle en la página de [Factores](#factors).

```{r}
# Convert to factor and define level order so "Other" and "Missing" are last
linelist <- linelist %>% 
  mutate(
    hospital = fct_relevel(hospital, 
      "St. Mark's Maternity Hospital (SMMH)",
      "Port Hospital", 
      "Central Hospital",
      "Military Hospital",
      "Other",
      "Missing"))

```


```{r}
levels(linelist$hospital)
```

### Gráfico de barras {#ggplot_basics_bars .unnumbered}  

Utiliza `geom_bar()` si deseas que la altura de las barras (o la altura de los componentes de las barras apiladas) refleje *el número de filas relevantes de los datos*. Estas barras tendrán huecos entre ellas, a menos que se ajuste la estética de `width =`.

* Proporciona sólo una asignación de columna de eje (normalmente el eje-x). Si proporcionas x e y, obtendrás un `Error: stat_count() can only have an x or y aesthetic.`
* Puedes crear barras apiladas añadiendo una asignación de Columba usando `fill =` dentro de `mapping = aes()`
* El eje opuesto se llamará por defecto "count" (recuento), ya que representa el número de filas

A continuación, hemos asignado el resultado (outcome) al eje-y, pero podría estar fácilmente asignarse al eje-x. Si tienes valores de caracteres más largos, a veces el gráfico se ve mejor si las barras se grafican de manera horizontal en vez de vertical ,poniendo la leyenda en la parte inferior del gráfico. Esto puede afectar el orden en el que aparecen los factores - en este caso los invertimos con `fct_rev()` para poner la categoría de "faltantes" y "otros" en la parte inferior. 

```{r, out.width=c('50%', '50%'), fig.show='hold'}
# A) Outcomes in all cases
ggplot(linelist %>% drop_na(outcome)) + 
  geom_bar(aes(y = fct_rev(hospital)), width = 0.7) +
  theme_minimal()+
  labs(title = "A) Number of cases by hospital",
       y = "Hospital")


# B) Outcomes in all cases by hosptial
ggplot(linelist %>% drop_na(outcome)) + 
  geom_bar(aes(y = fct_rev(hospital), fill = outcome), width = 0.7) +
  theme_minimal()+
  theme(legend.position = "bottom") +
  labs(title = "B) Number of recovered and dead Ebola cases, by hospital",
       y = "Hospital")

```





### `geom_col()` {.unnumbered}  

Utiliza `geom_col()` si deseas que la altura de las barras (o la altura de los componentes de las barras apiladas) refleje *valores* precalculados que existen en los datos. A menudo, se trata de recuentos sumarios o "agregados", o de proporciones.

Proporciona las asignaciones de columna para *ambos* ejes a `geom_col()`. Normalmente la columna del eje-x es discreta y la del eje-y es numérica.

Digamos que tenemos los un conjunto de datos denominado `outcomes`: 

```{r, echo = F}
outcomes <- linelist %>% 
  drop_na() %>% 
  group_by(outcome) %>% 
  count %>% 
  ungroup() %>% # Ungroup so proportion is out of total
  mutate(proportion = n/sum(n)*100) # Caculate percentage
  
outcomes #View full table
```



A continuación se muestra un código que utiliza `geom_col()` para crear gráficos de barras sencillos que muestren la distribución de los resultados de los pacientes con Ebola. Con geom_col, es necesario especificar tanto x como y. Aquí x es la variable categórica a lo largo del eje-x, e y es la columna de proporciones precalculada denominada `proportion`.

```{r, fig.height = 3, fig.width=4.5}
# Outcomes in all cases
ggplot(outcomes) + 
  geom_col(aes(x=outcome, y = proportion)) +
  labs(subtitle = "Number of recovered and dead Ebola cases")

```

Para mostrar los desgloses por hospital, necesitaríamos que nuestra tabla contuviera más información, y que estuviera en formato "largo". Creamos esta tabla con las frecuencias de las categorías combinadas `outcome` y `hospital` (véase la página de [Agrupar datos](#grouping-data) para obtener consejos de agrupación). 

```{r, fig.height = 4, fig.width=6}
outcomes2 <- linelist %>% 
  drop_na(outcome) %>% 
  count(hospital, outcome) %>%  # get counts by hospital and outcome
  group_by(hospital) %>%        # Group so proportions are out of hospital total
  mutate(proportion = n/sum(n)*100) # calculate proportions of hospital total

head(outcomes2) # Preview data
```

A continuación, creamos el ggplot añadiendo algunas asignaciones de formato:

  * **Cambio de posición del eje**: Intercambiamos los ejes con `coord_flip()` para poder leer los nombres de los hospitales.
  * **Columnas de lado a lado**: Se ha añadido un argumento de `position = "dodge"` para que las barras de muerte (Death) y recuperación (Recover) se presenten una al lado de la otra en lugar de apiladas. Ten en cuenta que las barras apiladas aparecen de manera predeterminada.
  * **Ancho de columna**: Se especifica el "ancho", para graficar las columnas a la mitad del ancho posible.
  * **Orden de las columnas**: Se ha invertido el orden de las categorías en el eje-y para que "Otros" y "Faltantes" aparezcan últimos, con `scale_x_discrete(limits=rev)`. Ten en cuenta que usamos eso en lugar de `scale_y_discrete` porque la variable hospital se asigno al eje-x en el en el argumento de `aes()`, aunque visualmente se vea en el eje-y. Hacemos esto porque ggplot parece presentar las categorías al revés a menos que le digamos que no lo haga.
  * **Otros detalles**: Añadimos etiquetas/títulos y colores dentro de las funciones de `labs` y `scale_fill_color` respectivamente.
  
```{r, fig.height = 4, fig.width=8}

# Outcomes in all cases by hospital
ggplot(outcomes2) +  
  geom_col(
    mapping = aes(
      x = proportion,                 # show pre-calculated proportion values
      y = fct_rev(hospital),          # reverse level order so missing/other at bottom
      fill = outcome),                # stacked by outcome
    width = 0.5)+                    # thinner bars (out of 1)
  theme_minimal() +                  # Minimal theme 
  theme(legend.position = "bottom")+
  labs(subtitle = "Number of recovered and dead Ebola cases, by hospital",
       fill = "Outcome",             # legend title
       y = "Count",                  # y axis title
       x = "Hospital of admission")+ # x axis title
  scale_fill_manual(                 # adding colors manually
    values = c("Death"= "#3B1c8C",
               "Recover" = "#21908D" )) 

```


Ten en cuenta que las proporciones son binarias, por lo que podemos preferir omitir `recuperar` y mostrar sólo la proporción que murió. Esto es sólo a título ilustrativo.

Si se utiliza `geom_col()` con datos de fechas (por ejemplo, una epicurva a partir de datos agregados) - querrá ajustar el argumento `width =` para eliminar los "huecos" entre las barras. Si se utilizan datos diarios (en días), ajuste `width= 1`. Si se trata de datos semanales, la anchura seria `width = 7`. Los meses no son posibles de representar porque cada mes tiene un número diferente de días.


### geom_histogram() {.unnumbered}  

Los histogramas pueden parecerse a los gráficos de barras, pero son distintos porque miden la distribución de una variable *continua*. No hay huecos o espacios entre las "barras" y sólo se asigna una columna a `geom_histogram()`. Hay argumentos específicos para los histogramas como `bin_width =` y `breaks =` para especificar cómo se deben dividir los datos. La sección anterior sobre datos continuos y la página sobre [curvas epidémicas](#epidemic-curves) proporcionan detalles adicionales. 



## Recursos {#resources-23} 

Hay una gran cantidad de ayuda en línea, especialmente sobre ggplot. Consulta las siguietnes páginas con maerial en inglés:

* [hoja de trucos de ggplot2](http://r-statistics.co/ggplot2-cheatsheet.html)
* [otra hoja de trucos](https://biostats.w.uib.no/the-ggplot2-cheat-sheet-by-rstudio/)
* [página de fundamentos de tidyverse ggplot](https://ggplot2.tidyverse.org/reference/)
* [trazado de variables continuas](http://www.sthda.com/english/articles/32-r-graphics-essentials/131-plot-two-continuous-variables-scatter-graph-and-alternatives/)
* páginas de R for Data Science en español sobre [visualización de datos](https://es.r4ds.hadley.nz/visualización-de-datos.html)
* [gráficos para la comunicación](https://es.r4ds.hadley.nz/comunicar-con-gráficos.html) 

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/ggplot_basics.Rmd-->

# Consejos de ggplot {#ggplot-tips}

En esta página cubriremos consejos y trucos para hacer que tus ggplots sean nítidos y elegantes. Consulta la página sobre [Conceptos básicos de ggplot](#ggplot-basics) para conocer los fundamentos.

Hay varios [tutoriales extensos de **ggplot2**](https://ggplot2.tidyverse.org/) enlazados en la sección de Recursos. También puedes descargar esta [hoja de trucos de visualización de datos con ggplot](https://rstudio.com/resources/cheatsheets/) desde el sitio web de RStudio. Recomendamos encarecidamente que busques inspiración en la [galería de gráficos de R](https://www.r-graph-gallery.com/) y en [Data-to-viz](https://www.data-to-viz.com/caveats.html).



<!-- ======================================================= -->
## Preparación {#preparation-23}

### Cargar paquetes {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R. 

```{r}
pacman::p_load(
  tidyverse,      # includes ggplot2 and other
  rio,            # import/export
  here,           # file locator
  stringr,        # working with characters   
  scales,         # transform numbers
  ggrepel,        # smartly-placed labels
  gghighlight,    # highlight one part of plot
  RColorBrewer    # color scales
)
```

### Importar datos {.unnumbered}  

Para esta página, importamos el conjunto de datos de casos de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica para descargar `linelist` "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa los datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - ver la página de [importación y exportación](#import-and-export) para más detalles).

```{r,  echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

```

```{r, eval=F}
linelist <- rio::import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas de `linelist`.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




<!-- ======================================================= -->
## Escalas para el color, relleno, ejes, etc. {#ggplot_tips_colors}

En **ggplot2**, cuando la estética de los datos trazados (por ejemplo, el tamaño, el color, la forma, el relleno, el eje de trazado) se asigna a las columnas de los datos, la visualización exacta se puede ajustar con el correspondiente comando "scale". En esta sección explicamos algunos ajustes de escala comunes.  



### Esquemas de color {.unnumbered}

Una cosa que puede ser inicialmente difícil de entender con **ggplot2** es el control de los esquemas de color. Ten en cuenta que esta sección discute el color de los *objetos a representar* (geoms/shapes) como puntos, barras, líneas, mosaicos, etc. Para ajustar el color del texto accesorio, los títulos o el color de fondo, consulta la sección [Temas](#ggplot_basics_themes) de la página [Conceptos básicos de ggplot](#ggplot-basics).

Para controlar el "color" de los *objetos de la gráfica* se ajustará la estética del `color = ` (el color exterior) o la estética del relleno, `fill = ` (el color *interior*). Una excepción a este patrón es `geom_point()`, donde realmente sólo se llega a controlar `color = `, que controla el color del punto (interior y exterior).

Al establecer el color o el relleno puedes utilizar nombres de colores reconocidos por R como `"red"` (ver [lista completa](http://sape.inf.usi.ch/quick-reference/ggplot2/colour) o introducir `?colors`), o un color hexadecimal específico como `"#ff0505"`.

```{r, warning=F, message=F}
# histogram - 
ggplot(data = linelist, mapping = aes(x = age))+       # set data and axes
  geom_histogram(              # display histogram
    binwidth = 7,                # width of bins
    color = "red",               # bin line color
    fill = "lightblue")          # bin interior color (fill) 
```



Como se explica en la sección [asignación de datos al gráfico](#ggplot_basics_mapping) de [Conceptos básicos de ggplot](#ggplot-basics) sobre la estética como  `fill = ` y `color = `  puede definirse *fuera* de una sentencia `mapping = aes()` o *dentro* de ella. Si está *fuera* de `aes()`, el valor asignado debe ser estático (por ejemplo, `color = "blue"`) y se aplicará a *todos* los datos trazados por geom. Si está *dentro*, la estética debe asignarse a una columna, como `color = hospital`, y la expresión variará según el valor de esa fila en los datos. Algunos ejemplos: 

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
# Static color for points and for line
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     
  geom_point(color = "purple")+
  geom_vline(xintercept = 50, color = "orange")+
  labs(title = "Static color for points and line")

# Color mapped to continuous column
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     
  geom_point(mapping = aes(color = temp))+         
  labs(title = "Color mapped to continuous column")

# Color mapped to discrete column
ggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     
  geom_point(mapping = aes(color = gender))+         
  labs(title = "Color mapped to discrete column")

# bar plot, fill to discrete column, color to static value
ggplot(data = linelist, mapping = aes(x = hospital))+     
  geom_bar(mapping = aes(fill = gender), color = "yellow")+         
  labs(title = "Fill mapped to discrete column, static color")

```


### Escalas {#ggplot_tips_scales .unnumbered}  

Una vez que se asigna una columna a una estética de la gráfica (por ejemplo, `x = `, `y = `, `fill = `, `color = `...), tu gráfica ganará una escala / leyenda. Mira arriba cómo la escala puede ser continua, discreta, de fecha, etc. dependiendo del tipo de la columna asignada. Si tienes múltiples estéticas asignadas a las columnas, tu gráfico tendrá múltiples escalas.

Puedes controlar las escalas con la función `scales_()` apropiada. Las funciones de escala de **ggplot()** tienen 3 partes que se escriben así: `scale_AESTHETIC_METHOD()`.

1)  La primera parte, `scale_()`, es fija.
2)  La segunda parte, la ESTÉTICA, debe ser la estética para la que deseas ajustar la escala (`_fill_`, `_shape_`, `_color_`, `_size_`, `_alpha_`...) - las opciones aquí también incluyen `_x_` e `_y_`.
3)  La tercera parte, el MÉTODO, será `_discrete()`, `continuous()`, `_date()`, `_gradient()`, o `_manual()`  dependiendo del tipo de la columna y de *cómo* se quiera controlar. Hay otros, pero estos son los más utilizados.

Asegúrate de utilizar la función correcta para la escala. De lo contrario, tu comando de escala no parecerá cambiar nada. Si tienes varias escalas, puedes utilizar varias funciones de escala para ajustarlas. Por ejemplo:

### Argumentos de la escala {.unnumbered}  

Cada tipo de escala tiene sus propios argumentos, aunque hay cierto solapamiento. Consulta la función escribiendo  `?scale_color_discrete` en la consola de R para ver la documentación de los argumentos de la función.

Para escalas continuas, utiliza breaks = para proporcionar una secuencia de valores con `seq()` (`to = `, `from = `, y `by = ` como se muestra en el ejemplo siguiente. Fija `expand = c(0,0)` para eliminar el espacio de relleno alrededor de los ejes (esto se puede utilizar en cualquier escala `_x_` o `_y_`.

En el caso de las escalas discretas, puedes ajustar el orden de aparición de los niveles con los `breaks = `, y cómo se muestran los valores con el argumento `labels = `. Proporciona un vector de caracteres a cada uno de ellos (véase el ejemplo siguiente). También puedes dejar de lado `NA` fácilmente estableciendo `na.translate = FALSE`.

Los matices de las escalas de fechas se tratan más ampliamente en la página de [curvas epidémicas](#epidemic-curves).


### Ajustes manuales {.unnumbered}  

Uno de los trucos más útiles es utilizar las funciones de escalado "manual" para asignar explícitamente los colores que se deseen. Son funciones con la sintaxis `scale_xxx_manual()` (por ejemplo, `scale_colour_manual()` o `scale_fill_manual())`. Cada uno de los argumentos siguientes se demuestra en el ejemplo de código que sigue.

* Asignar colores a los valores de los datos con el argumento `values = `
* Especificar un color para `NA` con `na.value = `
* Cambiar cómo se *escriben* los valores en la leyenda con el argumento `labels = `
* Cambiar el título de la leyenda con `name = `

A continuación, creamos un gráfico de barras y mostramos cómo aparece por defecto, y luego con tres escalas ajustadas - la escala continua del eje-y, la escala discreta del eje-x, y el ajuste manual del relleno (color interior de la barra).


```{r, warning=F, message=F}
# BASELINE - no scale adjustment
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = outcome, fill = gender))+
  labs(title = "Baseline - no scale adjustments")

# SCALES ADJUSTED
ggplot(data = linelist)+
  
  geom_bar(mapping = aes(x = outcome, fill = gender), color = "black")+
  
  theme_minimal()+                   # simplify background
  
  scale_y_continuous(                # continuous scale for y-axis (counts)
    expand = c(0,0),                 # no padding
    breaks = seq(from = 0,
                 to = 3000,
                 by = 500))+
  
  scale_x_discrete(                   # discrete scale for x-axis (gender)
    expand = c(0,0),                  # no padding
    drop = FALSE,                     # show all factor levels (even if not in data)
    na.translate = FALSE,             # remove NA outcomes from plot
    labels = c("Died", "Recovered"))+ # Change display of values
    
  
  scale_fill_manual(                  # Manually specify fill (bar interior color)
    values = c("m" = "violetred",     # reference values in data to assign colors
               "f" = "aquamarine"),
    labels = c("m" = "Male",          # re-label the legend (use "=" assignment to avoid mistakes)
              "f" = "Female",
              "Missing"),
    name = "Gender",                  # title of legend
    na.value = "grey"                 # assign a color for missing values
  )+
  labs(title = "Adjustment of scales") # Adjust the title of the fill legend
```

### Escalas de ejes continuos {.unnumbered}  

Cuando los datos se mapean a los ejes del gráfico, éstos también pueden ajustarse con comandos de escalas. Un ejemplo común es el ajuste de la visualización de un eje (por ejemplo, el eje-y) que se asigna a una columna con datos continuos.

Es posible que queramos ajustar los descansos o la visualización de los valores en ggplot utilizando `scale_y_continuous()`. Como se indicó anteriormente, utiliza el argumento `breaks = ` para proporcionar una secuencia de valores que servirán como "saltos" a lo largo de la escala. Estos son los valores en los que se mostrarán los números. A este argumento, puedes proporcionar un vector `c()` que contenga los valores de ruptura deseados, o puedes proporcionar una secuencia regular de números utilizando la función `seq()` de R **base**. Esta función `seq()` acepta  `to = `, `from = `, y `by = `.

```{r, warning=F, message=F, out.width=c('50%', '50%'), fig.show='hold'}
# BASELINE - no scale adjustment
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = outcome, fill = gender))+
  labs(title = "Baseline - no scale adjustments")

# 
ggplot(data = linelist)+
  geom_bar(mapping = aes(x = outcome, fill = gender))+
  scale_y_continuous(
    breaks = seq(
      from = 0,
      to = 3000,
      by = 100)
  )+
  labs(title = "Adjusted y-axis breaks")

```



#### Mostrar porcentajes {.unnumbered}  

Si los valores de datos originales son proporciones, puedes mostrarlos fácilmente como porcentajes con "%" proporcionando `labels = scales::percent` en el comando de escalas, como se muestra a continuación.

Aunque una alternativa sería convertir los valores en caracteres y añadir un carácter "%" al final, este enfoque causará complicaciones porque tus datos ya no serán valores numéricos continuos. 


```{r, warning=F, message=F, out.width=c('50%', '50%'), fig.show='hold'}
# Original y-axis proportions
#############################
linelist %>%                                   # start with linelist
  group_by(hospital) %>%                       # group data by hospital
  summarise(                                   # create summary columns
    n = n(),                                     # total number of rows in group
    deaths = sum(outcome == "Death", na.rm=T),   # number of deaths in group
    prop_death = deaths/n) %>%                   # proportion deaths in group
  ggplot(                                      # begin plotting
    mapping = aes(
      x = hospital,
      y = prop_death))+ 
  geom_col()+
  theme_minimal()+
  labs(title = "Display y-axis original proportions")



# Display y-axis proportions as percents
########################################
linelist %>%         
  group_by(hospital) %>% 
  summarise(
    n = n(),
    deaths = sum(outcome == "Death", na.rm=T),
    prop_death = deaths/n) %>% 
  ggplot(
    mapping = aes(
      x = hospital,
      y = prop_death))+
  geom_col()+
  theme_minimal()+
  labs(title = "Display y-axis as percents (%)")+
  scale_y_continuous(
    labels = scales::percent                    # display proportions as percents
  )

```

#### Escala logarítmica {.unnumbered}  

Para transformar un eje continuo a escala logarítmica, añade `trans = "log2"` al comando de escala. A modo de ejemplo, creamos un dataframe de regiones con sus respectivos valores de `preparedness_index` y casos acumulados. 

```{r}
plot_data <- data.frame(
  region = c("A", "B", "C", "D", "E", "F", "G", "H", "I"),
  preparedness_index = c(8.8, 7.5, 3.4, 3.6, 2.1, 7.9, 7.0, 5.6, 1.0),
  cases_cumulative = c(15, 45, 80, 20, 21, 7, 51, 30, 1442)
)

plot_data
```

Los casos acumulados de la región "I" son mucho mayores que los de las demás regiones. En circunstancias como ésta, puedes optar por mostrar el eje-y utilizando una escala logarítmica para que el lector pueda ver las diferencias entre las regiones con menos casos acumulados.

```{r, warning=F, message=F, out.width=c('50%', '50%'), fig.show='hold'}
# Original y-axis
preparedness_plot <- ggplot(data = plot_data,  
       mapping = aes(
         x = preparedness_index,
         y = cases_cumulative))+
  geom_point(size = 2)+            # points for each region 
  geom_text(
    mapping = aes(label = region),
    vjust = 1.5)+                  # add text labels
  theme_minimal()

preparedness_plot                  # print original plot


# print with y-axis transformed
preparedness_plot+                   # begin with plot saved above
  scale_y_continuous(trans = "log2") # add transformation for y-axis
```



### Escalas de gradiente {.unnumbered}  

Las escalas de degradado de relleno pueden implicar matices adicionales. Los valores predeterminados suelen ser bastante agradables, pero es posible que desees ajustar los valores, los cortes, etc.

Para demostrar cómo ajustar una escala de colores continua, utilizaremos unos datos de la página de [Rastreo de contactos](#contact-tracing-1) que contiene las edades de los casos y de sus casos de origen.


```{r, warning=F, message=F}
case_source_relationships <- rio::import(here::here("data", "godata", "relationships_clean.rds")) %>% 
  select(source_age, target_age) 
```

A continuación, producimos un gráfico de densidad de mapa de calor "rasterizado". No vamos a desarrollar cómo (ver el enlace en el párrafo anterior), pero nos centraremos en cómo podemos ajustar la escala de colores. Lee más sobre la función `stat_density2d()` de **ggplot2**  [aquí](https://ggplot2.tidyverse.org/reference/geom_density_2d.html). Observa cómo la escala de relleno es *continua*.

```{r, warn=F, message=F}
trans_matrix <- ggplot(
    data = case_source_relationships,
    mapping = aes(x = source_age, y = target_age))+
  stat_density2d(
    geom = "raster",
    mapping = aes(fill = after_stat(density)),
    contour = FALSE)+
  theme_minimal()
```

Ahora mostramos algunas variaciones en la escala de relleno:

```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
trans_matrix
trans_matrix + scale_fill_viridis_c(option = "plasma")
```

Ahora mostramos algunos ejemplos de cómo ajustar realmente los puntos de ruptura de la escala: 

* `scale_fill_gradient()` acepta dos colores (high/low)
* `scale_fill_gradientn()` acepta un vector de cualquier longitud de colores a  `values = ` (los valores intermedios serán interpolados)
* Usa [scales::rescale()](https://www.rdocumentation.org/packages/scales/versions/0.4.1/topics/rescale) para ajustar la posición de los colores a lo largo del gradiente; reescala tu vector de posiciones para que esté entre 0 y 1.  


```{r, out.width=c('50%', '50%'), fig.show='hold', warning=F, message=F}
trans_matrix + 
  scale_fill_gradient(     # 2-sided gradient scale
    low = "aquamarine",    # low value
    high = "purple",       # high value
    na.value = "grey",     # value for NA
    name = "Density")+     # Legend title
  labs(title = "Manually specify high/low colors")

# 3+ colors to scale
trans_matrix + 
  scale_fill_gradientn(    # 3-color scale (low/mid/high)
    colors = c("blue", "yellow","red") # provide colors in vector
  )+
  labs(title = "3-color scale")

# Use of rescale() to adjust placement of colors along scale
trans_matrix + 
  scale_fill_gradientn(    # provide any number of colors
    colors = c("blue", "yellow","red", "black"),
    values = scales::rescale(c(0, 0.05, 0.07, 0.10, 0.15, 0.20, 0.3, 0.5)) # positions for colors are rescaled between 0 and 1
    )+
  labs(title = "Colors not evenly positioned")

# use of limits to cut-off values that get fill color
trans_matrix + 
  scale_fill_gradientn(    
    colors = c("blue", "yellow","red"),
    limits = c(0, 0.0002))+
  labs(title = "Restrict value limits, resulting in grey space")

```


### Paletas {.unnumbered}  

#### Colorbrewer y Viridis {.unnumbered}

En general, si deseas paletas predefinidas, puedes utilizar las funciones `scale_xxx_brewer` o `scale_xxx_viridis_y.`

Las funciones 'brewer' pueden dibujar desde las paletas de [colorbrewer.org](http://www.colorbrewer.org).

Las funciones 'viridis' se basan en las paletas viridis (¡difíciles para los daltónicos!), que "proporcionan mapas de color que son perceptivamente uniformes tanto en color como en blanco y negro. También están diseñadas para ser percibidas por espectadores con formas comunes de daltonismo". (lee más [aquí](https://ggplot2.tidyverse.org/reference/scale_viridis.html) y [aquí](https://bids.github.io/colormap/)). Define si la paleta es discreta, continua o con intervalos especificando esto al final de la función (por ejemplo, discreta es `scale_xxx_viridis_d`).

Se aconseja que pruebes tu esquema en este [simulador de daltonismo](https://www.color-blindness.com/coblis-color-blindness-simulator/). Si tienes un esquema de color rojo/verde, prueba en su lugar un esquema "caliente-frío" (rojo-azul) como se describe [aquí](https://www.visualisingdata.com/2019/08/five-ways-to-design-for-red-green-colour-blindness/#:~:text=The pink-red through to,green hues used by default.)

Aquí hay un ejemplo de la página de [Conceptos básicos de ggplot](#ggplot-basics), utilizando varios esquemas de color. 

```{r, out.width=c('50%'), fig.show='hold', warning=F, message=F} 
symp_plot <- linelist %>%                                         # begin with linelist
  select(c(case_id, fever, chills, cough, aches, vomit)) %>%     # select columns
  pivot_longer(                                                  # pivot longer
    cols = -case_id,                                  
    names_to = "symptom_name",
    values_to = "symptom_is_present") %>%
  mutate(                                                        # replace missing values
    symptom_is_present = replace_na(symptom_is_present, "unknown")) %>% 
  ggplot(                                                        # begin ggplot!
    mapping = aes(x = symptom_name, fill = symptom_is_present))+
  geom_bar(position = "fill", col = "black") +                    
  theme_classic() +
  theme(legend.position = "bottom")+
  labs(
    x = "Symptom",
    y = "Symptom status (proportion)"
  )

symp_plot  # print with default colors

#################################
# print with manually-specified colors
symp_plot +
  scale_fill_manual(
    values = c("yes" = "black",         # explicitly define colours
               "no" = "white",
               "unknown" = "grey"),
    breaks = c("yes", "no", "unknown"), # order the factors correctly
    name = ""                           # set legend to no title

  ) 

#################################
# print with viridis discrete colors
symp_plot +
  scale_fill_viridis_d(
    breaks = c("yes", "no", "unknown"),
    name = ""
  )


```



<!-- ======================================================= -->
## Cambiar el orden de las variables discretas {#change-order-of-discrete-variables}  

Cambiar el orden en que aparecen las variables discretas es a menudo difícil de entender para las personas que son nuevas en los gráficos de `ggplot2`. Sin embargo, es fácil de entender cómo hacer esto una vez que se entiende cómo `ggplot2` maneja las variables discretas por debajo. En general, si se utiliza una variable discreta, se convierte automáticamente en un tipo de factor - que ordena los factores por orden alfabético por defecto. Para manejar esto, simplemente tienes que reordenar los niveles de los factores para reflejar el orden en que te gustaría que aparecieran en el gráfico. Para obtener información más detallada sobre cómo reordenar los objetos de factor, consulta la página [Factores](#factors).

Podemos ver un ejemplo común utilizando los grupos de edad - por defecto el grupo de 5 a 9 años se colocará en medio de los grupos de edad (dado el orden alfanumérico), pero podemos moverlo detrás del grupo de 0 a 4 años del gráfico renivelando los factores.


```{r, , warning=F, message=F}
ggplot(
  data = linelist %>% drop_na(age_cat5),                         # remove rows where age_cat5 is missing
  mapping = aes(x = fct_relevel(age_cat5, "5-9", after = 1))) +  # relevel factor

  geom_bar() +
  
  labs(x = "Age group", y = "Number of hospitalisations",
       title = "Total hospitalisations by age group") +
  
  theme_minimal()


```

#### **ggthemr** {.unnnumbered}  

También considera utilizar el paquete **ggthemr**. Puedes descargar este paquete desde Github usando  [estas](https://github.com/Mikata-Project/ggthemr) instrucciones. Ofrece paletas que son muy agradables estéticamente, pero ten en cuenta que estas suelen tener un número máximo de valores que puede ser limitante si quieres más de 7 u 8 colores.






## Líneas de contorno  {#contour-lines}

Los gráficos de contorno son útiles cuando se tienen muchos puntos que pueden cubrirse unos a otros ("sobretrazado"). Los datos de la fuente de casos utilizados anteriormente se trazan de nuevo, pero de forma más sencilla utilizando `stat_density2d()` y `stat_density2d_filled()` para producir niveles de contorno discretos - como un mapa topográfico. Lee más sobre las estadísticas [aquí](https://ggplot2.tidyverse.org/reference/geom_density_2d.html).


```{r, out.width=c('50%'), fig.show='hold', warning=F, message=F}
case_source_relationships %>% 
  ggplot(aes(x = source_age, y = target_age))+
  stat_density2d()+
  geom_point()+
  theme_minimal()+
  labs(title = "stat_density2d() + geom_point()")


case_source_relationships %>% 
  ggplot(aes(x = source_age, y = target_age))+
  stat_density2d_filled()+
  theme_minimal()+
  labs(title = "stat_density2d_filled()")

```



## Distribuciones marginales  {#marginal-distributions}

Para mostrar las distribuciones en los bordes de un gráfico de dispersión `geom_point()`, puedes utilizar el paquete **ggExtra** y su función `ggMarginal()`. Guarda tu ggplot original como un objeto, y pásalo a `ggMarginal()` como se muestra a continuación. Estos son los argumentos clave:

* Debe especificar el `type = ` como "histogram", "density" "boxplot", "violin", o "densigram"
* Por defecto, los gráficos marginales aparecerán para ambos ejes. Puedes establecer `margins = ` a "x" o "y" si sólo quieres uno.
* Otros argumentos opcionales son `fill = ` (color de la barra), `color = ` (color de la línea), `size = ` (tamaño del gráfico en relación con el tamaño del margen, por lo que un número mayor hace que el gráfico marginal sea más pequeño).
* Puedes proporcionar otros argumentos específicos del eje a `xparams = ` e `yparams = `. Por ejemplo, para tener diferentes tamaños de cubos de histograma, como se muestra a continuación.

Puedes hacer que los gráficos marginales reflejen grupos (columnas a las que se les ha asignado un `color =  ` en su estética mapeada de `ggplot()`). Si este es el caso, establece el argumento `ggMarginal()` `groupColour = ` o `groupFill = ` a `TRUE`, como se muestra a continuación.

Lee más en [esta viñeta](https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html), en la [galería de gráficos de R](https://www.r-graph-gallery.com/277-marginal-histogram-for-ggplot2.html) o en la documentación de la función R `?ggMarginal.`

```{r, message=FALSE, warning=FALSE}
# Install/load ggExtra
pacman::p_load(ggExtra)

# Basic scatter plot of weight and age
scatter_plot <- ggplot(data = linelist)+
  geom_point(mapping = aes(y = wt_kg, x = age)) +
  labs(title = "Scatter plot of weight and age")
```

Para añadir histogramas marginales utiliza `type = "histogram"`. Opcionalmente puedes establecer `groupFill = TRUE` para obtener histogramas apilados.    

```{r, message=FALSE, warning=FALSE}
# with histograms
ggMarginal(
  scatter_plot,                     # add marginal histograms
  type = "histogram",               # specify histograms
  fill = "lightblue",               # bar fill
  xparams = list(binwidth = 10),    # other parameters for x-axis marginal
  yparams = list(binwidth = 5))     # other parameters for y-axis marginal
```

Gráfico de densidad marginal con valores agrupados/coloreados:

```{r, message=FALSE, warning=FALSE}

# Scatter plot, colored by outcome
# Outcome column is assigned as color in ggplot. groupFill in ggMarginal set to TRUE
scatter_plot_color <- ggplot(data = linelist %>% drop_na(gender))+
  geom_point(mapping = aes(y = wt_kg, x = age, color = gender)) +
  labs(title = "Scatter plot of weight and age")+
  theme(legend.position = "bottom")

ggMarginal(scatter_plot_color, type = "density", groupFill = TRUE)
```

Establece el argumento `size = ` para ajustar el tamaño relativo del gráfico marginal. Un número más pequeño hace un gráfico marginal más grande. También se establece el `color = `. A continuación se muestra un boxplot marginal, con la demostración del argumento `margins = ` por lo que aparece en un solo eje: 

```{r, message=FALSE, warning=FALSE}
# with boxplot 
ggMarginal(
  scatter_plot,
  margins = "x",      # only show x-axis marginal plot
  type = "boxplot")   
```



<!-- ======================================================= -->
## Etiquetado inteligente {#smart-labeling}  

En **ggplot2**, también es posible añadir texto a los gráficos. Sin embargo, esto viene con la notable limitación de que las etiquetas de texto a menudo chocan con los puntos de datos en un gráfico, haciendo que se vean desordenados o difíciles de leer. No hay una manera ideal de lidiar con esto en el paquete base, pero hay un complemento de **ggplot2**, conocido como **ggrepel** que hace que esto sea muy simple.

El paquete **ggrepel** proporciona dos nuevas funciones, `geom_label_repel()` y `geom_text_repel()`, que sustituyen a `geom_label()` y `geom_text()`. Basta con utilizar estas funciones en lugar de las funciones base para producir etiquetas ordenadas. Dentro de la función, mapea la estética `aes()` como siempre, pero incluye el argumento `label = ` al que le proporciona un nombre de columna que contenga los valores que deseas mostrar (por ejemplo, id de paciente, o nombre, etc.). Puedes hacer etiquetas más complejas combinando columnas y nuevas líneas (`\n`) dentro de `str_glue()` como se muestra a continuación.

Algunos consejos:

* Utiliza `min.segment.length = 0` para dibujar siempre segmentos de línea, o `min.segment.length = Inf` para no dibujarlos nunca
* Utiliza `size = ` fuera de `aes()` para establecer el tamaño del texto
* Utiliza `force = ` para cambiar el grado de repulsión entre las etiquetas y sus respectivos puntos (por defecto es 1)
* Incluye `fill = ` dentro de `aes()` para tener la etiqueta coloreada por el valor
  * Puede aparecer una letra "a" en la leyenda - añade `guides(fill = guide_legend(override.aes = aes(color = NA)))+` para eliminarla

Para verlo con mayor profundidad, consulta este [tutorial](https://ggrepel.slowkow.com/articles/examples.html)   

```{r, , warning=F, message=F}
pacman::p_load(ggrepel)

linelist %>%                                               # start with linelist
  group_by(hospital) %>%                                   # group by hospital
  summarise(                                               # create new dataset with summary values per hospital
    n_cases = n(),                                           # number of cases per hospital
    delay_mean = round(mean(days_onset_hosp, na.rm=T),1),    # mean delay per hospital
  ) %>% 
  ggplot(mapping = aes(x = n_cases, y = delay_mean))+      # send data frame to ggplot
  geom_point(size = 2)+                                    # add points
  geom_label_repel(                                        # add point labels
    mapping = aes(
      label = stringr::str_glue(
        "{hospital}\n{n_cases} cases, {delay_mean} days")  # how label displays
      ), 
    size = 3,                                              # text size in labels
    min.segment.length = 0)+                               # show all line segments                
  labs(                                                    # add axes labels
    title = "Mean delay to admission, by hospital",
    x = "Number of cases",
    y = "Mean delay (days)")
```

Puedes etiquetar sólo un subconjunto de los puntos de datos - utilizando la sintaxis estándar de `ggplot()` para proporcionar diferentes `data = ` para cada `geom` del gráfico. A continuación, se trazan todos los casos, pero sólo se etiquetan algunos.   

```{r, warning=F, message=FALSE}

ggplot()+
  # All points in grey
  geom_point(
    data = linelist,                                   # all data provided to this layer
    mapping = aes(x = ht_cm, y = wt_kg),
    color = "grey",
    alpha = 0.5)+                                              # grey and semi-transparent
  
  # Few points in black
  geom_point(
    data = linelist %>% filter(days_onset_hosp > 15),  # filtered data provided to this layer
    mapping = aes(x = ht_cm, y = wt_kg),
    alpha = 1)+                                                # default black and not transparent
  
  # point labels for few points
  geom_label_repel(
    data = linelist %>% filter(days_onset_hosp > 15),  # filter the data for the labels
    mapping = aes(
      x = ht_cm,
      y = wt_kg,
      fill = outcome,                                          # label color by outcome
      label = stringr::str_glue("Delay: {days_onset_hosp}d")), # label created with str_glue()
    min.segment.length = 0) +                                  # show line segments for all
  
  # remove letter "a" from inside legend boxes
  guides(fill = guide_legend(override.aes = aes(color = NA)))+
  
  # axis labels
  labs(
    title = "Cases with long delay to admission",
    y = "weight (kg)",
    x = "height(cm)")
```





<!-- ======================================================= -->
## Ejes temporales {#time-axes}

Trabajar con ejes de tiempo en `ggplot` puede parecer desalentador, pero se hace muy fácil con unas pocas funciones clave. Recuerda que cuando trabajes con el tiempo o la fecha debes asegurarte que las variables correctas están formateadas como tipo date o datetime - mira la página [Trabajar con fechas](#working-with-dates-1) para más información sobre esto, o la página [Curvas epidémicas](#epidemic-curves) (sección ggplot) para ver ejemplos.

El conjunto de funciones más útil para trabajar con fechas en `ggplot2` son las funciones de escala (`scale_x_date()`, `scale_x_datetime()`, y sus funciones afines del eje-y). Estas funciones permiten definir la frecuencia de las etiquetas de los ejes, y cómo formatear las etiquetas de los ejes. Para saber cómo dar formato a las fechas, vuelve a ver la sección de *trabajar con fechas*. Puedes utilizar los argumentos `date_breaks` y `date_labels` para especificar el aspecto de las fechas:

1.  `date_breaks` permite especificar la frecuencia con la que se producen las rupturas de los ejes - se puede pasar aquí una cadena (por ejemplo, `"3 months"`, or "`2 days"`)

2.  `date_labels` permite definir el formato en el que se muestran las fechas. Puedes pasar una cadena de formato de fecha a estos argumentos (por ejemplo, `"%b-%d-%Y"`):


```{r, , warning=F, message=F}
# make epi curve by date of onset when available
ggplot(linelist, aes(x = date_onset)) +
  geom_histogram(binwidth = 7) +
  scale_x_date(
    # 1 break every 1 month
    date_breaks = "1 months",
    # labels should show month then date
    date_labels = "%b %d"
  ) +
  theme_classic()

```



<!-- ======================================================= -->
## Resaltando {#highlighting}

Resaltar elementos específicos en un gráfico es una forma útil de llamar la atención sobre una instancia específica de una variable, a la vez que se proporciona información sobre la dispersión de los datos. Aunque esto no es fácil de hacer en **ggplot2** base, hay un paquete externo que puede ayudar a hacer esto conocido como **gghighlight**. Es fácil de usar dentro de la sintaxis de `ggplot`.

El paquete **gghighlight** utiliza la función `gghighlight()` para lograr este efecto. Para usar esta función, suministra una declaración lógica a la función - esto puede tener resultados bastante flexibles, pero aquí mostraremos un ejemplo de la distribución de edad de los casos en nuestro listado, resaltándolos por resultado.

```{r, , warning=F, message=F}
# load gghighlight
library(gghighlight)

# replace NA values with unknown in the outcome variable
linelist <- linelist %>%
  mutate(outcome = replace_na(outcome, "Unknown"))

# produce a histogram of all cases by age
ggplot(
  data = linelist,
  mapping = aes(x = age_years, fill = outcome)) +
  geom_histogram() + 
  gghighlight::gghighlight(outcome == "Death")     # highlight instances where the patient has died.

```

Esto también funciona bien con las funciones de facetas - ¡permite al usuario producir gráficos de facetas con los datos de fondo resaltados que no se aplican a la faceta! A continuación, contamos los casos por semana y trazamos las curvas de epidemia por hospital (`color = ` y `facet_wrap()` ajustado a la columna `hospital`).

```{r, , warning=F, message=F}

# produce a histogram of all cases by age
linelist %>% 
  count(week = lubridate::floor_date(date_hospitalisation, "week"),
        hospital) %>% 
  ggplot()+
  geom_line(aes(x = week, y = n, color = hospital))+
  theme_minimal()+
  gghighlight::gghighlight() +                      # highlight instances where the patient has died
  facet_wrap(~hospital)                              # make facets by outcome

```





## Representar múltiples conjuntos de datos {#plotting-multiple-datasets}

Ten en cuenta que alinear correctamente los ejes para trazar múltiples conjuntos de datos en el mismo gráfico puede ser difícil. Considera una de las siguientes estrategias:

* Fusionar los datos antes de trazarlos y convertirlos al formato "long" con una columna que refleje el conjunto de datos
* Utilizar **Cowplot** o un paquete similar para combinar dos gráficos (véase más abajo)






<!-- ======================================================= -->
## Combinar gráficos {#combine-plots}

Dos paquetes que son muy útiles para combinar gráficos son **cowplot** y **patchwork**. En esta página nos centraremos principalmente en **cowplot**, con el uso ocasional de **patchwork**.

Aquí está la [introducción en línea a cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html). Puedes leer la documentación más extensa de cada función en línea [aquí](https://www.rdocumentation.org/packages/cowplot/versions/1.1.1). A continuación cubriremos algunos de los casos de uso y funciones más comunes.

El paquete **cowplot** funciona en tándem con **ggplot2** - esencialmente, se utiliza para organizar y combinar ggplots y sus leyendas en figuras compuestas. También puede aceptar gráficos de R **base**.

```{r}
pacman::p_load(
  tidyverse,      # data manipulation and visualisation
  cowplot,        # combine plots
  patchwork       # combine plots
)
```


Mientras que las facetas (descritas en la página de [Conceptos básicos de ggplot](#ggplot-basics)) son un enfoque conveniente para el trazado, a veces no es posible obtener los resultados que deseas de su enfoque relativamente restrictivo. En este caso, se puede optar por combinar los gráficos pegándolos en un gráfico más grande. Hay tres paquetes bien conocidos que son excelentes para esto - **cowplot**, **gridExtra**, y **patchwork**. Sin embargo, estos paquetes hacen en gran medida las mismas cosas, por lo que nos centraremos en **cowplot** para esta sección.

### `plot_grid()` {.unnumbered}

El paquete **cowplot** tiene una gama bastante amplia de funciones, pero el uso más fácil de él se puede lograr mediante el uso de `plot_grid()`. Se trata de una forma de organizar los gráficos predefinidos en una cuadrícula. Podemos trabajar a través de otro ejemplo con el conjunto de datos de la malaria - aquí podemos trazar el total de casos por distrito, y también mostrar la curva epidémica en el tiempo.


```{r, , warning=F, message=F}
malaria_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) 

# bar chart of total cases by district
p1 <- ggplot(malaria_data, aes(x = District, y = malaria_tot)) +
  geom_bar(stat = "identity") +
  labs(
    x = "District",
    y = "Total number of cases",
    title = "Total malaria cases by district"
  ) +
  theme_minimal()

# epidemic curve over time
p2 <- ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +
  geom_col(width = 1) +
  labs(
    x = "Date of data submission",
    y =  "number of cases"
  ) +
  theme_minimal()

cowplot::plot_grid(p1, p2,
                  # 1 column and two rows - stacked on top of each other
                   ncol = 1,
                   nrow = 2,
                   # top plot is 2/3 as tall as second
                   rel_heights = c(2, 3))


```




### Combinar leyendas {.unnumbered}  

Si tus gráficos tienen la misma leyenda, combinarlos es relativamente sencillo. Simplemente utiliza el método de **cowplot** anterior para combinar los gráficos, pero elimina la leyenda de uno de ellos (de-duplica).

Si tus gráficos tienen leyendas diferentes, debes utilizar un enfoque alternativo:

1.  Crea y guarda tus gráficos *sin leyendas* utilizando `theme(legend.position = "none")`
2.  Extrae las leyendas de cada gráfica utilizando get_legend() como se muestra a continuación - *pero extrae las leyendas de los gráficos modificados para mostrar realmente la leyenda*
3.  Combina las leyendas en un panel de leyendas
4.  Combina el panel de gráficos y leyendas

Para comprobarlo, mostramos las dos gráficos por separado, y luego dispuestas en una cuadrícula con sus propias leyendas mostrando (uso feo e ineficiente del espacio):

```{r, out.width=c('50%'), fig.show='hold', warning=F, message=F}
p1 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, outcome) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+
  scale_fill_brewer(type = "qual", palette = 4, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  labs(title = "Cases by outcome")


p2 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, age_cat) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+
  scale_fill_brewer(type = "qual", palette = 1, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  theme(axis.text.y = element_blank())+
  labs(title = "Cases by age")

```

Así es como se ven los dos gráficos cuando se combinan usando `plot_grid()` sin combinar sus leyendas:

```{r, warning=F, message=F}
cowplot::plot_grid(p1, p2, rel_widths = c(0.3))
```

Y ahora mostramos cómo combinar las leyendas. Esencialmente lo que hacemos es definir cada gráfica *sin* su leyenda (`theme(legend.position = "none"`), y luego definimos la leyenda de cada gráfica *por separado*, utilizando la función `get_legend()` de **cowplot**. Cuando extraemos la leyenda del gráfico guardado, tenemos que añadir `+` la leyenda de nuevo, incluyendo la especificación de la colocación ("derecha") y pequeños ajustes para la alineación de las leyendas y sus títulos. A continuación, combinamos las leyendas verticalmente, y luego combinamos los dos gráficos con las leyendas recién combinadas. Voila! 

```{r, warning=F, message=F}

# Define plot 1 without legend
p1 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, outcome) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+
  scale_fill_brewer(type = "qual", palette = 4, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  theme(legend.position = "none")+
  labs(title = "Cases by outcome")


# Define plot 2 without legend
p2 <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, age_cat) %>% 
  ggplot()+
  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+
  scale_fill_brewer(type = "qual", palette = 1, na.value = "grey")+
  coord_flip()+
  theme_minimal()+
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  )+
  labs(title = "Cases by age")


# extract legend from p1 (from p1 + legend)
leg_p1 <- cowplot::get_legend(p1 +
                                theme(legend.position = "right",        # extract vertical legend
                                      legend.justification = c(0,0.5))+ # so legends  align
                                labs(fill = "Outcome"))                 # title of legend
# extract legend from p2 (from p2 + legend)
leg_p2 <- cowplot::get_legend(p2 + 
                                theme(legend.position = "right",         # extract vertical legend   
                                      legend.justification = c(0,0.5))+  # so legends align
                                labs(fill = "Age Category"))             # title of legend

# create a blank plot for legend alignment
#blank_p <- patchwork::plot_spacer() + theme_void()

# create legends panel, can be one on top of the other (or use spacer commented above)
legends <- cowplot::plot_grid(leg_p1, leg_p2, nrow = 2, rel_heights = c(.3, .7))

# combine two plots and the combined legends panel
combined <- cowplot::plot_grid(p1, p2, legends, ncol = 3, rel_widths = c(.4, .4, .2))

combined  # print


```

Esta solución fue aprendida de [este post](https://stackoverflow.com/questions/52060601/ggplot-multiple-legends-arrangement) con un arreglo menor para alinear las leyendas de [este post](https://github.com/wilkelab/cowplot/issues/33).


<span style="color: darkgreen;">***CONSEJO:*** Nota divertida: la "vaca" en **cowplot** viene del nombre del creador: Claus O. Wilke.</span>  


### Gráficos insertados {.unnumbered} 

Puedes insertar una gráfica en otra utilizando **cowplot**. Aquí hay cosas que hay que tener en cuenta:

* Define el gráfico principal con `theme_half_open()` de **cowplot**; puede ser mejor tener la leyenda arriba o abajo
* Define el gráfico de inserción. Lo mejor es tener un gráfico en el que no se necesite una leyenda. Puedes eliminar los elementos del tema del gráfico con `element_blank()` como se muestra a continuación.
* Combínalos aplicando `ggdraw()` al gráfico principal, y luego añadiendo `draw_plot()` en el gráfico de inserción y especificando las coordenadas (x e y de la esquina inferior izquierda), la altura y la anchura como proporción de todo el gráfico principal. 


```{r, out.width=c('100%'), fig.show='hold', warning=F, message=F}

# Define main plot
main_plot <- ggplot(data = linelist)+
  geom_histogram(aes(x = date_onset, fill = hospital))+
  scale_fill_brewer(type = "qual", palette = 1, na.value = "grey")+ 
  theme_half_open()+
  theme(legend.position = "bottom")+
  labs(title = "Epidemic curve and outcomes by hospital")


# Define inset plot
inset_plot <- linelist %>% 
  mutate(hospital = recode(hospital, "St. Mark's Maternity Hospital (SMMH)" = "St. Marks")) %>% 
  count(hospital, outcome) %>% 
  ggplot()+
    geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+
    scale_fill_brewer(type = "qual", palette = 4, na.value = "grey")+
    coord_flip()+
    theme_minimal()+
    theme(legend.position = "none",
          axis.title.y = element_blank())+
    labs(title = "Cases by outcome") 


# Combine main with inset
cowplot::ggdraw(main_plot)+
     draw_plot(inset_plot,
               x = .6, y = .55,    #x = .07, y = .65,
               width = .4, height = .4)

```


Esta técnica se explica mejor en estas dos viñetas:

[Laboratorio Wilke](https://wilkelab.org/cowplot/articles/drawing_with_on_plots.html)

[documentación de draw_plot()](https://www.rdocumentation.org/packages/cowplot/versions/1.1.1/topics/draw_plot)




<!-- ======================================================= -->
## Ejes dobles {#dual-axes}

Un eje-y secundario es a menudo una adición solicitada a un gráfico `ggplot2`. Aunque existe un fuerte debate sobre la validez de estos gráficos en la comunidad de visualización de datos, y a menudo no se recomiendan, es posible que tu jefe los quiera. A continuación, presentamos un método para conseguirlos: usa el paquete **cowplot** para combinar dos gráficos separados.

Este enfoque implica la creación de dos gráficos separados - uno con un eje-y a la izquierda, y el otro con un eje-y a la derecha. Ambos utilizarán un tema específico de `theme_cowplot()` y deben tener el mismo eje-x. Luego, en un tercer comando, los dos gráficos se alinean y se superponen. Las funcionalidades de **cowplot**, de las que ésta es sólo una, se describen en profundidad en este [sitio](https://wilkelab.org/cowplot/articles/aligning_plots.html).

Para demostrar esta técnica, superpondremos la curva epidémica con una línea del porcentaje semanal de pacientes fallecidos. Utilizamos este ejemplo porque la alineación de las fechas en el eje-x es más compleja que, por ejemplo, alinear un gráfico de barras con otro gráfico. Hay que tener en cuenta algunas cosas:

* Para la curva epidémica y la línea se agrupan en semanas antes de trazarlas *y* los `date_breaks` y `date_labels` son idénticos - lo hacemos para que los ejes-x de los dos gráficos sean los mismos cuando se superponen.
* El eje-y se mueve a la derecha para el gráfico 2 con el argumento `position = ` de `scale_y_continuous()`.
* Ambos gráficos hacen uso de `theme_cowplot()`

Observa que hay otro ejemplo de esta técnica en la página de [curvas epidémicas](#epidemic-curves): la superposición de la incidencia acumulada sobre la curva epidemica.

**Hacer el gráfico 1**
Esto es esencialmente la curva epidémica. Usamos `geom_area()` sólo para mostrar su uso (área bajo una línea, por defecto)

```{r, warning=F, message=F}
pacman::p_load(cowplot)            # load/install cowplot

p1 <- linelist %>%                 # save plot as object
     count(
       epiweek = lubridate::floor_date(date_onset, "week")) %>% 
     ggplot()+
          geom_area(aes(x = epiweek, y = n), fill = "grey")+
          scale_x_date(
               date_breaks = "month",
               date_labels = "%b")+
     theme_cowplot()+
     labs(
       y = "Weekly cases"
     )

p1                                      # view plot 
```

**Hacer el gráfico 2**
Crea el segundo gráfico mostrando una línea del porcentaje semanal de casos que murieron. 

```{r, warning=F, message=F}

p2 <- linelist %>%         # save plot as object
     group_by(
       epiweek = lubridate::floor_date(date_onset, "week")) %>% 
     summarise(
       n = n(),
       pct_death = 100*sum(outcome == "Death", na.rm=T) / n) %>% 
     ggplot(aes(x = epiweek, y = pct_death))+
          geom_line()+
          scale_x_date(
               date_breaks = "month",
               date_labels = "%b")+
          scale_y_continuous(
               position = "right")+
          theme_cowplot()+
          labs(
            x = "Epiweek of symptom onset",
            y = "Weekly percent of deaths",
            title = "Weekly case incidence and percent deaths"
          )

p2     # view plot
```

Ahora alineamos el gráfico utilizando la función `align_plots()`, especificando la alineación horizontal y vertical ("hv", también podría ser "h", "v", "none"). También especificamos la alineación de todos los ejes (top, bottom, left, y right) con "tblr". La salida es de tipo list (2 elementos).

Luego dibujamos los dos gráficos juntos usando `ggdraw()` (de **cowplot**) y referenciando las dos partes del objeto `aligned_plots`.

```{r, warning=F, message=F}
aligned_plots <- cowplot::align_plots(p1, p2, align="hv", axis="tblr")         # align the two plots and save them as list
aligned_plotted <- ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])  # overlay them and save the visual plot
aligned_plotted                                                                # print the overlayed plots

```



<!-- ### Statistical transformation {.unnumbered}   -->
<!-- Another way to do this to have the second axis be a direct transformation of the secondary axis.  -->

<!-- Differences in axis values will be purely cosmetic - if you want to show two different variables on one graph, with different y-axis scales for each variable, this will not work without some work behind the scenes. To obtain this effect, you will have to transform one of your variables in the data, and apply the same transformation *in reverse* when specifying the axis labels. Based on this, you can either specify the transformation explicitly (e.g. variable a is around 10x as large as variable b) or calculate it in the code (e.g. what is the ratio between the maximum values of each dataset). -->


<!-- The syntax for adding a secondary axis is very straightforward! When calling a `scale_xxx_xxx()` function (e.g. `scale_y_continuous()`), use the `sec.axis` argument to call the `sec_axis()` function. The `trans` argument in this function allows you to specify the label transformation for the axis - provide this in standard tidyverse syntax.  -->

<!-- For example, if we want to show the number of positive RDTs in the malaria dataset for facility 1, showing 0-4 year olds and all cases on chart: -->


<!-- ```{r, , warning=F, message=F} -->

<!-- # take malaria data from facility 1 -->
<!-- malaria_facility_1 <- malaria_data %>% -->
<!--   filter(location_name == "Facility 1") -->

<!-- # calculate the ratio between malaria_rdt_0-4 and malaria_tot  -->

<!-- tf_ratio <- max(malaria_facility_1$malaria_tot, na.rm = T) / max(malaria_facility_1$`malaria_rdt_0-4`, na.rm = T) -->

<!-- # transform the values in the dataset -->

<!-- malaria_facility_1 <- malaria_facility_1 %>% -->
<!--   mutate(malaria_rdt_0_4_tf = `malaria_rdt_0-4` * tf_ratio) -->


<!-- # plot the graph with dual axes -->

<!-- ggplot(malaria_facility_1, aes(x = data_date)) + -->
<!--   geom_line(aes(y = malaria_tot, col = "Total cases")) + -->
<!--   geom_line(aes(y = malaria_rdt_0_4_tf, col = "Cases: 0-4 years old")) + -->
<!--   scale_y_continuous( -->
<!--     name = "Total cases", -->
<!--     sec.axis = sec_axis(trans = ~ . / tf_ratio, name = "Cases: 0-4 years old") -->
<!--   ) + -->
<!--   labs(x = "date of data collection") + -->
<!--   theme_minimal() + -->
<!--   theme(legend.title = element_blank()) -->



<!-- ``` -->






<!-- ## Sparklines   -->

<!-- UNDER CONSTRUCTION   -->
<!-- (perhaps move to Tables for presentation page) -->




## Paquetes para ayudarte  {#packages-to-help-you}

Hay algunos paquetes de R muy buenos diseñados específicamente para ayudarte a navegar por **ggplot2**:

### Apuntar y clicar en **ggplot2** con **equisse** {.unnumbered}

"Este complemento te permite explorar interactivamente tus datos visualizándolos con el paquete ggplot2. Te permite dibujar gráficos de barras, curvas, gráficos de dispersión, histogramas, boxplot y objetos sf, y luego exportar el gráfico o recuperar el código para reproducir el gráfico."

Instala y luego lanza el complemento con el menú de RStudio o con `esquisse::esquisser()`.

Ver la [página de Github](https://github.com/dreamRs/esquisse) de esquisse

[Documentación adicional](https://dreamrs.github.io/esquisse/index.html)









## Miscelánea  {#miscellaneous}


### Pantalla numérica {.unnumbered}  

Puedes desactivar la notación científica ejecutando este comando antes de representar gráficamente.

```{r, eval=F}
options(scipen=999)
```

O aplicar `number_format()` del paquete **scales** a un valor o columna específicos, como se muestra a continuación.

Utiliza las funciones del paquete **scales** para ajustar fácilmente la forma en que se muestran los números. Pueden aplicarse a las columnas del dataframe, pero se muestran en los números individuales a modo de ejemplo.

```{r}
scales::number(6.2e5)
scales::number(1506800.62,  accuracy = 0.1,)
scales::comma(1506800.62, accuracy = 0.01)
scales::comma(1506800.62, accuracy = 0.01,  big.mark = "." , decimal.mark = ",")
scales::percent(0.1)
scales::dollar(56)
scales::scientific(100000)
```

## Recursos {#resources-24}

Inspiración [galería de gráficos de ggplot](https://www.tidyverse.org/blog/2018/07/ggplot2-3-0-0/)

[Directrices para la presentación de los datos de vigilancia](https://ecdc.europa.eu/sites/portal/files/documents/Guidelines for presentation of surveillance data-final-with-cover-for-we....pdf) del Centro Europeo para la Prevención y el Control de las Enfermedades (ecdc)

[Utilización de etiquetadoras para tiras de facetas](http://www.cookbook-r.com/Graphs/Facets_(ggplot2)/#modifying-facet-label-text) y [Etiquetadoras](https://ggplot2.tidyverse.org/reference/labellers.html)]

Ajuste del orden con factores  

[fct_reorder](https://forcats.tidyverse.org/reference/fct_reorder.html)  

[fct_inorder](https://forcats.tidyverse.org/reference/fct_inorder.html)  

[Cómo reordenar un boxplot](https://cmdlinetips.com/2019/02/how-to-reorder-a-boxplot-in-r/)  

[Reordenar una variable en ggplot2](https://www.r-graph-gallery.com/267-reorder-a-variable-in-ggplot2.html)  

[R for Data Science en español - Factores](https://es.r4ds.hadley.nz/factores.html)  

Leyendas  
[Ajustar el orden de las leyendas](https://stackoverflow.com/questions/38425908/reverse-stacking-order-without-affecting-legend-order-in-ggplot2-bar-charts)

Pies de foto   
[Alineación de las leyendas](https://stackoverflow.com/questions/64701500/left-align-ggplot-caption)

Etiquetas  
[ggrepel](https://ggrepel.slowkow.com/articles/examples.html)

Cheetsheets  
[Trazado bonito con ggplot2](http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/)




<!-- TO DO - Under construction -->


<!-- * Straight horizontal, vertical, or other line -->

<!-- You can also add straight lines to your plot with `geom_hline()` (horizontal), `geom_vline()` (vertical) or `geom_abline()` (with a specified y intercept and slope) -->


<!-- Using option `label_wrap_gen` in facet_wrap to have multiple strip lines -->
<!-- labels and colors of strips -->

<!-- Axis text vertical adjustment -->
<!-- rotation -->
<!-- Labellers -->

<!-- limit range with limit() and coord_cartesian(), ylim(), or scale_x_continuous() -->
<!-- theme_classic() -->

<!-- expand = c(0,0) -->
<!-- coord_flip() -->
<!-- tick marks -->

<!-- ggrepel -->
<!-- animations -->

<!-- remove -->
<!-- remove title -->
<!-- using fill = or color = in labs() -->
<!-- flip order / don't flip order -->
<!-- move location -->
<!-- color?    theme(legend.title = element_text(colour="chocolate", size=16, face="bold"))+ scale_color_discrete(name="This color is\ncalled chocolate!?") -->
<!-- Color of boxes behind points in legend  -->
<!--      theme(legend.key=element_rect(fill='pink'))   or use fill = NA to remove them. http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/  -->
<!-- Change size of symbols in legend only guides(colour = guide_legend(override.aes = list(size=4))) -->


<!-- Turn off a layer in the legend -->
<!-- geom_text(data=nmmaps, aes(date, temp, label=round(temp)), size=4) -->
<!-- geom_text(data=nmmaps, aes(date, temp, label=round(temp), size=4), show_guide=FALSE) -->

<!-- Force a legend even if there is no aes().  -->
<!-- ggplot(nmmaps, aes(x=date, y=o3))+ -->
<!--      geom_line(aes(color="Important line"))+ -->
<!--      geom_point(aes(color="My points")) -->
<!-- Control the shape in the legend with guides - a list with linetype and shape -->
<!-- ggplot(nmmaps, aes(x=date, y=o3))+geom_line(aes(color="Important line"))+ -->
<!--    geom_point(aes(color="Point values"))+ -->
<!--   scale_colour_manual(name='', values=c('Important line'='grey', 'Point values'='red'), guide='legend') + -->
<!--   guides(colour = guide_legend(override.aes = list(linetype=c(1,0) -->
<!--                                                       , shape=c(NA, 16)))) -->
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/ggplot_tips.Rmd-->


# Curvas epidémicas {#epidemic-curves}

```{r, out.width=c('75%'), echo=F, message=F}
knitr::include_graphics(here::here("images", "epicurve_top.png"))
```    


Una curva epidémica (también conocida como "curva epi") es un gráfico epidemiológico básico que se suele utilizar para visualizar el patrón temporal de aparición de la enfermedad entre un grupo o epidemia de casos.

El análisis de la curva epidémica puede revelar tendencias temporales, valores atípicos, la magnitud del brote, el periodo de exposición más probable, los intervalos de tiempo entre las generaciones de casos, e incluso puede ayudar a identificar el modo de transmisión de una enfermedad no identificada (por ejemplo, fuente puntual, fuente común continua, propagación de persona a persona). En el sitio web de los [CDC de EE.UU.](https://www.cdc.gov/training/quicklearns/epimode/index.html) se puede encontrar una lección en línea sobre la interpretación de las curvas epidémicas.

En esta página mostramos dos enfoques para producir curva epidémicas en R:

* El paquete **incidence2**, que puede producir una curva epi con simples comandos
* El paquete **ggplot2**, que permite una personalización avanzada mediante comandos más complejos

También se abordan casos de uso específicos como:

* Grágicos de datos de recuento agregados
* Facetado o producción de múltiplos gráficos pequeños
* Aplicación de medias móviles
* Mostrar qué datos son "provisionales" o están sujetos a retrasos en la presentación de informes
* Superposición de la incidencia de casos acumulados mediante un segundo eje

<!-- ======================================================= -->
## Preparación {#preparation-24}


### Paquetes {.unnumbered}  

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puede cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.
```{r message=F, warning=F}
pacman::p_load(
  rio,          # file import/export
  here,         # relative filepaths 
  lubridate,    # working with dates/epiweeks
  aweek,        # alternative package for working with dates/epiweeks
  incidence2,   # epicurves of linelist data
  i2extras,     # supplement to incidence2
  stringr,      # search and manipulate character strings
  forcats,      # working with factors
  RColorBrewer, # Color palettes from colorbrewer2.org
  tidyverse     # data management + ggplot2 graphics
) 
```


### Importar datos {.unnumbered}

En esta sección se utilizan dos conjuntos de datos de ejemplo:

* `Linelist` con casos individuales de una epidemia simulada
* Recuentos agregados por hospital de la misma epidemia simulada

Los datos se importan mediante la función `import()` del paquete **rio**. Consulta la página sobre [importación y exportación](#import-and-export) para conocer las distintas formas de importar datos.


```{r, echo=F, message=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the counts data into R
count_data <- linelist %>% 
  group_by(hospital, date_hospitalisation) %>% 
  summarize(n_cases = dplyr::n()) %>% 
  filter(date_hospitalisation > as.Date("2013-06-01")) %>% 
  ungroup()
```


**`Linelist` con casos**

Importamos los datos de casos de una epidemia de ébola simulada. Si deseas descargar los datos para seguirlos paso a paso, consulta las instrucciones en la página [Descargando el manual y los datos](#download-handbook-and-data). Asumimos que el archivo está en el directorio de trabajo, por lo que no se especifican subcarpetas en esta ruta de archivo.  

```{r, eval=F}
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



**Recuentos de casos agregados por hospital**

A efectos del manual, los datos de recuentos semanales agregados por hospital se crean a partir de  `linelist` con el siguiente código.

```{r, eval=F}
# import the counts data into R
count_data <- linelist %>% 
  group_by(hospital, date_hospitalisation) %>% 
  summarize(n_cases = dplyr::n()) %>% 
  filter(date_hospitalisation > as.Date("2013-06-01")) %>% 
  ungroup()
```

A continuación se muestran las primeras 50 filas:

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(count_data, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




### Establecer parámetros {.unnumbered}

Para la producción de un informe, es posible que desees establecer parámetros editables como la fecha para la que los datos sean actuales (la "data_date"). A continuación, puedes hacer referencia al objeto `data_date` en tu código cuando apliques filtros o en subtítulos dinámicos.

```{r set_parameters}
## set the report date for the report
## note: can be set to Sys.Date() for the current date
data_date <- as.Date("2015-05-15")
```



### Verificar las fechas {.unnumbered}

Verifica que cada columna de fecha relevante es de tipo Date y tiene un rango de valores apropiado. Puedes hacerlo simplemente utilizando `hist()` para histogramas, o `range()` con `na.rm=TRUE`, o con `ggplot()` como se indica a continuación.

```{r, out.width = c('50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# check range of onset dates
ggplot(data = linelist)+
  geom_histogram(aes(x = date_onset))
```



<!-- ======================================================= -->
## Epicurves con el paquete **incidence2** {#epicurves-with-incidence2-package}

A continuación mostramos cómo hacer curvas epidémicas utilizando el paquete **incidence2**. Los autores de este paquete han intentado que el usuario pueda crear y modificar curvas epidémicas sin necesidad de conocer la sintaxis de **ggplot2**. Gran parte de esta página está adaptada de las viñetas del paquete, que se pueden encontrar en la [página de github de **incidence2**](https://github.com/reconhub/incidence2).


<!-- ======================================================= -->
### Ejemplo sencillo {.unnumbered}

**Se requieren 2 pasos para trazar una curva epidémica con el paquete *incidence2*:**

1)  **Crear** un *objeto de incidencia* (utilizando la función `incidence()`)
     + Proporcionar los datos
     + Especificar la columna de fecha a `date_index = `
     + Especificar el `interval = ` en el que deben agregarse los casos (diario, semanal, mensual..)
     + Especificar cualquier columna de agrupación (por ejemplo, sexo, hospital, resultado)
2.  **Representarr** el objeto de incidencia
     + Especificar etiquetas, colores, títulos, etc.

A continuación, cargamos el paquete **incidence2**, creamos el objeto de incidencia a partir de `linelist` en la columna `date_onset` y agregamos los casos por día. A continuación, imprimimos un resumen del objeto de incidencia.

```{r, warning=F, message=F}
# load incidence2 package
pacman::p_load(incidence2)

# create the incidence object, aggregating cases by day
epi_day <- incidence(       # create incidence object
  x = linelist,             # dataset
  date_index = date_onset,  # date column
  interval = "day"          # date grouping interval
  )
```

El objeto **epi_day2** tiene el aspecto de un tibble (como una trama de datos) y puede imprimirse o manipularse como un dataframe. 

```{r}
class(epi_day)
```

Este es el aspecto que tiene cuando se imprime. Tiene una columna `date_index` y una columna `count`.

```{r}
epi_day
```

También puedes imprimir un resumen del objeto:

```{r}
# print summary of the incidence object
summary(epi_day)
```

Para *trazar* el objeto de incidencia, utiliza `plot`() en el *nombre del objeto de incidencia*. En segundo plano, se llama a la función `plot.incidence2()`, por lo que para leer la documentación específica de incidence2** se ejecutaría `?plot.incidence2.`

```{r}
# plot the incidence object
plot(epi_day)
```

Si notas muchas líneas blancas verticales diminutas, intenta ajustar el tamaño de la imagen. Por ejemplo, si exportas el gráfico con `ggsave()`, puede proporcionar números a `width = ` y `height = `. Si amplías el gráfico esas líneas pueden desaparecer.   



### Cambiar el intervalo de tiempo de la agregación de casos {.unnumbered}  
El argumento `interval = ` de `incidence()` define cómo se agrupan las observaciones en barras verticales.

**Especificar el intervalo**

**incidence2** proporciona flexibilidad y una sintaxis comprensible para especificar cómo quieres agregar tus casos en una curva epidémica de barras Proporcione un valor como los siguientes al argumento `interval = `. Puedes escribir cualquiera de los siguientes en plural (por ejemplo, "week**s**"), y puedes añadir números antes (por ejemplo, "3 months"). 

Opción de argumento | Más explicaciones 
------------------- | ------------------------------------ |
Número (1, 7, 13, 14, etc.) | Número de días por intervalo  
"week" | nota: el lunes es el día de inicio predeterminad
"2 weeks" | o 3, 4, 5...
"Sunday week" | semanas que comienzan en domingo (también se puede utilizar el Thursday, etc.) 
"2 Sunday weeks" | o 3, 4, 5...
"MMWRweek" | la semana comienza en domingo - ver US CDC
"month" | El 1 de mes 
"quarter" | 1er mes del trimestre  
"2 months" | o 3, 4, 5...
"year" | Primer día del año natural 


A continuación se muestran ejemplos de cómo se ven los diferentes intervalos cuando se aplican a linelist. Observa cómo el formato por defecto y la frecuencia de las *etiquetas de* fecha en el eje-x cambian a medida que cambia el intervalo de fecha.

```{r incidence, out.width=c('50%', '50%', '50%', '50%'), fig.show='hold', warning=F, message=F}
# Create the incidence objects (with different intervals)
##############################
# Weekly (Monday week by default)
epi_wk      <- incidence(linelist, date_onset, interval = "Monday week")

# Sunday week
epi_Sun_wk  <- incidence(linelist, date_onset, interval = "Sunday week")

# Three weeks (Monday weeks by default)
epi_2wk     <- incidence(linelist, date_onset, interval = "2 weeks")

# Monthly
epi_month   <- incidence(linelist, date_onset, interval = "month")

# Quarterly
epi_quarter   <- incidence(linelist, date_onset, interval = "quarter")

# Years
epi_year   <- incidence(linelist, date_onset, interval = "year")


# Plot the incidence objects (+ titles for clarity)
############################
plot(epi_wk)+      labs(title = "Monday weeks")
plot(epi_Sun_wk)+  labs(title = "Sunday weeks")
plot(epi_2wk)+     labs(title = "2 (Monday) weeks")
plot(epi_month)+   labs(title = "Months")
plot(epi_quarter)+ labs(title = "Quarters")
plot(epi_year)+    labs(title = "Years")

```


<!-- **Begin at first case**   -->

<!-- If you want the intervals to begin at the first case, you can add the argument `standard = TRUE` to the `incidence()` command. This only works if the interval is either "week", "month", "quarter" or "year".   -->

**Primera fecha**

Puedes especificar opcionalmente un valor de tipo Date (por ejemplo, `as.Date("2016-05-01")`) a `firstdate = ` en el comando `incidence()`. Si se da, los datos se recortarán a este rango y los intervalos comenzarán en esta fecha.



### Grupos {.unnumbered}

Los grupos se especifican en el comando `incidence()`, y pueden utilizarse para colorear las barras o para facetar los datos. Para especificar los grupos en tus datos, escribe el nombre de la(s) columna(s) en el argumento `groups = ` del comando `incidence()` (sin comillas alrededor del nombre de la columna). Si especificas varias columnas, pon sus nombres dentro de `c()`.

Puedes especificar que los casos con valores faltantes en las columnas de agrupación sean listados como un grupo `NA` distinto estableciendo `na_as_group = TRUE`. De lo contrario, se excluirán del gráfico.

* Para *colorear las barras por una columna de agrupación*, debes proporcionar de nuevo el nombre de la columna en `fill = ` del comando `plot()`.

* Para *crear una faceta basada en una columna de agrupación*, consulta la sección siguiente sobre facetas con **incidence2**.

En el ejemplo siguiente, los casos de todo el brote se agrupan por su categoría de edad. Los valores faltantes se incluyen como grupo. El intervalo de la curva epidémica es de semanas.


```{r, message=F, warning=F}
# Create incidence object, with data grouped by age category
age_outbreak <- incidence(
  linelist,                # dataset
  date_index = date_onset, # date column
  interval = "week",       # Monday weekly aggregation of cases
  groups = age_cat,        # age_cat is set as a group
  na_as_group = TRUE)      # missing values assigned their own group

# plot the grouped incidence object
plot(
  age_outbreak,             # incidence object with age_cat as group
  fill = age_cat)+          # age_cat is used for bar fill color (must have been set as a groups column above)
labs(fill = "Age Category") # change legend title from default "age_cat" (this is a ggplot2 modification)
```
<span style="color: darkgreen;">***CONSEJO:*** Cambia el título de la leyenda añadiendo `+` el comando de **ggplot2** `labs(fill = "su título")`</span>  

También puedes hacer que las barras agrupadas se muestren una al lado de la otra estableciendo `stack = FALSE` en `plot()`, como se muestra a continuación:

```{r, warning=F, message=F}
# Make incidence object of monthly counts. 
monthly_gender <- incidence(
 linelist,
 date_index = date_onset,
 interval = "month",
 groups = gender            # set gender as grouping column
)

plot(
  monthly_gender,   # incidence object
  fill = gender,    # display bars colored by gender
  stack = FALSE)    # side-by-side (not stacked)
``` 

Puedes establecer el argumento `na_as_group = ` a FALSE en el comando `incidence()` para eliminar las filas con valores faltantes del gráfico. 




### Datos filtrados {.unnumbered}

Para trazar la curva epidémica de un subconjunto de datos:

1)  Filtrar los datos del listado
2)  Proporcionar los datos filtrados al comando `incidence()`
3)  Trazar el objeto de incidencia

El ejemplo siguiente utiliza datos filtrados para mostrar sólo los casos del Central Hospital.

```{r, warning=F, message=F}
# filter the linelist
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# create incidence object using filtered data
central_outbreak <- incidence(central_data, date_index = date_onset, interval = "week")

# plot the incidence object
plot(central_outbreak, title = "Weekly case incidence at Central Hospital")
```




### Recuentos agregados {.unnumbered}

Si los datos originales son agregados (recuentos), cuando crees el objeto de incidencia con `incidence()` proporciona el nombre de la columna que contiene los recuentos de casos al argumento `count = `.

Por ejemplo, este dataframe `count_data` son casos de `linelist` agregados en recuentos diarios por hospital. Las primeras 50 filas tienen este aspecto: 

```{r message=FALSE, echo=F}
DT::datatable(head(count_data,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Si comienzas tu análisis con datos de recuento diario como el conjunto de datos anterior, tu comando `incidence()` para convertirlo en una curva epidémica semanal por hospital tendría el siguiente aspecto:

```{r}
epi_counts <- incidence(              # create weekly incidence object
  count_data,                         # dataset with counts aggregated by day
  date_index = date_hospitalisation,  # column with dates
  count = n_cases,                    # column with counts
  interval = "week",                  # aggregate daily counts up to weeks
  groups = hospital                   # group by hospital
  )

# plot the weekly incidence epi curve, with stacked bars by hospital
plot(epi_counts,                      # incidence object
     fill = hospital)                 # color the bars by hospital
```




### Facetas/pequeños múltiplos {.unnumbered}  

Facetar los datos por grupos (es decir, producir "pequeños múltiples"):

1.  Especificar la columna a facetar en `groups = ` 

2.  Utilizar el comando facet_plot() en lugar de `plot()`

3.  Especificar qué columnas de agrupación utilizar como `fill = ` y cuáles utilizar como `facets = `

A continuación, establecemos las columnas `hospital` y  `outcome` como columnas de agrupación en el comando `incidence()`. A continuación, en `facet_plot()` trazamos la curva epidémica, especificando que queremos una curva epidémica diferente para cada hospital y que dentro de cada curva epidémica las barras deben estar apiladas y coloreadas por `outcome`.
 

```{r, warning=F, message=F}
epi_wks_hosp_out <- incidence(
  linelist,                      # dataset
  date_index = date_onset,       # date column
  interval = "month",            # monthly bars  
  groups = c(outcome, hospital)  # both outcome and hospital are given as grouping columns
  )

# plot
incidence2::facet_plot(
  epi_wks_hosp_out,      # incidence object
  facets = hospital,     # facet column
  fill = outcome)        # fill column

```

Ten en cuenta que el paquete **ggtree** (utilizado para mostrar árboles filogenéticos) también tiene una función `facet_plot()` - por eso especificamos incidence2::facet_plot() arriba.



### Modificaciones con `plot()` {.unnumbered} 

Una curva epidémica producida por **incidence2** puede ser modificada a través de estos argumentos *dentro de la función `plot()`*.

**Aquí están los argumentos de `plot()` que modifican la apariencia de las barras:**

Argumento | Descripción | Ejemplos
------------------|---------------------------------------|-------------------
`fill = `|Color de la barra. Nombre de color o Nombre de la columna previamente especificada en `groups = ` en el comando `incidence()` |`fill = "red"`, o `fill = gender`  
`color = ` |Colorea alrededor de cada barra, o de cada agrupación dentro de una barra|`border = "white"` 
`legend = `|Ubicación de la leyenda|Puede ser "bottom", "top", "left", "right", o "none"  
`alpha = `|Transparencia de las barras/cajas|1 es totalmente opaco, 0 es totalmente transparente
`width = `|Valor entre 0 y 1 que indica el tamaño de las barras relativo a su intervalo de tiempo|`width = .7`  
`show_cases = `|Lógico; si es TRUE, cada caso se muestra como una caja. Muestra mejor en brotes pequeños.|`show_cases = TRUE`  

**Aquí están los argumentos de `plot()` que modifican el eje de la fecha:**

Argumento(s)|Descripción
----------------------|----------------------------------------------------
`centre_dates = `|TRUE/FALSE en cuanto a si la fecha aparece bajo el centro de las barras, o al principio de las mismas     
`date_format = `| Ajusta el formato de visualización de la fecha con la sintaxis strptime ("%"). Sólo funciona si `centre_dates = FALSE` (detalles más abajo). 
`n.breaks = `|Número aproximado de interrupciones de la etiqueta del eje-x  
`angle = `|Ángulo de las etiquetas de fecha del eje-x (número de grados)  
`size = `|Tamaño del texto en puntos 

Ten en cuenta que el argumento `date_breaks = ` sólo funciona si `centre_dates = FALSE`. Proporciona un valor de carácter entre comillas utilizando la sintaxis strptime que se indica a continuación, como se detalla en la página [Trabajar con fechas](#working-with-dates-1). Puedes utilizar `\n` para una "nueva línea".

%d = Número de día del mes (5, 17, 28, etc.)

%j = Número de día del año (día juliano 001-366)

%a = Día de la semana abreviado (Mon, Tue, Wed, etc.)

%A = Día de la semana completo (Monday, Tuesday, etc.))

%w = Número del día de la semana (0-6, el domingo es 0)

%u = Número del día de la semana (1-7, el lunes es 1)

%W = Número de la semana (00-53, el lunes es el comienzo de la semana)

%U = Número de la semana (01-53, el domingo es el comienzo de la semana)

%m = Número del mes (ej. 01, 02, 03, 04)

%b = Mes abreviado (Jan, Feb, etc. )

%B = Mes completo (January, February, etc.)

%y = Año de 2 dígitos (por ejemplo, 89)

%Y = Año de 4 dígitos (por ejemplo, 1989)

%h = horas (reloj de 24 horas)

%m = minutos

%s = segundos

%z = desviación de GMT

%Z = Zona horaria (carácter)


<!-- <span style="color: darkgreen;">**_TIP:_** For breaks every "nth" interval (e.g. every 4th), use `n.breaks = nrow(i)/n` (where “i” is your incidence object name and “n” is a number). If your data are grouped, you will need to multiply "n" by the number of unique groups.</span>   -->



**Estos son los argumentos de `plot()` que modifican las etiquetas de los gráficos:**

Argumento(s)|Descripción
----------------------|----------------------------------------------------
`title = `|Título del gráfico
`xlab = `|Título del eje-x
`ylab = `|Título del eje-y 
`size = `|Tamaño del texto del eje-x en pts (utiliza ggplot's theme() para ajustar otros tamaños)  


Un ejemplo que utiliza muchos de los argumentos anteriores:

```{r, warning=F, message=F}
# filter the linelist
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# create incidence object using filtered data
central_outbreak <- incidence(
  central_data,
  date_index = date_onset,
  interval = "week",
  groups = outcome)

# plot incidence object
plot(
  central_outbreak,
  fill = outcome,                       # box/bar color
  legend = "top",                       # legend on top
  title = "Cases at Central Hospital",  # title
  xlab = "Week of onset",               # x-axis label
  ylab = "Week of onset",               # y-axis label
  show_cases = TRUE,                    # show each case as an individual box
  alpha = 0.7,                          # transparency 
  border = "grey",                      # box border
  angle = 30,                           # angle of date labels
  centre_dates = FALSE,                 # date labels at edge of bar
  date_format = "%a %d %b %Y\n(Week %W)" # adjust how dates are displayed
  )
```

Para ajustar aún más la apariencia del gráfico, consulta la sección siguiente sobre modificaciones con `ggplot()`.






### Modificaciones con ggplot2 {.unnumbered}

Puedes modificar aún más un gráfico de **incidence2** añadiendo modificaciones de **ggplot2** con un `+` después del cierre de la función `plot()` de **incidence**, como se demuestra a continuación.

A continuación, el gráfico de **incidence2** termina y luego se utilizan los comandos de **ggplot2** para modificar los ejes, añadir una leyenda y ajustar la fuente en negrita y el tamaño del texto.

Ten en cuenta que si añades `scale_x_date()`, la mayor parte del formato de fecha de `plot()` se sobrescribirá. Consulta la sección de curvas epidémicas de `ggplot()` y la página del Manual [Consejos de ggplot](#ggplot-tips) para más opciones.

```{r, warning=F, message=F}
# filter the linelist
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")

# create incidence object using filtered data
central_outbreak <- incidence(
  central_data,
  date_index = date_onset,
  interval = "week",
  groups = c(outcome))

# plot incidence object
plot(
  central_outbreak,
  fill = outcome,                       # box/bar color
  legend = "top",                       # legend on top
  title = "Cases at Central Hospital",  # title
  xlab = "Week of onset",               # x-axis label
  ylab = "Week of onset",               # y-axis label
  show_cases = TRUE,                    # show each case as an individual box
  alpha = 0.7,                          # transparency 
  border = "grey",                      # box border
  centre_dates = FALSE,                   
  date_format = "%a %d %b\n%Y (Week %W)", 
  angle = 30                           # angle of date labels
  )+
  
  scale_y_continuous(
    breaks = seq(from = 0, to = 30, by = 5),  # specify y-axis increments by 5
    expand = c(0,0))+                         # remove excess space below 0 on y-axis
  
  # add dynamic caption
  labs(
    fill = "Patient outcome",                               # Legend title
    caption = stringr::str_glue(                            # dynamic caption - see page on characters and strings for details
      "n = {central_cases} from Central Hospital
      Case onsets range from {earliest_date} to {latest_date}. {missing_onset} cases are missing date of onset and not shown",
      central_cases = nrow(central_data),
      earliest_date = format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y'),
      latest_date = format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y'),      
      missing_onset = nrow(central_data %>% filter(is.na(date_onset)))))+
  
  # adjust bold face, and caption position
  theme(
    axis.title = element_text(size = 12, face = "bold"),    # axis titles larger and bold
    axis.text = element_text(size = 10, face = "bold"),     # axis text size and bold
    plot.caption = element_text(hjust = 0, face = "italic") # move caption to left
  )
  
```




### Cambiar los colores  {.unnumbered}  

#### Especificar una paleta {.unnumbered}  

Proporciona el nombre de una paleta predefinida al argumento `col_pal = ` en `plot()`. El paquete **incidence2** viene con 2 paletas predefinidas: "vibrant" y "muted". En "vibrant" los primeros 6 colores son distintos y en "muted" los primeros 9 colores son distintos. Después de estos números, los colores son interpolaciones/intermediarios de otros colores. Estas paletas predefinidas se pueden encontrar en [este sitio web](https://personal.sron.nl/~pault/#sec:qualitative). Las paletas excluyen el gris, que está reservado para los datos que faltan (utiliza `na_color = ` para cambiar este valor por defecto).

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
# Create incidence object, with data grouped by age category  
age_outbreak <- incidence(
  linelist,
  date_index = date_onset,   # date of onset for x-axis
  interval = "week",         # weekly aggregation of cases
  groups = age_cat)

# plot the epicurve with default palette
plot(age_outbreak, fill = age_cat, title = "'vibrant' default incidence2 palette")

# plot with different color palette
#plot(age_outbreak, fill = age_cat, col_pal = muted, title = "'muted' incidence2 palette")
```

También puedes utilizar una de las paletas de R **base** (pon el nombre de la paleta *sin* comillas). 

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
# plot with base R palette
plot(age_outbreak, fill = age_cat, col_pal = heat.colors, title = "base R heat.colors palette")

# plot with base R palette
plot(age_outbreak, fill = age_cat, col_pal = rainbow, title = "base R rainbow palette")
```

También puedes añadir una paleta de colores del paquete **viridis** o del paquete **RColorBrewer**. Primero hay que cargar esos paquetes, y luego añadir sus respectivas funciones `scale_fill_*()` con un `+`, como se muestra a continuación.

```{r out.width = c('50%', '50%'), fig.show='hold', warning = F, message = F}
pacman::p_load(RColorBrewer, viridis)

# plot with color palette
plot(age_outbreak, fill = age_cat, title = "Viridis palette")+
  scale_fill_viridis_d(
    option = "inferno",     # color scheme, try also "plasma" or the default
    name = "Age Category",  # legend name
    na.value = "grey")      # for missing values

# plot with color palette
plot(age_outbreak, fill = age_cat, title = "RColorBrewer palette")+
  scale_fill_brewer(
    palette = "Dark2",      # color palette, try also Accent, Dark2, Paired, Pastel1, Pastel2, Set1, Set2, Set3
    name = "Age Category",  # legend name
    na.value = "grey")      # for missing values
```


#### Especificar manualmente {.unnumbered}  

Para especificar los colores manualmente, añade la función `scale_fill_manual()` de **ggplot2** a la función `plot()` con un `+` y proporciona el vector de nombres de colores o códigos HEX al argumento `values = `. El número de colores listados debe ser igual al número de grupos. Ten en cuenta que si los valores faltantes son un grupo - pueden ser convertidos a un valor de carácter como "Missing" durante la preparación de los datos con la función `fct_explicit_na()` como se explica en la página sobre [Factores](#factors).

```{r out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}
# manual colors
plot(age_outbreak, fill = age_cat, title = "Manually-specified colors")+
  scale_fill_manual(
    values = c("darkgreen", "darkblue", "purple", "grey", "yellow", "orange", "red", "lightblue"),  # colors
    name = "Age Category")      # Name for legend
```

Como se menciona en la página [Consejos de ggplot](#ggplot-tips), puedes crear tus propias paletas utilizando `colorRampPalette()` sobre un vector de colores y especificando el número de colores que deseas. Esta es una buena manera de obtener muchos colores en una rampa especificando unos pocos.  

```{r}
my_cols <- c("darkgreen", "darkblue", "purple", "grey", "yellow", "orange")
my_palette <- colorRampPalette(my_cols)(12)  # expand the 6 colors above to 12 colors
my_palette
```
          
          
### Ajustar el orden de los niveles {.unnumbered}  

Para ajustar el orden de aparición de los grupos (en el gráfico y en la leyenda), la columna de agrupación debe ser de tipo Factor. Consulta la página sobre [Factores](#factors) para obtener más información.

En primer lugar, veamos una curva epidémica semanal por hospital con la ordenación por defecto: 

```{r, message=F, warning=F}
# ORIGINAL - hospital NOT as factor
###################################

# create weekly incidence object, rows grouped by hospital and week
hospital_outbreak <- incidence(
  linelist,
  date_index = date_onset, 
  interval = "week", 
  groups = hospital)

# plot incidence object
plot(hospital_outbreak, fill = hospital, title = "ORIGINAL - hospital not a factor")
```

Ahora, para ajustar el orden de manera que los "Missing" y "Otros" estén en la parte superior de la curva epidémica podemos hacer lo siguiente:

* Cargar el paquete **forcats**, para trabajar con factores
* Ajustar los datos - en este caso vamos a definir un nuevo dataset (`plot_data`) en el que:
  * la columna `gender` se define como un factor el orden de los niveles se establecen con `fct_relevel()` de manera que "Other" y "Missing" son los primeros, por lo que aparecen en la parte superior de las barras
* El objeto de incidencia se crea y se traza como antes
* Añadimos modificaciones de **ggplot2 **
  * `scale_fill_manual()` para asignar manualmente los colores para que "Missing" sea gris y "Other" sea beige
 



```{r, message=F, warning=F}
# MODIFIED - hospital as factor
###############################

# load forcats package for working with factors
pacman::p_load(forcats)

# Convert hospital column to factor and adjust levels
plot_data <- linelist %>% 
  mutate(hospital = fct_relevel(hospital, c("Missing", "Other"))) # Set "Missing" and "Other" as top levels


# Create weekly incidence object, grouped by hospital and week
hospital_outbreak_mod <- incidence(
  plot_data,
  date_index = date_onset, 
  interval = "week", 
  groups = hospital)

# plot incidence object
plot(hospital_outbreak_mod, fill = hospital)+
  
  # manual specify colors
  scale_fill_manual(values = c("grey", "beige", "darkgreen", "green2", "orange", "red", "pink"))+                      

  # labels added via ggplot
  labs(
      title = "MODIFIED - hospital as factor",   # plot title
      subtitle = "Other & Missing at top of epicurve",
      y = "Weekly case incidence",               # y axis title  
      x = "Week of symptom onset",               # x axis title
      fill = "Hospital")                         # title of legend     
```

<span style="color: darkgreen;">***CONSEJO:*** Si deseas invertir el orden de la leyenda solamente, añade este comando `guides(fill = guide_legend(reverse = TRUE))`de **ggplot2**.</span>  



### Líneas de cuadrícula verticales {.unnumbered}  

Si utilizas la configuración predeterminada de **incidence2**, puedes observar que las líneas de cuadrícula verticales aparecen en cada etiqueta de fecha y una vez entre cada etiqueta de semana. Esto puede dar lugar a que las líneas de cuadrícula se crucen con la parte superior de algunas barras.

<!-- [TO DO Note this paragraph is not applicable with version 1.0.0 of incidence2). You can specify the interval for the gridlines by adding **ggplot2**'s `scale_x_date()` command to your **incidence2** plot. Within it, specify the intervals for `date_breaks = ` and `date_minor_breaks = ` (e.g. "weeks" or "3 weeks" or "months"). Note that use of `scale_x_date()` will over-ride any formatting of the date labels in `plot()`, so you will need to specify any string format to `date_labels = ` as below.   -->

Puedes eliminar todas las líneas de la cuadrícula añadiendo el comando `theme_classic()` de **ggplot2**.

```{r, warning=F, message=F, out.width = c('50%', '50%', '50%'), fig.show='hold'}
# make incidence object
a <- incidence(
  central_data,
  date_index = date_onset,
  interval = "Monday weeks"
)

# Default gridlines
plot(a, title = "Default lines")

# Specified gridline intervals
# NOT WORKING WITH INCIDENCE2 1.0.0
# plot(a, title = "Weekly lines")+
#   scale_x_date(
#     date_breaks = "4 weeks",      # major vertical lines align on weeks
#     date_minor_breaks = "weeks",  # minor vertical lines every week
#     date_labels = "%a\n%d\n%b")   # format of date labels

# No gridlines
plot(a, title = "No lines")+
  theme_classic()                 # remove all gridlines
```

Ten en cuenta, sin embargo, que si utiliza semanas, los argumentos `date_breaks` y `date_minor_breaks` sólo funcionan para las semanas del *lunes*. Si tus semanas emepiezan por otro día de la semana tendrás que proporcionar manualmente un vector de fechas a los argumentos `breaks = ` y `minor_breaks = `. Consulta la sección de **ggplot2** para ver ejemplos de esto utilizando `seq.Date()`.

### Incidencia acumulada {.unnumbered}  

En versiones anteriores de **incidence2** se podía usar la función `cumulate()`. Esto se ha eliminado en la versión más reciente del paquete. 

<!-- You can easily produce a plot of cumulative incidence by passing the incidence object to the **incidence2** command `cumulate()` and then to `plot()`. This also works with `facet_plot()`. -->

<!-- The code below is provided  -->
<!-- ```{r, eval=F} -->
<!-- # make weekly incidence object -->
<!-- wkly_inci <- incidence( -->
<!--   linelist, -->
<!--   date_index = date_onset, -->
<!--   interval = "week" -->
<!-- ) -->

<!-- # plot cumulative incidence -->
<!-- wkly_inci %>% -->
<!--   cumulate() %>% -->
<!--   plot() -->
<!-- ``` -->


Ver la sección más abajo en esta página para el método alternativo para trazar la incidencia acumulativa con **ggplot2** - por ejemplo para superponer una línea de incidencia acumulativa sobre una curva epidémica. 

### Media móvil  {.unnumbered}

Puedes añadir una media móvil a un gráfico de **incidence2** fácilmente con `add_rolling_average()` del paquete **i2extras**. Pasa tu objeto incidence2 a esta función, y luego a `plot()`. Establece en `before =`  el número de días anteriores que deseas incluir en la media móvil (por defecto es 2). Si tus datos están agrupados, la media móvil se calculará por grupo.

```{r, warning=F, message=F}
rolling_avg <- incidence(                    # make incidence object
  linelist,
  date_index = date_onset,
  interval = "week",
  groups = gender) %>% 
  
  i2extras::add_rolling_average(before = 6)  # add rolling averages (in this case, by gender)

# plot
plot(rolling_avg) # faceted automatically because rolling average on groups
```

Para aprender a aplicar las medias móviles de forma más general sobre los datos, consulta la página del Manual sobre [medias móviles](#moving-averages-1).


<!-- ======================================================= -->
## Curvas epidémicas con ggplot2 {#epicurves-with-ggplot2}

El uso de `ggplot()` para construir tu curva epidémica permite más flexibilidad y personalización, pero requiere más esfuerzo y comprensión de cómo funciona `ggplot()`.

A diferencia de lo que ocurre con el paquete **incidence2**, hay que controlar *manualmente* la agregación de los casos por tiempo (en semanas, meses, etc.) *y* los intervalos de las etiquetas en el eje de fechas. Esto debe gestionarse cuidadosamente.

Estos ejemplos utilizan un subconjunto de los datos de `linelist`: sólo los casos del Hospital Central.


```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r}
central_data <- linelist %>% 
  filter(hospital == "Central Hospital")
```

```{r, eval=F, echo=F}
detach("package:tidyverse", unload=TRUE)
library(tidyverse)
```

Para producir una curva epidémica con `ggplot()` hay tres elementos principales:  

* Un histograma, con los casos de la lista de líneas agregados en "bins" distinguidos por puntos específicos de "ruptura".  
* Escalas para los ejes y sus etiquetas  
* Temas para la apariencia del gráfico, incluyendo títulos, etiquetas, subtítulos, etc.


### Especificaciones de las barras {.unnumbered}  

Aquí mostramos cómo especificar cómo se agregarán los casos en los intervalos del histograma ("barras"). Es importante reconocer que la agregación de los casos en los intervalos del histograma **no son** necesariamente los mismos intervalos que las fechas que aparecerán en el eje-x.

A continuación se muestra el código más sencillo para producir curvas epidémicas diarias y semanales.

En el comando general `ggplot()` se proporciona el conjunto de datos en `data = `. Sobre esta base, se añade la geometría de un histograma con un `+`. Dentro de `geom_histogram()`, mapeamos la estética de tal manera que la columna `date_onset` se mapea al eje-x. También dentro de `geom_histogram()` pero *no* dentro de `aes()` establecemos la anchura de las barras del histograma con  `binwidth = `, en días. Si esta sintaxis de **ggplot2** es confusa, revisa la página sobre [Conceptos básicos de ggplot](#ggplot-basics).

<span style="color: orange;">***PRECAUCIÓN:*** El trazado de casos semanales mediante el uso de `binwidth = 7` inicia la primera barra  de 7 días en el primer caso, ¡que podría ser cualquier día de la semana! Para crear semanas específicas, véase la sección siguiente.</span>


``` {r ggplot_simple,  out.width = c('50%', '50%'), fig.show='hold', warning= F, message = F}
# daily 
ggplot(data = central_data) +          # set data
  geom_histogram(                      # add histogram
    mapping = aes(x = date_onset),     # map date column to x-axis
    binwidth = 1)+                     # cases binned by 1 day 
  labs(title = "Central Hospital - Daily")                # title

# weekly
ggplot(data = central_data) +          # set data 
  geom_histogram(                      # add histogram
      mapping = aes(x = date_onset),   # map date column to x-axis
      binwidth = 7)+                   # cases binned every 7 days, starting from first case (!) 
  labs(title = "Central Hospital - 7-day bins, starting at first case") # title
```

Observamos que el primer caso de este conjunto de datos del Hospital Central tuvo un inicio de síntomas el:

```{r}
format(min(central_data$date_onset, na.rm=T), "%A %d %b, %Y")
```

**Para especificar manualmente las pausas del histograma, *no* utilices el argumento `binwidth = `, y en su lugar suministra un vector de fechas a `breaks = `.**

Crea el vector de fechas con la función `seq.Date()` de R **base**. Esta función espera argumentos `to = `, `from = `, y `by = `. Por ejemplo, el comando siguiente devuelve fechas mensuales que comienzan en el 15 de enero y terminan en el 28 de junio.

```{r}
monthly_breaks <- seq.Date(from = as.Date("2014-02-01"),
                           to = as.Date("2015-07-15"),
                           by = "months")

monthly_breaks   # print
```

Este vector puede proporcionarse a `geom_histogram()` como  `breaks = `:

```{r, warning=F, message=F}
# monthly 
ggplot(data = central_data) +  
  geom_histogram(
    mapping = aes(x = date_onset),
    breaks = monthly_breaks)+         # provide the pre-defined vector of breaks                    
  labs(title = "Monthly case bins")   # title
```

Una secuencia simple de fechas semanales puede ser devuelta estableciendo  `by = "week"`. Por ejemplo:

```{r}
weekly_breaks <- seq.Date(from = as.Date("2014-02-01"),
                          to = as.Date("2015-07-15"),
                          by = "week")
```

 
Una alternativa a la provisión de fechas específicas de inicio y fin es escribir un código *dinámico* para que los intervalos semanales comiencen *el lunes anterior al primer caso*. **Utilizaremos estos vectores de fechas a lo largo de los ejemplos siguientes.** 
     
```{r}
# Sequence of weekly Monday dates for CENTRAL HOSPITAL
weekly_breaks_central <- seq.Date(
  from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 1), # monday before first case
  to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 1), # monday after last case
  by   = "week")
```  

Descompongamos el código anterior, que es bastante desalentador:

* El valor "from" (fecha más temprana) se crea de la siguiente manera: el valor mínimo de fecha (`min()` con `na.rm=TRUE`) en la columna `date_onset` se introduce en `floor_date()` del paquete **lubridate.** `floor_date()` ajustado a "week" devuelve la fecha de inicio de la "semana" de esos casos, dado que el día de inicio de cada semana es un lunes (`week_start = 1`).
* Asimismo, el valor "to" (fecha final) se crea utilizando la función inversa `ceiling_date()` para devolver el lunes *posterior* al último caso.
* El argumento "by" de `seq.Date()` puede establecerse en cualquier número de días, semanas o meses.
* Utiliza `week_start = 7` para las semanas de domingo

Como vamos a utilizar estos vectores de fechas a lo largo de esta página, también definimos uno para todo el brote (el anterior es sólo para el Hospital Central).

```{r}
# Sequence for the entire outbreak
weekly_breaks_all <- seq.Date(
  from = floor_date(min(linelist$date_onset, na.rm=T),   "week", week_start = 1), # monday before first case
  to   = ceiling_date(max(linelist$date_onset, na.rm=T), "week", week_start = 1), # monday after last case
  by   = "week")
```

Estas salidas de `seq.Date()` pueden utilizarse para crear los saltos de las casillas del histograma, pero también los saltos de las etiquetas de fecha, que pueden ser independientes de las casillas. Lea más sobre las etiquetas de fecha en secciones posteriores.

<span style="color: darkgreen;">***CONSEJO:*** Para un comando `ggplot()` más sencillo, guarda los saltos de cubo y los saltos de etiqueta de fecha como vectores con nombre por adelantado, y simplemente proporciona sus nombres a `breaks = `..</span>  







### Ejemplo de curva epidémica semanal {.unnumbered}  

**A continuación se muestra un código de ejemplo detallado para producir curvas epidémicas semanales para las semanas del lunes, con barras alineadas, etiquetas de fecha y líneas de cuadrícula verticales.** Esta sección es para el usuario que necesita el código rápidamente. Para entender cada aspecto (temas, etiquetas de fecha, etc.) en profundidad, continúa con las secciones siguientes. Es importante tener en cuenta:

* Las *pausas del histograma* se definen con `seq.Date()`, como se ha explicado anteriormente, para comenzar el lunes anterior al caso más antiguo y terminar el lunes posterior al último caso
* El intervalo de las *etiquetas de fecha* se especifica mediante `date_breaks = `  dentro de `scale_x_date()`
* El intervalo de líneas verticales menores entre etiquetas de fecha se especifica en `date_minor_breaks =`
* `expand = c(0,0)` en los ejes x e y elimina el exceso de espacio a cada lado de los ejes, lo que también asegura que las etiquetas de fecha comiencen desde la primera barra.

```{r, warning=F, message=F}
# TOTAL MONDAY WEEK ALIGNMENT
#############################
# Define sequence of weekly breaks
weekly_breaks_central <- seq.Date(
      from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 1), # Monday before first case
      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 1), # Monday after last case
      by   = "week")    # bins are 7-days 


ggplot(data = central_data) + 
  
  # make histogram: specify bin break points: starts the Monday before first case, end Monday after last case
  geom_histogram(
    
    # mapping aesthetics
    mapping = aes(x = date_onset),  # date column mapped to x-axis
    
    # histogram bin breaks
    breaks = weekly_breaks_central, # histogram bin breaks defined previously
    
    # bars
    color = "darkblue",     # color of lines around bars
    fill = "lightblue"      # color of fill within bars
  )+ 
    
  # x-axis labels
  scale_x_date(
    expand            = c(0,0),           # remove excess x-axis space before and after case bars
    date_breaks       = "4 weeks",        # date labels and major vertical gridlines appear every 3 Monday weeks
    date_minor_breaks = "week",           # minor vertical lines appear every Monday week
    date_labels       = "%a\n%d %b\n%Y")+ # date labels format
  
  # y-axis
  scale_y_continuous(
    expand = c(0,0))+             # remove excess y-axis space below 0 (align histogram flush with x-axis)
  
  # aesthetic themes
  theme_minimal()+                # simplify plot background
  
  theme(
    plot.caption = element_text(hjust = 0,        # caption on left side
                                face = "italic"), # caption in italics
    axis.title = element_text(face = "bold"))+    # axis titles in bold
  
  # labels including dynamic caption
  labs(
    title    = "Weekly incidence of cases (Monday weeks)",
    subtitle = "Note alignment of bars, vertical gridlines, and axis labels on Monday weeks",
    x        = "Week of symptom onset",
    y        = "Weekly incident cases reported",
    caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```


#### Semanas dominicales {.unnumbered}  

Para conseguir el gráfico anterior para las semanas desde los domingos son necesarias algunas modificaciones, ya que los `date_breaks = "weeks"` sólo funcionan para las semanas de los lunes.

* Los puntos de ruptura de las *franjas del histograma* deben fijarse en los domingos (`week_start = 7`)
* Dentro de `scale_x_date()`, los saltos de fecha similares deben proporcionarse a `breaks = ` y `minor_breaks = ` para asegurar que las etiquetas de fecha y las líneas verticales de la cuadrícula se alineen los domingos.

Por ejemplo, el comando `scale_x_date()` para las semanas del domingo podría tener este aspecto:

```{r, eval=F}
scale_x_date(
    expand = c(0,0),
    
    # specify interval of date labels and major vertical gridlines
    breaks = seq.Date(
      from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7), # Sunday before first case
      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7), # Sunday after last case
      by   = "4 weeks"),
    
    # specify interval of minor vertical gridline 
    minor_breaks = seq.Date(
      from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7), # Sunday before first case
      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7), # Sunday after last case
      by   = "week"),
   
    # date label format
    date_labels = "%a\n%d %b\n%Y")+         # day, above month abbrev., above 2-digit year

```



### Agrupar/colorear por valor {.unnumbered}

Las barras del histograma pueden colorearse por grupos y "apilarse". Para designar la columna de agrupación, haz los siguientes cambios. Consulta la página de [Conceptos básicos de ggplot](#ggplot-basics) para más detalles.

* Dentro del mapeo estético del histograma `aes()`, asigna el nombre de la columna a los argumentos `group =` y `fill = `
* Elimina cualquier argumento `fill = ` *fuera* de `aes()`, ya que anulará el de dentro
* Los argumentos *dentro de* `aes()` se aplicarán *por grupo*, mientras que los de *fuera* se aplicarán a todas las barras (por ejemplo, es posible que quieras `color = ` fuera, para que cada barra tenga el mismo borde)

Este es el aspecto que tendría el comando `aes()` para agrupar y colorear las barras por gender:

```{r, eval=F}
aes(x = date_onset, group = gender, fill = gender)
```

Aquí se aplica: 

```{r, warning=F, message=F}
ggplot(data = linelist) +     # begin with linelist (many hospitals)
  
  # make histogram: specify bin break points: starts the Monday before first case, end Monday after last case
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = hospital,       # set data to be grouped by hospital
      fill = hospital),       # bar fill (inside color) by hospital
    
    # bin breaks are Monday weeks
    breaks = weekly_breaks_all,   # sequence of weekly Monday bin breaks for whole outbreak, defined in previous code       
    
    # Color around bars
    color = "black")
```


### Ajustar los colores {.unnumbered}  

* Para establecer *manualmente* el relleno de cada grupo, utiliza `scale_fill_manual()` (nota: `scale_color_manual()` es diferente).
  * Utiliza el argumento `values = ` para aplicar un vector de colores.
  * Utiliza `na.value = ` para especificar un color para los valores `NA`.
  * Utiliza el argumento `labels = ` para cambiar el texto de los elementos de la leyenda. Para estar seguro, proporciónalo como un vector, como `c("old" = "new", "old" = "new")` o ajusta los valores en los propios datos.
  * Utiliza `name = ` para dar un título adecuado a la leyenda
* Consulta la página sobre [Conceptos básicos de ggplot](#ggplot-basics) para obtener más información sobre escalas y paletas de colores.  

```{r, warning=F, message=F}
ggplot(data = linelist)+           # begin with linelist (many hospitals)
  
  # make histogram
  geom_histogram(
    mapping = aes(x = date_onset,
        group = hospital,          # cases grouped by hospital
        fill = hospital),          # bar fill by hospital
    
    # bin breaks
    breaks = weekly_breaks_all,        # sequence of weekly Monday bin breaks, defined in previous code
    
    # Color around bars
    color = "black")+              # border color of each bar
  
  # manual specification of colors
  scale_fill_manual(
    values = c("black", "orange", "grey", "beige", "blue", "brown"),
    labels = c("St. Mark's Maternity Hospital (SMMH)" = "St. Mark's"),
    name = "Hospital") # specify fill colors ("values") - attention to order!
```



### Ajustar el orden de los niveles {.unnumbered}  

El orden en que se apilan las barras agrupadas se ajusta mejor clasificando la columna de agrupación como tipo Factor. A continuación, puedes designar el orden de los niveles de los factores (y sus etiquetas de visualización). Consulta la página sobre [Factores](#factors) o [consejos de ggplot](#ggplot-tips) para obtener más detalles.

Antes de realizar el gráfico, utiliza la función `fct_relevel()` del paquete **forcats** para convertir la columna de agrupación en de tipo factor y ajustar manualmente el orden de los niveles, como se detalla en la página sobre [Factores](#factors).

```{r}
# load forcats package for working with factors
pacman::p_load(forcats)

# Define new dataset with hospital as factor
plot_data <- linelist %>% 
  mutate(hospital = fct_relevel(hospital, c("Missing", "Other"))) # Convert to factor and set "Missing" and "Other" as top levels to appear on epicurve top

levels(plot_data$hospital) # print levels in order
```

En el siguiente gráfico, las únicas diferencias con respecto al anterior es que la columna `hospital` se ha consolidado como en el caso anterior, y utilizamos `guides()` para invertir el orden de la leyenda, de modo que "Missing" se encuentra en la parte inferior de la leyenda. 

```{r, warning=F, message=F}
ggplot(plot_data) +                     # Use NEW dataset with hospital as re-ordered factor
  
  # make histogram
  geom_histogram(
    mapping = aes(x = date_onset,
        group = hospital,               # cases grouped by hospital
        fill = hospital),               # bar fill (color) by hospital
    
    breaks = weekly_breaks_all,         # sequence of weekly Monday bin breaks for whole outbreak, defined at top of ggplot section
    
    color = "black")+                   # border color around each bar
    
  # x-axis labels
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space before and after case bars
    date_breaks       = "3 weeks",      # labels appear every 3 Monday weeks
    date_minor_breaks = "week",         # vertical lines appear every Monday week
    date_labels       = "%d\n%b\n'%y")+ # date labels format
  
  # y-axis
  scale_y_continuous(
    expand = c(0,0))+                   # remove excess y-axis space below 0
  
  # manual specification of colors, ! attention to order
  scale_fill_manual(
    values = c("grey", "beige", "black", "orange", "blue", "brown"),
    labels = c("St. Mark's Maternity Hospital (SMMH)" = "St. Mark's"),
    name = "Hospital")+ 
  
  # aesthetic themes
  theme_minimal()+                      # simplify plot background
  
  theme(
    plot.caption = element_text(face = "italic", # caption on left side in italics
                                hjust = 0), 
    axis.title = element_text(face = "bold"))+   # axis titles in bold
  
  # labels
  labs(
    title    = "Weekly incidence of cases by hospital",
    subtitle = "Hospital as re-ordered factor",
    x        = "Week of symptom onset",
    y        = "Weekly cases")
```

<span style="color: darkgreen;">***CONSEJO:*** Para invertir solamente el orden de la leyenda, añade este comando **ggplot2**: `guides(fill = guide_legend(reverse = TRUE))`.</span>  





### Ajustar la leyenda {.unnumbered}

Lee más sobre las leyendas y las escalas en la página [Consejos de ggplot](#ggplot-tips). Aquí hay algunos puntos destacados:

* Edita el título de la leyenda, ya sea en la función de escala o con `labs(fill = "Título de la leyenda")` (si estás usando `color = ` estético, entonces usa `labs(color = "")`)
* `theme(legend.title = element_blank())` para no tener título de leyenda
* `theme(legend.position = "top")` ("bottom", "left", "right", o "none" para eliminar la leyenda)
* `theme(legend.direction = "horizontal")` leyenda horizontal
* `guides(fill = guide_legend(reverse = TRUE))` para invertir el orden de la leyenda  







### Barras de lado a lado {.unnumbered}  

La visualización lado a lado de las barras de grupo (en lugar de apiladas) se especifica dentro de `geom_histogram()` con `position = "dodge"` colocado fuera de `aes()`.

Si hay más de dos grupos de valores, éstos pueden resultar difíciles de leer. Considera la posibilidad de utilizar un gráfico facetado (múltiples pequeños). Para mejorar la legibilidad en este ejemplo, se han eliminado los valores de género que faltan.

```{r, warning=F, message=F}
ggplot(central_data %>% drop_na(gender))+   # begin with Central Hospital cases dropping missing gender
    geom_histogram(
        mapping = aes(
          x = date_onset,
          group = gender,         # cases grouped by gender
          fill = gender),         # bars filled by gender
        
        # histogram bin breaks
        breaks = weekly_breaks_central,   # sequence of weekly dates for Central outbreak - defined at top of ggplot section
        
        color = "black",          # bar edge color
        
        position = "dodge")+      # SIDE-BY-SIDE bars
                      
  
  # The labels on the x-axis
  scale_x_date(expand            = c(0,0),         # remove excess x-axis space below and after case bars
               date_breaks       = "3 weeks",      # labels appear every 3 Monday weeks
               date_minor_breaks = "week",         # vertical lines appear every Monday week
               date_labels       = "%d\n%b\n'%y")+ # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+             # removes excess y-axis space between bottom of bars and the labels
  
  #scale of colors and legend labels
  scale_fill_manual(values = c("brown", "orange"),  # specify fill colors ("values") - attention to order!
                    na.value = "grey" )+     

  # aesthetic themes
  theme_minimal()+                                               # a set of themes to simplify plot
  theme(plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
        axis.title = element_text(face = "bold"))+               # axis titles in bold
  
  # labels
  labs(title    = "Weekly incidence of cases, by gender",
       subtitle = "Subtitle",
       fill     = "Gender",                                      # provide new title for legend
       x        = "Week of symptom onset",
       y        = "Weekly incident cases reported")
```




### Límites del eje {.unnumbered}  

Hay dos maneras de limitar la extensión de los valores del eje.

Por lo general, la forma preferida es utilizar el comando `coord_cartesian()`, que acepta `xlim = c(min, max)` y `ylim = c(min, max)` (donde proporcionas los valores mínimos y máximos). Esto actúa como un "zoom" sin eliminar realmente ningún dato, lo que es importante para las estadísticas y las medidas de resumen.

Alternativamente, puedes establecer valores de fecha máximos y mínimos utilizando `limits = c()` dentro de `scale_x_date()`. Por ejemplo: 

```{r eval=F}
scale_x_date(limits = c(as.Date("2014-04-01"), NA)) # sets a minimum date but leaves the maximum open.  
```

Asimismo, si deseas que el eje-x se extienda hasta una fecha concreta (por ejemplo, la fecha actual), aunque no se hayan notificado nuevos casos, puedes utilizar 

```{r eval=F}
scale_x_date(limits = c(NA, Sys.Date()) # ensures date axis will extend until current date  
```

<span style="color: red;">***PELIGRO:*** Ten cuidado al establecer los cortes o límites de la escala del eje-y (por ejemplo, de 0 a 30 por 5: `seq(0, 30, 5)`). Tales números estáticos pueden cortar tu gráfica demasiado si los datos cambian para superar el límite!</span>


### Ejes de fecha etiquetas/cuadrículas {.unnumbered} 

<span style="color: darkgreen;">***CONSEJO:*** Recuerda que las **etiquetas** de los ejes de fecha son independientes de la agregación de los datos en barras, pero visualmente puede ser importante alinear las franjas, las etiquetas de fecha y las líneas verticales de la cuadrícula.</span>

Para **modificar las etiquetas de fecha y las líneas de la cuadrícula**, utiliza `scale_x_date()` de una de estas maneras:

* **Si los intervalos de tu histograma son días, semanas de lunes, meses o años**:
  * Utiliza `date_breaks = ` para especificar el intervalo de las etiquetas y las líneas principales de la cuadrícula (por ejemplo, "day", "week", "3 weeks", "month", o "year")
  * Utiliza `date_minor_breaks = ` para especificar el intervalo de las líneas verticales menores (entre las etiquetas de fecha)
  * Añade `expand = c(0,0)` para comenzar las etiquetas en la primera barra
  * Usa `date_labels = ` para especificar el formato de las etiquetas de fecha - mira la página de [trabajar con fechas](working-with-dates) para consejos (usa `\n` para una nueva línea)
* **Si las franjas de tu histograma son semanas de domingo**:
  * Usa `breaks = ` y `minor_breaks = ` proporcionando una secuencia de saltos de fecha para cada una
  * Puedes seguir utilizando `date_labels = ` y `expand = ` para formatear, como se ha descrito anteriormente

Algunas notas:

* Consulta la sección de apertura de ggplot para obtener instrucciones sobre cómo crear una secuencia de fechas utilizando `seq.Date()`.
* Consulta [esta página](https://rdrr.io/r/base/strptime.html) o la página [Trabajar con fechas](#working-with-dates-1) para obtener consejos sobre la creación de etiquetas con fechas.



#### Demostraciones {.unnumbered}

A continuación se hace una demostración de gráficos en los que los intervalos y las etiquetas de los gráficos/líneas de la cuadrícula están alineados y no alineados:

```{r fig.show='hold', class.source = 'fold-hide', warning=F, message=F}
# 7-day bins + Monday labels
#############################
ggplot(central_data) +
  geom_histogram(
    mapping = aes(x = date_onset),
    binwidth = 7,                 # 7-day bins with start at first case
    color = "darkblue",
    fill = "lightblue") +
  
  scale_x_date(
    expand = c(0,0),               # remove excess x-axis space below and after case bars
    date_breaks = "3 weeks",       # Monday every 3 weeks
    date_minor_breaks = "week",    # Monday weeks
    date_labels = "%a\n%d\n%b\n'%y")+  # label format
  
  scale_y_continuous(
    expand = c(0,0))+              # remove excess space under x-axis, make flush
  
  labs(
    title = "MISALIGNED",
    subtitle = "! CAUTION: 7-day bars start Thursdays at first case\nDate labels and gridlines on Mondays\nNote how ticks don't align with bars")



# 7-day bins + Months
#####################
ggplot(central_data) +
  geom_histogram(
    mapping = aes(x = date_onset),
    binwidth = 7,
    color = "darkblue",
    fill = "lightblue") +
  
  scale_x_date(
    expand = c(0,0),                  # remove excess x-axis space below and after case bars
    date_breaks = "months",           # 1st of month
    date_minor_breaks = "week",       # Monday weeks
    date_labels = "%a\n%d %b\n%Y")+    # label format
  
  scale_y_continuous(
    expand = c(0,0))+                # remove excess space under x-axis, make flush 
  
  labs(
    title = "MISALIGNED",
    subtitle = "! CAUTION: 7-day bars start Thursdays with first case\nMajor gridlines and date labels at 1st of each month\nMinor gridlines weekly on Mondays\nNote uneven spacing of some gridlines and ticks unaligned with bars")


# TOTAL MONDAY ALIGNMENT: specify manual bin breaks to be mondays
#################################################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # histogram breaks set to 7 days beginning Monday before first case
    breaks = weekly_breaks_central,    # defined earlier in this page
    
    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "4 weeks",           # Monday every 4 weeks
    date_minor_breaks = "week",        # Monday weeks 
    date_labels = "%a\n%d %b\n%Y")+      # label format
  
  scale_y_continuous(
    expand = c(0,0))+                # remove excess space under x-axis, make flush 
  
  labs(
    title = "ALIGNED Mondays",
    subtitle = "7-day bins manually set to begin Monday before first case (28 Apr)\nDate labels and gridlines on Mondays as well")


# TOTAL MONDAY ALIGNMENT WITH MONTHS LABELS:
############################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # histogram breaks set to 7 days beginning Monday before first case
    breaks = weekly_breaks_central,            # defined earlier in this page
    
    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "months",            # Monday every 4 weeks
    date_minor_breaks = "week",        # Monday weeks 
    date_labels = "%b\n%Y")+          # label format
  
  scale_y_continuous(
    expand = c(0,0))+                # remove excess space under x-axis, make flush 
  
  theme(panel.grid.major = element_blank())+  # Remove major gridlines (fall on 1st of month)
          
  labs(
    title = "ALIGNED Mondays with MONTHLY labels",
    subtitle = "7-day bins manually set to begin Monday before first case (28 Apr)\nDate labels on 1st of Month\nMonthly major gridlines removed")


# TOTAL SUNDAY ALIGNMENT: specify manual bin breaks AND labels to be Sundays
############################################################################
ggplot(central_data) + 
  geom_histogram(
    mapping = aes(x = date_onset),
    
    # histogram breaks set to 7 days beginning Sunday before first case
    breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7),
                      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7),
                      by   = "7 days"),
    
    color = "darkblue",
    
    fill = "lightblue") + 
  
  scale_x_date(
    expand = c(0,0),
    # date label breaks and major gridlines set to every 3 weeks beginning Sunday before first case
    breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7),
                      to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7),
                      by   = "3 weeks"),
    
    # minor gridlines set to weekly beginning Sunday before first case
    minor_breaks = seq.Date(from = floor_date(min(central_data$date_onset, na.rm=T),   "week", week_start = 7),
                            to   = ceiling_date(max(central_data$date_onset, na.rm=T), "week", week_start = 7),
                            by   = "7 days"),
    
    date_labels = "%a\n%d\n%b\n'%y")+  # label format
  
  scale_y_continuous(
    expand = c(0,0))+                # remove excess space under x-axis, make flush 
  
  labs(title = "ALIGNED Sundays",
       subtitle = "7-day bins manually set to begin Sunday before first case (27 Apr)\nDate labels and gridlines manually set to Sundays as well")

```





### Datos agregados {.unnumbered} 

A menudo, en lugar de un listado, se comienza con recuentos agregados de instalaciones, distritos, etc. Se puede hacer una curva epidémica con `ggplot()` pero el código será ligeramente diferente. Esta sección utilizará los datos de `count_data` que fue importado anteriormente, en la sección de preparación de datos. Este conjunto de datos es `linelist` agregado a los recuentos de día-hospital. A continuación se muestran las primeras 50 filas.

```{r message=FALSE, warning=F, echo=F}
# display the linelist data as a table
DT::datatable(head(count_data, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


#### Representar recuentos diarios {.unnumbered}  

Podemos trazar una curva epidémica diaria a partir de estos *recuentos diarios*. Aquí están las diferencias con el código:

* Dentro del mapeo estético `aes()`, especifica `y = ` como columna de recuento (en este caso, el nombre de la columna es `n_cases`)
* Añadir el argumento `stat = "identity"` dentro de `geom_histogram()`, que especifica que la altura de la barra debe ser el valor `y =` y no el número de filas, como es el valor por defecto
* Añade el argumento `width = ` para evitar las líneas blancas verticales entre las barras. Para los datos diarios, establece el valor 1. Para los datos semanales, escribe 7. Para los datos de recuento mensual, las líneas blancas son un problema (cada mes tiene un número diferente de días) - considera la posibilidad de transformar el eje x en un factor categórico ordenado (meses) y utilizar `geom_col()`.

```{r, message=FALSE, warning=F}
ggplot(data = count_data)+
  geom_histogram(
   mapping = aes(x = date_hospitalisation, y = n_cases),
   stat = "identity",
   width = 1)+                # for daily counts, set width = 1 to avoid white space between bars
  labs(
    x = "Date of report", 
    y = "Number of cases",
    title = "Daily case incidence, from daily count data")
```

#### representar recuentos semanales {.unnumbered}

Si tus datos ya son recuentos de casos por semana, podrían parecerse a este conjunto de datos (llamado `count_data_weekly`):

```{r, warning=F, message=F, echo=F}
# Create weekly dataset with epiweek column
count_data_weekly <- count_data %>%
  mutate(epiweek = lubridate::floor_date(date_hospitalisation, "week")) %>% 
  group_by(hospital, epiweek, .drop=F) %>% 
  summarize(n_cases_weekly = sum(n_cases, na.rm=T))   
```

A continuación se muestran las primeras 50 filas de `count_data_weekly`. Puedes ver que los recuentos se han agregado en semanas. Cada semana se muestra por el primer día de la semana (lunes por defecto).

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(count_data_weekly, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Ahora trace de manera que `x = `la columna `epiweek`. Recuerda añadir `y = ` la columna de recuentos al mapeo estético, y añadir `stat = "identity"` como se ha explicado anteriormente.

```{r, warning=F, message=F}
ggplot(data = count_data_weekly)+
  
  geom_histogram(
    mapping = aes(
      x = epiweek,           # x-axis is epiweek (as class Date)
      y = n_cases_weekly,    # y-axis height in the weekly case counts
      group = hospital,      # we are grouping the bars and coloring by hospital
      fill = hospital),
    stat = "identity")+      # this is also required when plotting count data
     
  # labels for x-axis
  scale_x_date(
    date_breaks = "2 months",      # labels every 2 months 
    date_minor_breaks = "1 month", # gridlines every month
    date_labels = '%b\n%Y')+       #labeled by month with year below
     
  # Choose color palette (uses RColorBrewer package)
  scale_fill_brewer(palette = "Pastel2")+ 
  
  theme_minimal()+
  
  labs(
    x = "Week of onset", 
    y = "Weekly case incidence",
    fill = "Hospital",
    title = "Weekly case incidence, from aggregated count data by hospital")
```




### Medias móviles {.unnumbered}

Consulta la página sobre [medias móviles](#moving-averages-1) para obtener una descripción detallada y varias opciones. A continuación se muestra una opción para calcular medias móviles con el paquete **slider**. En este enfoque, *la media móvil se calcula antes de representarla*:

1.  Agrega los datos en recuentos según sea necesario (diario, semanal, etc.) (véase la página de [Agrupar datos](#grouping-data))
2.  Crea una nueva columna para contener la media móvil, creada con `slide_index()` del paquete **slider**
3.  Dibuja la media móvil como una `geom_line()` encima (después) del histograma de la curva epidémica

Es muy útil la [viñeta en línea del paquete **slider**](https://cran.r-project.org/web/packages/slider/vignettes/slider.html) 


```{r, warning=F, message=F}
# load package
pacman::p_load(slider)  # slider used to calculate rolling averages

# make dataset of daily counts and 7-day moving average
#######################################################
ll_counts_7day <- linelist %>%    # begin with linelist
  
  ## count cases by date
  count(date_onset, name = "new_cases") %>%   # name new column with counts as "new_cases"
  drop_na(date_onset) %>%                     # remove cases with missing date_onset
  
  ## calculate the average number of cases in 7-day window
  mutate(
    avg_7day = slider::slide_index(    # create new column
      new_cases,                       # calculate based on value in new_cases column
      .i = date_onset,                 # index is date_onset col, so non-present dates are included in window 
      .f = ~mean(.x, na.rm = TRUE),    # function is mean() with missing values removed
      .before = 6,                     # window is the day and 6-days before
      .complete = FALSE),              # must be FALSE for unlist() to work in next step
    avg_7day = unlist(avg_7day))       # convert class list to class numeric


# plot
######
ggplot(data = ll_counts_7day) +  # begin with new dataset defined above 
    geom_histogram(              # create epicurve histogram
      mapping = aes(
        x = date_onset,          # date column as x-axis
        y = new_cases),          # height is number of daily new cases
        stat = "identity",       # height is y value
        fill="#92a8d1",          # cool color for bars
        colour = "#92a8d1",      # same color for bar border
        )+ 
    geom_line(                   # make line for rolling average
      mapping = aes(
        x = date_onset,          # date column for x-axis
        y = avg_7day,            # y-value set to rolling average column
        lty = "7-day \nrolling avg"), # name of line in legend
      color="red",               # color of line
      size = 1) +                # width of line
    scale_x_date(                # date scale
      date_breaks = "1 month",
      date_labels = '%d/%m',
      expand = c(0,0)) +
    scale_y_continuous(          # y-axis scale
      expand = c(0,0),
      limits = c(0, NA)) +       
    labs(
      x="",
      y ="Number of confirmed cases",
      fill = "Legend")+ 
    theme_minimal()+
    theme(legend.title = element_blank())  # removes title of legend
```




### Facetas/pequeñas múltiples {.unnumbered}

Al igual que con otros ggplots, puedes crear gráficos facetados ("pequeños múltiples"). Como se explica en la página [Consejos de ggplot](#ggplot-tips) de este manual, puedes utilizar `facet_wrap()` o `facet_grid()`. Aquí lo mostramos con `facet_wrap()`. Para las curvas epidémicas, `facet_wrap()` es típicamente más fácil, ya que es probable que sólo necesites facetar una columna.

La sintaxis general es `facet_wrap(rows ~ cols)`, donde a la izquierda de la tilde (`~`) está el nombre de una columna que se extiende a través de las "filas" del gráfico facetado, y a la derecha de la tilde está el nombre de una columna que se extiende a través de las "columnas" del gráfico facetado. Lo más sencillo es utilizar un nombre de columna, a la derecha de la tilde: `facet_wrap(~age_cat)`.  


**Ejes libres**
Tendrás que decidir si las escalas de los ejes para cada faceta son "fijas" (por defecto), o "libres" (lo que significa que cambiarán en función de los datos dentro de la faceta). Haz esto con el argumento `scales = ` dentro de `facet_wrap()` especificando "free_x" o "free_y", o "free".

**Número de columnas y filas de las facetas**
Se puede especificar con `ncol = ` y `nrow = ` dentro de `facet_wrap()`.

**Orden de los paneles**
Para cambiar el orden de aparición, cambia el orden de los niveles de la columna de factores utilizada para crear las facetas.

**Estética**
El tamaño y tipo de la fuente, el color de la franja, etc, se pueden modificar mediante  `theme()` con argumentos como:  

* `strip.text = element_text()` (size, colour, face, angle..(tamaño, color, cara, ángulo)
* `strip.background = element_rect()` (e.g. element_rect(fill="grey"))  
* `strip.position = ` (posición "bottom", "top", "left", o "right" (Abajo, arriba, izquierda o derecha))  


**Etiquetas de banda**
Las etiquetas de los gráficos de facetas pueden modificarse a través de las "etiquetas" de la columna como factor, o mediante el uso de un "etiquetador".  

Haz un etiquetador como este, usando la función `as_labeller()` de **ggplot2**. A continuación, proporciona el argumento `labeller = ` en `facet_wrap()` como se muestra a continuación.

```{r, class.source = 'fold-show'}
my_labels <- as_labeller(c(
     "0-4"   = "Ages 0-4",
     "5-9"   = "Ages 5-9",
     "10-14" = "Ages 10-14",
     "15-19" = "Ages 15-19",
     "20-29" = "Ages 20-29",
     "30-49" = "Ages 30-49",
     "50-69" = "Ages 50-69",
     "70+"   = "Over age 70"))
```

**Un ejemplo de gráfico facetado** - facetado por la columna `age_cat`.


```{r, warning=F, message=F}
# make plot
###########
ggplot(central_data) + 
  
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = age_cat,
      fill = age_cat),    # arguments inside aes() apply by group
      
    color = "black",      # arguments outside aes() apply to all data
        
    # histogram breaks
    breaks = weekly_breaks_central)+  # pre-defined date vector (see earlier in this page)
                      
  # The labels on the x-axis
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space below and after case bars
    date_breaks       = "2 months",     # labels appear every 2 months
    date_minor_breaks = "1 month",      # vertical lines appear every 1 month 
    date_labels       = "%b\n'%y")+     # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+                       # removes excess y-axis space between bottom of bars and the labels
  
  # aesthetic themes
  theme_minimal()+                                           # a set of themes to simplify plot
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
    axis.title = element_text(face = "bold"),
    legend.position = "bottom",
    strip.text = element_text(face = "bold", size = 10),
    strip.background = element_rect(fill = "grey"))+         # axis titles in bold
  
  # create facets
  facet_wrap(
    ~age_cat,
    ncol = 4,
    strip.position = "top",
    labeller = my_labels)+             
  
  # labels
  labs(
    title    = "Weekly incidence of cases, by age category",
    subtitle = "Subtitle",
    fill     = "Age category",                                      # provide new title for legend
    x        = "Week of symptom onset",
    y        = "Weekly incident cases reported",
    caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```

Consulta este [enlace](https://ggplot2.tidyverse.org/reference/labellers.html) para obtener más información sobre las etiquetadoras.




#### Conjunto de la Epidemia como fondo de la faceta {.unnumbered}


Para mostrar el conjunto de la epidemia como fondo de cada faceta, añade la función `gghighlight()` con paréntesis vacíos al ggplot. Esto es del paquete **gghighlight**. Observa que el máximo del eje Y en todas las facetas se basa ahora en el pico de toda la epidemia. Hay más ejemplos de este paquete en la página [Consejos de ggplot](#highlighting). 

```{r, warning=F, message=F}
ggplot(central_data) + 
  
  # epicurves by group
  geom_histogram(
    mapping = aes(
      x = date_onset,
      group = age_cat,
      fill = age_cat),  # arguments inside aes() apply by group
    
    color = "black",    # arguments outside aes() apply to all data
    
    # histogram breaks
    breaks = weekly_breaks_central)+     # pre-defined date vector (see top of ggplot section)                
  
  # add grey epidemic in background to each facet
  gghighlight::gghighlight()+
  
  # labels on x-axis
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space below and after case bars
    date_breaks       = "2 months",     # labels appear every 2 months
    date_minor_breaks = "1 month",      # vertical lines appear every 1 month 
    date_labels       = "%b\n'%y")+     # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+  # removes excess y-axis space below 0
  
  # aesthetic themes
  theme_minimal()+                                           # a set of themes to simplify plot
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
    axis.title = element_text(face = "bold"),
    legend.position = "bottom",
    strip.text = element_text(face = "bold", size = 10),
    strip.background = element_rect(fill = "white"))+        # axis titles in bold
  
  # create facets
  facet_wrap(
    ~age_cat,                          # each plot is one value of age_cat
    ncol = 4,                          # number of columns
    strip.position = "top",            # position of the facet title/strip
    labeller = my_labels)+             # labeller defines above
  
  # labels
  labs(
    title    = "Weekly incidence of cases, by age category",
    subtitle = "Subtitle",
    fill     = "Age category",                                      # provide new title for legend
    x        = "Week of symptom onset",
    y        = "Weekly incident cases reported",
    caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```


#### Una faceta con datos {.unnumbered}  

Si quieres tener una caja de facetas que contenga todos los datos, duplica todo el conjunto de datos y trata los duplicados como un solo valor de facetas. Una función de "ayuda" `CreateAllFacet()` a continuación puede ayudar con esto (gracias a esta [entrada del blog](https://stackoverflow.com/questions/18933575/easily-add-an-all-facet-to-facet-wrap-in-ggplot2)). Cuando se ejecuta, el número de filas se duplica y habrá una nueva columna llamada `facet` en la que las filas duplicadas tendrán el valor "all", y las filas originales tienen el valor original de la columna `facet`. Ahora sólo tienes que hacer la faceta con la columna `facet`.

Aquí está la función de ayuda. Ejecútala para que esté disponible para ti. 

```{r}
# Define helper function
CreateAllFacet <- function(df, col){
     df$facet <- df[[col]]
     temp <- df
     temp$facet <- "all"
     merged <-rbind(temp, df)
     
     # ensure the facet value is a factor
     merged[[col]] <- as.factor(merged[[col]])
     
     return(merged)
}
```

Ahora aplica la función de ayuda a los datos, en la columna `age_cat`:

```{r}
# Create dataset that is duplicated and with new column "facet" to show "all" age categories as another facet level
central_data2 <- CreateAllFacet(central_data, col = "age_cat") %>%
  
  # set factor levels
  mutate(facet = fct_relevel(facet, "all", "0-4", "5-9",
                             "10-14", "15-19", "20-29",
                             "30-49", "50-69", "70+"))

# check levels
table(central_data2$facet, useNA = "always")
```

Los cambios más importantes en el comando `ggplot()` son:

* Los datos utilizados son ahora `central_data2` (el doble de filas, con la nueva columna "facet")
* La etiquetadora tendrá que ser actualizada, si se utiliza
* Opcional: para conseguir facetas apiladas verticalmente: la columna de la faceta se mueve al lado de las filas de la ecuación y a la derecha se sustituye por "." (`facet_wrap(facet\~.)`), y `ncol = 1 `. También puede ser necesario ajustar la anchura y la altura de la imagen png guardada (ver `ggsave()` en [Conceptos básicos de ggplot](#ggplot-basics)).

```{r, fig.height=12, fig.width=5, warning=F, message=F}
ggplot(central_data2) + 
  
  # actual epicurves by group
  geom_histogram(
        mapping = aes(
          x = date_onset,
          group = age_cat,
          fill = age_cat),  # arguments inside aes() apply by group
        color = "black",    # arguments outside aes() apply to all data
        
        # histogram breaks
        breaks = weekly_breaks_central)+    # pre-defined date vector (see top of ggplot section)
                     
  # Labels on x-axis
  scale_x_date(
    expand            = c(0,0),         # remove excess x-axis space below and after case bars
    date_breaks       = "2 months",     # labels appear every 2 months
    date_minor_breaks = "1 month",      # vertical lines appear every 1 month 
    date_labels       = "%b\n'%y")+     # date labels format
  
  # y-axis
  scale_y_continuous(expand = c(0,0))+  # removes excess y-axis space between bottom of bars and the labels
  
  # aesthetic themes
  theme_minimal()+                                           # a set of themes to simplify plot
  theme(
    plot.caption = element_text(face = "italic", hjust = 0), # caption on left side in italics
    axis.title = element_text(face = "bold"),
    legend.position = "bottom")+               
  
  # create facets
  facet_wrap(facet~. ,                            # each plot is one value of facet
             ncol = 1)+            

  # labels
  labs(title    = "Weekly incidence of cases, by age category",
       subtitle = "Subtitle",
       fill     = "Age category",                                      # provide new title for legend
       x        = "Week of symptom onset",
       y        = "Weekly incident cases reported",
       caption  = stringr::str_glue("n = {nrow(central_data)} from Central Hospital; Case onsets range from {format(min(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')} to {format(max(central_data$date_onset, na.rm=T), format = '%a %d %b %Y')}\n{nrow(central_data %>% filter(is.na(date_onset)))} cases missing date of onset and not shown"))
```








## Datos provisionales  {#tentative-data}

Los datos más recientes que se muestran en las curvas epidémicas deben marcarse a menudo como provisionales, o sujetos a retrasos en los informes. Esto puede hacerse añadiendo una línea vertical y/o un rectángulo sobre un número determinado de días. Aquí hay dos opciones:


1.  Utiliza `annotate()`:
     + Para una línea utiliza `annotate(geom = "segment")`. Proporciona `x`, `xend`, `y`, e `yend`. Ajusta el tamaño, el tipo de línea (`lty)` y el color.
     + Para un rectángulo utiliza `annotate(geom = "rect")`. Proporciona `xmin/xmax/ymin/ymax`. Ajusta el color y el alpha.
2.  Agrupar los datos por estado tentativo y colorear esas barras de forma diferente

<span style="color: orange;">***PRECAUCIÓN:*** Puedes intentar `geom_rect()` para dibujar un rectángulo, pero el ajuste de la transparencia no funciona en un contexto de listado. Esta función superpone un rectángulo para cada observación/fila!. Utiliza un alfa muy bajo (por ejemplo, 0,01), u otro enfoque. </span>

### Uso de `annotate()` {.unnumbered}

* Dentro de `annotate(geom = "rect")`, los argumentos `xmin` y `xmax` deben tener entradas del tipo Date.
* Ten en cuenta que, como estos datos se agregan en barras semanales, y la última barra se extiende hasta el lunes siguiente al último punto de datos, la región sombreada puede parecer que abarca 4 semanas
* Este es un [ejemplo de `annotate()` en línea](https://ggplot2.tidyverse.org/reference/annotate.html)


```{r, warning=F, message=F}
ggplot(central_data) + 
  
  # histogram
  geom_histogram(
    mapping = aes(x = date_onset),
    
    breaks = weekly_breaks_central,   # pre-defined date vector - see top of ggplot section
    
    color = "darkblue",
    
    fill = "lightblue") +

  # scales
  scale_y_continuous(expand = c(0,0))+
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "1 month",           # 1st of month
    date_minor_breaks = "1 month",     # 1st of month
    date_labels = "%b\n'%y")+          # label format
  
  # labels and theme
  labs(
    title = "Using annotate()\nRectangle and line showing that data from last 21-days are tentative",
    x = "Week of symptom onset",
    y = "Weekly case indicence")+ 
  theme_minimal()+
  
  # add semi-transparent red rectangle to tentative data
  annotate(
    "rect",
    xmin  = as.Date(max(central_data$date_onset, na.rm = T) - 21), # note must be wrapped in as.Date()
    xmax  = as.Date(Inf),                                          # note must be wrapped in as.Date()
    ymin  = 0,
    ymax  = Inf,
    alpha = 0.2,          # alpha easy and intuitive to adjust using annotate()
    fill  = "red")+
  
  # add black vertical line on top of other layers
  annotate(
    "segment",
    x     = max(central_data$date_onset, na.rm = T) - 21, # 21 days before last data
    xend  = max(central_data$date_onset, na.rm = T) - 21, 
    y     = 0,         # line begins at y = 0
    yend  = Inf,       # line to top of plot
    size  = 2,         # line size
    color = "black",
    lty   = "solid")+   # linetype e.g. "solid", "dashed"

  # add text in rectangle
  annotate(
    "text",
    x = max(central_data$date_onset, na.rm = T) - 15,
    y = 15,
    label = "Subject to reporting delays",
    angle = 90)
```


La misma línea vertical negra se puede conseguir con el código de abajo, pero usando `geom_vline()` se pierde la capacidad de controlar la altura:

```{r, eval=F}
geom_vline(xintercept = max(central_data$date_onset, na.rm = T) - 21,
           size = 2,
           color = "black")
```



### Color de las barras {.unnumbered}  

Un enfoque alternativo podría ser ajustar el color o la visualización de las propias barras de datos tentativos. Podrías crear una nueva columna en la etapa de preparación de los datos y utilizarla para agrupar los datos, de manera que el `aes(fill = )` de los datos tentativos pueda tener un color o un alfa diferente al de las otras barras.

```{r, message=F, warning=F}
# add column
############
plot_data <- central_data %>% 
  mutate(tentative = case_when(
    date_onset >= max(date_onset, na.rm=T) - 7 ~ "Tentative", # tenative if in last 7 days
    TRUE                                       ~ "Reliable")) # all else reliable

# plot
######
ggplot(plot_data, aes(x = date_onset, fill = tentative)) + 
  
  # histogram
  geom_histogram(
    breaks = weekly_breaks_central,   # pre-defined data vector, see top of ggplot page
    color = "black") +

  # scales
  scale_y_continuous(expand = c(0,0))+
  scale_fill_manual(values = c("lightblue", "grey"))+
  scale_x_date(
    expand = c(0,0),                   # remove excess x-axis space below and after case bars
    date_breaks = "3 weeks",           # Monday every 3 weeks
    date_minor_breaks = "week",        # Monday weeks 
    date_labels = "%d\n%b\n'%y")+      # label format
  
  # labels and theme
  labs(title = "Show days that are tentative reporting",
    subtitle = "")+ 
  theme_minimal()+
  theme(legend.title = element_blank())                 # remove title of legend
  
```


## Etiquetas de fecha de varios niveles  {#multi-level-date-labels}

Si deseas etiquetas de fecha de varios niveles (por ejemplo, mes y año) *sin duplicar los niveles de etiquetas inferiores*, considera uno de los enfoques siguientes:

Recuerda - puedes utilizar herramientas como `\n` *dentro de* los argumentos `date_labels` o `labels` para poner partes de cada etiqueta en una nueva línea inferior. Sin embargo, el código de abajo le ayuda a usar años o meses (por ejemplo) en una línea inferior *y sólo una vez*. Algunas notas sobre el código de abajo:

* Los recuentos de casos se agregan en semanas por motivos estéticos. Véase la página de Epicurves (sección de datos agregados) para más detalles.
* Se utiliza una línea `geom_area()` en lugar de un histograma, ya que el enfoque de facetas que se presenta a continuación no funciona bien con los histogramas.


**Agregar a los recuentos semanales**

```{r out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F}

# Create dataset of case counts by week
#######################################
central_weekly <- linelist %>%
  filter(hospital == "Central Hospital") %>%   # filter linelist
  mutate(week = lubridate::floor_date(date_onset, unit = "weeks")) %>%  
  count(week) %>%                              # summarize weekly case counts
  drop_na(week) %>%                            # remove cases with missing onset_date
  complete(                                    # fill-in all weeks with no cases reported
    week = seq.Date(
      from = min(week),   
      to   = max(week),
      by   = "week"),
    fill = list(n = 0))                        # convert new NA values to 0 counts
```

**Hacer gráficos**

```{r, warning=F, message=F}
# plot with box border on year
##############################
ggplot(central_weekly) +
  geom_area(aes(x = week, y = n),    # make line, specify x and y
            stat = "identity") +             # because line height is count number
  scale_x_date(date_labels="%b",             # date label format show month 
               date_breaks="month",          # date labels on 1st of each month
               expand=c(0,0)) +              # remove excess space on each end
  scale_y_continuous(
    expand  = c(0,0))+                       # remove excess space below x-axis
  facet_grid(~lubridate::year(week), # facet on year (of Date class column)
             space="free_x",                
             scales="free_x",                # x-axes adapt to data range (not "fixed")
             switch="x") +                   # facet labels (year) on bottom
  theme_bw() +
  theme(strip.placement = "outside",         # facet labels placement
        strip.background = element_rect(fill = NA, # facet labels no fill grey border
                                        colour = "grey50"),
        panel.spacing = unit(0, "cm"))+      # no space between facet panels
  labs(title = "Nested year labels, grey label border")


# plot with no box border on year
#################################
ggplot(central_weekly,
       aes(x = week, y = n)) +              # establish x and y for entire plot
  geom_line(stat = "identity",              # make line, line height is count number
            color = "#69b3a2") +            # line color
  geom_point(size=1, color="#69b3a2") +     # make points at the weekly data points
  geom_area(fill = "#69b3a2",               # fill area below line
            alpha = 0.4)+                   # fill transparency
  scale_x_date(date_labels="%b",            # date label format show month 
               date_breaks="month",         # date labels on 1st of each month
               expand=c(0,0)) +             # remove excess space
  scale_y_continuous(
    expand  = c(0,0))+                      # remove excess space below x-axis
  facet_grid(~lubridate::year(week),        # facet on year (of Date class column)
             space="free_x",                
             scales="free_x",               # x-axes adapt to data range (not "fixed")
             switch="x") +                  # facet labels (year) on bottom
  theme_bw() +
  theme(strip.placement = "outside",                     # facet label placement
          strip.background = element_blank(),            # no facet lable background
          panel.grid.minor.x = element_blank(),          
          panel.border = element_rect(colour="grey40"),  # grey border to facet PANEL
          panel.spacing=unit(0,"cm"))+                   # No space between facet panels
  labs(title = "Nested year labels - points, shaded, no label border")
```

Las técnicas anteriores fueron adaptadas de [este](https://stackoverflow.com/questions/44616530/axis-labels-on-two-lines-with-nested-x-variables-year-below-months) y [este](https://stackoverflow.com/questions/20571306/multi-row-x-axis-labels-in-ggplot-line-chart) post en stackoverflow.com.






<!-- ======================================================= -->
## Doble eje {#dual-axis}  

Aunque hay fuertes discusiones sobre la validez de los ejes duales dentro de la comunidad de visualización de datos, muchos supervisores de epi todavía quieren ver una curva epidémica o un gráfico similar con un porcentaje superpuesto con un segundo eje. Esto se discute más ampliamente en la página [Consejos de ggplot](#plotting-multiple-datasets), pero a continuación se muestra un ejemplo utilizando el método **cowplot**:

* Se hacen dos gráficos distintos y luego se combinan con el paquete **cowplot**.
* Los gráficos deben tener exactamente el mismo eje x (límites establecidos) o de lo contrario los datos y las etiquetas no se alinearán
* Cada uno de ellos utiliza `theme_cowplot()` y uno de ellos tiene el eje-y desplazado a la derecha del gráfico

```{r, warning=F, message=F}
#load package
pacman::p_load(cowplot)

# Make first plot of epicurve histogram
#######################################
plot_cases <- linelist %>% 
  
  # plot cases per week
  ggplot()+
  
  # create histogram  
  geom_histogram(
    
    mapping = aes(x = date_onset),
    
    # bin breaks every week beginning monday before first case, going to monday after last case
    breaks = weekly_breaks_all)+  # pre-defined vector of weekly dates (see top of ggplot section)
        
  # specify beginning and end of date axis to align with other plot
  scale_x_date(
    limits = c(min(weekly_breaks_all), max(weekly_breaks_all)))+  # min/max of the pre-defined weekly breaks of histogram
  
  # labels
  labs(
      y = "Daily cases",
      x = "Date of symptom onset"
    )+
  theme_cowplot()


# make second plot of percent died per week
###########################################
plot_deaths <- linelist %>%                        # begin with linelist
  group_by(week = floor_date(date_onset, "week")) %>%  # create week column
  
  # summarise to get weekly percent of cases who died
  summarise(n_cases = n(),
            died = sum(outcome == "Death", na.rm=T),
            pct_died = 100*died/n_cases) %>% 
  
  # begin plot
  ggplot()+
  
  # line of weekly percent who died
  geom_line(                                # create line of percent died
    mapping = aes(x = week, y = pct_died),  # specify y-height as pct_died column
    stat = "identity",                      # set line height to the value in pct_death column, not the number of rows (which is default)
    size = 2,
    color = "black")+
  
  # Same date-axis limits as the other plot - perfect alignment
  scale_x_date(
    limits = c(min(weekly_breaks_all), max(weekly_breaks_all)))+  # min/max of the pre-defined weekly breaks of histogram
  
  
  # y-axis adjustments
  scale_y_continuous(                # adjust y-axis
    breaks = seq(0,100, 10),         # set break intervals of percent axis
    limits = c(0, 100),              # set extent of percent axis
    position = "right")+             # move percent axis to the right
  
  # Y-axis label, no x-axis label
  labs(x = "",
       y = "Percent deceased")+      # percent axis label
  
  theme_cowplot()                   # add this to make the two plots merge together nicely
```

Ahora utiliza **cowplot** para superponer los dos gráficos. Se ha prestado atención a la alineación del eje-x, al lado del eje-y y al uso de `theme_cowplot()`.

```{r, warning=F, message=F}
aligned_plots <- cowplot::align_plots(plot_cases, plot_deaths, align="hv", axis="tblr")
ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])
```




## Incidencia acumulada {#cumulative-incidence-1}

Nota: Si utilizas **incidence2**, consulta la sección sobre cómo puede producirse la incidencia acumulada con una función simple. Esta página abordará cómo calcular la incidencia acumulada y dibujarla con `ggplot()`.

Si se empieza con una lista de casos, crea una nueva columna que contenga el número acumulado de casos por día en un brote utilizando `cumsum()` de R **base**:   

```{r}
cumulative_case_counts <- linelist %>% 
  count(date_onset) %>%                # count of rows per day (returned in column "n")   
  mutate(                         
    cumulative_cases = cumsum(n)       # new column of the cumulative number of rows at each date
    )
```

A continuación se muestran las 10 primeras filas:

```{r message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(cumulative_case_counts, 10), rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```



Esta columna acumulativa puede entonces ser dibujada contra `date_onset`, usando `geom_line()`:

```{r, warning=F, message=F}
plot_cumulative <- ggplot()+
  geom_line(
    data = cumulative_case_counts,
    aes(x = date_onset, y = cumulative_cases),
    size = 2,
    color = "blue")

plot_cumulative
```


También se puede superponer a la curva epidémica, con doble eje utilizando el método **cowplot** descrito anteriormente y en la página [Consejos de ggplot](#ggplot-tips):

```{r, warning=F, message=F}
#load package
pacman::p_load(cowplot)

# Make first plot of epicurve histogram
plot_cases <- ggplot()+
  geom_histogram(          
    data = linelist,
    aes(x = date_onset),
    binwidth = 1)+
  labs(
    y = "Daily cases",
    x = "Date of symptom onset"
  )+
  theme_cowplot()

# make second plot of cumulative cases line
plot_cumulative <- ggplot()+
  geom_line(
    data = cumulative_case_counts,
    aes(x = date_onset, y = cumulative_cases),
    size = 2,
    color = "blue")+
  scale_y_continuous(
    position = "right")+
  labs(x = "",
       y = "Cumulative cases")+
  theme_cowplot()+
  theme(
    axis.line.x = element_blank(),
    axis.text.x = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks = element_blank())
```

Ahora utiliza **cowplot** para superponer los dos gráficos. Se ha prestado atención a la alineación del eje-x, al lado del eje-y y al uso de `theme_cowplot()`.

```{r, warning=F, message=F}
aligned_plots <- cowplot::align_plots(plot_cases, plot_cumulative, align="hv", axis="tblr")
ggdraw(aligned_plots[[1]]) + draw_plot(aligned_plots[[2]])
```


<!-- ======================================================= -->
## Recursos {#resources-25}








```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/epicurves.Rmd-->


# Pirámides de población y escalas de Likert {#demographic-pyramids-and-likert-scales}


```{r, out.width = c('50%', '50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "pop_pyramid_baseline.png"))

knitr::include_graphics(here::here("images", "likert.png"))
```


Las pirámides demográficas son útiles para mostrar distribuciones de edad y género. Se puede utilizar un código similar para visualizar los resultados de las preguntas de las encuestas tipo Likert (por ejemplo, "Muy de acuerdo", "De acuerdo", "Neutral", "En desacuerdo", "Muy en desacuerdo"). En esta página cubrimos lo siguiente:

* Pirámides rápidas y sencillas con el paquete **apyramid**
* Más pirámides personalizables con `ggplot()`
* Visualización de datos demográficos "de referencia" en el fondo de la pirámide
* Utilización de gráficos de tipo  pirámide para mostrar otros tipos de datos (por ejemplo, respuestas a preguntas de encuestas **tipo Likert**)


<!-- ======================================================= -->
## Preparación {#preparation-23}

### Cargar paquetes {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de **.** Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r}
pacman::p_load(rio,       # to import data
               here,      # to locate files
               tidyverse, # to clean, handle, and plot the data (includes ggplot2 package)
               apyramid,  # a package dedicated to creating age pyramids
               janitor,   # tables and cleaning data
               stringr)   # working with strings for titles, captions, etc.
```




### Importar datos {.unnumbered}  

Para empezar, importamos la lista de casos limpia de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica aquí para descargar linelist "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa los datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - vea la página de [importación y exportación](#import-and-export) para más detalles).

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas del listado.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Limpieza {.unnumbered}  

Para hacer una pirámide demográfica tradicional de edad/género, primero hay que limpiar los datos de la siguiente manera:

* Debe limpiarse la columna `gender`.
* Dependiendo del método, la edad debe ser almacenada como un número o en una columna de *categoría de edad*.

Si se utilizan categorías de edad, los valores de las columnas deben corregirse ordenados, ya sea por defecto alfanumérico o intencionadamente al convertirlo en de tipo factor.

A continuación utilizamos `tabyl()` de **janitor** para inspeccionar las columnas `gender` y `age_cat5`.

```{r}
linelist %>% 
  tabyl(age_cat5, gender)
```

También realizamos un histograma rápido de la columna `age` para asegurarnos de que está limpia y correctamente clasificada:

```{r}
hist(linelist$age)
```


<!-- ======================================================= -->
## paquete **apyramid** {#apyramid-package}

El paquete **apyramid** es un producto del proyecto [R4Epis](https://r4epis.netlify.com/). Puedes leer más sobre este paquete [aquí](https://cran.r-project.org/web/packages/apyramid/vignettes/intro.html). Te permite hacer rápidamente una pirámide de edad. Para situaciones más matizadas consulta, más abajo, la sección sobre [el uso de ggplot()](#demo_pyr_gg). Puedes leer más sobre el paquete **apyramid** en su página de ayuda introduciendo `?age_pyramid` en la consola de R.

### Datos individualizados {.unnumbered}  

Utilizando el conjunto de datos de `linelist` limpio, podemos crear una pirámide de edad con un simple comando `age_pyramid()`. En este comando:

* En el argumento `data = ` se establece el dataframe `linelist` 
* En el argumento `age_group = ` (para el eje Y) se establece la columna `age` categórica (entre comillas)
* En el argumento `split_by = ` (para el eje x) se establece la columna `gender`

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender")
```


La pirámide puede mostrarse con el porcentaje de todos los casos en el eje x, en lugar de los recuentos, incluyendo `proportional = TRUE`.

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender",
                      proportional = TRUE)
```


Cuando se utiliza el paquete **agepyramid**, si la columna `split_by` es binaria (por ejemplo, male/female, o yes/no), el resultado aparecerá como una pirámide. Sin embargo, si hay más de dos valores en la columna `split_by` (sin incluir `NA`), la pirámide aparecerá como un gráfico de barras facetadas con barras grises en el "fondo" que indican el rango de los datos no facetados para ese grupo de edad. En este caso, los valores de `split_by` = aparecerán como etiquetas en la parte superior de cada panel de facetas. Por ejemplo, a continuación se muestra lo que ocurre si a `split_by` = se le asigna la columna `hospital`.

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "hospital")  
```

#### Valores faltantes {.unnumbered}  

Las filas que tienen valores faltantes `NA` en las columnas `split_by =` o `age_group = `, si se codifican como `NA`, no producirán el aspecto mostrado arriba. Por defecto, estas filas no se mostrarán. Sin embargo, puede especificar que aparezcan, en un gráfico de barras adyacente y como un grupo de edad separado en la parte superior, especificando `na.rm = FALSE`.

```{r, warning=F, message=F}
apyramid::age_pyramid(data = linelist,
                      age_group = "age_cat5",
                      split_by = "gender",
                      na.rm = FALSE)         # show patients missing age or gender
```

#### Proporciones, colores y estética {.unnumbered}  

Por defecto, las barras muestran los recuentos (no el %), se muestra una línea media discontinua para cada grupo y los colores son verde/morado. Cada uno de estos parámetros puede ajustarse, como se muestra a continuación:

También puede añadir comandos adicionales de `ggplot()` al gráfico utilizando la sintaxis estándar de `ggplot()` "+", como temas estéticos y ajustes de etiquetas:

```{r, warning=F, message=F}
apyramid::age_pyramid(
  data = linelist,
  age_group = "age_cat5",
  split_by = "gender",
  proportional = TRUE,              # show percents, not counts
  show_midpoint = FALSE,            # remove bar mid-point line
  #pal = c("orange", "purple")      # can specify alt. colors here (but not labels)
  )+                 
  
  # additional ggplot commands
  theme_minimal()+                               # simplfy background
  scale_fill_manual(                             # specify colors AND labels
    values = c("orange", "purple"),              
    labels = c("m" = "Male", "f" = "Female"))+
  labs(y = "Percent of all cases",              # note x and y labs are switched
       x = "Age categories",                          
       fill = "Gender", 
       caption = "My data source and caption here",
       title = "Title of my plot",
       subtitle = "Subtitle with \n a second line...")+
  theme(
    legend.position = "bottom",                          # legend to bottom
    axis.text = element_text(size = 10, face = "bold"),  # fonts/sizes
    axis.title = element_text(size = 12, face = "bold"))
```



### Datos agregados {.unnumbered}  

Los ejemplos anteriores suponen que sus datos están en formato de listado, con una fila por observación. Si los datos ya están agregados en recuentos por categoría de edad, puedes seguir utilizando el paquete **apyramid**, como se muestra a continuación.

Para la demostración, agregamos los datos del listado en recuentos por categoría de edad y género, en un formato "ancho". Esto simulará como si sus datos estuvieran agregados desde el principios. Aprende más sobre [Agrupar datos](#grouping-data) y [Pivotar datos](#pivoting-data) en sus respectivas páginas.

```{r, warning=F, message=F}
demo_agg <- linelist %>% 
  count(age_cat5, gender, name = "cases") %>% 
  pivot_wider(
    id_cols = age_cat5,
    names_from = gender,
    values_from = cases) %>% 
  rename(`missing_gender` = `NA`)
```

...lo que hace que el conjunto de datos tenga el siguiente aspecto: con columnas para la categoría age, y recuentos de male, de female y de missing.

```{r, echo=F, warning=F, message=F}
# View the aggregated data
DT::datatable(demo_agg, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Para configurar estos datos para la pirámide de edad, pivotaremos los datos para que sean "largos" con la función `pivot_longer()` de **dplyr**. Esto se debe a que `ggplot()` generalmente prefiere datos "largos", y **apyramid** está utilizando `ggplot()`.

```{r, warning=F, message=F}
# pivot the aggregated data into long format
demo_agg_long <- demo_agg %>% 
  pivot_longer(
    col = c(f, m, missing_gender),            # cols to elongate
    names_to = "gender",                # name for new col of categories
    values_to = "counts") %>%           # name for new col of counts
  mutate(
    gender = na_if(gender, "missing_gender")) # convert "missing_gender" to NA
``` 

```{r, echo=F, warning=F, message=F}
# View the aggregated data
DT::datatable(demo_agg_long, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

A continuación, utiliza los argumentos `split_by =` y `count =` de `age_pyramid()` para especificar las respectivas columnas de los datos:

```{r, warning=F, message=F}
apyramid::age_pyramid(data = demo_agg_long,
                      age_group = "age_cat5",# column name for age category
                      split_by = "gender",   # column name for gender
                      count = "counts")      # column name for case counts
```

Observa en lo anterior, que el orden de los factores "m" y "f" es diferente (pirámide invertida). Para ajustar el orden debes redefinir el género en los datos agregados como un Factor y ordenar los niveles como se desee. Consulta la página [Factores](#factors).




<!-- ======================================================= -->
## `ggplot()` {#demo_pyr_gg}

El uso de `ggplot()` para construir tu pirámide de edad permite más flexibilidad, pero requiere más esfuerzo y comprensión de cómo funciona `ggplot()`. También es más fácil cometer errores accidentalmente.

Para usar `ggplot()` para hacer pirámides demográficas, se crean dos gráficos de barras (uno para cada género), se convierten los valores de un gráfico en negativo y, finalmente, se invierten los ejes x e y para mostrar los gráficos de barras verticalmente, con sus bases encontrándose en el centro del gráfico.

### Preparación {.unnumbered}

Este enfoque utiliza la columna *numérica* `age`, no la columna *categórica* de `age_cat5`. Así que comprobaremos que el tipo de esta columna es efectivamente numérica.

```{r}
class(linelist$age)
```

Podrías utilizar la misma lógica que se indica a continuación para construir una pirámide a partir de datos categóricos utilizando `geom_col()` en lugar de `geom_histogram()`. 

<!-- ======================================================= -->
### Construcción del gráfico {.unnumbered} 

En primer lugar, hay que entender que para hacer una pirámide de este tipo utilizando `ggplot()` el planteamiento es el siguiente:

* Dentro de `ggplot()`, crea **dos** histogramas utilizando la columna numérica de la edad. Crea uno para cada uno de los dos valores de agrupación (en este caso los géneros masculino y femenino). Para ello, los datos para cada histograma se especifican dentro de sus respectivos comandos `geom_histogram()`, con los respectivos filtros aplicados a `linelist`.

* Un gráfico tendrá valores de recuento positivos, mientras que el otro tendrá sus recuentos convertidos a valores negativos - esto crea la "pirámide" con el valor `0` en el centro del gráfico. Los valores negativos se crean utilizando un término especial de **ggplot2** `..count..` y multiplicando por -1.

* El comando `coord_flip()` cambia los ejes X e Y, lo que hace que los gráficos se vuelvan verticales y se cree la pirámide.

* Por último, hay que modificar las etiquetas de los valores del eje de recuento para que aparezcan como recuentos "positivos" en ambos lados de la pirámide (a pesar de que los valores subyacentes en un lado sean negativos).

A continuación se muestra una versión **sencilla** de esto, utilizando `geom_histogram()`:

```{r, warning=F, message=F}
  # begin ggplot
  ggplot(mapping = aes(x = age, fill = gender)) +
  
  # female histogram
  geom_histogram(data = linelist %>% filter(gender == "f"),
                 breaks = seq(0,85,5),
                 colour = "white") +
  
  # male histogram (values converted to negative)
  geom_histogram(data = linelist %>% filter(gender == "m"),
                 breaks = seq(0,85,5),
                 mapping = aes(y = ..count..*(-1)),
                 colour = "white") +
  
  # flip the X and Y axes
  coord_flip() +
  
  # adjust counts-axis scale
  scale_y_continuous(limits = c(-600, 900),
                     breaks = seq(-600,900,100),
                     labels = abs(seq(-600, 900, 100)))
```

<span style="color: red;">***PELIGRO:*** Si los **límites** de tu eje de recuentos son demasiado bajos, y una barra de recuentos los sobrepasa, la barra desaparecerá por completo o se acortará artificialmente. Ten cuidado con esto si analizas datos que se actualizan de forma rutinaria. Evítalo haciendo que los límites del eje de recuentos se ajusten automáticamente a los datos, como se indica a continuación.</span>  

Hay muchas cosas que puedes cambiar/añadir a esta sencilla versión, entre ellas:

* Ajustar automáticamente la escala del eje de recuentos a sus datos (evita los errores que se comentan en la advertencia que aparece a continuación)
* Especificar manualmente los colores y las etiquetas de las leyendas

**Convertir recuentos en porcentajes**

Para convertir los recuentos en porcentajes (del total), hazlo en los datos antes de representarlos. A continuación, obtenemos los recuentos de age-gender, entonces desagrupamos con `ungroup()`, y luego mutamos con `mutate()` para crear nuevas columnas de porcentajes. Si quieres porcentajes por género, omite el paso de desagrupación.


```{r, warning=F, message=F}
# create dataset with proportion of total
pyramid_data <- linelist %>%
  count(age_cat5,
        gender,
        name = "counts") %>% 
  ungroup() %>%                 # ungroup so percents are not by group
  mutate(percent = round(100*(counts / sum(counts, na.rm=T)), digits = 1), 
         percent = case_when(
            gender == "f" ~ percent,
            gender == "m" ~ -percent,     # convert male to negative
            TRUE          ~ NA_real_))    # NA val must by numeric as well
```

Es importante que guardemos los valores máximo y mínimo para saber cuáles deben ser los límites de la escala. Estos se utilizarán en el comando `ggplot()` a continuación.  

```{r}
max_per <- max(pyramid_data$percent, na.rm=T)
min_per <- min(pyramid_data$percent, na.rm=T)

max_per
min_per
```

Finalmente hacemos el `ggplot()` sobre los datos porcentuales. Especificamos `scale_y_continuous()` para extender las longitudes predefinidas en cada dirección (positiva y "negativa"). Usamos `floor()` y `ceiling()` para redondear los decimales en la dirección apropiada (abajo o arriba) para el lado del eje. 

```{r, warning=F, message=F}
# begin ggplot
  ggplot()+  # default x-axis is age in years;

  # case data graph
  geom_col(data = pyramid_data,
           mapping = aes(
             x = age_cat5,
             y = percent,
             fill = gender),         
           colour = "white")+       # white around each bar
  
  # flip the X and Y axes to make pyramid vertical
  coord_flip()+
  

  # adjust the axes scales
  # scale_x_continuous(breaks = seq(0,100,5), labels = seq(0,100,5)) +
  scale_y_continuous(
    limits = c(min_per, max_per),
    breaks = seq(from = floor(min_per),                # sequence of values, by 2s
                 to = ceiling(max_per),
                 by = 2),
    labels = paste0(abs(seq(from = floor(min_per),     # sequence of absolute values, by 2s, with "%"
                            to = ceiling(max_per),
                            by = 2)),
                    "%"))+  

  # designate colors and legend labels manually
  scale_fill_manual(
    values = c("f" = "orange",
               "m" = "darkgreen"),
    labels = c("Female", "Male")) +
  
  # label values (remember X and Y flipped now)
  labs(
    title = "Age and gender of cases",
    x = "Age group",
    y = "Percent of total",
    fill = NULL,
    caption = stringr::str_glue("Data are from linelist \nn = {nrow(linelist)} (age or sex missing for {sum(is.na(linelist$gender) | is.na(linelist$age_years))} cases) \nData as of: {format(Sys.Date(), '%d %b %Y')}")) +
  
  # display themes
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(colour = "black"),
    plot.title = element_text(hjust = 0.5), 
    plot.caption = element_text(hjust=0, size=11, face = "italic")
    )

```



<!-- ======================================================= -->
### Comparación con una línea basal  {.unnumbered} 

Con la flexibilidad de `ggplot()`, se puede tener una segunda capa de barras en el fondo que represente la pirámide de población "verdadera" o "de referencia". Esto puede proporcionar una buena visualización para comparar lo observado con una referencia.

Importa y visualiza los datos de población (véase la página [Descargando el manual y los datos](#download-handbook-and-data)):

```{r echo=F}
# import the population demographics data
pop <- rio::import(here::here("data", "standardization", "country_demographics.csv"))
```

```{r eval=F}
# import the population demographics data
pop <- rio::import("country_demographics.csv")
```

```{r, echo=F, warning=F, message=F}
# display the linelist data as a table
DT::datatable(pop, rownames = FALSE, options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```


En primer lugar, algunos pasos de gestión de datos:

Aquí registramos el orden de las categorías de edad que queremos que aparezcan. Debido a algunas peculiaridades de la forma en que se implementa `ggplot()`, en este escenario específico es más fácil almacenar estos como un vector de caracteres y utilizarlos más tarde en la función de representación gráfica.

```{r}
# record correct age cat levels
age_levels <- c("0-4","5-9", "10-14", "15-19", "20-24",
                "25-29","30-34", "35-39", "40-44", "45-49",
                "50-54", "55-59", "60-64", "65-69", "70-74",
                "75-79", "80-84", "85+")
```

Combina los datos de la población y de los casos mediante la función `bind_rows()` de **dplyr**:

* En primer lugar, asegúrate que los nombres de las columnas, los valores de las categorías de edad y los valores del género son *exactamente los mismos*
* Haz que tengan la misma estructura de datos: columnas de categoría de edad, sexo, recuentos y porcentaje del total
* Agruparlas, una encima de la otra (`bind_rows()`)


```{r, warning=F, message=F}
# create/transform populaton data, with percent of total
########################################################
pop_data <- pop %>% 
  pivot_longer(      # pivot gender columns longer
    cols = c(m, f),
    names_to = "gender",
    values_to = "counts") %>% 
  
  mutate(
    percent  = round(100*(counts / sum(counts, na.rm=T)),1),  # % of total
    percent  = case_when(                                                        
     gender == "f" ~ percent,
     gender == "m" ~ -percent,               # if male, convert % to negative
     TRUE          ~ NA_real_))
```

Revisar el conjunto de datos de la población modificada

```{r, echo=F, warning=F, message=F}
# display the linelist data as a table
DT::datatable(pop_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Ahora implementa lo mismo para los casos de `linelist` Ligeramente diferente porque comienza con las filas de casos, no con los recuentos.
```{r, warning=F, message=F}
# create case data by age/gender, with percent of total
#######################################################
case_data <- linelist %>%
  count(age_cat5, gender, name = "counts") %>%  # counts by age-gender groups
  ungroup() %>% 
  mutate(
    percent = round(100*(counts / sum(counts, na.rm=T)),1),  # calculate % of total for age-gender groups
    percent = case_when(                                     # convert % to negative if male
      gender == "f" ~ percent,
      gender == "m" ~ -percent,
      TRUE          ~ NA_real_))
```

Revisa los datos de casos modificados

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(case_data, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Ahora los dos dataframes están combinados, uno encima del otro (tienen los mismos nombres de columna). Podemos "nombrar" cada uno de los dataframes, y utilizar el argumento `.id = ` para crear una nueva columna "data_source" que indicará de qué dataframe se originó cada fila. Podemos utilizar esta columna para filtrar en `ggplot()`.



```{r, warning=F, message=F}
# combine case and population data (same column names, age_cat values, and gender values)
pyramid_data <- bind_rows("cases" = case_data, "population" = pop_data, .id = "data_source")
```

Almacena los valores porcentuales máximo y mínimo, utilizados en la función de trazado para definir la extensión del gráfico (¡y no acortar ninguna barra!)  

```{r}
# Define extent of percent axis, used for plot limits
max_per <- max(pyramid_data$percent, na.rm=T)
min_per <- min(pyramid_data$percent, na.rm=T)
```

Ahora el gráfico se hace con `ggplot()`:

* Un gráfico de barras de los datos de población (barras más anchas y transparentes)
* Un gráfico de barras de los datos del caso (barras pequeñas y más sólidas)


```{r, warning=F, message=F}

# begin ggplot
##############
ggplot()+  # default x-axis is age in years;

  # population data graph
  geom_col(
    data = pyramid_data %>% filter(data_source == "population"),
    mapping = aes(
      x = age_cat5,
      y = percent,
      fill = gender),
    colour = "black",                               # black color around bars
    alpha = 0.2,                                    # more transparent
    width = 1)+                                     # full width
  
  # case data graph
  geom_col(
    data = pyramid_data %>% filter(data_source == "cases"), 
    mapping = aes(
      x = age_cat5,                               # age categories as original X axis
      y = percent,                                # % as original Y-axis
      fill = gender),                             # fill of bars by gender
    colour = "black",                               # black color around bars
    alpha = 1,                                      # not transparent 
    width = 0.3)+                                   # half width
  
  # flip the X and Y axes to make pyramid vertical
  coord_flip()+
  
  # manually ensure that age-axis is ordered correctly
  scale_x_discrete(limits = age_levels)+     # defined in chunk above
  
  # set percent-axis 
  scale_y_continuous(
    limits = c(min_per, max_per),                                          # min and max defined above
    breaks = seq(floor(min_per), ceiling(max_per), by = 2),                # from min% to max% by 2 
    labels = paste0(                                                       # for the labels, paste together... 
              abs(seq(floor(min_per), ceiling(max_per), by = 2)), "%"))+                                                  

  # designate colors and legend labels manually
  scale_fill_manual(
    values = c("f" = "orange",         # assign colors to values in the data
               "m" = "darkgreen"),
    labels = c("f" = "Female",
               "m"= "Male"),      # change labels that appear in legend, note order
  ) +

  # plot labels, titles, caption    
  labs(
    title = "Case age and gender distribution,\nas compared to baseline population",
    subtitle = "",
    x = "Age category",
    y = "Percent of total",
    fill = NULL,
    caption = stringr::str_glue("Cases shown on top of country demographic baseline\nCase data are from linelist, n = {nrow(linelist)}\nAge or gender missing for {sum(is.na(linelist$gender) | is.na(linelist$age_years))} cases\nCase data as of: {format(max(linelist$date_onset, na.rm=T), '%d %b %Y')}")) +
  
  # optional aesthetic themes
  theme(
    legend.position = "bottom",                             # move legend to bottom
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(colour = "black"),
    plot.title = element_text(hjust = 0), 
    plot.caption = element_text(hjust=0, size=11, face = "italic"))

```


<!-- ======================================================= -->
## Escalas de Likert {#likert-scale}

Las técnicas utilizadas para hacer una pirámide de población con `ggplot()` también se pueden utilizar para hacer gráficos de datos de encuestas en escala Likert.

```{r, eval=F, echo=F}
data_raw <- import("P:/Shared/equateur_mve_2020/lessons learned/Ebola After-Action Survey - HQ epi team (form responses).csv")


likert_data <- data_raw %>% 
  select(2, 4:11) %>% 
  rename(status = 1,
         Q1 = 2,
         Q2 = 3,
            Q3 = 4,
            Q4 = 5,
            Q5 = 6,
            Q6 = 7,
            Q7 = 8,
            Q8 = 9) %>% 
  mutate(status = case_when(
           stringr::str_detect(status, "Mar") ~ "Senior",
           stringr::str_detect(status, "Jan") ~ "Intermediate",
           stringr::str_detect(status, "Feb") ~ "Junior",
           TRUE ~ "Senior")) %>% 
  mutate(Q4 = recode(Q4, "Not applicable" = "Very Poor"))

table(likert_data$status)

rio::export(likert_data, here::here("data", "likert_data.csv"))
```

Importa los datos (consulta la página [Descargando el manual y los datos](#download-handbook-and-data) si lo deseas).

```{r echo=F}
# import the likert survey response data
likert_data <- rio::import(here::here("data", "likert_data.csv"))
```

```{r, eval=F}
# import the likert survey response data
likert_data <- rio::import("likert_data.csv")
```

Empieza con datos que tengan este aspecto, con una clasificación categórica de cada encuestado (`status` y sus respuestas a 8 preguntas en una escala tipo Likert de 4 puntos ("Muy pobre", "Pobre", "Bueno", "Muy bueno").

```{r, echo=F, message=FALSE}
# display the linelist data as a table
DT::datatable(likert_data, rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```

En primer lugar, algunos pasos de gestión de datos:

* Pivotar los datos a lo largo
* Crear una nueva columna  `direction` en función de si la respuesta fue generalmente "positiva" o "negativa"
* Establece el orden del nivel de factor para la columnas `status` y `Response`
* Almacena el valor de recuento máximo para que los límites del gráfico sean los adecuados


```{r, warning=F, message=F}
melted <- likert_data %>% 
  pivot_longer(
    cols = Q1:Q8,
    names_to = "Question",
    values_to = "Response") %>% 
  mutate(
    
    direction = case_when(
      Response %in% c("Poor","Very Poor")  ~ "Negative",
      Response %in% c("Good", "Very Good") ~ "Positive",
      TRUE                                 ~ "Unknown"),
    
    status = fct_relevel(status, "Junior", "Intermediate", "Senior"),
    
    # must reverse 'Very Poor' and 'Poor' for ordering to work
    Response = fct_relevel(Response, "Very Good", "Good", "Very Poor", "Poor")) 

# get largest value for scale limits
melted_max <- melted %>% 
  count(status, Question) %>% # get counts
  pull(n) %>%                 # column 'n'
  max(na.rm=T)                # get max
```


Ahora haz el gráfico. Como en las pirámides de edad anteriores, estamos creando dos gráficos de barras e invirtiendo los valores de uno de ellos a negativo.

Utilizamos `geom_bar()` porque nuestros datos son una fila por observación, no recuentos agregados. Utilizamos el término especial de **ggplot2** `..count..` en uno de los gráficos de barras para invertir los valores en negativo (*-1), y establecemos `position = "stack"` para que los valores se apilen unos encima de otros. 

```{r, warning=F, message=F}
# make plot
ggplot()+
     
  # bar graph of the "negative" responses 
     geom_bar(
       data = melted %>% filter(direction == "Negative"),
       mapping = aes(
         x = status,
         y = ..count..*(-1),    # counts inverted to negative
         fill = Response),
       color = "black",
       closed = "left",
       position = "stack")+
     
     # bar graph of the "positive responses
     geom_bar(
       data = melted %>% filter(direction == "Positive"),
       mapping = aes(
         x = status,
         fill = Response),
       colour = "black",
       closed = "left",
       position = "stack")+
     
     # flip the X and Y axes
     coord_flip()+
  
     # Black vertical line at 0
     geom_hline(yintercept = 0, color = "black", size=1)+
     
    # convert labels to all positive numbers
    scale_y_continuous(
      
      # limits of the x-axis scale
      limits = c(-ceiling(melted_max/10)*11,    # seq from neg to pos by 10, edges rounded outward to nearest 5
                 ceiling(melted_max/10)*10),   
      
      # values of the x-axis scale
      breaks = seq(from = -ceiling(melted_max/10)*10,
                   to = ceiling(melted_max/10)*10,
                   by = 10),
      
      # labels of the x-axis scale
      labels = abs(unique(c(seq(-ceiling(melted_max/10)*10, 0, 10),
                            seq(0, ceiling(melted_max/10)*10, 10))))) +
     
    # color scales manually assigned 
    scale_fill_manual(
      values = c("Very Good"  = "green4", # assigns colors
                "Good"      = "green3",
                "Poor"      = "yellow",
                "Very Poor" = "red3"),
      breaks = c("Very Good", "Good", "Poor", "Very Poor"))+ # orders the legend
     
    
     
    # facet the entire plot so each question is a sub-plot
    facet_wrap( ~ Question, ncol = 3)+
     
    # labels, titles, caption
    labs(
      title = str_glue("Likert-style responses\nn = {nrow(likert_data)}"),
      x = "Respondent status",
      y = "Number of responses",
      fill = "")+

     # display adjustments 
     theme_minimal()+
     theme(axis.text = element_text(size = 12),
           axis.title = element_text(size = 14, face = "bold"),
           strip.text = element_text(size = 14, face = "bold"),  # facet sub-titles
           plot.title = element_text(size = 20, face = "bold"),
           panel.background = element_rect(fill = NA, color = "black")) # black box around each facet
```


<!-- ======================================================= -->
## Recursos {#resources-26}

[documentación de apyramide](https://cran.r-project.org/web/packages/apyramid/vignettes/intro.html)


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/age_pyramid.Rmd-->


# Gráficos de calor {#heat-plots}

Los gráficos de calor, también conocidos como "Heatmaps", o mapas de calor" o "mosaicos de calor", pueden ser visualizaciones útiles cuando se trata de mostrar 3 variables (eje-x, eje-y y relleno). A continuación mostramos dos ejemplos:

* Una matriz visual de eventos de transmisión por edad ("quién infectó a quién")
* Seguimiento de las métricas de información en muchas instalaciones/jurisdicciones a lo largo del tiempo


```{r, out.width = c('50%', '50%'), fig.show='hold', warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "transmission_matrix.png"))

knitr::include_graphics(here::here("images", "heat_tile.png"))

```





<!-- ======================================================= -->
## Preparación {#preparation-25}

### Cargar paquetes {.unnumbered}  

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r}
pacman::p_load(
  tidyverse,       # data manipulation and visualization
  rio,             # importing data 
  lubridate        # working with dates
  )
```

**Conjuntos de datos**

Esta página utiliza los casos de `linelist` un brote simulado para la sección de la matriz de transmisión, y unos datos separados de recuentos diarios de casos de malaria por instalación para la sección de seguimiento de métricas. Se cargan y limpian en sus secciones individuales.







## Matriz de transmisión  {#transmission-matrix}

Los mapas de calor pueden ser útiles para visualizar matrices. Un ejemplo es la visualización de "quién-infectó-quién" en un brote. Esto supone que se tiene información sobre los eventos de transmisión.

Ten en cuenta que la página [Rastreo de contactos](#contact-tracing-1) contiene otro ejemplo de elaboración de una matriz de contactos del mapa de calor, utilizando unos datos diferentes (quizás más sencillo) en el que las edades de los casos y sus fuentes están perfectamente alineadas en la misma fila del dataframe. Estos mismos datos se utilizan para hacer un mapa de *densidad* en la página [Consejos de ggplot](#ggplot-tips). Este ejemplo comienza a partir de `linelist`  y, por lo tanto, implica una considerable manipulación de los datos antes de lograr un dataframe ploteable. Así que hay muchos escenarios para elegir...

Partimos de la lista de casos de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica para descargar `linelist` "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa los datos con la función `import()` del paquete **rio** (acepta muchos tipos de archivos como .xlsx, .rds, .csv - vea la página de [importación y exportación](#import-and-export) para más detalles).

A continuación se muestran las primeras 50 filas del listado para su demostración:



```{r, echo=F}
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```


```{r, eval=F}
linelist <- import("linelist_cleaned.rds")
```


En este `linelist`:

* Hay una fila por caso, identificada por `case_id`
* Hay una columna posterior `infector` que contiene el `case_id` del *infector*, que también es un caso en `linelist`


```{r message=FALSE, echo=F}
# display the population as a table
DT::datatable(head(linelist, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



### Preparación de los datos {.unnumbered}  

**Objetivo**: Necesitamos conseguir un dataframe de estilo "largo" que contenga una fila por cada posible ruta de transmisión edad-a-edad, con una columna numérica que contenga la proporción de esa fila de todos los eventos de transmisión observados en `linelist`.

Esto requerirá varios pasos de manipulación de datos para lograrlo:


#### Hacer el dataframe de casos {.unnumbered} 

Para empezar, creamos un dataframe de los casos, sus edades y sus infectadores - llamamos al dataframe `case_ages`. Las primeras 50 filas se muestran a continuación.

```{r}
case_ages <- linelist %>% 
  select(case_id, infector, age_cat) %>% 
  rename("case_age_cat" = "age_cat")
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(case_ages, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

#### Hacer un dataframe de infectores {.unnumbered}  

A continuación, creamos un dataframe de los infectores, que por el momento consta de una sola columna. Se trata de las identificaciones de los infectores del listado. No todos los casos tienen un infector conocido, por lo que eliminamos los valores que faltan. A continuación se muestran las primeras 50 filas.


```{r}
infectors <- linelist %>% 
  select(infector) %>% 
  drop_na(infector)
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(infectors, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

A continuación, utilizamos las uniones para obtener las edades de los infectores. Esto no es sencillo, ya que en `linelist`, las edades de los infectores no aparecen como tales. Conseguimos este resultado uniendo los casos de `linelist` con los infectores. Comenzamos con los infectores, y `left_join()` (añadimos)  `linelist` de tal manera que la columna de ID del `infector` del lado izquierdo del dataframe "base" se une a la columna `case_id` en el dataframe `linelist` en el lado derecho.

Así, los datos del registro de casos del infector en `linelist `(incluida la edad) se añaden a la fila del infector. A continuación se muestran las 50 primeras filas.

```{r}
infector_ages <- infectors %>%             # begin with infectors
  left_join(                               # add the linelist data to each infector  
    linelist,
    by = c("infector" = "case_id")) %>%    # match infector to their information as a case
  select(infector, age_cat) %>%            # keep only columns of interest
  rename("infector_age_cat" = "age_cat")   # rename for clarity
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(infector_ages, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

A continuación, combinamos los casos y sus edades con los infectores y sus edades. Cada uno de estos dataframes tiene la columna `infector`, por lo que se utiliza para la unión. Las primeras filas se muestran a continuación: 

```{r}
ages_complete <- case_ages %>%  
  left_join(
    infector_ages,
    by = "infector") %>%        # each has the column infector
  drop_na()                     # drop rows with any missing data
```


```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(ages_complete, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

A continuación, una simple tabulación cruzada de los recuentos entre los grupos de edad de los casos y de los infectantes. Se añaden etiquetas para mayor claridad.  

```{r}
table(cases = ages_complete$case_age_cat,
      infectors = ages_complete$infector_age_cat)
```


Podemos convertir esta tabla en un dataframe con `data.frame()` de R **base**, que también la convierte automáticamente al formato "long", que es el deseado por `ggplot()`. Las primeras filas se muestran a continuación.

```{r}
long_counts <- data.frame(table(
    cases     = ages_complete$case_age_cat,
    infectors = ages_complete$infector_age_cat))
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(long_counts, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


Ahora hacemos lo mismo, pero aplicamos `prop.table()` de R **base** a la tabla para que en lugar de recuentos obtengamos proporciones del total. Las primeras 50 filas se muestran a continuación.

```{r}
long_prop <- data.frame(prop.table(table(
    cases = ages_complete$case_age_cat,
    infectors = ages_complete$infector_age_cat)))
```

```{r message=FALSE, echo=F}
# display the shapefile as a table
DT::datatable(head(long_prop, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```




### Crear un gráfico de calor {.unnumbered}  

Ahora, finalmente, podemos crear el gráfico de calor con el paquete **ggplot2**, utilizando la función `geom_tile()`. Consulta la página [Consejos de ggplot](#ggplot-tips) para conocer más ampliamente las escalas de color/relleno, especialmente la función `scale_fill_gradient()`.

* En la estética `aes()` de `geom_tile()` establece la x y la y como la edad del caso y la edad del infector
* También en `aes()` establece el argumento `fill = ` en la columna `Freq` - este es el valor que se convertirá en un color de mosaico
* Establece un color de escala con `scale_fill_gradient()` - puedes especificar los colores high/low
  * Ten en cuenta que `scale_color_gradient()` es diferente. En este caso quieres que rellene
* Dado que el color se hace a través de "fill", puedes utilizar el argumento `fill = ` en `labs()` para cambiar el título de la leyenda  

```{r}
ggplot(data = long_prop)+       # use long data, with proportions as Freq
  geom_tile(                    # visualize it in tiles
    aes(
      x = cases,         # x-axis is case age
      y = infectors,     # y-axis is infector age
      fill = Freq))+            # color of the tile is the Freq column in the data
  scale_fill_gradient(          # adjust the fill color of the tiles
    low = "blue",
    high = "orange")+
  labs(                         # labels
    x = "Case age",
    y = "Infector age",
    title = "Who infected whom",
    subtitle = "Frequency matrix of transmission events",
    fill = "Proportion of all\ntranmsission events"     # legend title
  )
  
```



<!-- ======================================================= -->
## Informar sobre las métricas a lo largo del tiempo {#reporting-metrics-over-time}

A menudo, en el ámbito de la salud pública, uno de los objetivos es evaluar las tendencias a lo largo del tiempo de muchas entidades (instalaciones, jurisdicciones, etc.). Una forma de visualizar esas tendencias a lo largo del tiempo es un gráfico de calor en el que el eje de abscisas es el tiempo y en el eje de ordenadas están las numerosas entidades.



### Preparación de los datos {.unnumbered}

Comenzamos importando unos datos de informes diarios sobre la malaria procedentes de muchos centros. Los informes contienen una fecha, una provincia, un distrito y el recuento de paludismo. Consulta la página [Descargando el manual y los datos](#download-handbook-and-data) para saber cómo descargar estos datos. A continuación se muestran las primeras 30 filas:

```{r, echo=F}
facility_count_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  select(location_name, data_date, District, malaria_tot)
```

```{r, eval=F}
facility_count_data <- import("malaria_facility_count_data.rds")
```


```{r, echo=F}
DT::datatable(head(facility_count_data,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


#### Agregar y resumir {.unnumbered}

**El objetivo de este ejemplo** es transformar los recuentos diarios del *total* de casos de malaria del centro (vistos en la sección anterior) en estadísticas resumidas *semanales* de la declaración de cada centro, en este caso *la proporción de días por semana en que el centro notificó algún dato*. Para este ejemplo mostraremos los datos sólo para el **distrito de Spring**.

Para ello, realizaremos los siguientes pasos de gestión de datos:

1)  Filtrar los datos según convenga (por lugar, fecha)
2)  Crear una columna de semana utilizando `floor_date()` del paquete **lubridate **
     + Esta función devuelve la fecha de inicio de la semana de una fecha dada, utilizando una fecha de inicio especificada de cada semana (por ejemplo, "onday")
3)  Los datos se agrupan por las columnas  "location" y "week" para crear unidades de análisis de "instalación-semana"
4)  La función `summarise()` crea nuevas columnas para reflejar las estadísticas resumidas por grupo de facility-week:

     + Número de días por semana (7 - un valor estático)
     + Número de informes recibidos de la semana de la instalación (¡podrían ser más de 7!)
     + Suma de los casos de paludismo notificados por el centro-semana (sólo por interés)
     + Número de días *únicos* en la semana de la instalación para los que hay datos reportados
     + **Porcentaje de los 7 días por instalación-semana para los que se comunicaron datos**
5.  El dataframe se une con `right_join()` a una lista exhaustiva de todas las posibles combinaciones de semanas de instalaciones, para que el conjunto de datos esté completo. La matriz de todas las combinaciones posibles se crea aplicando `expand()` a esas dos columnas del dataframe tal y como se encuentra en ese momento en la cadena de pipes (representada por `.`). Como se utiliza un right_join(), se mantienen todas las filas del dataframe de `expand()` y se añaden a `agg_weeks` si es necesario. Estas nuevas filas aparecen con valores resumidos `NA` (missing). 


A continuación lo mostramos paso a paso:

```{r, message=FALSE, warning=FALSE}
# Create weekly summary dataset
agg_weeks <- facility_count_data %>% 
  
  # filter the data as appropriate
  filter(
    District == "Spring",
    data_date < as.Date("2020-08-01")) 
```

Ahora el conjunto de datos tiene `nrow(agg_weeks)` filas, cuando antes tenía `nrow(facility_count_data)`.

A continuación creamos una columna `week` que refleje la fecha de inicio de la semana para cada registro. Esto se consigue con la función `floor_date()` del paquete **lubridate**, que se establece como "week" y para que las semanas comiencen los lunes (día 1 de la semana - los domingos serían 7). A continuación se muestran las filas superiores.

```{r}
agg_weeks <- agg_weeks %>% 
  # Create week column from data_date
  mutate(
    week = lubridate::floor_date(                     # create new column of weeks
      data_date,                                      # date column
      unit = "week",                                  # give start of the week
      week_start = 1))                                # weeks to start on Mondays 
```

La nueva columna `week` puede verse en el extremo derecho del dataframe

```{r, echo=F}
DT::datatable(head(agg_weeks,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Ahora agrupamos los datos en semanas de instalaciones y los resumimos para producir estadísticas por  `facility-week`. Consulta la página sobre [tablas descriptivas](#descriptive-tables) para obtener consejos. La agrupación en sí misma no cambia el dataframe, pero afecta a la forma en que se calculan las estadísticas de resumen posteriores.

A continuación se muestran las filas superiores. Observa cómo las columnas han cambiado completamente para reflejar las estadísticas de resumen deseadas. Cada fila refleja una `facility-week`.

```{r, warning=F, message=F}
agg_weeks <- agg_weeks %>%   

  # Group into facility-weeks
  group_by(location_name, week) %>%
  
  # Create summary statistics columns on the grouped data
  summarize(
    n_days          = 7,                                          # 7 days per week           
    n_reports       = dplyr::n(),                                 # number of reports received per week (could be >7)
    malaria_tot     = sum(malaria_tot, na.rm = T),                # total malaria cases reported
    n_days_reported = length(unique(data_date)),                  # number of unique days reporting per week
    p_days_reported = round(100*(n_days_reported / n_days))) %>%  # percent of days reporting
  ungroup(location_name, week)
```

```{r, echo=F}
DT::datatable(head(agg_weeks,30), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Por último, ejecutamos el siguiente comando para asegurarnos que TODAS las semanas posibles de las instalaciones están presentes en los datos, incluso si antes no estaban.

Estamos utilizando un `right_join()` sobre sí mismo (el conjunto de datos está representado por ".") pero habiéndose expandido para incluir todas las combinaciones posibles de las columnas `week` y  `location_name`. Véase la documentación sobre la función `expand()` en la página sobre [Pivotar datos](#pivoting-data). Antes de ejecutar este código, el conjunto de datos contiene `nrow(agg_weeks)` filas.

```{r, message=F, warning=F}
# Create data frame of every possible facility-week
expanded_weeks <- agg_weeks %>% 
  tidyr::expand(location_name, week)  # expand data frame to include all possible facility-week combinations
```

Aquí está `expanded_weeks`: 

```{r, echo=F}
DT::datatable(expanded_weeks, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Antes de ejecutar este código, `agg_weeks` contiene `nrow(agg_weeks)` filas.   

```{r}
# Use a right-join with the expanded facility-week list to fill-in the missing gaps in the data
agg_weeks <- agg_weeks %>%      
  right_join(expanded_weeks) %>%                            # Ensure every possible facility-week combination appears in the data
  mutate(p_days_reported = replace_na(p_days_reported, 0))  # convert missing values to 0                           
```

Después de ejecutar este código, `agg_weeks` contiene `nrow(agg_weeks)` filas.  


<!-- ======================================================= -->
### Crear un gráfico de calor {.unnumbered}

`ggplot()` se realiza utilizando `geom_tile()` del paquete **ggplot2**:

* Las semanas en el eje-x se transforman en fechas, lo que permite utilizar `scale_x_date()`
* `location_name` en el eje y mostrará todos los nombres de las instalaciones
* `fill` (relleno) es `p_days_reported`, el rendimiento para ese establecimiento-semana (numérico)
* `scale_fill_gradient()` se utiliza en el relleno numérico, especificando los colores para el alto, el bajo y `NA`
* `scale_x_date()` se utiliza en el eje x especificando las etiquetas cada 2 semanas y su formato
* Los temas de visualización y las etiquetas pueden ajustarse según sea necesario




<!-- ======================================================= -->
### Básico {.unnumbered}  

A continuación se produce un gráfico de calor básico, utilizando los colores, escalas, etc., por defecto. Como se ha explicado anteriormente, dentro de `aes()` para `geom_tile()` debes proporcionar una columna del eje-x, una columna del eje-y **y** una columna para `fill = `. El relleno es el valor numérico que se presenta como color del mosaico.

```{r}
ggplot(data = agg_weeks)+
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported))
```

### Gráfico limpio {.unnumbered}

Podemos hacer que este gráfico se vea mejor añadiendo funciones adicionales **de ggplot2**, como se muestra a continuación. Consulta la página [Consejos de ggplot](#ggplot-tips) para más detalles.

```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # show data as tiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+                 # white gridlines
  
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # date axis
  scale_x_date(
    expand = c(0,0),             # remove extra space on sides
    date_breaks = "2 weeks",     # labels every 2 weeks
    date_labels = "%d\n%b")+     # format is day over month (\n in newline)
  
  # aesthetic themes
  theme_minimal()+                                  # simplify background
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),           # height of legend key
    legend.key.width  = grid::unit(0.6,"cm"),         # width of legend key
    
    axis.text.x = element_text(size=12),              # axis text size
    axis.text.y = element_text(vjust=0.2),            # axis text alignment
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"),  # axis title size and bold
    
    plot.title = element_text(hjust=0,size=14,face="bold"),  # title right-aligned, large, bold
    plot.caption = element_text(hjust = 0, face = "italic")  # caption right-aligned and italic
    )+
  
  # plot labels
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",           # legend title, because legend shows fill
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, May-July 2020",
       caption = "7-day weeks beginning on Mondays.")
```





<!-- ======================================================= -->
### Eje-y ordenado {.unnumbered}  

Actualmente, las instalaciones están ordenadas "alfanuméricamente" de abajo a arriba. Si deseas ajustar el orden de las instalaciones del eje-y, conviértelas en de tipo factor y proporciona el orden. Consulta la página sobre [Factores](#factors) para obtener consejos.

Como hay muchas instalaciones y no queremos escribirlas todas, intentaremos otro enfoque: ordenar las instalaciones en un dataframe y utilizar la columna de nombres resultante como orden de los niveles del factor. A continuación, la columna `location_name` se convierte en un factor, y el orden de sus niveles se establece en función del número total de días de notificación presentados por el centro en todo el período de tiempo.

Para ello, creamos un dataframe que representa el número total de informes por instalación, ordenados de forma ascendente. Podemos utilizar este vector para ordenar los niveles del factor en el gráfico. 

```{r}
facility_order <- agg_weeks %>% 
  group_by(location_name) %>% 
  summarize(tot_reports = sum(n_days_reported, na.rm=T)) %>% 
  arrange(tot_reports) # ascending order
```

Véase el dataframe más abajo: 

```{r, echo=F}
DT::datatable(facility_order, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```




Ahora utiliza una columna del dataframe anterior (`facility_order$location_name`) para que sea el orden de los niveles del factor `location_name` en el dataframe `agg_weeks`:

```{r, warning=F, message=F}
# load package 
pacman::p_load(forcats)

# create factor and define levels manually
agg_weeks <- agg_weeks %>% 
  mutate(location_name = fct_relevel(
    location_name, facility_order$location_name)
    )
```

Y ahora los datos se vuelven a representar, con location_name como factor ordenado: 

```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # show data as tiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+                 # white gridlines
  
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # date axis
  scale_x_date(
    expand = c(0,0),             # remove extra space on sides
    date_breaks = "2 weeks",     # labels every 2 weeks
    date_labels = "%d\n%b")+     # format is day over month (\n in newline)
  
  # aesthetic themes
  theme_minimal()+                                  # simplify background
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),           # height of legend key
    legend.key.width  = grid::unit(0.6,"cm"),         # width of legend key
    
    axis.text.x = element_text(size=12),              # axis text size
    axis.text.y = element_text(vjust=0.2),            # axis text alignment
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"),  # axis title size and bold
    
    plot.title = element_text(hjust=0,size=14,face="bold"),  # title right-aligned, large, bold
    plot.caption = element_text(hjust = 0, face = "italic")  # caption right-aligned and italic
    )+
  
  # plot labels
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",           # legend title, because legend shows fill
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, May-July 2020",
       caption = "7-day weeks beginning on Mondays.")
```





<!-- ======================================================= -->
### Mostrar valores {.unnumbered}  

Puedes añadir una capa `geom_text()` encima de los mosaicos, para mostrar los números reales de cada mosaico. Ten en cuenta que esto puede no parecer bonito si tiene muchos mosaicos pequeños.

Se ha añadido el siguiente código: `geom_text(aes(label = p_days_reported))`. Esto añade texto en cada mosaico. El texto que se muestra es el valor asignado al argumento `label = `, que en este caso se ha establecido en la misma columna numérica `p_days_reported` que también se utiliza para crear el gradiente de color.



  
```{r, message=FALSE, warning=FALSE}
ggplot(data = agg_weeks)+ 
  
  # show data as tiles
  geom_tile(
    aes(x = week,
        y = location_name,
        fill = p_days_reported),      
    color = "white")+                 # white gridlines
  
  # text
  geom_text(
    aes(
      x = week,
      y = location_name,
      label = p_days_reported))+      # add text on top of tile
  
  # fill scale
  scale_fill_gradient(
    low = "orange",
    high = "darkgreen",
    na.value = "grey80")+
  
  # date axis
  scale_x_date(
    expand = c(0,0),             # remove extra space on sides
    date_breaks = "2 weeks",     # labels every 2 weeks
    date_labels = "%d\n%b")+     # format is day over month (\n in newline)
  
  # aesthetic themes
  theme_minimal()+                                    # simplify background
  
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),           # height of legend key
    legend.key.width  = grid::unit(0.6,"cm"),         # width of legend key
    
    axis.text.x = element_text(size=12),              # axis text size
    axis.text.y = element_text(vjust=0.2),            # axis text alignment
    axis.ticks = element_line(size=0.4),               
    axis.title = element_text(size=12, face="bold"),  # axis title size and bold
    
    plot.title = element_text(hjust=0,size=14,face="bold"),  # title right-aligned, large, bold
    plot.caption = element_text(hjust = 0, face = "italic")  # caption right-aligned and italic
    )+
  
  # plot labels
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",           # legend title, because legend shows fill
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, May-July 2020",
       caption = "7-day weeks beginning on Mondays.")
```




<!-- ======================================================= -->
## Recursos {#resources-25}

[scale_fill_gradient()](https://ggplot2.tidyverse.org/reference/scale_gradient.html)  

[Galería de gráficos R - mapa de calor](https://ggplot2.tidyverse.org/reference/scale_gradient.html)  




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/heatmaps.Rmd-->


# Diagramas y gráficos {#diagrams-and-charts}


```{r out.width = c('50%'), fig.show='hold', echo=F}
knitr::include_graphics(here::here("images", "flow_chart.png"))
knitr::include_graphics(here::here("images", "sankey_diagram.png"))
```


Esta página cubre el código para producir:

* Diagramas de flujo utilizando **DiagrammeR** y el lenguaje `DOT`
* Diagramas aluviales/Diagramas de Sankey
* Calendario de eventos

<!-- * DAGs (Directed Acyclic Graphs)   -->
<!-- * GANTT charts   -->


<!-- ======================================================= -->
## Preparación {#preparation-28}

### Cargar paquetes {.unnumbered}  

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También es posible cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r}
pacman::p_load(
  DiagrammeR,     # for flow diagrams
  networkD3,      # For alluvial/Sankey diagrams
  tidyverse)      # data management and visualization
```

### Importar datos {.unnumbered}  

La mayor parte del contenido de esta página no requiere unos datos. Sin embargo, en la sección del diagrama de Sankey, utilizaremos la lista de casos de una epidemia de ébola simulada. Si quieres seguir esta parte, [clica para descargar linelist "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa los datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - consulta la página [importación y exportación](#import-and-export) para más detalles).

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas del listado.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
## Diagramas de flujo {#flow-diagrams}

Se puede utilizar el paquete R **DiagrammeR** para crear gráficos/gráficos de flujo. Pueden ser estáticos o ajustarse de forma dinámica en función de los cambios en unos datos.

**Herramientas**

La función `grViz()` se utiliza para crear un diagrama "Graphviz". Esta función acepta una *cadena de caracteres de entrada que contiene las instrucciones* para hacer el diagrama. Dentro de esa cadena, las instrucciones están escritas en un lenguaje diferente, llamado [DOT](https://graphviz.org/doc/info/lang.html) - es bastante fácil aprender lo básico.

**Estructura básica**

1)  Abre las instrucciones `grViz("`
2)  Especifica la dirección y el nombre del gráfico, y abre los paréntesis, por ejemplo, `digraph my_flow_chart {`
3)  Define los elementos del gráfico (layout, rank direction)
4)  Establece los nodos (create nodes)
5)  Establece las conexiones entre nodos
6)  Cierra las instrucciones `}")`

### Ejemplos sencillos {.unnumbered} 

A continuación, dos sencillos ejemplos

Un ejemplo mínimo:

```{r out.width='50%'}
# A minimal plot
DiagrammeR::grViz("digraph {
  
graph[layout = dot, rankdir = LR]

a
b
c

a -> b -> c
}")
```

Un ejemplo con un contexto de salud pública quizás más aplicado:

```{r out.width='50%'}
grViz("                           # All instructions are within a large character string
digraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,
         overlap = true,
         fontsize = 10]
  
  # nodes
  #######
  node [shape = circle,           # shape = circle
       fixedsize = true
       width = 1.3]               # width of circles
  
  Primary                         # names of nodes
  Secondary
  Tertiary

  # edges
  #######
  Primary   -> Secondary [label = ' case transfer']
  Secondary -> Tertiary [label = ' case transfer']
}
")
```

### Sintaxis  {.unnumbered}

**Sintaxis básica**

Los nombres de los nodos, o las etiquetas de las conexiones (edges), pueden separarse con espacios, punto y coma o nuevas líneas.

**Dirección del rango**

Se puede reorientar un gráfico para que se mueva de izquierda a derecha ajustando el argumento `rankdir` dentro de la sentencia del gráfico. El valor predeterminado es `TB` (top-bottom, de arriba a abajo), pero puede ser `LR` (Left-Right, de izquierda a derecha), `RL` o `BT`.

**Nombres de los nodos**

Los nombres de los nodos pueden ser palabras sueltas, como en el sencillo ejemplo anterior. Para utilizar nombres con varias palabras o caracteres especiales (por ejemplo, paréntesis, guiones), pon el nombre del nodo entre comillas simples (' '). Puede ser más fácil tener un nombre de nodo corto, y asignar una *etiqueta* como se muestra a continuación entre corchetes `[ ]`. Si quieres tener una nueva línea dentro del nombre del nodo, debes hacerlo a través de una etiqueta - utiliza `\n` en la etiqueta del nodo entre comillas simples, como se muestra a continuación.

**Subgrupos** 

Al definir las conexiones (aristas), se pueden crear subgrupos a ambos lados de la arista con corchetes (`{ }`). La arista se aplica entonces a todos los nodos en el corchete - es una forma abreviada.

**Diseños** 

* dot (establecer `rankdir` entre TB, LR, RL, BT, )
* neato  
* twopi  
* circo  


**Nodos - atributos editables**

* `label` (texto, entre comillas simples si es de varias palabras)  
* `fillcolor` (muchos colores posibles)  
* `fontcolor`  (color de la fuente)
* `alpha` (transparencia 0-1)  
* `shape` (ellipse, oval, diamond, egg, plaintext, point, square, triangle)  
* `style` (estilo)
* `sides`  (lados)
* `peripheries`  (periferia)
* `fixedsize` (h x w)  (tamaño fijo (alto x ancho))
* `height`  (alto)
* `width`  (ancho)
* `distortion`  (dstorsión)
* `penwidth` (ancho del borde de la forma)  
* `x` (left/right) (desplazamiento a la izquierda/derecha)
* `y` (up/down)  (desplazamiento arriba/abajo)
* `fontname`  (nombre de la fuente)
* `fontsize`  (tamaño de letra)
* `icon`  


**Conexioness - atributos editables**

* `arrowsize`  (tamaño de la flecha)
* `arrowhead` (normal, box, crow, curve, diamond, dot, inv, none, tee, vee)  
* `arrowtail`  (cola de flecha)
* `dir` (dirección, )  
* `style` (guiones,  ...)  
* `color`  
* `alpha`  
* `headport` (texto delante de la punta de la flecha)  
* `tailport` (texto detrás de la cola de flecha) 
* `fontname`  (nombre de la fuente)
* `fontsize`  (tamaño de letra)
* `fontcolor`  (color de la fuente)
* `penwidth` (anchura de la flecha)  
* `minlen` (longitud mínima)

**Nombres de los colores**: valores hexadecimales o nombres de colores "X11", véase [aquí para los detalles de X11](http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html)


### Ejemplos complejos  {.unnumbered}


El siguiente ejemplo amplía el surveillance_diagram, añadiendo nombres de nodos complejos, conexiones agrupadas, colores y estilos


```
DiagrammeR::grViz("               # All instructions are within a large character string
digraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            # layout top-to-bottom
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,           # shape = circle
       fixedsize = true
       width = 1.3]                      
  
  Primary   [label = 'Primary\nFacility'] 
  Secondary [label = 'Secondary\nFacility'] 
  Tertiary  [label = 'Tertiary\nFacility'] 
  SC        [label = 'Surveillance\nCoordination',
             fontcolor = darkgreen] 
  
  # edges
  #######
  Primary   -> Secondary [label = ' case transfer',
                          fontcolor = red,
                          color = red]
  Secondary -> Tertiary [label = ' case transfer',
                          fontcolor = red,
                          color = red]
  
  # grouped edge
  {Primary Secondary Tertiary} -> SC [label = 'case reporting',
                                      fontcolor = darkgreen,
                                      color = darkgreen,
                                      style = dashed]
}
")
```


```{r out.width='50%', echo=F}
DiagrammeR::grViz("               # All instructions are within a large character string
digraph surveillance_diagram {    # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            # layout top-to-bottom
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,           # shape = circle
       fixedsize = true
       width = 1.3]                      
  
  Primary   [label = 'Primary\nFacility'] 
  Secondary [label = 'Secondary\nFacility'] 
  Tertiary  [label = 'Tertiary\nFacility'] 
  SC        [label = 'Surveillance\nCoordination',
             fontcolor = darkgreen] 
  
  # edges
  #######
  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red,
                          color = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red,
                          color = red]
  
  # grouped edge
  {Primary Secondary Tertiary} -> SC [label = 'case reporting',
                                      fontcolor = darkgreen,
                                      color = darkgreen,
                                      style = dashed]
}
")
```

**Agrupaciones de subgráficos**

Para agrupar los nodos en clústeres de cajas, ponlos dentro del mismo subgrafo (`subgraph name {}`). Para que cada subgrafo se identifique dentro de una caja delimitadora, comienza el nombre del subgrafo con "cluster", como se muestra con las 4 cajas de abajo.


```
DiagrammeR::grViz("             # All instructions are within a large character string
digraph surveillance_diagram {  # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            
         overlap = true,
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,                  # shape = circle
       fixedsize = true
       width = 1.3]                      # width of circles
  
  subgraph cluster_passive {
    Primary   [label = 'Primary\nFacility'] 
    Secondary [label = 'Secondary\nFacility'] 
    Tertiary  [label = 'Tertiary\nFacility'] 
    SC        [label = 'Surveillance\nCoordination',
               fontcolor = darkgreen] 
  }
  
  # nodes (boxes)
  ###############
  node [shape = box,                     # node shape
        fontname = Helvetica]            # text font in node
  
  subgraph cluster_active {
    Active [label = 'Active\nSurveillance'] 
    HCF_active [label = 'HCF\nActive Search']
  }
  
  subgraph cluster_EBD {
    EBS [label = 'Event-Based\nSurveillance (EBS)'] 
    'Social Media'
    Radio
  }
  
  subgraph cluster_CBS {
    CBS [label = 'Community-Based\nSurveillance (CBS)']
    RECOs
  }

  
  # edges
  #######
  {Primary Secondary Tertiary} -> SC [label = 'case reporting']

  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red]
  
  HCF_active -> Active
  
  {'Social Media' Radio} -> EBS
  
  RECOs -> CBS
}
")

```


```{r out.width='120%', echo=F}
DiagrammeR::grViz("             # All instructions are within a large character string
digraph surveillance_diagram {  # 'digraph' means 'directional graph', then the graph name 
  
  # graph statement
  #################
  graph [layout = dot,
         rankdir = TB,            
         overlap = true,
         fontsize = 10]
  

  # nodes (circles)
  #################
  node [shape = circle,                  # shape = circle
       fixedsize = true
       width = 1.3]                      # width of circles
  
  subgraph cluster_passive {
    Primary   [label = 'Primary\nFacility'] 
    Secondary [label = 'Secondary\nFacility'] 
    Tertiary  [label = 'Tertiary\nFacility'] 
    SC        [label = 'Surveillance\nCoordination',
               fontcolor = darkgreen] 
  }
  
  # nodes (boxes)
  ###############
  node [shape = box,                     # node shape
        fontname = Helvetica]            # text font in node
  
  subgraph cluster_active {
    Active [label = 'Active\nSurveillance'] 
    HCF_active [label = 'HCF\nActive Search']
  }
  
  subgraph cluster_EBD {
    EBS [label = 'Event-Based\nSurveillance (EBS)'] 
    'Social Media'
    Radio
  }
  
  subgraph cluster_CBS {
    CBS [label = 'Community-Based\nSurveillance (CBS)']
    RECOs
  }

  
  # edges
  #######
  {Primary Secondary Tertiary} -> SC [label = 'case reporting']

  Primary   -> Secondary [label = 'case transfer',
                          fontcolor = red]
  Secondary -> Tertiary [label = 'case transfer',
                          fontcolor = red]
  
  HCF_active -> Active
  
  {'Social Media' Radio} -> EBS
  
  RECOs -> CBS
}
")

```


**Formas de los nodos**

El siguiente ejemplo, tomado de [este tutorial](http://rich-iannone.github.io/DiagrammeR/), muestra las formas de los nodos aplicados y una abreviatura de las conexiones de los bordes en serie

```{r out.width='75%'}
DiagrammeR::grViz("digraph {

graph [layout = dot, rankdir = LR]

# define the global styles of the nodes. We can override these in box if we wish
node [shape = rectangle, style = filled, fillcolor = Linen]

data1 [label = 'Dataset 1', shape = folder, fillcolor = Beige]
data2 [label = 'Dataset 2', shape = folder, fillcolor = Beige]
process [label =  'Process \n Data']
statistical [label = 'Statistical \n Analysis']
results [label= 'Results']

# edge definitions with the node IDs
{data1 data2}  -> process -> statistical -> results
}")
```


### Salidas  {.unnumbered}

Cómo manejar y guardar las salidas

* Las salidas aparecerán en el panel del Visor de RStudio, por defecto en la parte inferior derecha junto a Files, Plots, Packages, y Help.
* Para exportarlos puedes "Save as image" o "Copy to clipboard" desde el Visor. El gráfico se ajustará al tamaño especificado.




### Figuras parametrizadas {.unnumbered} 

Esta es una cita a este tutorial: https://mikeyharper.uk/flowcharts-in-r-using-diagrammer/

"Figuras parametrizadas": Una gran ventaja de diseñar figuras dentro de R es que podemos conectar las figuras directamente con nuestro análisis leyendo los valores de R directamente en nuestros diagramas de flujo. Por ejemplo, imagina que has creado un proceso de filtrado que elimina valores después de cada etapa de un proceso, puedes hacer que una figura muestre el número de valores que quedan en el conjunto de datos después de cada etapa de su proceso. Para hacer esto, puedes utilizar el símbolo `@@X` directamente dentro de la figura, y luego hacer referencia a esto en el pie de página del gráfico utilizando `[X]`:, donde X es el índice numérico único".

Te animamos a revisar este tutorial si te interesa la parametrización.


<!-- And below is some example code from this tutorial. -->

<!-- ```{r, eval=F} -->
<!-- # Define some sample data -->
<!-- data <- list(a=1000, b=800, c=600, d=400) -->


<!-- DiagrammeR::grViz(" -->
<!-- digraph graph2 { -->

<!-- graph [layout = dot] -->

<!-- # node definitions with substituted label text -->
<!-- node [shape = rectangle, width = 4, fillcolor = Biege] -->
<!-- a [label = '@@1'] -->
<!-- b [label = '@@2'] -->
<!-- c [label = '@@3'] -->
<!-- d [label = '@@4'] -->

<!-- a -> b -> c -> d -->

<!-- } -->

<!-- [1]:  paste0('Raw Data (n = ', data$a, ')') -->
<!-- [2]: paste0('Remove Errors (n = ', data$b, ')') -->
<!-- [3]: paste0('Identify Potential Customers (n = ', data$c, ')') -->
<!-- [4]: paste0('Select Top Priorities (n = ', data$d, ')') -->
<!-- ") -->

<!-- ``` -->



<!-- ### CONSORT diagram  {.unnumbered} -->

<!-- THIS SECTION IS UNDER CONSTRUCTION   -->

<!-- https://scriptsandstatistics.wordpress.com/2017/12/22/how-to-draw-a-consort-flow-diagram-using-r-and-graphviz/ -->

<!-- Note above is out of date via DiagrammeR -->




<!-- ======================================================= -->
## Diagramas Aluviales/Sankey {#alluvialsankey-diagrams}

### Cargar paquetes {.unnumbered}  

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

Cargamos el paquete **networkD3** para producir el diagrama, y también **tidyverse** para los pasos de preparación de datos.

```{r}
pacman::p_load(
  networkD3,
  tidyverse)
```

### Trazado desde los datos {.unnumbered} 

Trazado de las conexiones en unos datos. A continuación mostramos el uso de este paquete con `linelist` Aquí hay un [tutorial en línea](https://www.r-graph-gallery.com/321-introduction-to-interactive-sankey-diagram-2.html).

Comenzamos obteniendo los recuentos de casos para cada combinación única de categoría de edad y hospital. Hemos eliminado los valores con categoría de edad ausente para mayor claridad. También reetiquetamos las columnas `hospital` y `age_cat` como `source` y `target` respectivamente. Estos serán los dos lados del diagrama aluvial.

```{r}
# counts by hospital and age category
links <- linelist %>% 
  drop_na(age_cat) %>% 
  select(hospital, age_cat) %>%
  count(hospital, age_cat) %>% 
  rename(source = hospital,
         target = age_cat)
```

El conjunto de datos tiene ahora este aspecto: 

```{r message=FALSE, echo=F}
DT::datatable(links, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


Ahora creamos un dataframe de todos los nodos del diagrama, bajo la columna `name`. Esto consiste en todos los valores de `hospital` y `age_cat.` Observa que nos aseguramos de que todos son de tipo carácter antes de combinarlos. Ajustamos las columnas ID para que sean números en lugar de etiquetas:

```{r}
# The unique node names
nodes <- data.frame(
  name=c(as.character(links$source), as.character(links$target)) %>% 
    unique()
  )

nodes  # print
```
A continuación editamos el dataframe `links`, que hemos creado anteriormente con `count()`. Añadimos dos columnas numéricas `IDsource` e `IDtarget` que reflejarán/crearán los enlaces entre los nodos. Estas columnas contendrán los números de ruta (posición) de los nodos de origen y destino. Se resta 1 para que estos números de posición comiencen en 0 (no en 1). 

```{r}
# match to numbers, not names
links$IDsource <- match(links$source, nodes$name)-1 
links$IDtarget <- match(links$target, nodes$name)-1
```

El conjunto de datos `links` tiene ahora este aspecto:

```{r message=FALSE, echo=F}
DT::datatable(links, rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```

Ahora traza el diagrama Sankey con `sankeyNetwork()`. Puedes leer más sobre cada argumento ejecutando `?sankeyNetwork` en la consola. Ten en cuenta que a menos que establezcas `iterations = 0` el orden de los nodos puede no ser el esperado.


```{r}

# plot
######
p <- sankeyNetwork(
  Links = links,
  Nodes = nodes,
  Source = "IDsource",
  Target = "IDtarget",
  Value = "n",
  NodeID = "name",
  units = "TWh",
  fontSize = 12,
  nodeWidth = 30,
  iterations = 0)        # ensure node order is as in data
p
```



Este es un ejemplo en el que también se incluye el resultado del paciente. Obsérva que en el paso de preparación de los datos tenemos que calcular los recuentos de casos entre la edad y el hospital, y por separado entre el hospital y el resultado - y luego unir todos estos recuentos con `bind_rows()`.

```{r}
# counts by hospital and age category
age_hosp_links <- linelist %>% 
  drop_na(age_cat) %>% 
  select(hospital, age_cat) %>%
  count(hospital, age_cat) %>% 
  rename(source = age_cat,          # re-name
         target = hospital)

hosp_out_links <- linelist %>% 
    drop_na(age_cat) %>% 
    select(hospital, outcome) %>% 
    count(hospital, outcome) %>% 
    rename(source = hospital,       # re-name
           target = outcome)

# combine links
links <- bind_rows(age_hosp_links, hosp_out_links)

# The unique node names
nodes <- data.frame(
  name=c(as.character(links$source), as.character(links$target)) %>% 
    unique()
  )

# Create id numbers
links$IDsource <- match(links$source, nodes$name)-1 
links$IDtarget <- match(links$target, nodes$name)-1

# plot
######
p <- sankeyNetwork(Links = links,
                   Nodes = nodes,
                   Source = "IDsource",
                   Target = "IDtarget",
                   Value = "n",
                   NodeID = "name",
                   units = "TWh",
                   fontSize = 12,
                   nodeWidth = 30,
                   iterations = 0)
p

```


https://www.displayr.com/sankey-diagrams-r/



<!-- ======================================================= -->
## Calendario de eventos {#event-timelines}

Para hacer una línea de tiempo que muestre eventos específicos, puedes utilizar el paquete **vistime**.

Mira esta [viñeta](https://cran.r-project.org/web/packages/vistime/vignettes/vistime-vignette.html#ex.-2-project-planning)

```{r}
# load package
pacman::p_load(vistime,  # make the timeline
               plotly    # for interactive visualization
               )
```

```{r, echo=F}
# reference: https://cran.r-project.org/web/packages/vistime/vignettes/vistime-vignette.html#ex.-2-project-planning

data <- read.csv(text="event, group, start, end, color
                       Event 1, Group A,2020-01-22,2020-01-22, #90caf9
                       Event 1, Group B,2020-01-23,2020-01-23, #90caf9
                       Event 1, Group C,2020-01-23,2020-01-23, #1565c0
                       Event 1, Group D,2020-01-25,2020-01-25, #f44336
                       Event 1, Group E,2020-01-25,2020-01-25, #90caf9
                       Event 1, Group F,2020-01-26,2020-01-26, #8d6e63
                       Event 1, Group G,2020-01-27,2020-01-27, #1565c0
                       Event 1, Group H,2020-01-27,2020-01-27, #90caf9
                       Event 1, Group I,2020-01-27,2020-01-27,#90a4ae
                       Event 2, Group A,2020-01-28,2020-01-28,#fc8d62
                       Event 2, Group C,2020-01-28,2020-01-28, #6a3d9a
                       Event 2, Group J,2020-01-28,2020-01-28, #90caf9
                       Event 2, Group J,2020-01-28,2020-01-28, #fc8d62
                       Event 2, Group J,2020-01-28,2020-01-28, #1565c0
")
```

Este es el conjunto de datos de eventos con el que comenzamos:  

```{r message=FALSE, echo=F}
DT::datatable(data, rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```



```{r}
p <- vistime(data)    # apply vistime

library(plotly)

# step 1: transform into a list
pp <- plotly_build(p)

# step 2: Marker size
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "markers") pp$x$data[[i]]$marker$size <- 10
}

# step 3: text size
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "text") pp$x$data[[i]]$textfont$size <- 10
}


# step 4: text position
for(i in 1:length(pp$x$data)){
  if(pp$x$data[[i]]$mode == "text") pp$x$data[[i]]$textposition <- "right"
}

#print
pp

```



<!-- ======================================================= -->
## DAGs {#dags}

Puedes construir un DAG manualmente utilizando el paquete **DiagammeR** y el lenguaje DOT como se ha descrito anteriormente.

Como alternativa, existen paquetes como **ggdag** y **daggity**

[Viñeta de Introducción a los DAGs ggdag](https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html)   

[Inferencia causal con dags en R](https://www.r-bloggers.com/2019/08/causal-inference-with-dags-in-r/#:~:text=In%20a%20DAG%20all%20the,for%20drawing%20and%20analyzing%20DAGs.)  





<!-- ======================================================= -->
## Recursos {#resources-28}


Gran parte de lo anterior sobre el lenguaje DOT está adaptado del tutorial [de este sitio](https://mikeyharper.uk/flowcharts-in-r-using-diagrammer/)

Otro [tutorial más detallado sobre DiagammeR](http://rich-iannone.github.io/DiagrammeR/)

Esta página sobre [los diagramas de Sankey](https://www.displayr.com/sankey-diagrams-r/)




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/diagrams.Rmd-->


# Análisis de combinaciones {#combinations-analysis}

```{r echo=F, out.width= "75%", warning=F, message=F}
pacman::p_load(tidyverse,
               UpSetR,
               ggupset)

# Adds new symptom variables to the linelist, with random "yes" or "no" values 
linelist_sym <- linelist %>% 
  mutate(fever  = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.80, 0.20)),
         chills = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.20, 0.80)),
         cough  = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.9, 0.15)),
         aches  = sample(c("yes", "no"), nrow(linelist), replace = T, prob = c(0.10, 0.90)),
         vomit = sample(c("yes", "no"), nrow(linelist), replace = T))

linelist_sym_2 <- linelist_sym %>% 
     
  #mutate(fever = ifelse(fever == "yes", colnames(linelist)[which(colnames(linelist) == "fever")]))
   mutate(across(.cols = c(fever, chills, cough, aches, vomit),
                 .fns = ~+(.x == "yes")))   

     
  #mutate(across(c("fever", "chills", "cough", "aches", "vomit"), ~ifelse(.x = "yes", colnames(.)[which(colnames(.) == "fever")], 0)))   
  

# Make the plot
UpSetR::upset(
  select(linelist_sym_2, fever, chills, cough, aches, vomit),
  sets = c("fever", "chills", "cough", "aches", "vomit"),
  order.by = "freq",
  sets.bar.color = c("blue", "red", "yellow", "darkgreen", "orange"), # optional colors
  empty.intersections = "on",
  # nsets = 3,
  number.angles = 0,
  point.size = 3.5,
  line.size = 2, 
  mainbar.y.label = "Symptoms Combinations",
  sets.x.label = "Patients with Symptom")

```



Este análisis representa la frecuencia de diferentes **combinaciones** de valores/respuestas. En este ejemplo, se representa la frecuencia con la que los casos mostraron varias combinaciones de síntomas.

Este análisis también se suele llamar:

* **"Análisis de respuesta múltiple"**
* **"Análisis de conjuntos"**
* **"Análisis de combinaciones"**

En el ejemplo del gráfico anterior, se muestran cinco síntomas. Debajo de cada barra vertical hay una línea y puntos que indican la combinación de síntomas que refleja la barra de arriba. A la derecha, las barras horizontales reflejan la frecuencia de cada síntoma individual.

El primer método que mostramos utiliza el paquete **ggupset**, y el segundo utiliza el paquete **UpSetR**.




  



<!-- ======================================================= -->
## Preparación {#preparation-29}

### Cargar paquetes {.unnumbered}

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base**. Consulta la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r, warning=F, message=F}
pacman::p_load(
  tidyverse,     # data management and visualization
  UpSetR,        # special package for combination plots
  ggupset)       # special package for combination plots
```

<!-- ======================================================= -->
### Importar datos {.unnumbered}  


Para empezar, importamos la lista de casos limpia de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica aquí para descargar linelist "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds), (como archivo .rds). Importa los datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - ver la página de [importación y exportación](#import-and-export) para más detalles).


```{r, echo=F}
# import the linelist into R
linelist_sym <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import case linelist 
linelist_sym <- import("linelist_cleaned.rds")
```


Linelist incluye cinco variables "yes/no" sobre los síntomas declarados. Tendremos que transformar un poco estas variables para utilizar el paquete **ggupset** para hacer nuestro gráfico. Para ver los datos desplázate a la derecha para ver las variables de los síntomas).

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist_sym, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```



<!-- ======================================================= -->
### Reformular los valores {.unnumbered}  

Para alinearse con el formato esperado por **ggupset**, convertimos el "yes" y el "no" en el nombre real del síntoma, utilizando `case_when()` de **dplyr**. Si "no", establecemos el valor en blanco, por lo que los valores son NA o el síntoma.
 

```{r, warning=F, message=F}
# create column with the symptoms named, separated by semicolons
linelist_sym_1 <- linelist_sym %>% 
    # convert the "yes" and "no" values into the symptom name itself
    # if old value is "yes", new value is "fever", otherwise set to missing (NA)
     mutate(fever = ifelse(fever == "yes", "fever", NA), 
            chills = ifelse(chills == "yes", "chills", NA),
            cough = ifelse(cough == "yes", "cough", NA),
            aches = ifelse(aches == "yes", "aches", NA),
            vomit = ifelse(vomit == "yes", "vomit", NA))
```

Ahora hacemos dos columnas finales:

1.  Concatenando (pegar) todos los síntomas del paciente (una columna de caracteres)
2.  Conviertiendo la columna anterior en una de tipo *list*, para que pueda ser aceptada por **ggupset** para hacer la trama

Consulta la página sobre [Caracteres y cadenas](#characters-and-strings) para saber más sobre la función `unite()` de **stringr**

```{r, warning=F, message=F}
linelist_sym_1 <- linelist_sym_1 %>% 
  unite(col = "all_symptoms",
        c(fever, chills, cough, aches, vomit), 
        sep = "; ",
        remove = TRUE,
        na.rm = TRUE) %>% 
  mutate(
    # make a copy of all_symptoms column, but of class "list" (which is required to use ggupset() in next step)
    all_symptoms_list = as.list(strsplit(all_symptoms, "; "))
    )
```

En los datos nuevos observa las dos columnas del extremo derecho: los valores combinados pegados y la lista

```{r, echo=F, , warning=F, message=F}
DT::datatable(head(linelist_sym_1,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap')
```


<!-- ======================================================= -->
## **ggupset** {#ggupset}

Carga el paquete

```{r}
pacman::p_load(ggupset)
```


Crear el gráfico. Comenzamos con `ggplot()` y `geom_bar()`, pero luego añadimos la función especial `scale_x_upset()` de **ggupset**.  

```{r, warning=F, message=F}
ggplot(
  data = linelist_sym_1,
  mapping = aes(x = all_symptoms_list)) +
geom_bar() +
scale_x_upset(
  reverse = FALSE,
  n_intersections = 10,
  sets = c("fever", "chills", "cough", "aches", "vomit"))+
labs(
  title = "Signs & symptoms",
  subtitle = "10 most frequent combinations of signs and symptoms",
  caption = "Caption here.",
  x = "Symptom combination",
  y = "Frequency in dataset")

```
  
Puedes encontrar más información sobre ggupset [en línea](https://rdrr.io/cran/ggupset/man/scale_x_upset.html) o fuera de línea en la documentación del paquete en su pestaña de Ayuda de RStudio `?ggupset`.


<!-- ======================================================= -->
## `UpSetR` {#upsetr}

El paquete **UpSetR** permite una mayor personalización del gráfico, pero puede ser más difícil de ejecutar:


**Cargar paquete**

```{r}
pacman::p_load(UpSetR)
```

**Limpieza de datos**

Debemos convertir los valores de los síntomas de `linelist` en 1 / 0.

```{r}
linelist_sym_2 <- linelist_sym %>% 
     # convert the "yes" and "no" values into 1s and 0s
     mutate(fever = ifelse(fever == "yes", 1, 0), 
            chills = ifelse(chills == "yes", 1, 0),
            cough = ifelse(cough == "yes", 1, 0),
            aches = ifelse(aches == "yes", 1, 0),
            vomit = ifelse(vomit == "yes", 1, 0))
```

Si está interesado en un comando más eficiente, puede aprovechar la función `+()`, que convierte en 1s y 0s basándose en una sentencia lógica. Este comando utiliza la función `across()` para cambiar varias columnas a la vez (lea más en [Limpieza de datos y funciones básicas](#clean_across)).  

```{r, eval=F, echo=T}
# Efficiently convert "yes" to 1 and 0
linelist_sym_2 <- linelist_sym %>% 
  
  # convert the "yes" and "no" values into 1s and 0s
  mutate(across(c(fever, chills, cough, aches, vomit), .fns = ~+(.x == "yes")))
```


Ahora haz el gráfico usando la función personalizada `upset()` - utilizando sólo las columnas de síntomas. Debes designar qué "conjuntos" comparar (los nombres de las columnas de síntomas). Alternativamente, utiliza `nsets = ` y `order.by = "freq"` para mostrar sólo las X combinaciones principales.

```{r, warning=F, message=F}
# Make the plot
linelist_sym_2 %>% 
  UpSetR::upset(
       sets = c("fever", "chills", "cough", "aches", "vomit"),
       order.by = "freq",
       sets.bar.color = c("blue", "red", "yellow", "darkgreen", "orange"), # optional colors
       empty.intersections = "on",
       # nsets = 3,
       number.angles = 0,
       point.size = 3.5,
       line.size = 2, 
       mainbar.y.label = "Symptoms Combinations",
       sets.x.label = "Patients with Symptom")

```


<!-- ======================================================= -->
## Recursos {resources-29}

[La página de github de UpSetR](https://github.com/hms-dbmi/UpSetR)

[Una versión de app Shiny: puedes cargar tus propios datos](https://gehlenborglab.shinyapps.io/upsetr/)

[*documentación - difícil de interpretar](https://cran.r-project.org/web/packages/UpSetR/UpSetR.pdf)


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/combination_analysis.Rmd-->


# Cadenas de transmisión {#transmission-chains}

<!-- ======================================================= -->
## Resumen {#overview-7}

La principal herramienta para manejar, analizar y visualizar las cadenas de transmisión y los datos de rastreo de contactos es el paquete **epicontacts**, desarrollado por la gente de RECON. Prueba con el gráfico interactivo que se muestra a continuación pasando el cursor por encima de los nodos para obtener más información, arrastrándolos para moverlos y clicando sobre ellos para resaltar los casos posteriores.

```{r out.width=c('25%', '25%'), fig.show='hold', echo=F}

## install development version of epicontacts
if(
  !"epicontacts" %in% rownames(installed.packages()) |
  packageVersion("epicontacts") != "1.2.0"
) remotes::install_github("reconhub/epicontacts@timelines") #jfmont

## install and load packages
pacman::p_load(tidyverse, epicontacts, magrittr, here, webshot, visNetwork)

## load linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds")) %>%
  filter(!duplicated(case_id))


## generate contacts
contacts <- linelist %>%
  transmute(
    from = infector,
    to = case_id,
    location = sample(c("Community", "Nosocomial"), n(), TRUE),
    duration = sample.int(10, n(), TRUE)
  ) %>%
  drop_na(from)

## generate epicontacts
epic <- epicontacts::make_epicontacts(
  linelist = linelist,
  contacts = contacts, 
  directed = TRUE
)

## subset object
epic %<>% subset(
  node_attribute = list(date_onset = c(as.Date(c("2014-06-01", "2014-07-01"))))
) %>%
  thin("contacts")

## plot with date of onset as x-axis
plot(
  epic,
  x_axis = "date_onset",
  label = FALSE,
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  node_shape = "gender",
  shapes = c(f = "female", m = "male"),
  unlinked_pos = "bottom",
  date_labels = "%b %d %Y",
  node_size = 35,
  font_size = 20,
  arrow_size = 0.5,
  height = 800,
  width = 700,
  edge_linetype = "location",
  legend_width = 0.15,
  highlight_downstream = TRUE,
  selector = FALSE
)

```

<!-- ======================================================= -->
## Preparación {#preparation-30}

### Cargar paquetes {.unnumbered}  

Primero carga los paquetes estándar necesarios para la importación y manipulación de datos. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También se pueden cargar paquetes con `library()` desde R **base**. Consulta la página [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.
 
	
```{r transmission_chains_packages, eval = FALSE}
pacman::p_load(
   rio,          # File import
   here,         # File locator
   tidyverse,    # Data management + ggplot2 graphics
   remotes       # Package installation from github
)
```
	
Necesitarás la versión de desarrollo de **epicontacts**, que puede instalarse desde github utilizando la función `p_install_github()` de **pacman**. Sólo necesitas ejecutar este comando una vez, no cada vez que utilizas el paquete (a partir de entonces, puedes utilizar sólo `p_load()`).

```{r transmission_chains_epicontacts_install, eval = FALSE}
pacman::p_install_gh("reconhub/epicontacts@timeline")
```


### Importar datos {.unnumbered}

Importamos el conjunto de datos de casos de una epidemia de ébola simulada. Si deseas descargar los datos para seguirlos paso a paso, consulta las instrucciones en la página de [descargando el manual y los datos](#download-handbook-and-data). El conjunto de datos se importa utilizando la función `import()` del paquete **rio**. Consulta la página sobre [importación y exportación](#import-and-export) para conocer las distintas formas de importar datos.

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.xlsx")
```

A continuación se muestran las primeras 50 filas del listado. Son especialmente interesantes las columnas `case_id`, `generation`, `infector`, y `source`.    

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


### Creación de un objeto epicontacts {.unnumbered}

A continuación, tenemos que crear un objeto **epicontacts**, que requiere dos tipos de datos:

* un listado de casos en los que las columnas son variables y las filas corresponden a casos únicos
* una lista de bordes que definen los vínculos entre los casos sobre la base de sus identificadores únicos (pueden ser contactos, eventos de transmisión, etc.)

Como ya tenemos un listado, sólo tenemos que crear una lista de aristas entre los casos, más concretamente entre sus ID. Podemos extraer los enlaces de transmisión del listado vinculando la columna `infector` con la columna `case_id`. En este punto también podemos añadir "propiedades de borde", con lo que nos referimos a cualquier variable que describa el vínculo entre los dos casos, no los casos en sí. Por ejemplo, añadiremos una variable `location` que describa la ubicación del evento de transmisión, y una variable de duración que describa la duración del contacto en días.

En el código siguiente, la función `transmute()` de **dplyr** es similar a `mutate`, excepto que sólo mantiene las columnas que hemos especificado dentro de la función. La función `drop_na` filtrará cualquier fila en la que las columnas especificadas tengan un valor `NA`; en este caso, sólo queremos mantener las filas en las que se conoce el infector.

```{r transmission_chains_create_contacts,}
## generate contacts
contacts <- linelist %>%
  transmute(
    infector = infector,
    case_id = case_id,
    location = sample(c("Community", "Nosocomial"), n(), TRUE),
    duration = sample.int(10, n(), TRUE)
  ) %>%
  drop_na(infector)
```

Ahora podemos crear el objeto **epicontacts** utilizando la función `make_epicontacts`. Necesitamos especificar qué columna del listado apunta al identificador único del caso, así como qué columnas de los contactos apuntan a los identificadores únicos de los casos involucrados en cada enlace. Estos enlaces son direccionales en el sentido de que la infección va *del* infector *al* caso, por lo que necesitamos especificar los argumentos `from` y `to`. Por lo tanto, también establecemos el argumento `directed` a `TRUE`, que afectará a las operaciones futuras.

```{r transmission_chains_create_epicontacts,}
## generate epicontacts object
epic <- make_epicontacts(
  linelist = linelist,
  contacts = contacts,
  id = "case_id",
  from = "infector",
  to = "case_id",
  directed = TRUE
)
```
Al examinar los objetos **epicontacts**, podemos ver que la columna `case_id` del listado ha sido renombrada a `id` y las columnas `case_id` e `infector` de los contactos han sido renombradas a `from` y `to`. Esto garantiza la coherencia en las operaciones posteriores de manipulación, visualización y análisis.

```{r transmission_chains_view_epicontacts,}
## view epicontacts object
epic
```

<!-- ======================================================= -->
## Manipulación {#handling}

### Subconjunto {.unnumbered}

El método `subset()` para los objetos `epicontacts` permite, entre otras cosas, filtrar las redes en función de las propiedades del listado ("atributos de nodos") y de la base de datos de contactos ("atributos de aristas"). Estos valores deben pasarse como listas con nombre al argumento respectivo. Por ejemplo, en el código que sigue mantenemos en linelist sólo los casos masculinos que tienen una fecha de infección entre abril y julio de 2014 (las fechas se especifican como rangos), y los enlaces de transmisión que se produjeron en el hospital.

```{r transmission_chains_subset_nodes,}
sub_attributes <- subset(
  epic,
  node_attribute = list(
    gender = "m",
    date_infection = as.Date(c("2014-04-01", "2014-07-01"))
  ), 
  edge_attribute = list(location = "Nosocomial")
)
sub_attributes
```

Podemos utilizar la función `thin` para filtrar linelist para incluir los casos que se encuentran en los contactos estableciendo el argumento `what = "linelist"`, o filtrar los contactos para incluir los casos que se encuentran en linelist estableciendo el argumento `what = "contacts"`. En el código siguiente, filtramos aún más el objeto epicontactos para mantener sólo los enlaces de transmisión que implican los casos masculinos infectados entre abril y julio que habíamos filtrado anteriormente. Podemos ver que sólo dos enlaces de transmisión conocidos se ajustan a esa especificación.

```{r transmission_chains_thin,}
sub_attributes <- thin(sub_attributes, what = "contacts")
nrow(sub_attributes$contacts)
```

Además de la subdivisión por atributos de nodos y aristas, las redes pueden podarse para incluir sólo los componentes que están conectados a ciertos nodos. El argumento `cluster_id` toma un vector de IDs de casos y devuelve linelist de individuos que están vinculados, directa o indirectamente, a esos IDs. En el código siguiente, podemos ver que un total de 13 casos del listado están involucrados en los clusters que contienen `2ae019` y `71577a`.

```{r}
sub_id <- subset(epic, cluster_id = c("2ae019","71577a"))
nrow(sub_id$linelist)
```

El método `subset()` para los objetos `epicontacts` también permite filtrar por tamaño de cluster usando los argumentos `cs`, `cs_min` y `cs_max`. En el código siguiente, estamos manteniendo sólo los casos vinculados a clusters de 10 casos o más, y podemos ver que 271 casos del listado están involucrados en tales clusters.
    
```{r}   
sub_cs <- subset(epic, cs_min = 10)
nrow(sub_cs$linelist)
```

### Acceso a los IDs {.unnumbered}

La función `get_id()` recupera información sobre los ID de los casos en el conjunto de datos, y puede parametrizarse como sigue:

* **linelist**: IDs en los datos del listado
* **contacts**: IDs en el conjunto de datos de los contactos ("desde" y "hasta" combinados)
* **from**: IDs en la columna "from" de los datos del contacto
* **to** los identificadores de la columna "a" de los datos de los contactos
* **all**: Las identificaciones que aparecen en cualquier parte de cualquiera de los conjuntos de datos
* **common**: identificaciones que aparecen tanto en el conjunto de datos de contactos como en linelist

Por ejemplo, ¿cuáles son las diez primeras identificaciones de los datos de contactos?

```{r transmission_chains_get_ids,}
contacts_ids <- get_id(epic, "contacts")
head(contacts_ids, n = 10)
```

¿Cuántas identificaciones se encuentran tanto en linelist como en los contactos?
```{r transmission_chains_get_both,}
length(get_id(epic, "common"))
```

<!-- ======================================================= -->
## Visualización {#visualization}

### Representación básica {.unnumbered}

Todas las visualizaciones de los objetos **epicontacts** son manejadas por la función `plot`. En primer lugar, filtraremos el objeto **epicontacts** para incluir solo los casos con fechas de inicio en junio de 2014 utilizando la función de subconjunto, y solo incluiremos los contactos vinculados a esos casos utilizando la función `thin`.
	
```{r transmission_chains_basic_plot_sub,}
## subset epicontacts object
sub <- epic %>%
  subset(
    node_attribute = list(date_onset = c(as.Date(c("2014-06-30", "2014-06-01"))))
  ) %>%
 thin("contacts")
```

A continuación, podemos crear el gráfico básico e interactivo de la siguiente manera:

```{r transmission_chains_basic_plot,}
## plot epicontacts object
plot(
  sub,
  width = 700,
  height = 700
)
```

Puedes mover los nodos arrastrándolos, pasar por encima de ellos para obtener más información y clicar sobre ellos para resaltar los casos conectados.

Hay un gran número de argumentos para modificar este gráfico. Aquí cubriremos los principales, pero consulta la documentación a través de `?vis_epicontacts` (la función a la que se llama cuando se utiliza plot en un objeto **epicontacts**) para obtener una descripción completa de los argumentos de la función.

#### Visualizar los atributos de los nodos {.unnumbered}

El color, la forma y el tamaño del nodo se pueden asignar a una columna determinada en linelist utilizando los argumentos `node_color`, `node_shape` y `node_size`. Esto es similar a la sintaxis `aes` que puede reconocer **ggplot2**.

Los colores, formas y tamaños específicos de los nodos pueden especificarse de la siguiente manera:

* **Colores** a través del argumento `col_pal`, ya sea proporcionando una lista de nombres para la especificación manual de cada color como se hace a continuación, o proporcionando una función de paleta de colores como `colorRampPalette(c("black", "red", "orange"))`, que proporcionaría un gradiente de colores entre los especificados.

* **Formas** pasando una lista con nombre al argumento `shapes`, especificando una forma para cada elemento único en la columna del listado especificada por el argumento `node_shape`. Ver en `codeawesome` las formas disponibles.

* **Tamaño** pasando un rango de tamaño de los nodos al argumento `size_range`.

Aquí un ejemplo, donde el color representa el resultado, la forma el género y el tamaño la edad:

```{r transmission_chains_node_attribute,}
plot(
  sub, 
  node_color = "outcome",
  node_shape = "gender",
  node_size = 'age',
  col_pal = c(Death = "firebrick", Recover = "green"),
  shapes = c(f = "female", m = "male"),
  size_range = c(40, 60),
  height = 700,
  width = 700
)
```

#### Visualizar los atributos de los bordes {.unnumbered}

El color, tamaño y el tipo de línea de los bordes pueden asignarse a una columna determinada del dataframe de los contactos utilizando los argumentos `edge_color`, `edge_width` y `edge_linetype`. Los colores y tamaño específicos de los bordes se pueden especificar como sigue:

* **Colores** a través del argumento `edge_col_pal`, de la misma manera que se utiliza para `col_pal`.

* **Tamaño** pasando un rango de tamaño de los nodos al argumento `width_range`.

Aquí un ejemplo:

```{r transmission_chains_edge_attribute,}

plot(
  sub, 
  node_color = "outcome",
  node_shape = "gender",
  node_size = 'age',
  col_pal = c(Death = "firebrick", Recover = "green"),
  shapes = c(f = "female", m = "male"),
  size_range = c(40, 60),
  edge_color = 'location',
  edge_linetype = 'location',
  edge_width = 'duration',
  edge_col_pal = c(Community = "orange", Nosocomial = "purple"),
  width_range = c(1, 3),
  height = 700,
  width = 700
)

```

### Eje temporal {.unnumbered}

También podemos visualizar la red a lo largo de un eje temporal asignando el argumento `x_axis` a una columna del listado. En el ejemplo siguiente, el eje-x representa la fecha de inicio de los síntomas. También hemos especificado el argumento `arrow_size` para asegurarnos que las flechas no son demasiado grandes, y hemos establecido `label = FALSE` para que la figura esté menos recargada.

```{r transmission_chains_x_axis,}
plot(
  sub,
  x_axis = "date_onset",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

Hay un gran número de argumentos adicionales para especificar aún más cómo se visualiza esta red a lo largo de un eje temporal, que puede comprobar a través de `?vis_temporal_interactive` (la función que se llama cuando se utiliza `plot` en un objeto **epicontacts** con el `x_axis` especificado). A continuación veremos algunos.

#### Especificar la forma del árbol de transmisión {.unnumbered}

Hay dos formas principales que puede adoptar el árbol de transmisión, especificadas mediante el argumento `network_shape`. La primera es una forma ramificada, como se muestra arriba, en la que un borde recto conecta dos nodos cualesquiera. Esta es la representación más intuitiva, pero puede dar lugar a la superposición de aristas en una red densamente conectada. La segunda forma es `rectangle`, que produce un árbol parecido a una filogenia. Por ejemplo:

```{r transmission_chains_rectangle,}
plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

A cada nodo del caso se le puede asignar una posición vertical única mediante el argumento `position_dodge`. La posición de los casos no conectados (es decir, sin contactos reportados) se especifica utilizando el argumento `unlinked_pos`.

```{r transmission_chains_dodge,}
plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  position_dodge = TRUE,
  unlinked_pos = "bottom",
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

La posición del nodo padre respecto a los nodos hijos puede especificarse mediante el argumento `parent_pos`. La opción por defecto es colocar el nodo padre en el centro, sin embargo puede colocarse en la parte inferior (`parent_pos = 'bottom'`) o en la parte superior (`parent_pos = 'top'`).

```{r transmission_chains_parent_pos,}
plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  parent_pos = "top",
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
)
```

#### Cómo guardar gráficos y valores {.unnumbered}

Puedes guardar un gráfico como un archivo html interactivo y autónomo con la función `visSave` del paquete **VisNetwork**:

```{r transmission_chains_save, eval=F}

plot(
  sub,
  x_axis = "date_onset",
  network_shape = "rectangle",
  node_color = "outcome",
  col_pal = c(Death = "firebrick", Recover = "green"),
  parent_pos = "top",
  arrow_size = 0.5,
  node_size = 13,
  label = FALSE,
  height = 700,
  width = 700
) %>%
  visNetwork::visSave("network.html")

```

Guardar estas salidas de red como una imagen es desafortunadamente menos fácil y requiere que guardes el archivo como un html y luego tomes una captura de pantalla de este archivo usando el paquete **webshot**. En el código siguiente, estamos convirtiendo el archivo html guardado anteriormente en un PNG:

```{r transmission_chains_webshot, eval=F}
webshot(url = "network.html", file = "network.png")
```

### Líneas de tiempo {.unnumbered}

También se pueden incluir líneas de tiempo en la red, que se representan en el eje de abscisas de cada caso. Esto puede servir para visualizar la ubicación de los casos, por ejemplo, o el tiempo hasta el resultado. Para generar una línea de tiempo, tenemos que crear un data.frame de al menos tres columnas que indiquen el ID del caso, la fecha de inicio del "evento" y la fecha de finalización del "evento". También se puede añadir cualquier número de otras columnas que luego se pueden asignar a las propiedades de los nodos y aristas de la línea de tiempo. En el código siguiente, generamos una línea de tiempo que va desde la fecha de inicio de los síntomas hasta la fecha del desenlace, y mantenemos las variables de desenlace y hospital que utilizamos para definir la forma y el color de los nodos. Ten en cuenta que puede tener más de una fila/evento de la línea de tiempo por caso, por ejemplo si un caso es transferido entre varios hospitales.

```{r transmission_chains_create_timeline,}

## generate timeline
timeline <- linelist %>%
  transmute(
    id = case_id,
    start = date_onset,
    end = date_outcome,
    outcome = outcome,
    hospital = hospital
  )

```

A continuación, pasamos el elemento de la línea de tiempo al argumento `timeline`. Podemos mapear los atributos de la línea de tiempo a los colores, formas y tamaños de los nodos de la línea de tiempo de la misma manera definida en las secciones anteriores, excepto que tenemos *dos* nodos: el nodo de inicio y el nodo final de cada línea de tiempo, que tienen argumentos separados. Por ejemplo, `tl_start_node_color` define qué columna de la línea de tiempo se asigna al color del nodo inicial, mientras que `tl_end_node_shape` define qué columna de la línea de tiempo se asigna a la forma del nodo final. También podemos asignar el color, la anchura, el tipo de línea y las etiquetas al *borde de la línea de tiempo* mediante los argumentos `tl_edge_ `.

Consulta `?vis_temporal_interactive` (la función a la que se llama cuando se traza un objeto epicontacto) para obtener documentación detallada sobre los argumentos. Cada argumento está anotado también en el código de abajo:

```{r transmission_chains_vis_timeline,}

## define shapes
shapes <- c(
  f = "female",
  m = "male",
  Death = "user-times",
  Recover = "heartbeat",
  "NA" = "question-circle"
)

## define colours
colours <- c(
  Death = "firebrick",
  Recover = "green",
  "NA" = "grey"
)

## make plot
plot(
  sub,
  ## max x coordinate to date of onset
  x_axis = "date_onset",
  ## use rectangular network shape
  network_shape = "rectangle",
  ## mape case node shapes to gender column
  node_shape = "gender",
  ## we don't want to map node colour to any columns - this is important as the
  ## default value is to map to node id, which will mess up the colour scheme
  node_color = NULL,
  ## set case node size to 30 (as this is not a character, node_size is not
  ## mapped to a column but instead interpreted as the actual node size)
  node_size = 30,
  ## set transmission link width to 4 (as this is not a character, edge_width is
  ## not mapped to a column but instead interpreted as the actual edge width)
  edge_width = 4,
  ## provide the timeline object
  timeline = timeline,
  ## map the shape of the end node to the outcome column in the timeline object
  tl_end_node_shape = "outcome",
  ## set the size of the end node to 15 (as this is not a character, this
  ## argument is not mapped to a column but instead interpreted as the actual
  ## node size)
  tl_end_node_size = 15,
  ## map the colour of the timeline edge to the hospital column
  tl_edge_color = "hospital",
  ## set the width of the timeline edge to 2 (as this is not a character, this
  ## argument is not mapped to a column but instead interpreted as the actual
  ## edge width)
  tl_edge_width = 2,
  ## map edge labels to the hospital variable
  tl_edge_label = "hospital",
  ## specify the shape for everyone node attribute (defined above)
  shapes = shapes,
  ## specify the colour palette (defined above)
  col_pal = colours,
  ## set the size of the arrow to 0.5
  arrow_size = 0.5,
  ## use two columns in the legend
  legend_ncol = 2,
  ## set font size
  font_size = 15,
  ## define formatting for dates
  date_labels = c("%d %b %Y"),
  ## don't plot the ID labels below nodes
  label = FALSE,
  ## specify height
  height = 1000,
  ## specify width
  width = 1200,
  ## ensure each case node has a unique y-coordinate - this is very important
  ## when using timelines, otherwise you will have overlapping timelines from
  ## different cases
  position_dodge = TRUE
)

```

<!-- ======================================================= -->
## Análisis {#analysis}

### Resumiendo {.unnumbered}

Podemos obtener una visión general de algunas de las propiedades de la red utilizando la función `summary`.

```{r transmission_chains_summarise_epicontacts,}
## summarise epicontacts object
summary(epic)
```

Por ejemplo, podemos ver que sólo el 57% de los contactos tienen ambos casos en linelist; esto significa que no tenemos datos del listado sobre un número significativo de casos involucrados en estas cadenas de transmisión.

### Características de los pares {.unnumbered}

La función `get_pairwise()` permite procesar la(s) variable(s) del listado según cada par de los datos de contactos. En el siguiente ejemplo, la fecha de inicio de la enfermedad se extrae del listado para calcular la diferencia entre la fecha de inicio de la enfermedad para cada par. El valor que se obtiene de esta comparación representa el **intervalo de serie** (si).

```{r transmission_chains_pairwise,}
si <- get_pairwise(epic, "date_onset")   
summary(si)
tibble(si = si) %>%
  ggplot(aes(si)) +
  geom_histogram() +
  labs(
    x = "Serial interval",
    y = "Frequency"
  )
```

La función `get_pairwise()` interpretará el tipo de la columna que se utiliza para la comparación, y ajustará su método de comparación de los valores en consecuencia. Para los números y las fechas (como en el ejemplo de **si**), la función restará los valores. Cuando se aplica a columnas que son caracteres o categóricas, `get_pairwise()` pegará los valores. Dado que la función también permite un procesamiento arbitrario (véase el argumento "f"), estas combinaciones discretas pueden ser fácilmente tabuladas y analizadas.
    
```{r transmission_chains_pairwise_2,}
head(get_pairwise(epic, "gender"), n = 10)
get_pairwise(epic, "gender", f = table)
fisher.test(get_pairwise(epic, "gender", f = table))
```

En este caso, se observa una asociación significativa entre los vínculos de transmisión y el género.

### Identificación de clusters {.unnumbered}

La función `get_clusters()` puede utilizarse para identificar componentes conectados en un objeto `epicontacts`. En primer lugar, la utilizamos para recuperar un `data.frame` que contenga la información de los clusters:

```{r transmission_chains_cluster,}
clust <- get_clusters(epic, output = "data.frame")
table(clust$cluster_size)
ggplot(clust, aes(cluster_size)) +
  geom_bar() +
  labs(
    x = "Cluster size",
    y = "Frequency"
  )
```

Veamos los clusters más grandes. Para ello, añadimos la información de los clústers al objeto `epicontacts` y luego lo subconjuntamos para mantener sólo los clústers más grandes:

```{r transmission_chains_cluster_2,}
epic <- get_clusters(epic)
max_size <- max(epic$linelist$cluster_size)
plot(subset(epic, cs = max_size))
```

### Cálculo de grados {.unnumbered}

El grado de un nodo corresponde a su número de aristas o conexiones con otros nodos. `get_degree()` proporciona un método sencillo para calcular este valor para las redes de `epicontacts`. Un grado alto en este contexto indica un individuo que estuvo en contacto con muchos otros. El argumento `type` indica que queremos contar tanto el grado de entrada como el de salida, el argumento `only_linelist` indica que sólo queremos calcular el grado para los casos del listado.

```{r transmission_chains_degree,}
deg_both <- get_degree(epic, type = "both", only_linelist = TRUE)
```

¿Qué personas son las que tienen más de 10 contactos?

```{r}
head(sort(deg_both, decreasing = TRUE), 10)
```

¿Cuál es el número medio de contactos?

```{r}
mean(deg_both)
```

<!-- ======================================================= -->
## Recursos {#resources-30}

La [página de epicontacts](https://www.repidemicsconsortium.org/epicontacts/index.html) ofrece una visión general de las funciones del paquete e incluye algunas viñetas más detalladas.

La [página de github de epicontacts](http://github.com/reconhub/epicontacts) puede utilizarse para plantear problemas y solicitar funciones.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/transmission_chains.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---

# Árboles filogenéticos {#phylogenetic-trees-1}


<!-- ======================================================= -->

## Resumen {#overview-8}


Los **árboles filogenéticos** se utilizan para visualizar y describir el parentesco y la evolución de los organismos a partir de la secuencia de su código genético.

Pueden construirse a partir de secuencias genéticas utilizando métodos basados en la distancia (como el método de unión de vecinos) o métodos basados en los caracteres (como el método de máxima verosimilitud y el método Bayesiano Markov Chain Monte Carlo). La secuenciación de nueva generación (NGS, por sus siglas en inglés) se ha vuelto más económica y se está utilizando cada vez más en el área de salud pública para describir los patógenos causantes de enfermedades infecciosas. Los dispositivos de secuenciación portátil reducen el tiempo de respuesta y prometen facilitar los datos en tiempo real y así apoyar la investigación de brotes. Los datos de NGS se pueden utilizar para identificar el origen o la fuente de una cepa de un brote y su propagación, así como para determinar la presencia de genes de resistencia antimicrobiana. Para visualizar el parentesco genético entre muestras biológicas se construye un árbol filogenético.

Aquí aprenderemos a utilizar el paquete **ggtree**, que permite la visualización combinada de árboles filogenéticos con datos de muestra adicionales en forma de dataframe. Esto nos permitirá observar patrones y comprender mejor la dinámica de los brotes.

```{r, phylogenetic_trees_overview_graph, out.width=c('80%'), fig.align='center', fig.show='hold', echo = FALSE}

pacman::p_load(here, ggplot2, dplyr, ape, ggtree, treeio, ggnewscale)

tree <- ape::read.tree(here::here("data", "phylo", "Shigella_tree.txt"))

sample_data <- read.csv(here::here("data","phylo", "sample_data_Shigella_tree.csv"),sep=",", na.strings=c("NA"), head = TRUE, stringsAsFactors=F)


ggtree(tree, layout="circular", branch.length='none') %<+% sample_data + # %<+% añade el dataframe con datos de muestra al árbol
  aes(color=I(Belgium))+ # colorea las ramas de acuerdo con una variable en tu dataframe
  scale_color_manual(name = "Sample Origin", # nombre de tu esquema de color (aparecerá en la leyenda así) 
                    breaks = c("Yes", "No"), # las diferentes opciones en tu variable
                   labels = c("NRCSS Belgica", "Other"), # asigna nombres para las diferentes opciones en tu leyenda, sirve para formatearlas
                 values= c("blue", "black"), # asigna el color que desees a la variable 
                 na.value = "black") + # colorea los valores NA en negro 
  new_scale_color()+ # permite añadir un esquema de color adicional para otra variable
     geom_tippoint(aes(color=Continent), size=1.5)+ # color de la punta por continente. Puedes cambiar la forma añadiendo "shape = "
scale_color_brewer(name = "Continent",  # nombre de tu esquema de color (se mostrará en la leyenda así)
                       palette="Set1", # elegimos un conjunto de colores que vienen con el paquete Brewer
                   na.value="grey")+  # para los valores NA elegimos el color gris
  theme(legend.position= "bottom")

```

<!-- ======================================================= -->

## Preparación {#preparation-31}

### Cargar paquetes {.unnumbered}  

Este trozo de código muestra la carga de los paquetes necesarios. En este manual destacamos 'p_load()' de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puede cargar los paquetes instalados con library() de R **base**. Consulta la página [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R. 

```{r, phylogenetic_trees_loading_packages}
pacman::p_load(
  rio,             # importa/exportar
  here,            # rutas de fichero relativas
  tidyverse,       # manejo general de datos y visualización
  ape,             # para importar y exportar archivos filogenéticos
  ggtree,          # para visualizar archivos filogenéticos
  treeio,          # para visualizar archivos filogenéticos
  ggnewscale)      # para añadir capas adicionales de esquemas de color

```

### Importar datos {.unnumbered}  

Los datos de esta página pueden descargarse con las instrucciones de la página [Descargando el manual y los datos](#download-handbook-and-data).

Hay varios formatos diferentes en los que se puede almacenar un árbol filogenético (por ejemplo, Newick, NEXUS, Phylip). Uno de los más comunes es el formato de archivo Newick (.nwk), que es el estándar para representar árboles en forma legible por el ordenador. Esto significa que un árbol completo puede expresarse en un formato de cadena como "((t2:0,04,t1:0,34):0,89,(t5:0,37,(t4:0,03,t3:0,67):0,9):0,59);", enumerando todos los nodos y puntas, y su relación (longitud de rama) entre sí.

Nota: Es importante entender que el archivo del árbol filogenético en sí mismo no contiene datos de secuenciación, sino que es simplemente el resultado de las distancias genéticas entre las secuencias. Por lo tanto, no podemos extraer datos de secuenciación de un archivo de árbol.

En primer lugar, utilizamos la función read.tree() del paquete **ape** para importar un archivo de árbol filogenético de Newick en formato .txt, y lo almacenamos en un objeto tipo lista llamado "phylo". Si es necesario, utiliza la función `here()` del paquete **here** para especificar la ruta relativa del archivo.

Nota: En este caso el árbol newick se guarda como un archivo .txt para facilitar su manejo y descarga desde Github.

```{r, echo=F}
tree <- ape::read.tree(here::here("data", "phylo", "Shigella_tree.txt"))
```


```{r, echo=T, eval=F}
tree <- ape::read.tree("Shigella_tree.txt")
```

Inspeccionamos nuestro objeto árbol ('tree') y vemos que contiene 299 puntas (o muestras) y 236 nodos.

```{r}
tree
```

En segundo lugar, importamos una tabla almacenada en un archivo .csv con información adicional para cada muestra secuenciada, como el sexo, el país de origen y los atributos de resistencia antimicrobiana, utilizando la función `import()` del paquete **rio**:

```{r, echo=F}
sample_data <- import(here("data", "phylo", "sample_data_Shigella_tree.csv"))
```

```{r, echo=T, eval=F}
sample_data <- import("sample_data_Shigella_tree.csv")
```

A continuación se muestran las primeras 50 filas de los datos: 

```{r message=FALSE, echo=F}
DT::datatable(head(sample_data,50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

### Limpiar e inspeccionar {.unnumbered} 

Limpiamos e inspeccionamos nuestros datos: Para asignar los datos de muestra correctos al árbol filogenético, los valores de la columna `Sample_ID` en el dataframe `sample_data` deben coincidir con los valores de `tip.labels` en el archivo `tree`:

Comprobamos el formato de los `tip.labels` en el archivo de árbol mirando las 6 primeras entradas usando `head()` de R **base**.

```{r, phylogenetic_trees_inspect_sampledata}
head(tree$tip.label) 
```

También nos aseguramos de que la primera columna de nuestro dataframe `sample_data` sea `Sample_ID`. Miramos los nombres de las columnas de nuestro dataframe utilizando `colnames()` de R **base**.

```{r}
colnames(sample_data)   
```

Miramos los `Sample_IDs` en el dataframe para asegurarnos de que el formato es el mismo que en el `tip.label` (por ejemplo, las letras son todas mayúsculas, no hay barras bajas adicionales `_` entre las letras y los números, etc.)

```{r}
head(sample_data$Sample_ID) # volvemos a inspeccionar sólo los 6 primeros usando head()
```

También podemos comparar si todas las muestras están presentes en el archivo `tree` y viceversa, generando un vector lógico de TRUE o FALSE donde coinciden o no. Estos no se imprimen aquí, para simplificar.


```{r, eval=F}
sample_data$Sample_ID %in% tree$tip.label

tree$tip.label %in% sample_data$Sample_ID
```

Podemos utilizar estos vectores para mostrar cualquier ID que no esté en el árbol (no hay ninguno).  

```{r}
sample_data$Sample_ID[!tree$tip.label %in% sample_data$Sample_ID]
```

Al inspeccionar podemos ver que el formato de `Sample_ID` en el dataframe corresponde al formato de los nombres de las muestras en el `tip.labels`. No es necesario que estén clasificados en el mismo orden para que coincidan.

Estamos listos para empezar!




<!-- ======================================================= -->

## Visualización simple de un árbol {#simple-tree-visualization}


### Diferentes configuraciones de los árboles {.unnumbered} 

**ggtree** ofrece varios formatos de presentación y algunos pueden ser más adecuados que otros dependiendo del propósito específico. A continuación se muestran algunos ejemplos. Para otras opciones, consulta este [libro en línea](http://yulab-smu.top/treedata-book/chapter4.html).

A continuación, vemos algunos ejemplos de configuración de árboles:
```{r, phylogenetic_trees_example_formats, out.width=c('50%'), fig.show='hold'}

ggtree(tree)                                            # árbol lineal simple
ggtree(tree,  branch.length = "none")                   # árbol lineal simple con todas las puntas alineadas
ggtree(tree, layout="circular")                         # árbol circular simple
ggtree(tree, layout="circular", branch.length = "none") # árbol circular simple con todas las puntas alineadas

```

### Árbol simple con datos de muestra {.unnumbered}  

El operador **%<+%** se utiliza para conectar el dataframe `sample_data` con el archivo `tree`. La anotación más sencilla de un árbol es el agregado de los nombres de las muestras en las puntas, así como la coloración de las puntas y, si se desea, de las ramas:

Este es un ejemplo de árbol circular: 
```{r, phylogenetic_trees_adding_sampledata, fig.align='center', warning=F, message=F}

ggtree(tree, layout = "circular", branch.length = 'none') %<+% sample_data + # %<+% agrega un dataframe con datos de muestra al árbol
  aes(color = I(Belgium))+                       # colorea las ramas de acuerdo con una variable en tu dataframe
  scale_color_manual(
    name = "Sample Origin",                      # nombre de tu esquema de color (aparecerá en la leyenda así) 
    breaks = c("Yes", "No"),                     # las diferentes opciones en tu variable
    labels = c("NRCSS Belgium", "Other"),        # cómo quieres que se nombren las diferentes opciones en tu leyenda, permite formatearlas
    values = c("blue", "black"),                 # el color que deseas asignar a la variable  
    na.value = "black") +                        # colorea los valores no disponibles (NA) en negro 
  new_scale_color()+                             # permite añadir un esquema de color adicional para otra variable
    geom_tippoint(
      mapping = aes(color = Continent),          # color de la punta por continente Puedes cambiar la forma añadiendo "shape = "
      size = 1.5)+                               # define el tamaño de la punta
  scale_color_brewer(
    name = "Continent",                    # nombre de tu esquema de color (se mostrará en la leyenda así)
    palette = "Set1",                      # elegimos un conjunto de colores que vienen con el paquete de Brewer
    na.value = "grey") +                    # para los valores NA elegimos el color gris
  geom_tiplab(                             # añade el nombre de la muestra en la punta de su rama 
    color = 'black',                       # añade tantas líneas de texto como desees con + , pero es posible que tengas que ajustar el valor de desplazamiento para colocarlas una al lado de la otra
    offset = 1,
    size = 1,
    geom = "text",
    #align = TRUE
    )+    
  ggtitle("Phylogenetic tree of Shigella sonnei")+       # Nombre del árbol
  theme(
    axis.title.x = element_blank(), # elimina el título del eje x
    axis.title.y = element_blank(), # elimina el título del eje y
    legend.title = element_text(    # define el tamaño y el formato de la fuente del título de la leyenda
      face = "bold",
      size = 12),   
    legend.text=element_text(       # define el tamaño de letra y tipografía de la leyenda
      face = "bold",
      size = 10),  
    plot.title = element_text(      # define el tamaño de letra y tipografía del título del gráfico
      size = 12,
      face = "bold"),  
    legend.position = "bottom",     # define la posición de la leyenda
    legend.box = "vertical",        # define la posición de la leyenda
    legend.margin = margin())   
```

Podes exportar el gráfico de árbol con `ggsave()` como lo harías con cualquier otro objeto ggplot. Escrito de esta manera, ggsave() guarda la última imagen producida en la ruta de archivo que especifiques. Recordá que podes utilizar `here()` y rutas de archivo relativas para guardar fácilmente en subcarpetas, etc.

```{r, eval=F}
ggsave("example_tree_circular_1.png", width = 12, height = 14)

```


<!-- ======================================================= -->

## Manipulación de árboles {#tree-manipulation}

A veces puede tener un árbol filogenético muy grande y sólo le interesa una parte del árbol. Por ejemplo, si ha creado un árbol que incluye muestras históricas o internacionales para obtener una gran visión general sobre como pueden encajar tus datos en esos contextos. Pero luego, para ver más de cerca alguna parte tus datos, tendrás que inspeccionar sólo esa parte del árbol.

Dado que el archivo del árbol filogenético es el resultado del análisis de los datos de secuenciación, no podemos manipular el orden de los nodos y las ramas en el propio archivo. Estos ya han sido determinados en análisis anteriores a partir de los datos NGS en bruto. Sin embargo, podemos ampliar partes, ocultar partes e incluso subdividir partes del árbol. 

### Ampliar el zoom  {.unnumbered}  

Si en vez de "cortar" tu árbol, prefieres inspeccionar sólo una parte más de cerca, puedes hacer zoom para ver una parte específica.

En primer lugar, trazamos todo el árbol en formato lineal y añadimos etiquetas numéricas a cada nodo del árbol.
```{r, phylogenetic_trees_zoom_in, out.width=c('50%'), fig.show='hold', fig.align='center'}

p <- ggtree(tree,) %<+% sample_data +
  geom_tiplab(size = 1.5) +                # etiqueta las puntas de todas las ramas del árbol con el nombre de la muestra
  geom_text2(
    mapping = aes(subset = !isTip,
                  label = node),
    size = 5,
    color = "darkred",
    hjust = 1,
    vjust = 1)                            # etiqueta todos los nodos del árbol

p  # imprime en pantalla

```

Para hacer zoom en una rama en particular (la que sobresale a la derecha), utilizá `viewClade()` en el objeto ggtree `p` y proporcioná el número de nodo para verlo más de cerca:
```{r phylogenetic_trees_zoom_in_452, out.width=c('50%'), fig.show='hold', fig.align='center'}

viewClade(p, node = 452)

```

### Ramas colapsadas {.unnumbered}

Sin embargo, tal vez queramos ignorar esta rama, entonces podemos colapsarla en ese mismo nodo (nodo 452) utilizando `collapse()`. Definimos este árbol como `p_collapsed`.

```{r phylogenetic_trees_collapse_452, out.width=c('50%'), fig.show='hold', fig.align='center'}

p_collapsed <- collapse(p, node = 452)
p_collapsed
```

Como aclaración, cuando imprimimos p_collapsed, añadimos un `geom_point2()` (un diamante azul) en el nodo de la rama colapsada.
```{r}
p_collapsed + 
geom_point2(aes(subset = (node == 452)),  # asignamos un símbolo al nodo colapsado
            size = 5,                     # definimos el tamaño del símbolo
            shape = 23,                   # definimos la forma del símbolo
            fill = "steelblue")           # definimos el color del símbolo
```

### Subconjunto de un árbol {.unnumbered}

Si queremos hacer un cambio más permanente y crear un nuevo árbol reducido con el que trabajar, podemos subconjuntar parte de él con `tree_subset()`. Luego podemos guardarlo como un nuevo archivo de árbol newick o archivo .txt.

En primer lugar, inspeccionamos los nodos del árbol y las etiquetas de las puntas para decidir qué subconjunto se va a seleccionar.

```{r, phylogenetic_trees_subsetting, out.width=c('50%'), fig.show='hold', fig.align='center'}
ggtree(
  tree,
  branch.length = 'none',
  layout = 'circular') %<+% sample_data +               # añade los datos de la muestra usando el operador %<+%
  geom_tiplab(size = 1)+                                # etiqueta las puntas de todas las ramas del árbol con el nombre de la muestra
  geom_text2(
    mapping = aes(subset = !isTip, label = node),
    size = 3,
    color = "darkred") +                                # etiqueta todos los nodos del árbol
 theme(
   legend.position = "none",                            # elimina la leyenda
   axis.title.x = element_blank(),
   axis.title.y = element_blank(),
   plot.title = element_text(size = 12, face="bold"))
```

Ahora, digamos que hemos decidido crear un un subconjunto del árbol (o sub-árbol) con solo el nodo 528 (manteniendo las puntas dentro de esta rama después del nodo 528) y lo guardamos como un nuevo objeto `sub_tree1`:

```{r}
sub_tree1 <- tree_subset(
  tree,
  node = 528)                                            #Subconjuntamos el árbol en el nodo 528
```

Veamos el subconjunto del árbol 1:

```{r}
ggtree(sub_tree1) +
  geom_tiplab(size = 3) +
  ggtitle("Subset tree 1")
```

También podes hacer un subconjunto basado en una muestra en particular, especificando cuántos nodos "hacia atrás" queres incluir. Vamos a subconjuntar la misma parte del árbol basándonos en una muestra, en este caso S17BD07692, retrocediendo 9 nodos y lo guardamos como un nuevo objeto `sub_tree2`:

```{r}
sub_tree2 <- tree_subset(
  tree,
  "S17BD07692",
  levels_back = 9) # levels back define cuántos nodos hacia atrás quieres ir desde la punta de la muestra
```

Veamos el subconjunto del árbol 2:

```{r}
ggtree(sub_tree2) +
  geom_tiplab(size =3)  +
  ggtitle("Subset tree 2")

```

También podes guardar tu nuevo sub-árbol como un archivo Newick o incluso un archivo de texto utilizando la función `write.tree()` del paquete **ape**:

```{r, eval=F, phylogenetic_trees_write_tree}
# para guardar en formato .nwk 
ape::write.tree(sub_tree2, file='data/phylo/Shigella_subtree_2.nwk')

# para guardar en formato  .txt
ape::write.tree(sub_tree2, file='data/phylo/Shigella_subtree_2.txt')

```

### Rotación de nodos en un árbol  {.unnumbered} 


Como ya hemos dicho, no podemos cambiar el orden de las puntas o de los nodos en el árbol, ya que éste se basa en su parentesco genético y no está sujeto a manipulación visual. Pero podemos rotar las ramas alrededor de los nodos si eso facilita la visualización.

En primer lugar, trazamos nuestro nuevo sub-árbol 2 con las etiquetas de los nodos para elegir el nodo que queremos manipular y lo almacenamos en un objeto de ggtree plot `p`.

```{r, phylogenetic_trees_rotating_1, out.width=c('50%'), fig.show='hold', fig.align='center'}

p <- ggtree(sub_tree2) +  
  geom_tiplab(size = 4) +
  geom_text2(aes(subset=!isTip, label=node), # etiqueta todos los nodos del árbol
             size = 5,
             color = "darkred", 
             hjust = 1, 
             vjust = 1) 
p
```

Luego podemos manipular los nodos aplicando ggtree**::rotate()** o **ggtree::flip()**: Nota: para ilustrar qué nodos estamos manipulando aplicamos primero la función **geom_hilight()** de **ggtree** para resaltar las muestras en los nodos que nos interesan y almacenamos ese objeto gráfico de ggtree en un nuevo objeto `p1`.

```{r, phylogenetic_trees_rotating_2, out.width=c('50%'), fig.show='hold', fig.align='center'}

p1 <- p + geom_hilight(  # resalta el nodo 39 en azul, "extend =" permite definir la longitud del bloque de color
  node = 39,
  fill = "steelblue",
  extend = 0.0017) +  
geom_hilight(            # resalta el nodo 37 en amarillo
  node = 37,
  fill = "yellow",
  extend = 0.0017) +               
ggtitle("Original tree")


p1 # imprime en pantalla
```

Ahora podemos rotar el nodo 37 en el objeto `p1` para que las muestras del nodo 38 se muevan hacia abajo. Almacenamos el árbol rotado en un nuevo objeto p2

```{r, include=FALSE}
library("ggtree") #jfmont
```


```{r}
p2 <- ggtree::rotate(p1, 37) + 
      ggtitle("Rotated Node 37")


p2   # imprime en pantalla
```

O podemos usar el comando flip para rotar el nodo 36 en el objeto `p1` y mover el nodo 37 a la parte superior y el nodo 39 a la parte inferior. Almacenamos el árbol con nodos rotados como un nuevo objeto `p3`.
```{r}

p3 <- ggtree::flip(p1, 39, 37) +
      ggtitle("Rotated Node 36")


p3   # imprime en pantalla
```

### Ejemplo de sub-árbol con anotación de datos {.unnumbered}

Digamos que estamos investigando el grupo de casos con expansión clonal que se produjo en 2017 y 2018 representados en el nodo 39 de nuestro sub-árbol. Añadimos el año de aislamiento de la cepa, así como el historial de viajes y coloreamos por país para ver el origen de otras cepas estrechamente relacionadas genéticamente:

```{r, phylogenetic_trees_inspect_subset_example, out.width=c('80%'), fig.show='hold', fig.align='center', warning=F, message=F}
ggtree(sub_tree2) %<+% sample_data +     # usamos el operador %<+% para enlazar con sample_data
  geom_tiplab(                          # etiqueta las puntas de todas las ramas del árbol con el nombre de la muestra
    size = 2.5,
    offset = 0.001,
    #align = TRUE
    ) + 
  theme_tree2()+
  xlim(0, 0.015)+                       # establece los límites del eje x de nuestro árbol
  geom_tippoint(aes(color=Country),     # colorea la punta por continente
                size = 1.5)+ 
  scale_color_brewer(
    name = "Country", 
    palette = "Set1", 
    na.value = "grey")+
  geom_tiplab(                         # añade el año de aislamiento como etiqueta de texto en las puntas
    aes(label = Year),
    color = 'blue',
    offset = 0.0045,
    size = 3,
    linetype = "blank" ,
    geom = "text",
    #align = TRUE
    )+ 
  geom_tiplab(                          # añade el historial de viajes como una etiqueta de texto en las puntas, en color rojo
    aes(label = Travel_history),
    color = 'red',
    offset = 0.006,
    size = 3,
    linetype = "blank",
    geom = "text",
    #align = TRUE
    )+ 
  ggtitle("Phylogenetic tree of Belgian S. sonnei strains with travel history")+  # añade el título del árbol
  xlab("genetic distance (0.001 = 4 nucleotides difference)")+                   # añade una etiqueta en el eje x
 
  theme(
    axis.title.x = element_text(size = 10),
    axis.title.y = element_blank(),
    legend.title = element_text(face = "bold", size = 12),
    legend.text = element_text(face = "bold", size = 10),
    plot.title = element_text(size = 12, face = "bold"))

```

Nuestra observación apunta a un evento de importación de cepas procedentes de Asia, que luego circularon en Bélgica a lo largo de los años y parecen haber causado el último brote.

<!-- ======================================================= -->

## Árboles más complejos: añadir mapas térmicos de datos de muestra {#more-complex-trees-adding-heatmaps-of-sample-data}


Podemos añadir información más compleja, como la presencia categórica de genes de resistencia antimicrobiana y valores numéricos de resistencia realmente medida contra agentes antimicrobianos en forma de mapa de calor utilizando la función **ggtree::gheatmap()**.

Primero necesitamos graficar nuestro árbol (puede ser lineal o circular) y almacenarlo en un nuevo objeto ggtree `p`: Utilizaremos el sub-árbol de la parte 3).
```{r, phylogenetic_trees_sampledata_heatmap, out.width=c('60%'), fig.align='center', fig.show='hold'}

p <- ggtree(sub_tree2, branch.length='none', layout='circular') %<+% sample_data +
  geom_tiplab(size =3) + 
 theme(
   legend.position = "none",
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    plot.title = element_text(
      size = 12,
      face = "bold",
      hjust = 0.5,
      vjust = -15))
p

```

En segundo lugar, preparamos nuestros datos. Para visualizar las diferentes variables con nuevos esquemas de color, realizamos un subconjunto de nuestro dataframe a la variable deseada. Es importante añadir el `Sample_ID` como nombre de fila (rownames) de lo contrario los datos no coinciden con los `tip.labels` del árbol.

En nuestro ejemplo, queremos ver el género y las mutaciones que podrían conferir resistencia a la ciprofloxacina, un importante antibiótico de primera línea utilizado para tratar las infecciones por Shigella.

Creamos un dataframe para el género:

```{r, phylogenetic_trees_sampledata_heatmap_data}
gender <- data.frame("gender" = sample_data[,c("Gender")])
rownames(gender) <- sample_data$Sample_ID
```

Creamos un dataframe para las mutaciones en el gen gyrA, que confieren resistencia a la ciprofloxacina:
```{r}
cipR <- data.frame("cipR" = sample_data[,c("gyrA_mutations")])
rownames(cipR) <- sample_data$Sample_ID

```
Creamos un dataframe para la concentración inhibitoria mínima (CIM) medida en laboratorio para la ciprofloxacina:
```{r}
MIC_Cip <- data.frame("mic_cip" = sample_data[,c("MIC_CIP")])
rownames(MIC_Cip) <- sample_data$Sample_ID
```

Creamos un primer gráfico añadiendo un mapa de calor binario para el género al árbol filogenético y almacenándolo en un nuevo objeto de gráfico ggtree `h1`:
```{r, phylogenetic_trees_sampledata_heatmap_gender, out.width=c('70%'), fig.show='hold', fig.align='center'}

h1 <-  gheatmap(p, gender,                            # añadimos al árbol una capa de mapa de calor para el género del dataframe
                offset = 10,                          # offset desplaza el mapa de calor a la derecha
                width = 0.10,                         # width define el ancho de la columna del mapa de calor
                color = NULL,                         # color define el borde de las columnas del mapa de calor
         colnames = FALSE) +                          # oculta los nombres de las columnas del mapa de calor
  scale_fill_manual(name = "Gender",                  # define el esquema de colores y la leyenda para el género
                    values = c("#00d1b1", "purple"),
                    breaks = c("Male", "Female"),
                    labels = c("Male", "Female")) +
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())
h1

```

A continuación, añadimos información sobre las mutaciones en el gen gyrA, que confieren resistencia a la ciprofloxacina:

Nota: La presencia de mutaciones cromosómicas puntuales en los datos de secuenciación del genoma completo (WGS) se determinó previamente utilizando la herramienta PointFinder desarrollada por Zankari et al. (ver la sección de referencias adicionales)

En primer lugar, asignamos un nuevo esquema de colores a nuestro objeto gráfico `h1` y lo almacenamos en un objeto llamado `h2`. Esto nos permite definir y cambiar los colores para nuestra segunda variable en el mapa de calor.
```{r}
h2 <- h1 + new_scale_fill() 
```

A continuación, añadimos la segunda capa del mapa de calor a `h2` y almacenamos los gráficos combinados en un nuevo objeto `h3`:

```{r, phylogenetic_trees_sampledata_heatmap_cip_genes, out.width=c('80%'), fig.show='hold', fig.align='center'}

h3 <- gheatmap(h2, cipR,         # añade la segunda capa del mapa de calor que describe las mutaciones de resistencia a la ciprofloxacina
               offset = 12, 
               width = 0.10, 
               colnames = FALSE) +
  scale_fill_manual(name = "Ciprofloxacin resistance \n conferring mutation",
                    values = c("#fe9698","#ea0c92"),
                    breaks = c( "gyrA D87Y", "gyrA S83L"),
                    labels = c( "gyrA d87y", "gyrA s83l")) +
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())+
  guides(fill = guide_legend(nrow = 2,byrow = TRUE))
h3
```

Repetimos el proceso anterior, añadiendo primero una nueva capa de escala de colores a nuestro objeto existente `h3`, y luego añadiendo los datos continuos sobre la concentración inhibitoria mínima (CIM) de ciprofloxacina para cada cepa al objeto resultante `h4` para producir el objeto final `h5`:
```{r, phylogenetic_trees_sampledata_heatmap_cip_MIC, out.width=c('90%'), fig.show='hold', fig.align='center'}
# Primero añadimos el nuevo esquema de colores:
h4 <- h3 + new_scale_fill()

# luego combinamos los dos en una nueva gráfica:
h5 <- gheatmap(h4, MIC_Cip,  
               offset = 14, 
               width = 0.10,
                colnames = FALSE)+
  scale_fill_continuous(name = "MIC for Ciprofloxacin",  # aquí definimos un esquema de color de gradiente para la variable continua MIC
                      low = "yellow", high = "red",
                      breaks = c(0, 0.50, 1.00),
                      na.value = "white") +
   guides(fill = guide_colourbar(barwidth = 5, barheight = 1))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())
h5

```

Podemos hacer el mismo ejercicio para un árbol lineal:
```{r, phylogenetic_trees_sampledata_heatmap_linear_1, out.width=c('80%'), fig.show='hold', fig.align='center'}

p <- ggtree(sub_tree2) %<+% sample_data +
  geom_tiplab(size = 3) + # etiqueta las puntas
  theme_tree2()+
  xlab("genetic distance (0.001 = 4 nucleotides difference)")+
  xlim(0, 0.015)+
 theme(legend.position = "none",
      axis.title.y = element_blank(),
      plot.title = element_text(size = 12, 
                                face = "bold",
                                hjust = 0.5,
                                vjust = -15))
p
```

Primero añadimos el género:

```{r, phylogenetic_trees_sampledata_heatmap_linear_2, out.width=c('80%'), fig.show='hold', fig.align='center'}

h1 <-  gheatmap(p, gender, 
                offset = 0.003,
                width = 0.1, 
                color="black", 
         colnames = FALSE)+
  scale_fill_manual(name = "Gender",
                    values = c("#00d1b1", "purple"),
                    breaks = c("Male", "Female"),
                    labels = c("Male", "Female"))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())
h1
```


A continuación, añadimos las mutaciones de resistencia a la ciprofloxacina después de añadir otra capa de colores representando los genes que confieren resistencia antimicrobiana:


```{r, phylogenetic_trees_sampledata_heatmap_linear_3, out.width=c('80%'), fig.show='hold', fig.align='center'}

h2 <- h1 + new_scale_fill()
h3 <- gheatmap(h2, cipR,   
               offset = 0.004, 
               width = 0.1,
               color = "black",
                colnames = FALSE)+
  scale_fill_manual(name = "Ciprofloxacin resistance \n conferring mutation",
                    values = c("#fe9698","#ea0c92"),
                    breaks = c( "gyrA D87Y", "gyrA S83L"),
                    labels = c( "gyrA d87y", "gyrA s83l"))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 10),
        legend.box = "vertical", legend.margin = margin())+
  guides(fill = guide_legend(nrow = 2,byrow = TRUE))
 h3
```

A continuación, añadimos en forma de mapa de calor la concentración mínima inhibitoria determinada por el laboratorio (MIC):

```{r, phylogenetic_trees_sampledata_heatmap_linear_4, out.width=c('80%'), fig.show='hold', fig.align='center'}

h4 <- h3 + new_scale_fill()
h5 <- gheatmap(h4, MIC_Cip, 
               offset = 0.005,  
               width = 0.1,
               color = "black", 
                colnames = FALSE)+
  scale_fill_continuous(name = "MIC for Ciprofloxacin",
                      low = "yellow", high = "red",
                      breaks = c(0,0.50,1.00),
                      na.value = "white")+
   guides(fill = guide_colourbar(barwidth = 5, barheight = 1))+
   theme(legend.position = "bottom",
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 8),
        legend.box = "horizontal", legend.margin = margin())+
  guides(shape = guide_legend(override.aes = list(size = 2)))
h5

```


<!-- ======================================================= -->
## Recursos {#resources-29}

http://hydrodictyon.eeb.uconn.edu/eebedia/index.php/Ggtree# Clade_Colors

https://bioconductor.riken.jp/packages/3.2/bioc/vignettes/ggtree/inst/doc/treeManipulation.html

https://guangchuangyu.github.io/ggtree-book/chapter-ggtree.html

https://bioconductor.riken.jp/packages/3.8/bioc/vignettes/ggtree/inst/doc/treeManipulation.html

Ea Zankari, Rosa Allesøe, Katrine G Joensen, Lina M Cavaco, Ole Lund, Frank M Aarestrup, PointFinder: una novedosa herramienta web para la detección basada en WGS de la resistencia a los antimicrobianos asociada a mutaciones puntuales cromosómicas en patógenos bacterianos, Journal of Antimicrobial Chemotherapy, Volume 72, Issue 10, October 2017, Pages 2764-2768, [https://doi.org/10.1093/jac/dkx217](https://doi.org/10.1093/jac/dkx217)


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/phylogenetic_trees.Rmd-->


# Gráficos interactivos {#interactive-plots}

Últimamente es cada vez más común la necesidad de que la visualización de datos sea interactiva para el público. Por ello, es cada vez más habitual la realización de gráficos interactivos. Hay varias formas de realizarlos, pero las dos más comunes son empleando **plotly** y **shiny**.

En esta página nos centraremos en como convertir un gráfico `ggplot()` existente en un gráfico interactivo con **plotly**. Puedes leer más sobre **shiny** en la página [Dashboards con Shiny](#dashboards-with-shiny). Antes de comenzar, merece la pena mencionar que los gráficos interactivos sólo se pueden utilizar en documentos R markdown en formato HTML, no en documentos PDF o Word.

A continuación se muestra una curva epidémica  que se ha transformado en interactiva utilizando la integración de **ggplot2** y **plotly** (pasa el cursor por encima del gráfico, amplía la imagen o clica en los elementos de la leyenda para comprobarlo).

```{r plotly_demo, out.width=c('75%'), out.height=c('500px'), echo=F, warning=F, message=F}
pacman::p_load(plotly, rio, here, ggplot2, dplyr, lubridate)
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

## estos botones son superfluos/distraen
plotly_buttons_remove <- list('zoom2d','pan2d','lasso2d', 'select2d','zoomIn2d',
                              'zoomOut2d','autoScale2d','hoverClosestCartesian',
                              'toggleSpikelines','hoverCompareCartesian')

p <- linelist %>% 
  mutate(outcome = if_else(is.na(outcome), "Unknown", outcome),
         date_earliest = if_else(is.na(date_infection), date_onset, date_infection),
         week_earliest = floor_date(date_earliest, unit = "week",week_start = 1))%>% 
  count(week_earliest, outcome) %>% 
  ggplot()+
  geom_col(aes(week_earliest, n, fill = outcome))+
  xlab("Week of infection/onset") + ylab("Cases per week")+
  theme_minimal()

p %>% 
  ggplotly() %>% 
  partial_bundle() %>% 
  config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)

```

<!-- ======================================================= -->
## Preparación {#preparation-32}

### Cargar paquetes {.unnumbered}  

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()`.  Consulta la página [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.


```{r}
pacman::p_load(
  rio,       # importación/exportación
  here,      # rutas de archivos
  lubridate, # lubridate
  plotly,    # gráficos interactivos
  scales,    # porcentajes rápidos
  tidyverse  # gestión y visualización de datos
  ) 
```

### Comienza con un `ggplot()` {.unnumbered}

En esta página asumimos que comienzas con un gráfico `ggplot()` que deseas convertir en interactivo. Construiremos varios de estos gráficos en esta página, utilizando la base de datos `linelist`, la cual es ampliamente utilizada en este manual.  


### Importar datos {.unnumbered}

Para empezar, importamos la lista de casos limpia de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica aquí para descargar linelist "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importae los datos con la función `import()` del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - consulta la página de [importación y exportación](#import-and-export) para más detalles).

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# Importar base de datos linelist 
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas de la base de datos.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```






  
<!-- ======================================================= -->
## Trazar con `ggplotly()` {#plot-with-ggplotly}

La función `ggplotly()` del paquete **plotly** facilita la conversión de un `ggplot()` para que sea interactivo. Simplemente guarda tu `ggplot()` y luego pásaselo a la función `ggplotly()`.

A continuación, trazamos una línea simple que representa la proporción de casos que murieron en una semana determinada.

Comenzamos creando unos datos resumidos de cada semana epidemiológica y el porcentaje de casos con resultado conocido que murieron.
 

```{r}
weekly_deaths <- linelist %>%
  group_by(epiweek = floor_date(date_onset, "week")) %>%  # crear y agrupar los datos por la columna epiweek
  summarise(                                              # crear nuevo dataframe descriptivo 
    n_known_outcome = sum(!is.na(outcome), na.rm=T),      # número de casos por grupo con resultado conocido
    n_death  = sum(outcome == "Death", na.rm=T),          # número de casos por grupo que murieron
    pct_death = 100*(n_death / n_known_outcome)           # porcentaje de casos con resultado conocido que murieron
  )
```
Aquí están las primeras 50 filas de los datos `weekly_deaths`. 

```{r message=FALSE, echo=F}
DT::datatable(head(weekly_deaths, 50), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```
Luego creamos el gráfico con **ggplot2**, utilizando `geom_line()`. 

```{r, warning=F, message=F}
deaths_plot <- ggplot(data = weekly_deaths)+            # comenzar introduciendo los datos de  weekly deaths 
  geom_line(mapping = aes(x = epiweek, y = pct_death))  # hacer un gráfico de línea

deaths_plot   # imprimir
```


Podemos convertirlo en interactivo simplemente pasando este gráfico mediante un "pipe" a `ggplotly()`, como se muestra abajo. Pasa el cursor por encima de la línea para mostrar los valores x e y. Puedes ampliar el gráfico y arrastrarlo. También puedes ver los iconos en la parte superior derecha del gráfico. En orden, estos botones permiten:

* Descargar la vista actual como imagen PNG
* Acercarse con un cuadro de selección
* "Pan", o moverse a través de la gráfica clicando y arrastrando la gráfica
* Acercar, alejar o volver al zoom por defecto
* Restablecer los ejes por defecto
* Activar/desactivar las "líneas en pico" que son líneas punteadas desde el punto interactivo que se extienden a los ejes x e y
* Ajustes para que los datos se muestren cuando no se está sobre la línea

```{r}
deaths_plot %>% plotly::ggplotly()
```

Los datos agrupados también funcionan con `ggplotly()`. A continuación, realizaremos una curva epidemica  semanal agrupada por resultado. Las barras apiladas son interactivas. Prueba a clicar en los diferentes elementos de la leyenda (aparecerán/desaparecerán).


```{r plot_show, eval=F}
# Hacer curva epidémica con el paquete incidence2
p <- incidence2::incidence(
  linelist,
  date_index = date_onset,
  interval = "weeks",
  groups = outcome) %>% plot(fill = outcome)
```

```{r, echo=T, eval=F}
# Hacer interactivo
p %>% plotly::ggplotly()
```
  
```{r, warning = F, message = F, , out.width=c('95%'), out.height=c('500px'), echo=FALSE}
p %>% 
  ggplotly() %>% 
  partial_bundle() 
```
  
<!-- ======================================================= -->
## Modificaciones {#modifications}

### Tamaño del archivo {.unnumbered} 

Cuando se exportan imagenes en un HTML generado por R Markdown (¡como este libro!) es deseable que el gráfico tenga el menor tamaño de datos posible (y siempre que se pueda, evitar que esto tenga repercusiones negativas). Para ello, sólo hay que realizar "pipe" desde el gráfico interactivo a `partial_bundle()`, de **plotly**.

```{r plot_tidyshow, eval=F}
p <- p %>% 
  plotly::ggplotly() %>%
  plotly::partial_bundle()
```

### Botones {.unnumbered} 

Algunos de los botones de un plotly estándar son superfluos y pueden distraer, por lo que, si quieres,  puedes eliminarlos. Puedes hacer esto simplemente canalizando la haz pipe hacia `config()` de **plotly** y especifican qué botones eliminar. En el siguiente ejemplo especificamos por adelantado los nombres de los botones a eliminar, y los especificamos en el argumento `modeBarButtonsToRemove = `. También establecemos `displaylogo = FALSE` para eliminar el logo de plotly.

```{r plot_tidyshow2, eval=F}
## estos botones distraen y queremos eliminarlos
plotly_buttons_remove <- list('zoom2d','pan2d','lasso2d', 'select2d','zoomIn2d',
                              'zoomOut2d','autoScale2d','hoverClosestCartesian',
                              'toggleSpikelines','hoverCompareCartesian')

p <- p %>%            # redefinir el gráfico  interactivo sin estos botones
  plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)
```



<!-- ======================================================= -->
## Gráficos de calor {#heat-tiles}

Puedes hacer que casi cualquier gráfico de `ggplot()` sea interactivo, incluidos los gráficos de calor. En la página sobre [gráficos de calor](#heat-plots) puede leer cómo hacer el siguiente gráfico, que muestra la proporción de días a la semana en que determinados centros comunicaron datos a su provincia.

Aquí está el código, aunque en este capítulo no describiremos en profundidad como realizarlo.

```{r  message=F, warning=F}
# importar datos
facility_count_data <- rio::import(here::here("data", "malaria_facility_count_data.rds"))

# datos agregados en semanas para el distrito de Spring
agg_weeks <- facility_count_data %>% 
  filter(District == "Spring",
         data_date < as.Date("2020-08-01")) %>% 
  mutate(week = aweek::date2week(
    data_date,
    start_date = "Monday",
    floor_day = TRUE,
    factor = TRUE)) %>% 
  group_by(location_name, week, .drop = F) %>%
  summarise(
    n_days          = 7,
    n_reports       = n(),
    malaria_tot     = sum(malaria_tot, na.rm = T),
    n_days_reported = length(unique(data_date)),
    p_days_reported = round(100*(n_days_reported / n_days))) %>% 
  ungroup(location_name, week) %>% 
  right_join(tidyr::expand(., week, location_name)) %>% 
  mutate(week = aweek::week2date(week))

# crear plot
metrics_plot <- ggplot(agg_weeks,
       aes(x = week,
           y = location_name,
           fill = p_days_reported))+
  geom_tile(colour="white")+
  scale_fill_gradient(low = "orange", high = "darkgreen", na.value = "grey80")+
  scale_x_date(expand = c(0,0),
               date_breaks = "2 weeks",
               date_labels = "%d\n%b")+
  theme_minimal()+ 
  theme(
    legend.title = element_text(size=12, face="bold"),
    legend.text  = element_text(size=10, face="bold"),
    legend.key.height = grid::unit(1,"cm"),
    legend.key.width  = grid::unit(0.6,"cm"),
    axis.text.x = element_text(size=12),
    axis.text.y = element_text(vjust=0.2),
    axis.ticks = element_line(size=0.4),
    axis.title = element_text(size=12, face="bold"),
    plot.title = element_text(hjust=0,size=14,face="bold"),
    plot.caption = element_text(hjust = 0, face = "italic")
    )+
  labs(x = "Week",
       y = "Facility name",
       fill = "Reporting\nperformance (%)",
       title = "Percent of days per week that facility reported data",
       subtitle = "District health facilities, April-May 2019",
       caption = "7-day weeks beginning on Mondays.")

metrics_plot # imprimir
```

A continuación, lo convertimos en interactivo y lo modificamos para que los botones sean sencillos y disminuya el tamaño del archivo. 

```{r,  out.width=c('95%'), out.height=c('500px')}
metrics_plot %>% 
  plotly::ggplotly() %>% 
  plotly::partial_bundle() %>% 
  plotly::config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove)
```

<!-- ## Maps {.unnumbered}   -->

<!-- You can also make `ggplot()` GIS maps interactive, although it makes a bit more care.  -->

<!-- THIS SECTION IS UNDER CONSTRUCTION  -->

<!-- Although **plotly** works well with `ggplot2::geom_sf` in RStudio, when you try to include its outputs in R Markdown HTML files (like this book), it doesn't work well.   -->

<!-- So instead you can use {**plotly**}'s own mapping tools which can be tricky but are easy when you know how. Read on...   -->

<!-- We're going to use Covid-19 incidence across African countries for this example. The data used can be found on the [World Health Organisation website](https://covid19.who.int/table).   -->

<!-- You'll also need a new type of file, a GeoJSON, which is sort of similar to a shp file for those familiar with GIS. For this book, we used one from [here](https://geojson-maps.ash.ms).   -->

<!-- GeoJSON files are stored in R as complex lists and you'll need to maipulate them a little. -->

<!-- ```{r, echo=T,} -->
<!-- ## You need two new packages: {rjson} and {purrr} -->
<!-- pacman::p_load(plotly, rjson, purrr) -->

<!-- ## This is a simplified version of the WHO data -->
<!-- df <- rio::import(here::here("data", "gis", "covid_incidence.csv")) -->

<!-- ## Load your geojson file -->
<!-- geoJSON <- rjson::fromJSON(file=here::here("data", "gis", "africa_countries.geo.json")) -->

<!-- ## Here are some of the properties for each element of the object -->
<!-- head(geoJSON$features[[1]]$properties) -->

<!-- ``` -->


<!-- This is the tricky part. For {**plotly**} to match your incidence data to GeoJSON, the countries in the geoJSON need an id in a specific place in the list of lists. For this we need to build a basic function: -->
<!-- ```{r} -->
<!-- ## The property column we need to choose here is "sovereignt" as it is the names for each country -->
<!-- give_id <- function(x){ -->

<!--   x$id <- x$properties$sovereignt  ## Take sovereignt from properties and set it as the id -->

<!--   return(x) -->
<!-- } -->

<!-- ## Use {purrr} to apply this function to every element of the features list of the geoJSON object -->
<!-- geoJSON$features <- purrr::map(.x = geoJSON$features, give_id) -->
<!-- ``` -->

<!-- <!-- ======================================================= --> -->
<!-- ### Maps - plot {  } -->

<!-- UNDER CONSTRUCTION -->

<!-- ```{r, echo=FALSE, eval=FALSE, out.width=c('95%'), out.height=c('500px'),warning=F} -->
<!-- plotly::plot_ly() %>%  -->
<!--   plotly::add_trace(                    #The main plot mapping functionn -->
<!--     type="choropleth", -->
<!--     geojson=geoJSON, -->
<!--     locations=df$Name,          #The column with the names (must match id) -->
<!--     z=df$Cumulative_incidence,  #The column with the incidence values -->
<!--     zmin=0, -->
<!--     zmax=57008, -->
<!--     colorscale="Viridis", -->
<!--     marker=list(line=list(width=0)) -->
<!--   ) %>% -->
<!--   colorbar(title = "Cases per million") %>% -->
<!--   layout(title = "Covid-19 cumulative incidence", -->
<!--                  geo = list(scope = 'africa')) %>%  -->
<!--   config(displaylogo = FALSE, modeBarButtonsToRemove = plotly_buttons_remove) -->
<!-- ``` -->

<!-- ======================================================= -->
## Recursos {#resources-32}

Plotly no es sólo para R, también funciona bien con Python (y realmente con cualquier lenguaje de ciencia de datos, ya que está construido en JavaScript). Puedes leer más sobre él en el [sitio web de plotly](https://plotly.com/r/)


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/interactive_plots.Rmd-->

# (PART) Informes y Dashboards {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_reports_dashboards.Rmd-->


# Informes con R Markdown {#reports-with-r-markdown}

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/rmarkdown_overview.png"))
```

R Markdown es una herramienta ampliamente utilizada para crear documentos de salida automatizados, reproducibles y dignos de compartir, como por ejemplo, informes. Podés generar resultados estáticos o interactivos en Word, pdf, html, powerpoint y otros formatos.

Un script de R Markdown intercala código R y texto de tal manera que el script *se convierte en tu documento de salida*. Puedes crear un documento completo con formato, incluyendo texto narrativo (el texto puede ser dinámico y cambiar en función de sus datos), tablas, figuras, viñetas/números, bibliografías, etc.

Estos documentos pueden producirse para ser actualizados de forma rutinaria (por ejemplo, informes de vigilancia diarios) y/o ejecutarse sobre subconjuntos de datos (por ejemplo, informes para cada jurisdicción).

Otras páginas de este manual amplían este tema:

* La página [Organización de informes rutinarios](#organizing-routine-reports) muestra cómo "rutinizar" la producción de informes con carpetas autogeneradas con marca de tiempo.
* La página [Dashboards con R Markdown](#dashboards-with-r-markdown) explica cómo formatear un informe de R Markdown como un cuadro de mando o tablero de control.

Cabe destacar que el proyecto [R4Epis](https://r4epis.netlify.app/) ha desarrollado plantillas de scripts R Markdown para los escenarios de brotes y encuestas de uso frcuente en lugares en donde MSF maneja proyectos.


<!-- ======================================================= -->
## Preparación {#preparation-33}

**Antecedentes de R Markdown**

Explicamos algunos de los conceptos y paquetes involucrados:

* **Markdown** es un "lenguaje" que permite escribir un documento en texto plano, que se puede convertir a html y otros formatos. No es específico de R. Los archivos escritos en Markdown tienen una extensión '.md'.
* R **Markdown**: es una variación de markdown que *es específica de R* - te permite escribir un documento usando markdown para producir texto *y para integrar código R y mostrar sus resultados*. Los archivos R Markdown tienen la extensión '.Rmd'.
* **rmarkdown - el paquete**: Usado por R para convertir el archivo .Rmd en el tipo de documento de salida deseado. Su objetivo es convertir la sintaxis markdown (texto), por lo que también necesitamos...
* **knitr**: Este paquete de R leerá los trozos de código, los ejecutará y los "tejerá" dentro del documento. Así es como se incluyen las tablas y los gráficos junto al texto.
* **Pandoc**: Por último, pandoc convierte el documento de salida en word/pdf/powerpoint, etc. Es un software independiente de R, y viene instalado automáticamente con RStudio.

En resumen, el proceso que ocurre *en segundo plano* (¡no es necesario que conozcas todos estos pasos!) consiste en alimentar el archivo .Rmd a **knitr**, que ejecuta los trozos de código R y crea un nuevo archivo .md (markdown) que incluye el código R y su salida renderizada. El archivo .md es entonces procesado por pandoc para crear el producto final: un documento de Microsoft Word, un archivo HTML, un documento powerpoint, un pdf, etc.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/0_rmd.png"))
```

(fuente: https://rmarkdown.rstudio.com/authoring_quick_tour.html):

**Instalación**

Para crear una salida de R Markdown, necesitas tener instalado lo siguiente:

* El paquete **rmarkdown** (**knitr** también se instalará automáticamente)
* Pandoc, que debería venir instalado con RStudio. Si no utilizas RStudio, podés descargar Pandoc aquí: http://pandoc.org .
* Si querés generar una salida en PDF (un poco más complicado), necesitarás instalar LaTeX. Para los usuarios de R Markdown que no hayan instalado LaTeX antes, recomendamos que instalen TinyTeX (https://yihui.name/tinytex/)https://yihui.name/tinytex/). Puedes utilizar los siguientes comandos:

```{r, eval=F}
pacman::p_load(tinytex)     # install tinytex package
tinytex::install_tinytex()  # R command to install TinyTeX software
```

<!-- ======================================================= -->
## Cómo empezar {#getting-started}

### Instalar el paquete R rmarkdown {.unnumbered} 

Instalá el paquete R **rmarkdown**. En este manual destacamos p_load() de **pacman**, que instala el paquete si es necesario *y ademas* lo carga para su uso. También podés cargar los paquetes instalados con `library()` de R **base**. Consultá la página sobre [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.


```{r, eval=F}
pacman::p_load(rmarkdown)
```

### Iniciar un nuevo archivo Rmd {.unnumbered}  

En RStudio, abrí un nuevo archivo R markdown, comenzando con ‘File’, luego ‘New file’ luego ‘R markdown…'. 

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/1_gettingstarted.png"))
```

R Studio te dará algunas opciones de salida para elegir. En el ejemplo siguiente seleccionamos "HTML" porque queremos crear un documento html. El título y los nombres de los autores no son importantes. Si el tipo de documento de salida que desea no es uno de estos, no te preocupes - podés elegir cualquiera y cambiarlo en el script más tarde.

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/1_gettingstartedB.png"))
```

Esto abrirá un nuevo script .Rmd.

### Es importante saber {.unnumbered} 
 
**El directorio de trabajo**

El directorio de trabajo de un archivo markdown es el lugar donde se guarda el propio archivo Rmd. Por ejemplo, si el proyecto R está dentro de `~/Documents/projectX` y el archivo Rmd en sí está en una subcarpeta `~/Documents/projectX/markdownfiles/markdown.Rmd`, el código `read.csv("data.csv")` dentro del markdown buscará un archivo csv en la carpeta `markdownfiles`, y no en la carpeta raíz del proyecto donde los scripts dentro de los proyectos normalmente buscarían archivos de manera automática.

Para referirse a archivos en otro lugar, tendrá que utilizar la ruta completa del archivo o utilizar el paquete **here**. El paquete **here** establece el directorio de trabajo en la carpeta raíz del proyecto R y se explica en detalle en las páginas de [proyectos R](#r-projects) e [importación y exportación](#import-and-export) de este manual. Por ejemplo, para importar un archivo llamado "data.csv" desde la carpeta `projectX`, el código sería `import(here("data.csv"))`.

Ten en cuenta que no se recomienda el uso de `setwd()` en los scripts de R Markdown - sólo se aplica al trozo de código en el que está escrito.

**Trabajar en una unidad versus tu ordenador**

Debido a que R Markdown puede tener problemas con pandoc cuando se ejecuta en una unidad de red compartida, se recomienda que la carpeta esté ubicada en tu máquina local, por ejemplo, en un proyecto dentro de 'Mis Documentos'. Si utilizas Git (¡super recomendable!), esto te resultará familiar. Para más detalles, consulta las páginas del manual sobre [R en unidades de red](#r-on-network-drives) y [Errores y ayuda](#common-errors). 


<!-- ======================================================= -->
## Componentes de R Markdown {#r-markdown-components}

Un documento R Markdown puede ser editado en RStudio igual que un script estándar de R. Cuando se inicia un nuevo script de R Markdown, RStudio intenta ayudarnos mostrando una plantilla que explica las diferentes secciones de un script de R Markdown.

Lo que aparece a continuación es lo que veremos al iniciar un nuevo script Rmd destinado a producir un documento de salida en html (según la sección anterior).

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/2_defaultRMD.png"))
```

Como puedes ver, hay tres componentes básicos en un archivo Rmd: YAML, texto Markdown y trozos de código R.

Estos *crearán y se convertirán en la salida de su documento*. Consulta el siguiente diagrama:

```{r out.width = "100%", out.height="150%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/rmarkdown_translation.png"))
```



### Metadatos YAML  {.unnumbered}

Denominado 'metadatos YAML' o simplemente 'YAML', se encuentra en la parte superior del documento R Markdown. Esta sección del script le dirá a su archivo Rmd qué tipo de salida producir, preferencias de formato y otros metadatos como el título del documento, el autor y la fecha. Hay otros usos que no se mencionan aquí (pero a los que se hace referencia en 'Producción del documento de salida'). Ten en cuenta que la sangría es importante; los tabuladores no se aceptan, pero los espacios sí.

Esta sección debe comenzar con una línea que contenga sólo tres guiones `---` y debe cerrar con una línea que contenga sólo tres guiones `---`. Los parámetros YAML vienen en pares `key:value`. La colocación de los dos puntos en YAML es importante - los pares `key:value` están separados por dos puntos (¡no por signos de igualdad!).

El YAML debe comenzar con los metadatos del documento. El orden de estos parámetros YAML primarios (sin sangría) no importa. Por ejemplo:

```yaml
title: "My document"
author: "Me"
date: "`r Sys.Date()`"
```

Puedes utilizar código R en valores YAML escribiéndolo como código en línea (precedido por `r` entre comillas) pero también entre comillas (véase el ejemplo anterior para `date: `).

En la imagen de arriba, porque hemos seleccionado el tipo de documento de salida como html, podemos ver que el YAML dice `output: html_document`. Sin embargo, también podemos cambiar esto escribir `powerpoint_presentation` o `word_document` o incluso `pdf_document`. 


### Texto  {.unnumbered}

Esta es la narrativa de t u documento, incluyendo los títulos y encabezados. Está escrito en el lenguaje "markdown", que se utiliza en muchos otros programas.

A continuación se presentan las formas principales de escribir este texto. Podés consultar material de referencia más detallado disponible en R Markdown "cheatsheet" en el [sitio web de RStudio](https://rstudio.com/resources/cheatsheets/). 

#### Nuevas líneas {.unnumbered}  

En R Markdown, para iniciar una nueva línea, introducí *dos espacios** al final de la línea anterior y luego Enter/Return. Esto es una particularidad de R Markdown.   


#### Formato de texto {.unnumbered}  

Rodea el texto normal con estos caracteres para cambiar su apariencia en la salida.

* Guiones bajos (`_texto_`) o un asterisco simple (`*texto*`) para *poner en cursiva* (itálica)
* Doble asterisco (`**texto**`) para el **texto en negrita**
* Comillas invertidas `(texto)` para mostrar el texto como código

El aspecto real de la fuente puede establecerse utilizando plantillas específicas (especificadas en los metadatos YAML; ver sub-secciones de este capitulo).

#### Color {.unnumbered}  

No existe un mecanismo sencillo para cambiar el color del texto en R Markdown. Como solución, *si tu salida es un archivo HTML*, es añadir una línea de codigo HTML en el texto de Markdown. El siguiente código HTML imprimirá una línea de texto en negrita roja.

```md
<span style="color: red;">**_PELIGRO:_** Esto es una advertencia.</span>  
```

<span style="color: red;">**_PELIGRO:_** Esto es una advertencia.</span>  


#### Títulos y encabezamientos {.unnumbered}  

Un símbolo de almohadilla (hash #) delante de un texto en un script de R Markdown crea un encabezado. Esto es diferente que en un trozo de código R en el script, en el que un símbolo de almohadilla permite comentar/anotar/desactivar, como en un script normal de R.

Los distintos niveles de encabezamiento se establecen con diferentes números de símbolos de almohadilla al comienzo de una nueva línea. Un solo símbolo de almohadilla genera un título o encabezamiento primario. Dos símbolos hash generan un encabezamiento de segundo nivel. Los encabezamientos de tercer y cuarto nivel pueden hacerse con más símbolos hash sucesivamente.

```md
# Encabezamiento / título de primer nivel

## Encabezamiento de segundo nivel

### Encabezamiento de tercer nivel
```


#### Viñetas y numeración {.unnumbered}  

Utilizá asteriscos (`*`) para crear una lista de viñetas. Completá la frase anterior, introducí dos espacios, presioná Enter/Return *dos veces*, y luego comenzá tus viñetas. Incluí un espacio entre el asterisco y el texto de tu viñeta. Después de cada viñeta, introducí dos espacios y luego presioná Enter/Return. Las sub-viñetas funcionan de la misma manera pero con sangría. Los números funcionan de la misma manera, pero en lugar de un asterisco, escribí 1), 2), etc. El texto de tu script de R Markdown se vería como mostramos a continuación.


```md
Aquí están mis viñetas (hay dos espacios después de los dos puntos):

* Viñeta 1 (seguida de dos espacios y Enter/Return)
* Viñeta 2 (seguida de dos espacios y Enter/Return)
  * Sub-viñeta 1 (seguida de dos espacios y Enter/Return)
  * Sub-viñeta 2 (seguida de dos espacios y Enter/Return)
* Subbalanceo 2 (seguido de dos espacios y Enter/Return)

```


#### Comentar el texto {.unnumbered}

Puedes desactivar o "esconder" el texto de R Markdown del mismo modo que puede utilizar el "#" para desactivar una línea de código en un chunk de R. Simplemente resalta el texto y clica Ctrl+Mayús+c (Cmd+Mayús+c para Mac). El texto aparecerá rodeado de flechas y se volverá verde. No aparecerá en tu salida.  


```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/rmarkdown_hide_text.png"))
```


### Trozos de código (chunks)  {.unnumbered}

Las secciones del script que se dedican a ejecutar el código R se denominan "chunks" o trozos. Aquí es donde se pueden cargar paquetes, importar datos y realizar la gestión y visualización de datos propiamente dicha. Puede haber muchos trozos de código, por lo que puede ser de ayuda organizar tu código R en partes, quizás intercaladas con texto. Para tener en cuenta: estos trozos tendrán un color de fondo ligeramente diferente al de la parte narrativa del documento.

Cada trozo se abre con una línea que comienza con tres comillas invertidas y corchetes que contienen parámetros para el trozo (`{ }`). El trozo termina con otras tres comillas invertidas.

Puedes crear un nuevo fragmento escribiendo tú mismo las marcas, utilizando el atajo de teclado "Ctrl + Alt + i" (o Cmd + Shift + r en Mac), o clicando en el icono verde 'insert a new code chunk' en la parte superior de tu editor de scripts.

Algunas notas sobre el contenido de las corchetes `{ }`:

* Empiezan con una "r" para indicar que el nombre del idioma dentro del chunk es R
* Después de la r puedes asignarle un "nombre" al chunk - no es necesario pero puede ayudarte a organizar tu trabajo. Ten en cuenta que si nombras tus chunks, debes usar SIEMPRE nombres únicos o de lo contrario R se quejará cuando intentes procesarlos.
* Los corchetes pueden incluir también otras opciones, escritas como `tag=value`, como por ejemplo
* `eval = FALSE` para no ejecutar el código R
     * `echo = FALSE` para no imprimir o esconder el código fuente de R del chunk en el documento de salida
     * `warning = FALSE` para no imprimir las advertencias producidas por el código R
     * `message = FALSE` para no imprimir ningún mensaje producido por el código R
     * `include = TRUE/FALSE` para incluir (o no) los resultados generados por los trozos (por ejemplo, los gráficos) en el documento de salida
     * `out.width = ` y `out.height = ` - asigna proporciones de ancho  y largo, por ejemplo `out.width = "75%"`
     * `fig.align = "center"` ajusta cómo se alinea una figura en la página
     * `fig.show = 'hold'` si tu chunk imprime múltiples figuras y querés imprimirlas una al lado de la otra usá también la función` out.width = c("33%", "67%")`. También podés establecer como `fig.show='asis'` para mostrarlas debajo del código que las genera, `'hide'` para ocultarlas, o `'animate'` para concatenar varias figuras en una animación.
* La cabecera de un trozo debe escribirse en *una sola línea*
* Intentá evitar puntos, barras bajas y espacios. Utiliza guiones ( - ) en su lugar si necesitas un separador. 

Leé más extensamente sobre las opciones de **knitr** [aquí](https://yihui.org/knitr/options/).

Algunas de estas opciones pueden configurarse usando los botones de configuración situados en la parte superior derecha del chunk. Aquí puedes especificar qué partes del chunk quieres incluir en el documento renderizado, es decir, el código, las salidas generadas y las advertencias. Estas preferencias aparecerán escritas como código dentro de los corchetes, por ejemplo, si especificás que querés mostrar sólo la salida (‘Show output only’) aparecerá `echo=FALSE` entre los corchetes.


```{r out.width = "80%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/3_chunk.png"))
```

También hay dos flechas en la parte superior derecha de cada trozo, que son útiles para ejecutar el código dentro de un trozo, o todo el código en trozos anteriores. Pasa el cursor por encima de estos iconos para ver lo que hacen.

Para que las opciones globales se apliquen a todos los chunks del script, podés configurar esto dentro del primer chunk de código R en el script. Por ejemplo, para sólo muestrar las salidas generadas por cada trozo de código y no el propio código, podés incluir este comando en el trozo de código R: 

```{r, eval=F}
knitr::opts_chunk$set(echo = FALSE) 
```



#### Código R en el texto {.unnumbered}  

También se puede insertar un mínimo de código R entre comillas invertidas (back ticks) intercalado entre el texto narrativo. Dentro de las comillas invertidas, comenzá el código con la letra "r" y un espacio, para que RStudio sepa que debe evaluar el código como código R. Ver el siguiente ejemplo.

El ejemplo siguiente muestra múltiples niveles de encabezamiento, viñetas, y utiliza el código (`Sys.Date()`) para obtener y mostrar la fecha actual.

```{r out.width = "80%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/2_text.png"))
```


El ejemplo anterior es sencillo (muestra la fecha actual), pero utilizando la misma sintaxis puede mostrar valores producidos por un código R más complejo (por ejemplo, para calcular el mínimo, la mediana o el máximo de una columna). También podés integrar objetos R o valores que han sido creados en trozos de código R anteriores.

Como ejemplo, el siguiente script calcula la proporción de casos que tienen menos de 18 años, utilizando funciones **tidyverse**, y crea los objetos `less18`, `total` y `less18prop`. Este valor dinámico se inserta en el texto narrativo. Vemos cómo queda cuando se teje en un documento de Word.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/3_chunktext.png"))
```


### Imágenes {.unnumbered} 

Hay dos maneras de incluir imágenes en R Markdown:

```{r, eval=F}
![]("path/to/image.png")  
```

Si el código anterior no funciona, probá utilizar `knitr::include_graphics()`  

```{r, eval=F}
knitr::include_graphics("path/to/image.png")
```

(recordá que podes declarar la ruta de tu archivo usando el paquete **here**)

```{r, eval=F}
knitr::include_graphics(here::here("path", "to", "image.png"))
```


### Tablas {.unnumbered}

Creá una tabla utilizando guiones ( - ) y barras ( | ). El número de guiones entre las barras determina el número de espacios en la celda a patrir del cual el texto comienza a envolverse.


```md
Column  1 |Column   2 |Column 3
----------|-----------|--------
Cell A    |Cell B     |Cell C
Cell D    |Cell E     |Cell F
```

El código anterior produce la siguiente tabla: 

Column 1 |Column  2 |Column 3
---------|----------|--------
Cell A   |Cell B    |Cell C
Cell D   |Cell E    |Cell F


### Secciones con pestañas  {.unnumbered} 

Para las salidas HTML, podés organizar las secciones con "pestañas". Basta con añadir `.tabset` entre las llaves `{ }` que se colocan *después de un encabezamiento*. Todos los subtítulos debajo de ese encabezado (hasta el próximo encabezado del mismo nivel) aparecerán como pestañas en las que el usuario puedes clicar. Lee más [aquí](https://bookdown.org/yihui/rmarkdown-cookbook/html-tabs.html)



```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/tabbed_script.png"))
knitr::include_graphics(here::here("images", "markdown/tabbed_view.gif"))

```


Puedes añadir una opción adicional `.tabset-pills` después de `.tabset` para dar a las pestañas una apariencia "en forma de píldora". Ten en cuenta que al ver la salida HTML con etiquetas, la funcionalidad de búsqueda Ctrl+f sólo buscará en las etiquetas "activas", no en las ocultas. 





<!-- ======================================================= -->
## Estructura de los archivos {#file-structure}

Hay varias maneras de estructurar el archivo de R Markdown y sus scripts de R asociados. Cada una tiene sus ventajas y desventajas:  

* R Markdown autónomo - todo lo necesario para el informe se importa o se crea dentro del R Markdown  
  * Ubicar otros archivos - Podés ejecutar scripts R externos con el comando `source()` y utilizar sus salidas en el Rmd
  * Scripts hijos - un mecanismo alternativo para `source()`  
* Utilizar un "archivo de ejecución" - Ejecutar comandos en un script R *antes* de renderizar el R Markdown


### Rmd autónomo  {.unnumbered}

Para un informe relativamente sencillo, puedes optar por organizar tu script de R Markdown de manera que sea "autosuficiente" y no implique utilizar ningún script externo.

Todo lo que se necesite para ejecutar el R Markdown se importa o se crea dentro del archivo Rmd, incluyendo todos los trozos de código y la carga de paquetes. Este enfoque "autosuficiente" es apropiado cuando no necesitás hacer mucho procesamiento de datos (por ejemplo, cuando se importa un archivo de datos limpio o semilimpio) y el procesamiento del R Markdown no tomará demasiado tiempo.

En este escenario, una organización lógica del script de R Markdown podría ser:

1.  Establecer las opciones globales de **knitr**
2.  Cargar paquetes
3.  Importar los datos
4.  Procesar los datos
5.  Generar resultados (tablas, gráficos, etc.)
6.  Guardar los resultados, si es el caso (.csv, .png, etc.)

#### Obtener otros archivos {.unnumbered}

Una variación del enfoque "autocontenido" es hacer que los trozos de código R Markdown busquen (ejecuten) scripts de R externos. Esto puede hacer que tu script de R Markdown sea menos desordenado, más simple y más fácil de organizar. También puede ayudar si quieres mostrar las cifras finales al principio del informe. En este enfoque, el script de R Markdown final simplemente combina las salidas preprocesadas en un documento.

Una forma de hacerlo es proporcionando los scripts de R (ruta del archivo y nombre con extensión) al comando `source()` R **base**.

```{r, eval=F}
source("your-script.R", local = knitr::knit_global())
# or sys.source("your-script.R", envir = knitr::knit_global())
```

Tené en cuenta que cuando utilizás `source()` *dentro* de R Markdown, los archivos externos se  ejecutarán durante el *curso del procesamiento de tu archivo Rmd*. Por lo tanto, cada script se ejecuta cada vez que se procesa el informe. Por lo tanto, utilizar comandos `source()` *dentro del R Markdown* no acelera el tiempo de ejecución, ni ayuda mucho a la depuración, ya que el error producido todavía se imprimirá al producir el R Markdown.

Una alternativa es utilizar la opción `child = ` **knitr**. #EXPLICAR MÁS PARA HACER

Debes ser consciente de los distintos *entornos* de R. Los objetos creados dentro de un entorno no estarán necesariamente disponibles para el entorno utilizado por R Markdown.


### Ejecutar archivo  {.unnumbered} 

Este enfoque implica utilizar el script de R que contiene el comando(s) `render()` para preprocesar los objetos que se introducen en el R markdown.

Por ejemplo, podés cargar los paquetes, cargar y limpiar los datos, e incluso crear los gráficos de interés antes de ejecutar`render()`. Estos pasos pueden ocurrir en el script de R, o en otros scripts que se convocan. Siempre y cuando estos comandos ocurran en la misma sesión de RStudio y los objetos se guarden en el entorno, los objetos pueden ser convocados dentro del contenido de Rmd. Entonces R markdown sólo se utilizará para el paso final, es decir, para producir la salida con todos los objetos pre-procesados. Esto es mucho más fácil de depurar si se genera algún error.

Este enfoque es útil por las siguientes razones:

* Mensajes de error más informativos - estos mensajes serán generados por el script de R, no por el R Markdown. Los errores de R Markdown tienden a informar qué trozo tuvo un problema, pero no te dirán qué línea.
* Si ejecutás muchos pasos de procesamiento antes de usar el comando `render()` - se ejecutarán sólo una vez.

En el ejemplo siguiente, tenemos un script de R en el que preprocesamos el objeto `data` en el entorno de R y luego procesamos "create_output.Rmd" usando `render()`.


```{r, eval=F}
data <- import("datafile.csv") %>%       # Cargar datos y guardarlos en el entorno
  select(age, hospital, weight)          # Seleccionar columnas

rmarkdown::render(input = "create_output.Rmd")   # Crear archivo Rmd 
```



### Estructura de carpetas  {.unnumbered} 

El flujo de trabajo también se refiere a la estructura general de las carpetas, como tener una carpeta de 'salida' para los documentos y figuras creados, y carpetas de 'datos' o 'entradas' para los datos depurados. No entramos en más detalles aquí, pero echa un vistazo a la página de [organización de informes rutinarios](#organizing-routine-reports).




## Producir el documento {#producing-the-document}  

Puedes generar el documento de las siguientes maneras:

* Manualmente haciendo click sobre el botón "Knit" en la parte superior del editor de scripts de RStudio (rápido y fácil)
* Ejecutando el comando `render()` (ejecutado fuera del script de R Markdown)  


### Opción 1: botón "Knit" {.unnumbered}  

Cuando tengas el archivo Rmd abierto, cliqueá el botón 'Knit' en la parte superior del archivo.

R Studio te mostrará el progreso dentro de una pestaña 'R Markdown' cerca de la consola R. El documento se abrirá automáticamente cuando esté completo.

El documento se guardará en la misma carpeta que tu script de R markdown, y con el mismo nombre de archivo (excepto la extensión). Obviamente, esto no es ideal para el control de versiones (se sobreescribirá cada vez que se haga un knit, a menos que se mueva manualmente), ya que entonces puede que tengas que renombrar el archivo (por ejemplo, añadir una fecha).

Este es el botón de acceso directo de RStudio para la función `render()` de **rmarkdown**. Este enfoque sólo es compatible con un R markdown autocontenido, donde todos los componentes necesarios existen o se convocan dentro del archivo. 

```{r out.width = "90%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/4_progress.png"))
```



### Opción 2: comando `render()`  {.unnumbered}

Otra forma de producir el documento de salida de R Markdown es ejecutar la función `render()` (del paquete **rmarkdown**). Debés ejecutar este comando *fuera* del script de R Markdown, ya sea en un script de R separado (a menudo llamado "archivo de ejecución"), o como un comando independiente en la consola de R.

```{r, eval=F}
rmarkdown::render(input = "my_report.Rmd")
```

Al igual que con "knit", la configuración predeterminada guardará la salida Rmd en la misma carpeta que el script Rmd, con el mismo nombre de archivo (excepto la extensión del archivo). Por ejemplo, "mi_informe.Rmd" cuando se procese creará "mi_informe.docx" si se procesa a un documento de Word. Sin embargo, al usar `render()` existe la opción de usar diferentes configuraciones. `render()` puede aceptar argumentos que incluyen:

* `output_format =` Este es el formato del documento salida al que se va a convertir (por ejemplo, `"html_document"`, `"pdf_document"`, `"word_document"`, o `"all"`). Esto también se puede especificar en el YAML dentro del script de R Markdown.
* `output_file = ` Este es el nombre del archivo de salida (y la ruta del archivo). Se puede crear a través de funciones de R como `here()` o `str_glue()` como se demuestra a continuación.
* `output_dir = `Este es un directorio de salida (carpeta) para guardar el archivo. Esto te permite elegir un directorio distinto en el que se guarda el archivo Rmd.
* `output_options = ` Podés proporcionar una lista de opciones que anulen las del YAML del script
* `output_yaml = ` Podés proporcionar la ruta a un archivo .yml que contenga las especificaciones YAML
* `params = ` Ver la sección de parámetros más abajo
* Ver la lista completa [aquí](https://pkgs.rstudio.com/rmarkdown/reference/render.html)

Como ejemplo, para mejorar el control de versiones, el siguiente comando guardará el archivo de salida dentro de una subcarpeta 'outputs', con la fecha actual en el nombre del archivo. Para crear el nombre del archivo, se utiliza la función `str_glue()` del paquete **stringr** para "pegar" las cadenas estáticas (escritas sin formato) con el código dinámico de R (escrito entre corchetes). Por ejemplo, si es 10 de abril de 2021, el nombre del archivo será "Informe_2021-04-10.docx". Consultá la página sobre [Caracteres y cadenas](#characters-and-strings) para obtener más detalles sobre `str_glue()`.

```{r, eval=F}
rmarkdown::render(
  input = "create_output.Rmd",
  output_file = stringr::str_glue("outputs/Report_{Sys.Date()}.docx")) 
```

A medida que el archivo se procesa, la consola de RStudio mostrará el progreso hasta el 100%, y un mensaje final para indicar que la renderización se ha completado.



###  Opción 3: paquete **reportfactory** {.unnumbered} 

El paquete de R **reportfactory** ofrece un método alternativo de organización y compilación de informes R Markdown *para situaciones en las que se ejecutan informes de forma rutinaria (por ejemplo, diariamente, semanalmente...).* Facilita la compilación de múltiples archivos R Markdown y la organización de sus resultados. En esencia, proporciona una "fábrica" desde la que se pueden ejecutar los informes R Markdown, obtener automáticamente carpetas con fecha y hora para los resultados, y tener un control de versiones "ligero".

Leé más sobre este flujo de trabajo en la página sobre la [organización de informes rutinarios](#organizing-routine-reports). 



<!-- ======================================================= -->
## Informes parametrizados {#parameterised-reports}

Podés utilizar la parametrización para generar informes dinámicos, de forma que pueda ejecutarse con una configuración específica (por ejemplo, una fecha o lugar concretos o con determinadas opciones de procesamiento). A continuación, nos centramos en los aspectos básicos, pero hay más [detalles en línea](https://bookdown.org/yihui/rmarkdown/parameterized-reports.html) sobre los informes parametrizados.

Utilizando el listado de casos de Ébola como ejemplo, digamos que queremos ejecutar un informe de diario vigilancia estándar para cada hospital. Mostramos cómo se puede hacer esto usando parámetros.

*Importante: los informes dinámicos también son posibles sin la estructura formal de parámetros (sin params:), utilizando simples objetos R en un script adyacente. Esto se explica al final de esta sección.*


### Establecer parámetros  {.unnumbered}

Existen varias opciones para especificar los valores de los parámetros para tu documento de salida de R Markdown.

#### Opción 1: Establecer parámetros dentro de YAML {.unnumbered}

Editá el YAML para incluir una opción `params`:, con declaraciones indentadas para cada parámetro a definir. En este ejemplo creamos los parámetros `date` y `hospital`, y especificamos sus valores. Estos valores están sujetos a cambios cada vez que se ejecuta el informe. Si utilizás el botón "Knit" para producir la salida, los parámetros estarán predeterminados por estos valores. Del mismo modo, si utilizás `render()` los parámetros tendrán estos valores por defecto a menos que se especifiquen de otra manera en el comando `render()`.


```yaml
---
title: Informe de vigilancia
output: html_document
params:
 date: 2021-04-10
 hospital: Hospital Central
---
```

En un segundo plano, los valores de los parámetros están contenidos en una lista de sólo lectura llamada `params`. Así, puedes insertar los valores de los parámetros en el código de R como lo harías con otro objeto/valor de R en tu entorno. Simplemente escriba `params$` seguido del nombre del parámetro. Por ejemplo `params$hospital` para representar el nombre del hospital ("Hospital Central" por defecto).

Tené en cuenta que los parámetros también pueden tener valores `true` o `false`, y por lo tanto estos pueden ser incluidos en sus opciones de **knitr** dentro de un chunk de R. Por ejemplo, puedes establecer `{r, eval=params$run}` en lugar de `{r, eval=FALSE}`, y ahora si el chunk se ejecuta o no depende del valor de un parámetro `run:`.

Los parámetros que son fechas, serán introducidos como una cadena. Por lo tanto, para que `params$date` se interprete en el código de R, es probable que tenga que ser envuelto con `as.Date()` o una función similar para convertir al tipo Date.




#### Opción 2: Establecer los parámetros dentro de `render()` {.unnumbered}  

Como se ha mencionado anteriormente, como alternativa a cliquear el botón "Knit" para producir la salida es ejecutar la función `render()` desde un script independiente. En este último caso, se pueden especificar los parámetros a utilizar con el argumento `params = ` de `render()`.

Hay que tener en cuenta que los valores de los parámetros asignados aquí *sobrescribirán* sus valores predeterminados si aparacen el YAML. Escribimos los valores entre comillas ya que en este caso deben ser definidos como valores de carácter/cadena.

El siguiente comando genera "surveillance_report.Rmd", especifica un nombre dinámico para el archivo de salida y una carpeta, y proporciona un `list()` de dos parámetros y sus valores al argumento `params = `.

```{r, eval=F}
rmarkdown::render(
  input = "surveillance_report.Rmd",  
  output_file = stringr::str_glue("outputs/Report_{Sys.Date()}.docx"),
  params = list(date = "2021-04-10", hospital  = "Central Hospital"))
```


#### Opción 3: Establecer los parámetros mediante una interfaz gráfica de usuario {.unnumbered}  

Para obtener una experiencia más interactiva, se puede utilizar la interfaz gráfica de usuario (GUI, por sus siglas en ingles) para seleccionar manualmente los valores de los parámetros. Para ello, podemos clicar en el menú desplegable situado junto al botón ‘Knit’ y elegir ‘Knit with parameters’.

Aparecerá una ventana que te permitirá introducir los valores de los parámetros establecidos en el YAML del documento.

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/5_parametersGUI.png"))
```

Se puede lograr lo mismo a través del comando `render()` especificando `params = "ask"`, como se demuestra a continuación.

```{r, eval=F}
rmarkdown::render(
  input = "surveillance_report.Rmd",  
  output_file = stringr::str_glue("outputs/Report_{Sys.Date()}.docx"),
  params = “ask”)
```


Sin embargo, la asignación de valores en esta ventana emergente está sujeta a errores y faltas de ortografía. Es posible  añadir restricciones a los valores que se pueden introducir a través de los menús desplegables. Podés hacerlo añadiendo en el YAML especificaciones para cada entrada `params:`.

* `label: ` es el título para ese menú desplegable en particular
* `value: ` es el valor predeterminado (inicial)
* `input: ` establecer `select` para utilizar un menú desplegable  
* `choices: ` Indique los valores opcionales en el menú desplegable  

A continuación, estas especificaciones están escritas para el parámetro `hospital`  

```yaml
---
title: Surveillance report
output: html_document
params:
 date: 2021-04-10
 hospital: 
  label: “Town:”
  value: Central Hospital
  input: select
  choices: [Central Hospital, Military Hospital, Port Hospital, St. Mark's Maternity Hospital (SMMH)]
---
```

Al procesarlo (con el botón 'knit with parameters' o con `render()`, la ventana emergente tendrá opciones desplegables para seleccionarlos.

```{r out.width = "50%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/5_parametersGUIB.png"))
```




### Ejemplo parametrizado  {.unnumbered}

El siguiente código crea parámetros para `date` y `hospital`, que se utilizan en el R Markdown como `params$date` y  `params$hospital`, respectivamente.

En la salida del informe resultante, los datos se filtran al hospital específico, y el título del gráfico se refiere al hospital y a la fecha correctos. En este caso utilizamos el archivo "linelist_cleaned.rds", pero sería especialmente adecuado que la propia lista de casos tuviera también un sello de fecha para alinearse con la fecha parametrizada.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/6_Rmdexample.png"))
```

Si se procesa esto se obtiene la salida final con la fuente y el diseño predeterminados.

```{r out.width = "80%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/6_RmdexampleB.png"))
```


### Parametrización sin parámetros  {.unnumbered}

Si estás procesando un archivo R Markdown con `render()` desde un script independiente, puede crear el mismo efecto de parametrización sin usar la funcionalidad `params:`.

Por ejemplo, en el *script de R* que contiene el comando `render()`, podés simplemente definir `hospital` y `date` como dos objetos R (valores) antes del comando `render()`. En el R Markdown, no sería necesario tener una sección `params:` en el YAML, y nos referiríamos al objeto `date` en lugar de `params$date` y a `hospital` en lugar de `params$hospital`.

```{r, eval=F}
# This is a R script that is separate from the R Markdown

# define R objects
hospital <- "Central Hospital"
date <- "2021-04-10"

# Render the R markdown
rmarkdown::render(input = "create_output.Rmd") 
```

Este enfoque significa que no se puede procesar con “knit with parameters”, ni utilizar la interfaz gráfica, ni incluir opciones de procesamiento dentro de los parámetros. Sin embargo, permite simplificar el código, lo cual puede resultar ventajoso.


<!-- ======================================================= -->

## Informes en bucle  {#looping-reports}

Es posible que queramos ejecutar un informe varias veces, variando los parámetros de entrada, para producir un informe para cada jurisdicción/unidad. Esto puede hacerse utilizando herramientas para la *iteración*, que se explican en detalle en la página sobre [Iteración, bucles y listas](#iteration-loops-and-lists). Las opciones incluyen el paquete **purrr**, o el uso de un *for loop* como se explica a continuación.

A continuación, utilizamos un simple *for loop* para generar un informe de vigilancia para todos los hospitales de interés. Esto se hace con un solo comando (en lugar de cambiar manualmente el parámetro del hospital uno por uno). El comando para generar los informes debe existir en un script separado *fuera* del informe Rmd. Este script también contendrá objetos definidos para "hacer un bucle" - la fecha de hoy, y un vector de nombres de hospitales para hacer un bucle. 


```{r, eval=F}
hospitals <- c("Central Hospital",
                "Military Hospital", 
                "Port Hospital",
                "St. Mark's Maternity Hospital (SMMH)") 
```

A continuación, introducimos estos valores uno a uno en el comando `render()` mediante un bucle, que ejecuta el comando una vez por cada valor del vector `hospitales`. La letra "i" representa la posición del índice (del 1 al 4) del hospital que se está utilizando en esa iteración, de modo que "lista_de_hospitales[1]` sería "Hospital Central". Esta información se suministra en dos lugares en el comando `render()`:  

1) Al nombre del archivo, de forma que el nombre del archivo de la primera iteración, si se produce el 10 de abril de 2021, sería "Informe_Hospital Central_2021-04-10.docx", guardado en la subcarpeta 'output' del directorio de trabajo.  
2) A `params = ` de forma que el Rmd utilice el nombre del hospital internamente siempre que se llame al valor `params$hospital` (por ejemplo, para filtrar los datos sólo a un hospital determinado). En este ejemplo, se crearían cuatro archivos, uno por cada hospital.

```{r, eval=F}
for(i in 1:length(hospitals)){
  rmarkdown::render(
    input = "surveillance_report.Rmd",
    output_file = str_glue("output/Report_{hospitals[i]}_{Sys.Date()}.docx"),
    params = list(hospital  = hospitals[i]))
}       
```



<!-- In the scenario where you are f not using this strict form of parameterisation but saving objects to the environment, as discussed at the end of the parameterisation section, the render function would look like this: -->

<!-- ```md -->
<!-- for(i in 1:length(hospital_list)){ -->
<!-- rmarkdown::render("surveillance_report.Rmd", -->
<!--                   output_file = paste0("output/Report_", hospital_list[i], refdate, ".docx") -->
<!-- }        -->
<!-- ``` -->
<!-- The text within the markdown would then need to refer to `hospital_list[i]` and `refdate`.  -->






<!-- ======================================================= -->
## Plantillas {#templates}  

Utilizando un documento de plantilla que contenga cualquier formato deseado, podés ajustar la estética del archivo de salida Rmd. Podés crear, por ejemplo, un archivo de MS Word o Powerpoint que contenga páginas/diapositivas con las dimensiones, marcas de agua, fondos y fuentes deseadas.

### Documentos en Word  {.unnumbered}

Para crear una plantilla, iniciá un nuevo documento de Word (o utiliza uno ya existente con el formato deseado), y editá las fuentes definiendo los Estilos. En el Estilo, los encabezados 1, 2 y 3 se refieren a los distintos niveles de encabezado de markdown (`# Header 1`, `## Header 2` and `### Header 3`, respectivamente). Cliqueá con el botón derecho en el estilo y selectioná 'modificar' para cambiar el formato de la fuente, así como el párrafo (por ejemplo, podés introducir saltos de página antes de ciertos estilos que pueden ayudar con el espaciado). Otros aspectos del documento de Word, como los márgenes, el tamaño de la página, los encabezados, etc., pueden modificarse como un documento de Word normal en el que se trabaja directamente. 

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/7_template.png"))
```

### Documentos en Powerpoint {.unnumbered}

Como en el caso anterior, creá un nuevo conjunto de diapositivas o utiliza un archivo PowerPoint existente con el formato deseado. Para seguir editando, cliqueá en "Ver" y "Patrón de diapositivas". Desde aquí se puede cambiar la apariencia de la diapositiva "maestra" editando el formato del texto en los cuadros de texto, así como el fondo y las dimensiones del página.

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/8_ppttemplate.png"))
```

Desgraciadamente, la edición de archivos PowerPoint es un poco menos flexible:

* Un encabezado de primer nivel (`# Header 1`) se convertirá automáticamente en el título de una nueva diapositiva,
* El texto del `# Header 2` no aparecerá como subtítulo, sino como texto dentro del cuadro de texto principal de la diapositiva (a menos que encuentre una manera de manipular la vista del Patrón).
* Los gráficos y las tablas resultantes irán automáticamente a nuevas diapositivas. Tendrás que combinarlos, por ejemplo con la función **patchwork** para combinar ggplots, para que aparezcan en la misma página. Esta [entrada del blog](https://mattherman.info/blog/ppt-patchwork/) trata el uso del paquete patchwork para colocar múltiples imágenes en una diapositiva.

En el [paquete **oficcer **](https://davidgohel.github.io/officer/) encontrarás una herramienta para trabajar más a fondo con las presentaciones de PowerPoint.




### Integración de plantillas en el YAML  {.unnumbered}

Una vez preparada la plantilla, el detalle de la misma puede añadirse en el YAML del Rmd debajo de la línea 'output' y debajo de donde se especifica el tipo de documento (que va a una línea aparte). Para las plantillas de diapositivas de PowerPoint se puede utilizar `reference_doc`.

Lo más fácil es guardar la plantilla en la misma carpeta en la que está el archivo Rmd (como en el ejemplo siguiente), o en una subcarpeta dentro de ella.

```yaml
---
title: Surveillance report
output: 
 word_document:
  reference_docx: "template.docx"
params:
 date: 2021-04-10
 hospital: Central Hospital
template:
 
---
```

### Formateo de archivos HTML {.unnumbered}

Los archivos HTML no utilizan plantillas, pero pueden tener los estilos configurados dentro del YAML. Los HTML son documentos interactivos y particularmente flexibles. Aquí cubrimos algunas opciones básicas.

* Tabla de contenidos: Podemos añadir una tabla de contenidos con `toc: true`, y también especificar que permanezca visible ("flotante") al desplazarse, con `toc_float: true`.

* Temas: Nos referimos a algunos temas prearmados, que provienen de una biblioteca de temas de Bootswatch. En el siguiente ejemplo utilizamos cerulean. Otras opciones son: journal, flatly, darkly, readable, spacelab, united, cosmo, lumen, paper, sandstone, simplex y yeti.

* Resaltar: Configurando esto se cambia el aspecto del texto resaltado (por ejemplo, el código dentro de los trozos que se muestran). Los estilos disponibles son default, tango, pygments, kate, monochrome, espresso, zenburn, haddock, breezedark y textmate.

He aquí un ejemplo de cómo integrar las opciones anteriores en el YAML.

```yaml
---
title: "HTML example"
output:
  html_document:
    toc: true
    toc_float: true
    theme: cerulean
    highlight: kate
    
---
```

A continuación se muestran dos ejemplos de salidas HTML ambas con tablas de contenido flotantes pero con diferentes estilos de tema y resaltado seleccionados:


```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/9_html.png"))
```


## Contenido dinámico {#dynamic-content}  

En una salida HTML, el contenido de tu informe puede ser dinámico. A continuación, veremos algunos ejemplos:

### Tablas  {.unnumbered} 

En un informe HTML, se puede imprimir un dataframe / tibble de manera que el contenido sea dinámico, con filtros y barras de desplazamiento. Hay varios paquetes que ofrecen esta capacidad.

Para hacer esto con el paquete **DT**, como se utiliza en este manual, se puede insertar un trozo de código como este:

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "markdown/10_dynamictable.png"))
```

La función `datatable()` imprimirá el dataframe proporcionado como una tabla dinámica para el lector. Puedes establecer `rownames = FALSE` para simplificar el extremo izquierdo de la tabla. `filter = "top"` proporciona un filtro sobre cada columna. En el argumento `options()` proporciona una lista de otras especificaciones. A continuación incluimos dos: `pageLength = 5` determina que el número de filas a mostrar sea 5 (las filas restantes se pueden ver paginando a través de flechas), y `scrollX=TRUE` habilita una barra de desplazamiento en la parte inferior de la tabla (para visualizar las columnas que se extienden a la extrema derecha).

Si tu conjunto de datos es muy grande, considerá mostrar sólo las filas superiores envolviendo los datos en `head()`. 


### Widgets HTML  {.unnumbered}

Los [widgets HTML para R](http://www.htmlwidgets.org/) son un tipo especial de paquetes de R que permiten una mayor interactividad utilizando bibliotecas de JavaScript. Puedes incorporarlos en salidas HTML R Markdown.

Algunos ejemplos comunes de estos widgets son:

* Plotly (utilizado en la página de este manual y en la página de [Gráficos interactivos](#interactive-plots)
* visNetwork (utilizado en la página de [Cadenas de transmisión](#transmission-chains) de este manual)
* Leaflet (Folleto) (utilizado en la página [Conceptos básicos de los SIG](#gis-basics) de este manual)
* dygraphs (útil para mostrar interactivamente los datos de las series temporales)
* DT (`datatable()`) (se utiliza para mostrar tablas dinámicas con filtro, ordenación, etc.)

La función `ggplotly()` de **plotly** es particularmente fácil de usar. Consúltalo en la sección en la página de [Gráficos interactivos](#plot-with-ggplotly).


## Recursos {#resources-33}

Podés encontrar más información en:

* https://bookdown.org/yihui/rmarkdown/
* https://rmarkdown.rstudio.com/articles_intro.html

Aquí encontras una buena explicación de markdown vs knitr vs Rmarkdown: https://stackoverflow.com/questions/40563479/relationship-between-r-markdown-knitr-pandoc-and-bookdown


```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/rmarkdown.Rmd-->

# Organización de informes rutinarios {#organizing-routine-reports}

Esta página cubre el paquete **reportfactory**, que es un *complemento para el uso de R Markdown para los informes*.

Este paquete facilita la ejecucion de reportes de rutina, especialmente la compilación de múltiples archivos R Markdown y la organización de sus resultados. En esencia, proporciona una "fábrica" desde la que se pueden ejecutar los informes R Markdown, obtener automáticamente carpetas con fecha y hora para guardar los archivos de salida, y generar un control de versiones "ligero".

**reportfactory** es uno de los paquetes desarrollados por RECON (R Epidemics Consortium). Aquí está su [sitio web](https://www.repidemicsconsortium.org/) y su [Github](https://github.com/reconverse). 


## Preparación {#preparation-34}

### Cargar paquetes {.unnumbered}  

En RStudio, instalá la última versión del paquete **reportfactory** desde Github.

Podés hacerlo a través del paquete **pacman** con `p_load_current_gh()` que forzará la instalación de la última versión desde Github. Proporcioná la cadena de caracteres "reconverse/reportfactory", que especifica la organización de Github (reconverse) y el repositorio (reportfactory). También puede utilizar `install_github()` del paquete **remotes**, como alternativa.

```{r, eval=FALSE}
# Instalá y cargá la última versión del paquete desde Github
pacman::p_load_current_gh("reconverse/reportfactory")
#remotes::install_github("reconverse/reportfactory") # alternativa
```


## Nueva fábrica {#new-factory}

Para crear una nueva fábrica, ejecutá la función `new_factory()`. Esto creará una nueva carpeta de proyecto R autocontenida con las siguientes características predeterminadas:

* La fábrica se añadirá a tu directorio de trabajo
* El nombre del proyecto R de la fábrica será "new_factory.Rproj"
* Tu sesión de RStudio se "trasladará" a este proyecto R

```{r, eval=F}
# Este comando creará una fabrica en el directorio de trabajo
new_factory()
```

Mirando dentro de la fábrica, se puede ver que las subcarpetas y algunos archivos se han creado de manera automática.


```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_new2.png"))
```

* La carpeta *report_sources* contendrá tus scripts R Markdown, que generan sus informes
* La carpeta de *outputs* contendrá el informe de salida (por ejemplo, HTML, Word, PDF, etc.)
* La carpeta de *scripts* puede utilizarse para guardar otros scripts de R (por ejemplo, los que se convican en tus scripts de Rmd)
* La carpeta de *data* puede utilizarse para guardar tus datos (se incluyen las subcarpetas "raw" (datos brutos) y "clean" (datos limpios))
* Un archivo *.here*, para que puedas utilizar el paquete **here** para convocar a los archivos de las subcarpetas gracias a su relación con esta carpeta raíz (véase la página de [proyectos en R](#r-projects) para más detalles)
* Se ha creado un archivo *gitignore* en caso de que se vincule este proyecto R a un repositorio de Github (ver [Control de versiones y colaboración con Github])
* Un archivo README vacío, en caso de que uses un repositorio de Github


<span style="color: orange;">**_PRECAUCIÓN::_** dependiendo de la configuración de tu ordenador, los archivos como ".here" pueden existir pero estar ocultos.</span>  

A continuación mencionamos configuraciones predeterminadas que tal vez quieras ajustar con el comando `new_factory()`: 

* `factory = ` Proporciona un nombre para la carpeta de fábrica (por defecto es "new_factory")
* `path = ` Designa una ruta de archivo para la nueva fábrica (por defecto es el directorio de trabajo)
* `report_sources = ` Proporciona un nombre alternativo para la subcarpeta que contiene los scripts R Markdown (por defecto es "report_sources")
* `outputs = ` Proporciona un nombre alternativo para la carpeta que contiene los resultados del informe (por defecto es "outputs")

Ver `?new_factory` para ver una lista completa de los argumentos.

Cuando creás la nueva fábrica, tu sesión de R se transfiere al nuevo proyecto R, por lo que debés cargar de nuevo el paquete **reportfactory**.

```{r, eval=FALSE}
pacman::p_load(reportfactory)
```

Ahora podés ejecutar el comando `factory_overview()` para ver la estructura interna (todas las carpetas y archivos) de la fábrica. 

```{r, eval=F}
factory_overview()            # muestra la estructura de la fábrica en la consola
```

El siguiente "árbol" de las carpetas y archivos de la fábrica se imprime en la consola de R. Fijáte que en la carpeta "data" hay subcarpetas para los datos "raw" y "clean", y datos CSV de ejemplo. También hay "example_report.Rmd" en la carpeta "report_sources". 

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_overview.png"))
```


## Crear un informe {#create-a-report}

Desde la fábrica del proyecto R, creá un informe R Markdown como lo harías normalmente, y guardálo en la carpeta "report_sources". Consultá la página de [R Markdown](reports-with-r-markdown) para obtener instrucciones. A modo de ejemplo, hemos añadido lo siguiente a la fábrica:

* Un nuevo script de R markdown titulado "daily_sitrep.Rmd", guardado dentro de la carpeta "report_sources".
* Datos para el informe ("linelist_cleaned.rds") guardados en la subcarpeta "clean" dentro de la carpeta "data"

Ejecutando factory_overview() podemos ver el archivo R Markdown en la carpeta "report_sources" y el archivo de datos en la carpeta de datos "clean" (resaltado):

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_overview2.png"))
```

A continuación mostramos una captura de pantalla del comienzo del archivo de Markdown "daily_sitrep.Rmd". Podés ver que el formato de salida está configurado para ser HTML, a través de la cabecera YAML `output: html_document`.

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_new_rmd.png"))
```

En este sencillo script, hay comandos para:

* Cargar los paquetes necesarios
* Importar los datos del listado de casos utilizando una ruta de archivo del paquete **here** (lea más en la página sobre [Importación y exportación](#import-and-export)) 

```{r, eval=F}
linelist <- import(here("data", "clean", "linelist_cleaned.rds"))
```

* Imprimir una tabla de resumen de casos, y exportarla con `export()` como un archivo .csv
* Imprimir una epicurva, y exportarla con `ggsave()` como un archivo .png

Podés revisar la lista de informes R Markdown en la carpeta "report_sources" con este comando:

```{r, eval=F}
list_reports()
```



## Compilar {#compile} 

En una fábrica de informes, "compilar" un informe de R Markdown implica que se ejecutará el script .Rmd y se producirá la salida (como se especifica en el script YAML, por ejemplo, como HTML, Word, PDF, etc.).

*La fábrica creará automáticamente una carpeta con fecha y hora para las salidas en la carpeta "outputs".*

El informe de salida y los archivos generados por el script (por ejemplo, csv, png, xlsx) se guardarán en esta carpeta. Además, el propio script Rmd se guardará en esta carpeta, así tendrás un registro de esa versión del script.

Esto contrasta con el comportamiento normal de un R Markdown "tejido", que guarda las salidas en la ubicación del script Rmd. Este comportamiento por defecto puede resultar en carpetas abarrotadas y desordenadas. El objetivo de la fábrica es mejorar la organización de archivos cuando uno necesita ejecutar informes con frecuencia.

### Compilar por nombre  {.unnumbered}   

Podés compilar un informe específico ejecutando `compile_reports()` y proporcionando el nombre del script Rmd (sin la extensión .Rmd) a `reports = `. Para simplificar, podés omitir `reports = ` y simplemente escribir el nombre R Markdown entre comillas, como se indica a continuación.  

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_compile1.png"))
```


Este comando compilaría sólo el informe "daily_sitrep.Rmd", guardando el informe de HTML, y las exportaciones de la tabla de .csv y la epicurva de .png en una subcarpeta con fecha y hora específicas, dentro de la carpeta "outputs".

Tené en cuenta que si proporcionás la extensión .Rmd, debés escribir la extensión tal como aparece en el nombre del archivo (.rmd vs. .Rmd).

También hay que tener en cuenta que, al compilar, es posible que aparezcan temporariamente varios archivos en la carpeta "report_sources", pero pronto desaparecerán al ser transferidos a la carpeta "outputs".

### Compilación por número  {.unnumbered}

También se puede especificar el script Rmd a compilar proporcionando un número o vector de números a `reports = `. Los números deben alinearse con el orden en que aparecen los informes cuando se ejecuta `list_reports()`.


```{r, eval=F}
# Compilar el segundo y el cuarto Rmd en la carpeta "report_sources"
compile_reports(reports = c(2, 4))
```



### Compilar todos  {.unnumbered}

Puedes compilar *todos* los informes R Markdown en la carpeta "report_sources" usando el argumento `reports = ` a TRUE.

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_compile_all.png"))
```


### Compilar desde la subcarpeta {.unnumbered} 

Podés añadir subcarpetas a la carpeta "report_sources". Para ejecutar un informe R Markdown desde una subcarpeta, simplemente proporcioná el nombre de la carpeta a `subfolder = `. A continuación se muestra un ejemplo de código para compilar un informe Rmd localizado en una subcarpeta de "report_sources".

```{r, eval=F}
compile_reports(
     reports = "summary_for_partners.Rmd",
     subfolder = "for_partners")
```

Podés compilar todos los informes Rmd dentro de una subcarpeta proporcionando el nombre de la subcarpeta a `reports = `, con una barra al final, como se indica a continuación.

```{r, eval=F}
compile_reports(reports = "for_partners/")
```


### Parametrización  {.unnumbered}

Como indicamos en la página sobre [Informes con R Markdown](#reports-with-r-markdown), podés ejecutar informes con parámetros especificados. Podés pasar estos parámetros como una lista a `compile_reports()` a través del argumento `params = `. Por ejemplo, en este informe ficticio hay tres parámetros proporcionados a los informes de R Markdown.

```{r, eval=F}
compile_reports(
  reports = "daily_sitrep.Rmd",
  params = list(most_recent_data = TRUE,
                region = "NORTHERN",
                rates_denominator = 10000),
  subfolder = "regional"
)
```


### Utilizar un "run-file"  {.unnumbered} 

Si tenés que ejecutar varios informes, podés crear un script de R que contenga todos los comandos `compile_reports()`. Un usuario puede simplemente ejecutar todos los comandos en este script de R y todos los informes se compilarán. Puedes guardar este "archivo de ejecución" (run file) en la carpeta "scripts".


## Salidas {#outputs-1}   

Después de haber compilado los informes unas cuantas veces, la carpeta "outputs" tendría este aspecto (los resaltados se han añadido para mayor claridad): 


```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_overview_all.png"))
```


* Dentro de "outputs", se han creado subcarpetas para cada informe Rmd
* Dentro de ellas, se han creado otras subcarpetas para cada compilación única
  * Están marcados con fecha y hora ("2021-04-23_T11-07-36" significa 23 de abril de 2021 a las 11:07:36)
  * Podés editar el formato de la fecha/hora. Ver `?compile_reports`
* Dentro de cada carpeta compilada de fecha/hora, se almacena el resultado del informe (por ejemplo, HTML, PDF, Word) junto con el script Rmd (¡control de versiones!) y cualquier otro archivo exportado (por ejemplo, table.csv, epidemic_curve.png)

Esta es una vista dentro de una de las carpetas con fecha/hora, para el informe "daily_sitrep". La ruta del archivo está resaltada en amarillo para enfatizar.


```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_compile_folder.png"))
```


Por último, a continuación mostramos una captura de pantalla del informe de salida de HTML .


```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "factory_html.png"))
```

Podés utilizar `list_outputs()` para ver una lista de las salidas.




## Miscelánea  {#miscellaneous-1} 

### Knit  {.unnumbered}

Si querés "procesar" uno de tus informes R Markdown cliqueando el botón "Knit" podés hacerlo. En este caso, por defecto, las salidas aparecerán en la carpeta donde se guarda el Rmd - la carpeta "report_sources". En versiones anteriores de **reportfactory**, la presencia de cualquier archivo que no sea Rmd en la carpeta "report_sources" impediría la compilación, pero esto ya no es así. Es posible ejecutar `compile_reports()` y no se producirá ningún error. 

### Scripts  {.unnumbered} 

Te recomendamos utilizar la carpeta "scripts" para almacenar "archivos de ejecución" o scripts .R que se originan en tus scripts .Rmd. Consultá la página sobre [R Markdown](#reports-with-r-markdown) para obtener consejos sobre cómo estructurar tu código en varios archivos. 


### Extras  {.unnumbered}

* Con **reportfactory**, podés utilizar la función `list_deps()` para listar todos los paquetes requeridos en todos los informes de toda la fábrica.

* Hay un paquete de acompañamiento en desarrollo llamado **rfextras** que ofrece más funciones de ayuda para asistirte en la construcción de informes, tales como:
  * `load_scripts()` - carga todos los scripts .R en una carpeta determinada (la carpeta "scripts" por defecto)
  * `find_latest() `- encuentra la última versión de un archivo (por ejemplo, el último conjunto de datos)




<!-- ======================================================= -->
## Recursos {#resources-34}

Consultá la [página de Github del paquete **reportfactory**](https://github.com/reconverse/reportfactory)

Consultá la [página de Github del paquete **rfextras**](https://github.com/reconhub/rfextras) 

```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/reportfactory.Rmd-->


# Dashboards con R Markdown {#dashboards-with-r-markdown}

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_output.png"))
```


Esta página cubrirá el uso básico del paquete **flexdashboard**. Este paquete permite formatear fácilmente la salida de R Markdown como un dashboard (panel de control o cuadro de mandos) y páginas. El contenido del panel puede ser texto, figuras/tablas estáticas o gráficos interactivos.

Ventajas de **flexdashboard**:

* Requiere una codificación mínima de R no estándar - con muy poca práctica puedes crear rápidamente un panel de control
* El Dashboard puede enviarse por correo electrónico a los compañeros como un archivo HTML autónomo, sin necesidad de servidor
* Puedes combinar **flexdashboard** con **shiny**, **ggplotly** y otros *"widgets html"* para añadir interactividad

Desventajas de **flexdashboard**:

* Menos personalización en comparación con el uso de **Shiny** para crear un panel de control

En la sección de Recursos se pueden encontrar tutoriales muy completos sobre el uso de **flexdashboard** que sirvieron de base a esta página. A continuación describimos las características principales y damos un ejemplo de construcción de un dashboard para explorar un brote, utilizando los datos de `linelist`.


## Preparación {#preparation-35}

### Cargar paquetes {.unnumbered}  

En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base**. Consulta la página [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r}
pacman::p_load(
  rio,             # data import/export     
  here,            # locate files
  tidyverse,       # data management and visualization
  flexdashboard,   # dashboard versions of R Markdown reports
  shiny,           # interactive figures
  plotly           # interactive figures
)
```

### Importar datos {.unnumbered}  

Importamos los datos de casos de una epidemia de ébola simulada. Si quieres seguir el proceso, [clica para descargar linelist "limpio"](https://github.com/epirhandbook/Epi_R_handbook/raw/master/data/case_linelists/linelist_cleaned.rds) (como archivo .rds). Importa los datos con la función import() del paquete **rio** (maneja muchos tipos de archivos como .xlsx, .csv, .rds - mira la página de [importación y exportación](#import-and-export) para más detalles).

```{r, echo=F}
# import the linelist into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, eval=F}
# import the linelist
linelist <- import("linelist_cleaned.rds")
```

A continuación se muestran las primeras 50 filas del listado.

```{r, message=FALSE, echo=F}
# display the linelist data as a table
DT::datatable(head(linelist, 50), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```


## Crear un nuevo R Markdown  {#create-new-r-markdown}

Una vez instalado el paquete, crea un nuevo archivo R Markdown clicando en *File > New file > R Markdown.*

```{r out.width = "100%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_new1.png"))
```


En la ventana que se abre, selecciona "From Template" y selecciona la plantilla "Flex Dashboard". A continuación, pedirá que nombres el documento. En el ejemplo de esta página, nombraremos nuestro R Markdown como "outbreak_dashboard.Rmd".
  

```{r out.width = "100%", out.height="75%", fig.align = "center", echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_new2.png"))
```




## El script  {#the-script}

El script es un script de R Markdown, y por lo tanto tiene los mismos componentes y organización que se describen en la página sobre [Informes con R Markdown](#reports-with-r-markdown). Volvemos a revisar brevemente estos y destacamos las diferencias con otros formatos de salida de R Markdown.

### YAML {.unnumbered} 

En la parte superior del script está la cabecera "YAML". Esta debe comenzar con tres guiones `---` y debe cerrarse con tres guiones `---`. Los parámetros YAML vienen en pares `key:value`. La **sangría y la colocación de los dos puntos en YAML es importante** - los pares `key:value` están separados por dos puntos (¡no por signos de igualdad!).

El YAML debe comenzar con los metadatos del documento. El orden de estos parámetros YAML primarios (sin sangría) no importa. Por ejemplo:

```{r, eval=F}
title: "My document"
author: "Me"
date: "`r Sys.Date()`"
```

Puedes utilizar código R en los valores YAML poniéndolo como código en línea (precedido por `r` entre comillas) pero también entre comillas (véase más arriba para la fecha).

Un parámetro YAML necesario es `output: `, que especifica el tipo de archivo que se producirá (por ejemplo, `html_document`, `pdf_document`, `word_document`, o `powerpoint_presentation`). En el caso de **flexdashboard** el valor de este parámetro es un poco confuso - debe establecerse como `output:flexdashboard::flex_dashboard`. Ten en cuenta los dos puntos simples y dobles, y el guión bajo. Este parámetro de salida YAML suele ir seguido de *dos puntos adicionales* y de subparámetros con sangría (ver parámetros `orientation: ` y `vertical_layout: ` más abajo). 

```{r, eval=F}
title: "My dashboard"
author: "Me"
date: "`r Sys.Date()`"
output:
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: scroll
```

Como se muestra arriba, se utilizan sangrías (2 espacios) para los subparámetros. En este caso, no olvides poner dos puntos adicionales después del primario, como `key:value: `.

Si procede, los valores lógicos deben indicarse en YAML en minúsculas (`true`, `false`, `null`). Si los dos puntos forman parte del valor (por ejemplo, en el título), escribe el valor entre comillas. Revisa los ejemplos en las secciones siguientes.


### Trozos de código {.unnumbered}  

Un script de R Markdown puede contener múltiples "trozos" de código (Chunk) - estas son áreas del script donde se puede escribir código R de varias líneas y funcionan como mini scripts R.

Los trozos de código se crean con tres signos de acento grave (```) y corchetes con una "r" minúscula dentro. El fragmento se cierra con otros tres acentos graves (acento atrás). Puedes crear un nuevo fragmento escribiéndolo tú mismo, utilizando el atajo de teclado "Ctrl + Alt + i" (o Cmd + Shift + r en Mac), o clicando en el icono verde 'insertar un nuevo fragmento de código' en la parte superior de tu editor de scripts. A continuación se ofrecen muchos ejemplos. 


### Texto narrativo {.unnumbered}  

Fuera de un "trozo" de código R, puedes escribir texto narrativo. Como se describe en la página sobre [Informes con R Markdown](#reports-with-r-markdown), puedes poner el texto en cursiva rodeándolo con un asterisco (*), o en negrita rodeándolo con dos asteriscos (**). Recuerda que las viñetas y los esquemas de numeración son sensibles a las nuevas líneas, a la sangría y a terminar una línea con dos espacios.

También puedes insertar código R en línea en el texto, como se describe en la página [Informes con R Markdown](#reports-with-r-markdown), rodeando el código con puntos suspensivos y comenzando el comando con "r": `` ` 1+1` `` (véase el ejemplo con la fecha anterior).



### Encabezados {.unnumbered}  

Los diferentes niveles de encabezamiento se establecen con diferentes números de símbolos hash, como se describe en la página [Informes con R Markdown](#reports-with-r-markdown).

En **flexdashboard**, un encabezado primario (#) crea una "página" del dashboard. Los encabezados de segundo nivel (##) crean una columna o una fila dependiendo de su parámetro `orientation:` (ver detalles más abajo). Los encabezados de tercer nivel (###) crean paneles para gráficos, diagramas, tablas, texto, etc. 

```md
# Título de primer nivel (página)

## Título de segundo nivel (fila o columna)

### Título de tercer nivel (panel para gráfico, diagrama, etc.)
```





## Atributos de la sección  {#section-attributes}

Al igual que en un R Markdown normal, puedes especificar los atributos que se aplicarán a las partes del cuadro de mando incluyendo las opciones `key=value` después de un encabezado, entre llaves `{ }`. Por ejemplo, en un típico informe HTML R Markdown podrías organizar los sub-encabezados en pestañas con `## My heading {.tabset}`.

Ten en cuenta que estos atributos se escriben después de un *título* en una parte de texto del script. Son diferentes a las opciones de **knitr** insertadas dentro en la parte superior de los trozos de código R, como `out.height = `.

Los atributos de sección específicos de **flexdashboard** incluyen:

* `{data-orientation=}` Establece la orientación de las filas `rows` o de las columnas `columns`.{orientación de los datos=} . Si tu dashboard tiene varias páginas, añade este atributo a cada una de ellas para indicar la orientación (se explica con más detalle en [la sección de diseño](#layout)).
* `{data-width=}` y `{data-height=}` establecen el tamaño relativo de los gráficos, columnas y filas dispuestos en la misma dimensión (horizontal o vertical). Los tamaños absolutos se ajustan para llenar mejor el espacio en cualquier dispositivo de visualización gracias al motor [flexbox](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Using_CSS_flexible_boxes).
     * La altura de las figuras también depende de si se establece el parámetro YAML `vertical_layout: fill` or `vertical_layout: scroll`. Si se establece en scroll, la altura de la figura reflejará la opción tradicional `fig.height = ` en el fragmento de código de R.
     * Consulta la documentación completa sobre el tamaño en el [sitio web de flexdashboard](https://rmarkdown.rstudio.com/flexdashboard/articles/using.html#sizing)
* `{.hidden}` Utiliza esto para excluir una página específica de la barra de navegación
* `{data-navbar=}` Utilízalo en un encabezado a nivel de página para anidarlo dentro de un menú desplegable de la barra de navegación. Indica el nombre (entre comillas) del menú desplegable. Véase el ejemplo siguiente.


## Diseño {#layout}  

Ajusta el diseño de tu panel de control de las siguientes maneras:

* Añadir páginas, columnas/filas y gráficos con encabezados R Markdown (por ejemplo, #, ## o ###)  
* Ajustar la orientación de los parámetros YAML: `orientation:` a `rows` o `columns`  
* Especificar si el diseño llena el navegador o permite el desplazamiento  
* Añadir pestañas a un título de sección concreto


### Páginas {.unnumbered}  

Los encabezados de primer nivel (#) en el R Markdown representarán las "páginas" del cuadro de mando. Por defecto, las páginas aparecerán en una barra de navegación a lo largo de la parte superior del dashboard. 

```{r, out.height = c('100%'), out.width = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_pages_top_script.png"))
```


```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_pages_top_view.png"))
```



Puedes agrupar las páginas en un "menú" dentro de la barra de navegación superior añadiendo el atributo `{data-navmenu=}` al título de la página. Ten cuidado: no incluyas espacios alrededor del signo de igualdad, de lo contrario no funcionará. 

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_navmenu_script.png"))
```


Esto es lo que produce el script: 


```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_navmenu_view.png"))
```

También puedes convertir una página o una columna en una "barra lateral" en el lado izquierdo del panel de control añadiendo el atributo `{.sidebar}`. Puede contener texto (visible desde cualquier página) o, si has integrado una interactividad **Shiny,** puede ser útil para contener controles de entrada del usuario, como deslizadores o menús desplegables. 

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_sidebar_script.png"))
```

Esto es lo que produce el script: 

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_sidebar_view.png"))
```




### Orientación {.unnumbered} 

Añade el parámetro `orientation: ` yaml para indicar cómo deben interpretarse los encabezados de segundo nivel (##) de R Markdown - como `orientation: columns` o `orientation: rows`.

Los encabezados de segundo nivel (##) se interpretarán como nuevas columnas o filas en función de este ajuste de orientación.

Si estableces `orientation: columns`, las cabeceras de segundo nivel crearán nuevas columnas en el dashboard. El siguiente dashboard tiene una página, que contiene dos columnas, con un total de tres paneles. Puedes ajustar el ancho relativo de las columnas con `{data-width=}` como se muestra a continuación.  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_columns_script.png"))
```

Esto es lo que produce el script:

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_columns_view.png"))
```

Si estableces `orientation: rows`, los encabezados de segundo nivel crearán nuevas filas en lugar de columnas. A continuación se muestra el mismo script que el anterior, pero con `orientation: rows` para que los encabezados de segundo nivel produzcan filas en lugar de columnas. Puedes ajustar la *altura* relativa de las filas con `{data-height=}` como se muestra a continuación. 

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_rows_script.png"))
```

Esto es lo que produce el script:

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_rows_view.png"))
```

Si tu dashboard tiene varias páginas, puedes designar la orientación para cada página específica añadiendo el atributo `{data-orientation=}` a la cabecera de cada página (especifica `rows` o `columns` sin comillas).

### Pestañas {.unnumbered}

Puedes dividir el contenido en pestañas con el atributo `{.tabset}`, como en otras salidas HTML R Markdown.

Simplemente añade este atributo después del título deseado. Los subtítulos bajo ese encabezado se mostrarán como pestañas. Por ejemplo, en el script de ejemplo que aparece a continuación, la columna 2 de la derecha (##) se modifica para que la curva epidémica y los paneles de la tabla (###) se muestren en pestañas.

Puedes hacer lo mismo con las filas si su orientación es de filas.

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_tabs_script.png"))
```

Esto es lo que produce el script: 

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_tabs_view.png"))
```


## Añadir contenido {#adding-content} 

Comencemos a construir un panel de control. Nuestro sencillo panel de control tendrá 1 página, 2 columnas y 4 paneles. Construiremos los paneles pieza por pieza para la demostración.

Puedes incluir fácilmente salidas estándar de R, como texto, ggplots y tablas (véase la página [Tablas para presentaciones](#tables-for-presentation)). Simplemente codifícalos dentro de un fragmento de código R como lo harías con cualquier otro script de R Markdown.

Nota: puedes descargar el script Rmd terminado y el resultado del Dashboard en HTML - ver la página [descargando el manual y los datos](#download-handbook-and-data). 


### Texto {.unnumbered}  

Puedes escribir el texto de Markdown e incluir el código *en línea* como para cualquier otra salida de R Markdown. Consulta la página [Informes con R Markdown](#reports-with-r-markdown) para obtener más detalles.

En este dashboard incluimos un panel de texto resumido que incluye un texto dinámico que muestra la última fecha de hospitalización y el número de casos notificados en el brote.

### Tablas {.unnumbered} 

Puedes incluir trozos de código R que impriman salidas como tablas. Pero la salida se verá mejor y responderá al tamaño de la ventana si utilizas la función `kable()` de **knitr** para mostrar las tablas. Las funciones de **flextable** pueden producir tablas acortadas / cortadas.

Por ejemplo, a continuación alimentamos `linelist()` a través de un comando `count()` para producir una tabla resumen de casos por hospital. Finalmente, la tabla se enlaza a `knitr::kable()` y el resultado tiene una barra de desplazamiento a la derecha. Puedes leer más sobre la personalización de la tabla con `kable()` y **kableExtra** [aquí](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html).

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_tables_script.png"))
```

Esto es lo que produce el script:

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_tables_view.png"))
```


Si deseas mostrar una tabla dinámica que permita al usuario filtrar, ordenar y/o clicar a través de las "páginas" del dataframe, utiliza el paquete **DT** y su función `datatable()`, como en el código siguiente.

En el código de ejemplo que sigue, se imprime `linelist` del dataframe. Se puede establecer `rownames = FALSE` para conservar el espacio horizontal, y `filter = "top"` para tener filtros en la parte superior de cada columna. Se puede proporcionar una lista de otras especificaciones a `options = `. A continuación, establecemos `pageLength = ` para que aparezcan 5 filas y `scrollX = ` para que el usuario pueda utilizar una barra de desplazamiento en la parte inferior para desplazarse horizontalmente. El argumento `class = 'white-space: nowrap'` asegura que cada fila sea sólo una línea (no varias líneas). Puedes consultar otros argumentos y valores posibles [aquí](https://rstudio.github.io/DT/?_ga=2.2810736.1321860763.1619286819-369061888.1601594705) o introduciendo `?datatable`

```{r, eval=F}
DT::datatable(linelist, 
              rownames = FALSE, 
              options = list(pageLength = 5, scrollX = TRUE), 
              class = 'white-space: nowrap' )
```

### Gráficos {.unnumbered}  

Puedes imprimir gráficos en un panel de control como lo harías en un script de R. En nuestro ejemplo, utilizamos el paquete **incidence2** para crear una "epicurva" por grupo de edad con dos simples comandos (véase la página de [curvas epidémicas](#epidemic-curves)). Sin embargo, podrías utilizar `ggplot()` e imprimir un gráfico de la misma manera.

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_plots_script.png"))
```

Esto es lo que produce el script:

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_plots_view.png"))
```


### Gráficos interactivos {.unnumbered} 

También puedes pasar un ggplot estándar u otro objeto de gráfico a `ggplotly()` del paquete **plotly** (véase la página de [gráficos interactivos](#plot-with-ggplotly)). Esto hará que el gráfico sea interactivo, permitirá al lector hacer un "zoom", y mostrará sobre el dashboard el valor de cada punto de datos (en este escenario el número de casos por semana y el grupo de edad en la curva).

```{r, eval=F}
age_outbreak <- incidence(linelist, date_onset, "week", groups = age_cat)
plot(age_outbreak, fill = age_cat, col_pal = muted, title = "") %>% 
  plotly::ggplotly()
```

Esto es lo que parece en el dashboard (gif). Esta funcionalidad interactiva seguirá funcionando incluso si envías por correo electrónico el Dashboard como un archivo estático (no en línea en un servidor).

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_ggplotly.gif"))
```

### Widgets HTML {.unnumbered}

Los [widgets HTML para R](http://www.htmlwidgets.org/) son un tipo especial de paquetes R que aumentan la interactividad utilizando bibliotecas JavaScript. Se pueden incrustar en salidas R Markdown (como un flexdashboard) y en dashboards de Shiny.

Algunos ejemplos comunes de estos widgets son:

* Plotly (utilizado en la página de este manual y en la página de [Plots interativos](#interactive-plots))
* visNetwork (utilizado en la página de [cadenas de transmisión](#transmission-chains) de este manual)
* Leaflet (utilizado en la página [conceptos básicos de los SIG](#gis-basics) de este manual)
* dygraphs (útil para mostrar interactivamente los datos de las series temporales)
* DT (`datatable()`) (utilizado para mostrar tablas dinámicas con filtro, ordenación, etc.)

A continuación mostramos la adición de una cadena de transmisión de epidemias que utiliza visNetwork al dashboard. El guión muestra sólo el nuevo código añadido a la sección "Columna 2" del script R Markdown. Puedes encontrar el código en la página de cadenas de transmisión de este manual. 

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_chain_script.png"))
```

Esto es lo que produce el script: 

```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "flexdashboard_chain.gif"))
```



## Organización del código  {#code-organization}

Puedes elegir tener todo el código dentro del script de R Markdown **flexdashboard**. Alternativamente, para tener un script de dashboard más limpio y conciso, puedes elegir llamar al código/figuras que están alojadas o creadas en scripts R externos. Esto se describe con mayor detalle en la página [Informes con R Markdown](#reports-with-r-markdown).


## Shiny  {#shiny-1}

La integración del paquete R **shiny** puede hacer que tus Dashboards sean aún más reactivos a la entrada del usuario. Por ejemplo, puedes hacer que el usuario selecciona una jurisdicción, o un rango de fechas, y hacer que los paneles reaccionen a su elección (por ejemplo, filtrar los datos mostrados). Para incrustar la reactividad de **shiny** en el **flexdashboard**, sólo tienes que hacer unos pocos cambios en tu script de R Markdown en el **flexdashboard**.

También se puede utilizar **shiny** para producir aplicaciones/dashboards *sin* flexdashboard. La página del manual sobre [Dashboards con Shiny](#dashboards-with-shiny) ofrece una visión general de este enfoque, incluyendo consejos sobre la sintaxis de **Shiny**, la estructura de los archivos de la aplicación y las opciones para compartir/publicar (incluyendo opciones de servidor gratuito). Esta sintaxis y los consejos generales se traducen también en el contexto de **flexdashboard**.

La incrustación de **shiny** en **el flexdashboard supone**, sin embargo, un cambio fundamental en tu flexdashboard. Ya no producirá una salida HTML que puedas enviar por correo electrónico y que cualquiera puede abrir y ver. En su lugar, será una "aplicación". El botón "Knit" en la parte superior del script será reemplazado por un icono "Run document", que abrirá una instancia del dashboard interactivo localmente en tu ordenador.

Para compartir tu panel de control, ahora será necesario que:

* Enviar el script Rmd al espectador, ellos lo abren en R en su ordenador, y ejecutan la aplicación, o

* La aplicación/dashboard se aloja en un servidor accesible para el espectador

Por lo tanto, la integración de **shiny** tiene ventajas, pero también complicaciones. Si la facilidad de compartir por correo electrónico es una prioridad y no necesitas las capacidades reactivas de shiny, considera la reducida interactividad que ofrece ggplotly() como se ha demostrado anteriormente.

A continuación damos un ejemplo muy sencillo utilizando el mismo "outbreak_dashboard.Rmd" que el anterior. Una amplia documentación sobre la integración de Shiny en **flexdashboard** está disponible en línea [aquí](https://rmarkdown.rstudio.com/flexdashboard/shiny.html).



### Ajustes {.unnumbered}  

Habilitar **shiny** en un **flexdashboard** añadiendo el parámetro `YAML runtime: ` shiny en el mismo nivel de sangría que `output: `, como se indica a continuación:

```md
---
title: "Dashboard del brote (demo Shiny)"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
runtime: shiny
---

```

También es conveniente habilitar una "barra lateral" para albergar los widgets de entrada de Shiny que recogerán la información del usuario. Como se explicó anteriormente, crea una columna e indica la opción `{.sidebar}` para crear una barra lateral en el lado izquierdo. Dentro de esta columna se pueden añadir trozos de texto R que contengan los comandos `input` de **shiny**.

Si tu aplicación/panel está alojado en un servidor y puede tener varios usuarios simultáneos, nombra el primer trozo de código R como `global`. Incluye los comandos para importar/cargar tus datos en este chunk. Este chunk con nombre especial es tratado de manera diferente, y los datos importados dentro de él sólo se importan una vez (no continuamente) y están disponibles para todos los usuarios. Esto mejora la velocidad de arranque de la aplicación.

### Ejemplo trabajado {.unnumbered}  

Aquí adaptamos el script flexdashboard "outbreak_dashboard.Rmd" para incluir **shiny**. Añadiremos la capacidad de que el usuario seleccione un hospital de un menú desplegable, y que la curva epidémica refleje sólo los casos de ese hospital, con un título de gráfico dinámico. Hacemos lo siguiente:

* Añadir `runtime: shiny` al YAML
* Renombrar el chunk de configuración como `global`
* Crear una barra lateral que contenga:
  * Código para crear un vector de nombres únicos de hospitales
  * Un comando `selectInput()` (menú desplegable **Shiny**) con la elección de los nombres de los hospitales. La selección se guarda como `hospital_choice`, a la que se puede hacer referencia en código posterior como `input$hospital_choice`
* El código de la curva epidémica (columna 2) está envuelto dentro de `renderPlot({ })`, incluyendo:
  * Un filtro en los datos que restringe la columna hospital al valor actual de `input$hospital_choice`
  * Un título de gráfico dinámico que incorpora `input$hospital_choice`

Ten en cuenta que cualquier código que haga referencia a un valor de `input$` debe estar dentro de una función `render({})` (para ser reactiva).

Aquí está la parte superior del script, incluyendo el YAML, el chunk global y la barra lateral:

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_shiny_script1.png"))
```
  
Aquí está la Columna 2, con el gráfico de la epicurva reactiva:

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_shiny_script2.png"))
```

Y aquí está el Dashboard:  

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "flexdashboard_shiny_view.gif"))
```




### Otros ejemplos {.unnumbered} 

Para leer un ejemplo relacionado con la salud de un Shiny-flexdashboard que utiliza la interactividad de **Shiny** y el widget de mapeo **leaflet**, consulta este capítulo del libro en línea [Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny](https://www.paulamoraga.com/book-geospatial/sec-dashboardswithshiny.html). 




## Compartir {#sharing}  

Los Dashboards que no contengan elementos Shiny producirán un archivo HTML (.html), que puede enviarse por correo electrónico (si el tamaño lo permite). Esto es útil, ya que puedes enviar el informe del "dashboard" y no tener que configurar un servidor para alojarlo como un sitio web.

Si has incrustado **shiny**, no podrás enviar una salida por correo electrónico, pero puedes enviar el propio script a un usuario de R, o alojar el Dashboard en un servidor como se ha explicado anteriormente. 


## Recursos {#resources-35} 

A continuación se pueden encontrar excelentes tutoriales que informaron esta página. Si los revisas, lo más probable es que en una hora puedas tener tu propio Dashboard.

https://bookdown.org/yihui/rmarkdown/dashboards.html

https://rmarkdown.rstudio.com/flexdashboard/

https://pkgs.rstudio.com/flexdashboard/articles/using.html

https://pkgs.rstudio.com/flexdashboard/articles/examples.html
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/flexdashboard.Rmd-->


# Dashboards con Shiny {#dashboards-with-shiny}

Los Dashboards (cuadros de mando o tableros de control) suelen ser una buena forma de compartir los resultados de los análisis con otras personas. Elaborar un cuadro de mando con **shiny** requiere un conocimiento relativamente avanzado del lenguaje R, pero ofrece una personalización y unas posibilidades increíbles.

<!-- One of the largest drawbacks of `R` is its usability for people who are new to or have no experience with programming languages. While these skills are very valuable, most people will find that this represents a barrier to sharing analyses, especially in multidisciplinary environments. It requires some work to maintain an `R` installation, and not everyone will be comfortable running shared code, even if it's well documented and easy to read. This is *especially* true when users have to change parameters of code!  -->

<!-- R based dashboards are also advantageous in that they centralise how code is run - when the same code is run on different machines, often people will have to deal with differing file paths, different R versions, and different package installations. For this reason, dashboards are a great way to share code with others in a user friendly way! -->

Se recomienda que alguien que esté aprendiendo a usar Dashboards con **shiny** tenga buenos conocimientos de transformación y visualización de datos, y se sienta cómodo depurando código y escribiendo funciones. Trabajar con dashboards no es intuitivo cuando se empieza, y es difícil de entender a veces, pero es una gran habilidad para aprender y se hace mucho más fácil con la práctica.

Esta página dará una breve visión general de cómo hacer Dashboards con **shiny** y sus extensiones. Para un método alternativo de hacer dashboards que es más rápido, más fácil, pero quizás menos personalizable, ver la página sobre **flextable** ([Dashboards with R Markdown](#dashboards-with-r-markdown)). 



## Preparación {#preparation-36} 


### Cargar paquetes {.unnumbered}  

En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base**. Consulta la página [Fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

Comenzamos instalando el paquete R **Shiny**:

```{r, eval = FALSE}
pacman::p_load("shiny")
```


### Importar datos {.unnumbered}  

Si quieres seguir esta página, consulta esta sección del [Manual de descarga y datos](#data_shiny). Hay enlaces para descargar los scripts de R y los archivos de datos que producen la aplicación final de Shiny.

Si intentas reconstruir la aplicación utilizando estos archivos, ten en cuenta la estructura de carpetas del proyecto R que se crea en el transcurso de la demostración (por ejemplo, carpetas para "data" y para "funcs").  



<!-- ======================================================= -->
## Estructura de una app Shiny {#the-structure-of-a-shiny-app}

### Estructuras básicas de archivos {.unnumbered}  

Para entender `Shiny`, primero tenemos que entender cómo funciona la estructura de archivos de una aplicación. Deberíamos crear un nuevo directorio antes de empezar. Esto puede hacerse más fácil eligiendo *Nuevo proyecto* en *Rstudio*, y eligiendo *Aplicación Web Shiny*. Esto creará la estructura básica de una aplicación shiny para ti.

Al abrir este proyecto, notarás que ya hay un archivo `.R` llamado *app.R.* Es *esencial* que tengamos una de las dos estructuras básicas de archivos:

1.  Un archivo llamado *app.R*, *o*
2.  Dos archivos, uno llamado *ui.R* y el otro *server.R*

En esta página, utilizaremos el primer enfoque de tener un archivo llamado *app.R.* Aquí hay un script de ejemplo:

```{r, eval = FALSE}
# an example of app.R

library(shiny)

ui <- fluidPage(

    # Application title
    titlePanel("My app"),

    # Sidebar with a slider input widget
    sidebarLayout(
        sidebarPanel(
            sliderInput("input_1")
        ),

        # Show a plot 
        mainPanel(
           plotOutput("my_plot")
        )
    )
)

# Define server logic required to draw a histogram
server <- function(input, output) {
     
     plot_1 <- reactive({
          plot_func(param = input_1)
     })
     
    output$my_plot <- renderPlot({
       plot_1()
    })
}


# Run the application 
shinyApp(ui = ui, server = server)


```


Si abres este archivo, te darás cuenta de que hay dos objetos definidos: uno llamado `ui` (interfaz de usuario) y otro llamado `server` (servidor). Estos objetos *deben* ser definidos en *todas* las aplicaciones shiny y son fundamentales para la estructura de la propia aplicación. De hecho, la única diferencia entre las dos estructuras de archivos descritas anteriormente es que en la estructura 1, tanto `ui` como `server` están definidos en un solo archivo, mientras que en la estructura 2 están definidos en archivos separados. Nota: también podemos (y deberíamos si tenemos una aplicación más grande) tener otros archivos .R en nuestra estructura que podemos llamar con `source()` desde nuestra aplicación.



### El servidor y la Interfaz de Usuario (ui) {.unnumbered} 

A continuación, tenemos que entender lo que *hacen* realmente los objetos `server` y `ui`. *En pocas palabras, se trata de dos objetos que interactúan entre sí cada vez que el usuario interactúa con la app shiny.*

El elemento de interfaz de usuario de una aplicación Shiny es, en un nivel básico, el código R que crea una interfaz HTML. Esto significa todo lo que se *muestra* en la UI de una app. Esto generalmente incluye:

* "Widgets" - menús desplegables, casillas de verificación, deslizadores, etc. con los que puede interactuar el usuario
* Gráficos, tablas, etc. - resultados que se generan con el código R
* Aspectos de la navegación de una aplicación: pestañas, paneles, etc.
* Texto genérico, hipervínculos, etc.
* Elementos HTML y CSS (abordados más adelante)

Lo más importante que hay que entender sobre la UI es que *recibe entradas* del usuario y *muestra salidas* del servidor. No hay código *activo* que se ejecute en la UI *en ningún momento* - todos los cambios que se ven en la UI pasan por el servidor (más o menos). Así que tenemos que hacer nuestros gráficos, descargas, etc en el servidor

El servidor de la app shiny es donde se ejecuta todo el código una vez que la aplicación se inicia. La forma en que esto funciona es un poco confusa. La función del servidor *reaccionará* efectivamente a la interfaz del usuario con la UI, y ejecutará trozos de código en respuesta. Si las cosas cambian en el servidor, estas serán pasadas de vuelta a la UI, donde pueden verse los cambios. Es importante destacar que el código en el servidor se ejecutará *de forma no consecutiva* (o es mejor pensarlo así). Básicamente, cada vez que una entrada de la ui afecte a un trozo de código en el servidor, éste se ejecutará automáticamente, y se producirá y mostrará esa salida.

Probablemente todo esto suene muy abstracto por ahora, así que tendremos que sumergirnos en algunos ejemplos para tener una idea clara de cómo funciona realmente.


### Antes de empezar a crear una app {.unnumbered}

Antes de empezar a construir una aplicación, es muy útil saber *qué* quieres construir. Dado que tu interfaz de usuario estará escrita en código, no puedes visualizar realmente lo que estás construyendo a menos que tengas como objetivo algo específico. Por esta razón, es inmensamente útil mirar muchos ejemplos de aplicaciones Shiny para tener una idea de lo que puedes hacer - ¡incluso mejor si puedes mirar el código fuente detrás de estas aplicaciones! Algunos de los mejores recursos para ello son:

* La [galería de aplicaciones de Rstudio](https://shiny.rstudio.com/gallery/)

Una vez que tengas una idea de lo que es posible, también es útil hacer un mapa de cómo quieres que sea la tuya; puedes hacerlo en papel o en cualquier software de dibujo (PowerPoint, MS paint, etc.). Es útil empezar con algo sencillo para tu primera aplicación. Tampoco hay que avergonzarse de utilizar el código que encuentres en Internet de una buena aplicación como plantilla para tu trabajo: es mucho más fácil que construir algo desde cero.


## Construir una interfaz de usuario {#building-a-ui}

Cuando construimos nuestra aplicación, es más fácil trabajar en la interfaz de usuario (UI) primero para que podamos ver lo que estamos haciendo, y no arriesgarnos a que la aplicación falle debido a cualquier error del servidor. Como se mencionó anteriormente, a menudo es bueno utilizar una plantilla cuando se trabaja en la interfaz de usuario. Hay una serie de diseños estándar que se pueden utilizar con shiny que están disponibles en el paquete base de shiny, pero vale la pena señalar que también hay una serie de extensiones del paquete como `shinydashboard`. Utilizaremos un ejemplo del paquete shiny básico para empezar.

Una interfaz de usuario Shiny se define generalmente como una serie de funciones anidadas, en el siguiente orden

1.  Una función que define el diseño general (la más básica es `fluidPage()`, pero hay más disponibles)
2.  Paneles dentro del diseño como:
     -  una barra lateral (`sidebarPanel()`)
     -  un panel "principal" (`mainPanel()`)
     -  una pestaña (`tabPanel()`)
     -  una "columna" genérica `(column()`)
3.  Widgets y salidas: pueden conferir entradas al servidor (widgets) o salidas del servidor (salidas)
     -  Los widgets suelen tener el estilo de `xxxInput()`, por ejemplo, `selectInput()`
     -  Las salidas suelen tener el estilo de `xxxOutput()`, por ejemplo, `plotOutput()`
     
Vale la pena repetir que estos datos no se pueden visualizar fácilmente de forma abstracta, por lo que es mejor ver un ejemplo. Consideremos la posibilidad de crear una aplicación básica que visualice nuestros datos de recuento de instalaciones de malaria por distrito. Estos datos tienen muchos parámetros diferentes, por lo que sería estupendo que el usuario final pudiera aplicar algunos filtros para ver los datos por grupo de edad/distrito según su criterio. Podemos utilizar un diseño Shiny muy simple para empezar - el diseño de la barra lateral. Se trata de un diseño en el que los widgets se colocan en una barra lateral a la izquierda, y el gráfico se coloca a la derecha.

Planifiquemos nuestra aplicación: podemos empezar con un selector que nos permita elegir el distrito donde queremos visualizar los datos, y otro que nos permita visualizar el grupo de edad que nos interesa. Con estos filtros pretendemos mostrar una epicurva que refleje estos parámetros. Para ello necesitamos:

1.  Dos menús desplegables que nos permiten elegir el distrito que queremos y el grupo de edad que nos interesa.
2.  Un área donde podemos mostrar nuestra epicurva resultante.

Esto podría ser algo así:

```{r, eval = FALSE}

library(shiny)

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = TRUE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         )

    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve")
    )
    
  )
)


```


Cuando se ejecuta app.R con el código de interfaz de usuario anterior (sin código activo en la parte del `server` de app.R), el diseño aparece con el siguiente aspecto: ten en cuenta que no habrá ningún gráfico si no hay un servidor que lo represente, ¡pero nuestras entradas están funcionando!

```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "shiny", "simple_UI_view.png"))
```

Esta es una buena oportunidad para discutir cómo funcionan los widgets - nota que cada widget está aceptando un `inputId`, una `label` (etiqueta), y una serie de otras opciones que son específicas para el tipo de widget. Este `inputId` es extremadamente importante - estos son los IDs que se utilizan para pasar la información de la UI al servidor. Por esta razón, *deben ser únicos*. Deberías hacer un esfuerzo para denominarlos con algo sensato, y específico a lo que están interactuando en casos de aplicaciones más grandes.

Deberías leer la documentación cuidadosamente para conocer todos los detalles sobre lo que hace cada uno de estos widgets. Los widgets pasarán tipos específicos de datos al servidor dependiendo del tipo de widget, y esto debe entenderse completamente. Por ejemplo, `selectInput()` pasará un dato de tipo carácter al servidor:

- Si seleccionamos *Spring* para el primer widget aquí, pasará el objeto carácter `"Spring"` al servidor.
- Si seleccionamos dos elementos del menú desplegable, aparecerán como un vector de caracteres (por ejemplo, `c("Primavera", "Bolo")`).

Otros widgets pasarán diferentes tipos de objetos al servidor. Por ejemplo:

- `numericInput()` pasará un objeto de tipo numérico al servidor
- `checkboxInput()` pasará un objeto de tipo lógico al servidor (`TRUE` o `FALSE`)

También vale la pena tener en cuenta el *nombre del vector* que usaremos para los datos de edad aquí. Para muchos widgets, el uso de un vector para las opciones mostrará los *nombres* del vector como las opciones de visualización, pero pasará el *valor* seleccionado del vector al servidor. Por ejemplo, aquí alguien puede seleccionar "15+" en el menú desplegable, y la interfaz de usuario pasará `"malaria_rdt_15"` al servidor, que resulta ser el nombre de la columna que nos interesa.

Hay un montón de widgets que puedes utilizar para hacer muchas cosas con tu aplicación. Los widgets también permiten cargar archivos en la aplicación y descargar resultados. También hay algunas excelentes extensiones de shiny que te dan acceso a más widgets que el shiny básico - el paquete **shinyWidgets** es un gran ejemplo de esto. Para ver algunos ejemplos puedes consultar los siguientes enlaces:

- [Galería de widgets de Shiny](https://shiny.rstudio.com/gallery/widget-gallery.html)
- [Galería de shinyWidgets](https://github.com/dreamRs/shinyWidgets)



## Cargar datos en nuestra app {#loading-data-into-our-app}

El siguiente paso en el desarrollo de nuestra aplicación es poner en marcha el servidor. Para ello, sin embargo, tenemos que conseguir algunos datos en nuestra aplicación, y averiguar todos los cálculos que vamos a hacer. Una aplicación Shiny no es fácil de depurar, ya que a menudo no está claro de dónde provienen los errores, por lo que es ideal desarrollar el código todo nuestro procesamiento de datos y visualización  antes de empezar a hacer el propio servidor.

Así que dado que queremos hacer una aplicación que muestre epicurvas que cambien en base a la entrada del usuario, deberíamos pensar en qué código necesitaríamos para ejecutar esto en un script normal de R. Necesitaremos:

1.  Cargar nuestros paquetes
2.  Cargar nuestros datos
3.  Transformar nuestros datos
4.  Desarrollar una *función* para visualizar nuestros datos en función de las entradas del usuario

Esta lista es bastante sencilla, y no debería ser demasiado difícil de hacer. Ahora es importante pensar qué partes de este proceso deben hacerse una *sola vez* y qué partes deben *ejecutarse en respuesta a las entradas del usuario*. Esto se debe a que las aplicaciones Shiny generalmente ejecutan algún código antes de ejecutarse, que sólo se realiza una vez. Ayudará al rendimiento de nuestra aplicación si la mayor parte de nuestro código puede ser trasladado a esta sección. Para este ejemplo, sólo necesitamos cargar nuestros datos/paquetes y hacer transformaciones básicas una vez, así que podemos poner ese código *fuera del servidor*. Esto significa que lo único que necesitaremos en el servidor es el código para visualizar nuestros datos. Vamos a desarrollar todos estos componentes en un script primero. Sin embargo, ya que estamos visualizando nuestros datos con una función, también podemos poner el código *de la función fuera del servidor* para que nuestra función esté en el entorno cuando la aplicación se ejecute.

Primero vamos a cargar nuestros datos. Ya que estamos trabajando con un nuevo proyecto, y queremos limpiarlo, podemos crear un nuevo directorio llamado data, y añadir nuestros datos de malaria allí. Podemos ejecutar este código de abajo en un script de prueba que eventualmente borraremos cuando limpiemos la estructura de nuestra aplicación.

```{r, echo = TRUE}
pacman::p_load("tidyverse", "lubridate")

# read data
malaria_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  as_tibble()

print(malaria_data)


```


Será más fácil trabajar con estos datos si utilizamos estándares de datos ordenados, por lo que también debemos transformarlos en un formato de datos más largo, donde el grupo de edad es una columna, y los casos son otra columna. Podemos hacer esto fácilmente usando lo que hemos aprendido en la página de [Pivotar datos](#pivoting-data).


```{r, echo = TRUE}

malaria_data <- malaria_data %>%
  select(-newid) %>%
  pivot_longer(cols = starts_with("malaria_"), names_to = "age_group", values_to = "cases_reported")

print(malaria_data)

```

Y con esto hemos terminado de preparar nuestros datos! Esto tacha los puntos 1, 2 y 3 de nuestra lista de cosas a desarrollar para nuestro "script de prueba de R". La última tarea, y la más difícil, será construir una función para producir una epicurva basada en parámetros definidos por el usuario. Como se mencionó anteriormente, se *recomienda encarecidamente* que cualquier persona que aprenda shimy primero mire la sección sobre la programación funcional ([Escribir funciones](#writing-functions-1)) para entender cómo funciona esto!

Al definir nuestra función, puede ser difícil pensar en los parámetros que queremos incluir. Para la programación funcional con shiny, cada parámetro relevante tendrá generalmente un widget asociado a él, así que pensar en esto suele ser bastante fácil. Por ejemplo, en nuestra aplicación actual, queremos ser capaces de filtrar por distrito, y tener un widget para ello, por lo que podemos añadir un parámetro de distrito para reflejar esto. *No* tenemos ninguna funcionalidad de la aplicación para filtrar por centro (por ahora), así que no necesitamos añadir esto como parámetro. Empecemos haciendo una función con tres parámetros:

1.  Los datos básicos
2.  El distrito de elección
3.  El grupo de edad elegido

```{r}

plot_epicurve <- function(data, district = "All", agegroup = "malaria_tot") {
  
  if (!("All" %in% district)) {
    data <- data %>%
      filter(District %in% district)
    
    plot_title_district <- stringr::str_glue("{paste0(district, collapse = ', ')} districts")
    
  } else {
    
    plot_title_district <- "all districts"
    
  }
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  data <- data %>%
    filter(age_group == agegroup)
  
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  if (agegroup == "malaria_tot") {
      agegroup_title <- "All ages"
  } else {
    agegroup_title <- stringr::str_glue("{str_remove(agegroup, 'malaria_rdt')} years")
  }
  
  
  ggplot(data, aes(x = data_date, y = cases_reported)) +
    geom_col(width = 1, fill = "darkred") +
    theme_minimal() +
    labs(
      x = "date",
      y = "number of cases",
      title = stringr::str_glue("Malaria cases - {plot_title_district}"),
      subtitle = agegroup_title
    )
  
  
  
}

```


No entraremos en grandes detalles sobre esta función, ya que su funcionamiento es relativamente sencillo. Una cosa a tener en cuenta, sin embargo, es que debemos gestionar los errores devolviendo `NULL` cuando de otro modo daría un error. Esto se debe a que cuando un servidor Shiny produce un objeto `NULL` en lugar de un objeto gráfico, ¡no se mostrará nada en la interfaz de usuario! Esto es importante, ya que de lo contrario los errores a menudo harán que la aplicación deje de funcionar.

Otra cosa a tener en cuenta es el uso del operador `%in%` cuando se evalúa la entrada del `district`. Como se mencionó anteriormente, esto podría llegar como un vector de caracteres con múltiples valores, por lo que el uso de `%in%` es más flexible que, por ejemplo, `==`.

Vamos a probar nuestra función!

```{r, echo = TRUE, warning = FALSE}

plot_epicurve(malaria_data, district = "Bolo", agegroup = "malaria_rdt_0-4")

```

Con nuestra función ya trabajando, ahora tenemos que entender cómo va a encajar todo esto en nuestra aplicación Shiny. Hemos mencionado el concepto de *código de inicio* antes, pero vamos a ver cómo podemos incorporar esto en la estructura de nuestra aplicación. Hay dos maneras de hacerlo.

1.  Escribe este código en tu archivo *app.R* al principio del script (por encima de la interfaz de usuario), o
2.  Crea un nuevo archivo en el directorio de tu aplicación llamado *global.R*, y pon el código de inicio en él.

Vale la pena señalar en este punto que generalmente es más fácil, especialmente con aplicaciones más grandes, utilizar la segunda estructura de archivos, ya que permite separar su estructura de una manera sencilla. Vamos a desarrollar completamente este script global.R ahora. Esto es lo que podría parecer:


```{r, eval = F}
# global.R script

pacman::p_load("tidyverse", "lubridate", "shiny")

# read data
malaria_data <- rio::import(here::here("data", "malaria_facility_count_data.rds")) %>% 
  as_tibble()

# clean data and pivot longer
malaria_data <- malaria_data %>%
  select(-newid) %>%
  pivot_longer(cols = starts_with("malaria_"), names_to = "age_group", values_to = "cases_reported")


# define plotting function
plot_epicurve <- function(data, district = "All", agegroup = "malaria_tot") {
  
  # create plot title
  if (!("All" %in% district)) {            
    data <- data %>%
      filter(District %in% district)
    
    plot_title_district <- stringr::str_glue("{paste0(district, collapse = ', ')} districts")
    
  } else {
    
    plot_title_district <- "all districts"
    
  }
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  # filter to age group
  data <- data %>%
    filter(age_group == agegroup)
  
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  if (agegroup == "malaria_tot") {
      agegroup_title <- "All ages"
  } else {
    agegroup_title <- stringr::str_glue("{str_remove(agegroup, 'malaria_rdt')} years")
  }
  
  
  ggplot(data, aes(x = data_date, y = cases_reported)) +
    geom_col(width = 1, fill = "darkred") +
    theme_minimal() +
    labs(
      x = "date",
      y = "number of cases",
      title = stringr::str_glue("Malaria cases - {plot_title_district}"),
      subtitle = agegroup_title
    )
  
  
  
}



```


Fácil! Una gran característica es qe shiny entenderá para qué sirven los archivos llamados *app.R*, *server.R*, *ui.R* y *global.R*, por lo que no es necesario conectarlos entre sí mediante ningún código. Así que sólo con tener este código en *global.R* en el directorio adecuado se ejecutará antes de que iniciemos nuestra app!

También debemos tener en cuenta que mejoraría la organización de nuestra aplicación si movemos la función de dibujar a su propio archivo - esto será especialmente útil a medida que las aplicaciones se hacen más grandes. Para hacer esto, podríamos hacer otro directorio llamado *funcs*, y poner esta función en un archivo llamado *plot_epicurve.R.* Podríamos entonces leer esta función a través del siguiente comando en *global.R*

```{r, eval = F}

source(here("funcs", "plot_epicurve.R"), local = TRUE)

```

Ten en cuenta que *siempre* debes especificar `local = TRUE` en las aplicaciones shiny, ya que afectará a la obtención de recursos cuando/si la aplicación se publica en un servidor.

## Desarrollar un servidor de app {#developing-an-app-server}

Ahora que tenemos la mayor parte de nuestro código, sólo tenemos que desarrollar nuestro servidor. Esta es la pieza final de nuestra aplicación, y es probablemente la más difícil de entender. El servidor es una gran función de R, pero es útil pensar en él como una serie de funciones más pequeñas, o tareas que la aplicación puede realizar. Es importante entender que estas funciones no se ejecutan en un orden lineal. Hay un orden en ellas, pero no es necesario entenderlo del todo cuando se empieza con Shiny. A un nivel muy básico, estas tareas o funciones se activarán cuando haya un cambio en las entradas del usuario que las afecte, *a menos que el desarrollador las haya configurado para que se comporten de forma diferente*. De nuevo, todo esto es bastante abstracto, pero vamos a repasar primero los tres tipos básicos de *objetos shiny*

1.  Fuentes reactivas - este es otro término para las entradas del usuario. El servidor shiny tiene acceso a las salidas de la UI a través de los widgets que hemos programado. Cada vez que los valores de estos se cambian, esto se pasa al servidor.

2.  Conductores reactivos - estos son objetos que existen *sólo* dentro del servidor Shiny. En realidad no los necesitamos para aplicaciones simples, pero producen objetos que sólo pueden ser vistos dentro del servidor, y utilizados en otras operaciones. Generalmente dependen de fuentes reactivas.

3.  Puntos finales: son las salidas que se pasan del servidor a la interfaz de usuario. En nuestro ejemplo, esto sería la epicurva que estamos produciendo.

Con esto en mente vamos a construir nuestro servidor paso a paso. Vamos a mostrar nuestro código de interfaz de usuario de nuevo aquí sólo para referencia:

```{r, eval = FALSE}

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = TRUE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         )

    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve")
    )
    
  )
)


```

De este código UI tenemos:

- Dos entradas:
  - Selector de distrito (con un inputId de `select_district`)
  - Selector de grupo de edad (con un inputId de `select_agegroup`)
- Una salida:
  - La epicurva (con un outputId de `malaria_epicurve`)

Como hemos dicho anteriormente, estos nombres únicos que hemos asignado a nuestras entradas y salidas son cruciales. *Deben ser únicos* y se utilizan para pasar información entre la ui y el servidor. En nuestro servidor, accedemos a nuestras entradas a través de la sintaxis `input$inputID` y a las salidas y las pasamos a la ui a través de la sintaxis `output$output_name` ¡Veamos un ejemplo, porque de nuevo esto es difícil de entender de otra manera!

```{r, eval = FALSE}

server <- function(input, output, session) {
  
  output$malaria_epicurve <- renderPlot(
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  )
  
}


```


El servidor para una aplicación simple como esta es en realidad bastante sencillo. Te darás cuenta de que el servidor es una función con tres parámetros - `input`, `output`, and `session` - esto no es tan importante para entender por ahora, pero es importante seguir esta configuración. En nuestro servidor sólo tenemos una tarea - esta procesa un gráfico basado en la función que hicimos antes, y las entradas del servidor. Fíjate en que los nombres de los objetos de entrada y salida se corresponden exactamente con los de la interfaz de usuario.

Para entender los fundamentos de cómo el servidor reacciona a las entradas del usuario, debes tener en cuenta que la salida sabrá (a través del paquete subyacente) cuando las entradas cambian, y volver a ejecutar esta función para crear un gráfico cada vez que cambian. Ten en cuenta que aquí también utilizamos la función `renderPlot()` - esta es de una familia de funciones específicas del tipo que pasan esos objetos a una salida ui. Hay una serie de funciones que se comportan de manera similar, pero hay que asegurarse de que la función utilizada coincide con el tipo de objeto que se está pasando a la ui. Por ejemplo:

- `renderText()` - enviar texto a la ui
- `renderDataTable` - envía una tabla interactiva a la ui.

Recuerda que estos también necesitan coincidir con la *función de* salida utilizada en la ui - así que `renderPlot()` se empareja con `plotOutput()`, y `renderText()` se empareja con `textOutput()`.

Así que finalmente hemos hecho una aplicación que funciona! Podemos ejecutarla clicando el botón Ejecutar aplicación en la parte superior derecha de la ventana de script en Rstudio. Debes tener en cuenta que puedes elegir ejecutar tu aplicación en tu navegador por defecto (en lugar de Rstudio), lo que reflejará con mayor precisión el aspecto que tendrá la aplicación para otros usuarios.


```{r, out.width = c('100%'), out.height = c('100%'), echo=F}
knitr::include_graphics(here::here("images", "shiny", "app_simple_view.gif"))
```


¡Es divertido observar que en la consola R, la aplicación está "escuchando". Hablando de reactividad!  

```{r, echo=F}
knitr::include_graphics(here::here("images", "shiny", "listening.png"))
```


<!-- TO DO: *ADD SOMETHING ON DOWNLOADING A ZIP FILE OF THE APP?*  -->



## Añadir más funcionalidad {#adding-more-functionality}

En este punto tenemos finalmente una aplicación en funcionamiento, pero tenemos muy poca funcionalidad. Tampoco hemos rascado la superficie de lo que shiny puede hacer, ¡así que hay mucho más que aprender! Vamos a seguir construyendo nuestra aplicación actual añadiendo algunas características adicionales. Algunas cosas que podría ser bueno añadir:

1.  Algunos textos explicativos
2.  Un botón de descarga para nuestra gráfica - esto proporcionaría al usuario una versión de alta calidad de la imagen que está generando en la aplicación
3.  Un selector de instalaciones específicas
4.  Otra página del panel de control: podría mostrar una tabla con nuestros datos.

Esto es mucho para agregar, pero podemos usarlo para aprender en el camino un montón de diferentes características de Shiny. Hay mucho que aprender sobre Shiny (puede ser *muy* avanzado, pero es de esperar que una vez que los usuarios tienen una mejor idea de cómo usarlo pueden llegar a ser más cómodo usando fuentes de aprendizaje externas también).

### Añadir texto estático {.unnumbered} 

Vamos a hablar primero de la adición de texto estático a nuestra aplicación Shiny. Añadir texto a nuestra aplicación es extremadamente fácil, una vez que se tiene un conocimiento básico de la misma. Dado que el texto estático no cambia en la aplicación shiny (si quieres que cambie, puedes utilizar las funciones de *procesado de texto* en el servidor), todo el texto estático de shiny se añade generalmente en la interfaz de usuario de la aplicación. No vamos a entrar en detalles, pero puedes añadir un número de elementos diferentes a su ui (e incluso personalizados) mediante la interfaz de R con *HTML* y *css*.

HTML y css son lenguajes que intervienen explícitamente en el diseño de la interfaz de usuario. No es necesario entenderlos demasiado bien, pero *HTML* crea objetos en la interfaz de usuario (como un cuadro de texto, o una tabla), y *css* se utiliza generalmente para cambiar el estilo y la estética de esos objetos. Shiny tiene acceso a una gran variedad de *etiquetas HTML* - éstas están presentes para los objetos que se comportan de una manera específica, como los encabezados, los párrafos de texto, los saltos de línea, las tablas, etc. Podemos utilizar algunos de estos ejemplos así:

- `h1()` - esta es una etiqueta de *encabezado*, que hará que el texto adjunto sea automáticamente más grande, y cambiará los valores predeterminados en cuanto a la fuente, el color, etc. (dependiendo del tema general de tu aplicación). Puedes acceder a subtítulos *cada vez más pequeños* con `h2()` hasta `h6()` también. El uso es así:
  * `h1("mi cabecera - sección 1")`

- `p()` - esta es una etiqueta de *párrafo*, que hará que el texto encerrado sea similar al texto de un cuerpo de texto. Este texto se envolverá automáticamente, y será de un tamaño relativamente pequeño (los pies de página podrían ser más pequeños, por ejemplo). Piensa en ello como el cuerpo de texto de un documento de Word. El uso es así:
  * `p("Este es un cuerpo de texto más grande donde explico la función de mi aplicación")`

- `tags$b()` y` tags$i()` - se utilizan para poner `tags$b()` en negrita (bold) y `tags$i()` en cursiva el texto que se incluya entre los paréntesis.

- `tags$ul()`, `tags$ol()` y `tags$li()` - son etiquetas utilizadas para crear *listas*. Todas ellas se utilizan dentro de la sintaxis siguiente, y permiten al usuario crear una lista ordenada (`tags$ol()`; es decir, numerada) o desordenada (`tags$ul()`, es decir, con viñetas). `tags$li()` se utiliza para marcar los elementos de la lista, independientemente del tipo de lista que se utilice. p. ej:

```{r, eval = F}

tags$ol(
  
  tags$li("Item 1"),
  
  tags$li("Item 2"),
  
  tags$li("Item 3")
  
)

```

- `br()` y `hr()` - estas etiquetas crean *saltos de línea* y *líneas horizontales* (con un salto de línea) respectivamente. Utilízalas para separar las secciones de tu aplicación y el texto. No es necesario pasar ningún elemento a estas etiquetas (los paréntesis pueden permanecer vacíos).

- `div()` - esta es una etiqueta *genérica* que puede *contener cualquier cosa,* y puede tener *cualquier nombre*. Una vez que avances en el diseño de la interfaz de usuario, puedes utilizarlas para compartimentar tu interfaz de usuario, dar estilos específicos a determinadas secciones y crear interacciones entre el servidor y los elementos de la interfaz de usuario. No vamos a entrar en detalles, pero vale la pena conocerlos.

Ten en cuenta que se puede acceder a cada uno de estos objetos a través de `tags$...` o para algunos, sólo la función. Estos son efectivamente sinónimos, pero puede ayudar a utilizar el estilo `tags$...` si prefieres ser más explícito y no sobrescribir las funciones accidentalmente. Esta no es en absoluto una lista exhaustiva de etiquetas disponibles. Hay una lista completa de todas las etiquetas disponibles en shiny [aquí](https://shiny.rstudio.com/articles/tag-glossary.html) e incluso se pueden utilizar más insertando HTML directamente en su ui!

Si te sientes seguro, también puedes añadir cualquier *elemento de estilo css* a tus etiquetas HTML con el argumento `style` en cualquiera de ellas. No vamos a entrar en detalles sobre cómo funciona esto, pero un consejo para probar los cambios estéticos en una interfaz de usuario es utilizar el modo de inspector de HTML en Chrome (de tu aplicación Shiny que está ejecutando en el navegador), y editar el estilo de los objetos tu mismo!

Vamos a añadir algo de texto a nuestra aplicación

```{r, eval = F}

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         h4("Options"),
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = TRUE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         ),
    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve"),
      br(),
      hr(),
      p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
    tags$ul(
      tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
      tags$li(tags$b("data_date"), " - the date the data were collected at"),
      tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
      tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
      tags$li(tags$b("District"), " - the district the data were collected at"),
      tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
      tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
    )
    
  )
)
)



```

```{r, echo=F}
knitr::include_graphics(here::here("images", "shiny", "app_text_view.png"))
```


### Añadir un enlace {.unnumbered}

Para añadir un enlace a una página web, utiliza `tags$a()` con el enlace y el texto a mostrar como se muestra a continuación. Para tener como un párrafo independiente, escríbelo dentro de `p()`. Para tener sólo algunas palabras de una frase enlazada, divide la frase en partes y utiliza `tags$a()` para la parte hipervinculada. Para que el enlace se abra en una *nueva* ventana del navegador, añade `target = "_blank"` como argumento.

```{r, eval=F}
tags$a(href = "www.epiRhandbook.com", "Visit our website!")
```



### Añadir un botón de descarga {.unnumbered}

Pasemos a la segunda de las tres características. Un botón de descarga es una cosa bastante común para añadir a una aplicación y es bastante fácil de hacer. Tenemos que añadir otro Widget a nuestra ui, y tenemos que añadir otra salida a nuestro servidor para adjuntarlo. También podemos introducir *conductores reactivos* en este ejemplo!

Vamos a actualizar nuestra interfaz de usuario primero - esto es fácil ya que Shiny viene con un widget llamado `downloadButton()` - vamos a darle un `inputId` y una `label` (etiqueta).

```{r, eval = FALSE}

ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = c(
                   "All",
                   "Spring",
                   "Bolo",
                   "Dingo",
                   "Barnard"
              ),
              selected = "All",
              multiple = FALSE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         ),
         # horizontal line
         hr(),
         downloadButton(
           outputId = "download_epicurve",
           label = "Download plot"
         )

    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve"),
      br(),
      hr(),
      p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
      tags$ul(
        tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
        tags$li(tags$b("data_date"), " - the date the data were collected at"),
        tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
        tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
        tags$li(tags$b("District"), " - the district the data were collected at"),
        tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
        tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
      )
      
    )
    
  )
)


```
 
Observa que también hemos añadido una etiqueta `hr()` - esto añade una línea horizontal que separa nuestros widgets de control de nuestros widgets de descarga. Esta es otra de las etiquetas HTML que hemos discutido anteriormente.

Ahora que tenemos nuestra ui lista, necesitamos añadir el componente del servidor. Las descargas se realizan en el servidor con la función `downloadHandler()`. De manera similar a nuestra trama, necesitamos adjuntarla a una salida que tenga el mismo inputId que el botón de descarga. Esta función toma dos argumentos - `filename` y `content` - ambos son funciones. Como podrás adivinar, `filename` se utiliza para especificar el nombre del archivo descargado, y `content` se utiliza para especificar lo que debe ser descargado. content contiene una función que usarías para guardar los datos localmente - así que si estuvieras descargando un archivo csv podrías usar `rio::export()`. Como estamos descargando un gráfico, usaremos `ggplot2::ggsave()`. Veamos cómo programaríamos esto (aún no lo añadiremos al servidor).

```{r, eval = FALSE}

server <- function(input, output, session) {
  
  output$malaria_epicurve <- renderPlot(
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  )
  
  output$download_epicurve <- downloadHandler(
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
}


```


Observa que la función `content` siempre toma un argumento `file`, que ponemos donde se especifica el nombre del archivo de salida. También puedes notar que estamos repitiendo código aquí - estamos usando nuestra función `plot_epicurve()` dos veces en este servidor, una para la descarga y otra para la imagen mostrada en la aplicación. Aunque esto no afecta masivamente al rendimiento, significa que el código para generar este gráfico tendrá que ejecutarse cuando el usuario cambie los widgets que especifican el distrito y el grupo de edad, *y* de nuevo cuando quiera descargar el gráfico. En aplicaciones más grandes, decisiones subóptimas como ésta ralentizarán cada vez más las cosas, así que es bueno aprender a hacer nuestra aplicación más eficiente en este sentido. Lo que tendría más sentido es si tuviéramos una forma de ejecutar el código de la epicurva cuando los distritos/grupos de edad cambien, *y dejar que eso sea utilizado por* las funciones `renderPlot()` y `downloadHandler()`. Aquí es donde entran los conductores reactivos!

Los conductores reactivos son objetos que se crean en el servidor shiny de forma *reactiva*, pero no se emiten - sólo pueden ser utilizados por otras partes del servidor. Hay varios tipos de conductores *reactivos*, pero vamos a repasar los dos básicos.

1. `reactive()` - este es el conductor reactivo más básico - reaccionará siempre que cualquier entrada utilizada dentro de él cambie (por  nuestros widgets de distrito/grupo de edad)
2. `eventReactive()` - este conductor rectivo funciona igual que `reactive()`, excepto que el usuario puede especificar qué entradas hacen que se vuelva a ejecutar. Esto es útil si tu conductor reactivo tarda mucho en procesar, pero esto se explicará más adelante.

Veamos los dos ejemplos:

```{r, eval = FALSE}

malaria_plot_r <- reactive({
  
  plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  
})


# only runs when the district selector changes!
malaria_plot_er <- eventReactive(input$select_district, {
  
  plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  
})



```

Cuando usamos la configuración de `eventReactive()`, podemos especificar qué entradas hacen que se ejecute este trozo de código - esto no nos es muy útil por el momento, así que podemos dejarlo por ahora. Ten en cuenta que puedes incluir múltiples entradas con `c()`

Veamos cómo podemos integrar esto en el código de nuestro servidor:


```{r, eval = FALSE}

server <- function(input, output, session) {
  
  malaria_plot <- reactive({
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup)
  })
  
  
  
  output$malaria_epicurve <- renderPlot(
    malaria_plot()
  )
  
  output$download_epicurve <- downloadHandler(
    
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             malaria_plot(),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
}


```

Puedes ver que sólo estamos llamando a la salida del reactivo que hemos definido en nuestras funciones de descarga y representación gráfica. Una cosa que hay que tener en cuenta y que suele confundir a la gente es que hay que utilizar las salidas de los reactivos como si fueran funciones, por lo que *hay que añadir paréntesis vacíos al final de los mismos* (es decir, `malaria_plot()` es correcto, y `malaria_plot` no lo es). Ahora que hemos añadido esta solución nuestra aplicación es un poco más ordenada, más rápida y más fácil de cambiar ya que todo el código que ejecuta la función epicurve está en un solo lugar.


```{r, echo=F}
knitr::include_graphics(here::here("images", "shiny", "download_button_view.png"))
```


### Añadir un selector de instalaciones {.unnumbered} 

Pasemos a nuestra siguiente función: un selector para instalaciones específicas. Implementaremos otro parámetro en nuestra función para poder pasarlo como argumento desde nuestro código. Vamos a ver cómo hacer esto primero - sólo funciona con los mismos principios que los otros parámetros que hemos establecido. Actualicemos y probemos nuestra función.


```{r, echo = TRUE}

plot_epicurve <- function(data, district = "All", agegroup = "malaria_tot", facility = "All") {
  
  if (!("All" %in% district)) {
    data <- data %>%
      filter(District %in% district)
    
    plot_title_district <- stringr::str_glue("{paste0(district, collapse = ', ')} districts")
    
  } else {
    
    plot_title_district <- "all districts"
    
  }
  
  # si no hay datos restantes, devuelve NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  data <- data %>%
    filter(age_group == agegroup)
  
  
  # si no hay datos restantes, devuelve NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }
  
  if (agegroup == "malaria_tot") {
      agegroup_title <- "All ages"
  } else {
    agegroup_title <- stringr::str_glue("{str_remove(agegroup, 'malaria_rdt')} years")
  }
  
    if (!("All" %in% facility)) {
    data <- data %>%
      filter(location_name == facility)
    
    plot_title_facility <- facility
    
  } else {
    
    plot_title_facility <- "all facilities"
    
  }
  
  # if no remaining data, return NULL
  if (nrow(data) == 0) {
    
    return(NULL)
  }

  
  
  ggplot(data, aes(x = data_date, y = cases_reported)) +
    geom_col(width = 1, fill = "darkred") +
    theme_minimal() +
    labs(
      x = "date",
      y = "number of cases",
      title = stringr::str_glue("Malaria cases - {plot_title_district}; {plot_title_facility}"),
      subtitle = agegroup_title
    )
  
  
  
}
```

Vamos a probarlo:  

```{r, warning=F, message=F}

plot_epicurve(malaria_data, district = "Spring", agegroup = "malaria_rdt_0-4", facility = "Facility 1")

```


Con todas las instalaciones en nuestros datos, no está muy claro qué instalaciones corresponden a qué distritos, y el usuario final tampoco lo sabrá. Esto puede hacer que el uso de la aplicación sea poco intuitivo. Por esta razón, debemos hacer que las opciones de instalaciones en la interfaz de usuario cambien dinámicamente a medida que el usuario cambia de distrito, de modo que una filtra a la otra. Dado que tenemos tantas variables que estamos utilizando en las opciones, también podríamos querer generar algunas de nuestras opciones para la ui en nuestro archivo *global.R a partir de los datos*. Por ejemplo, podemos añadir este trozo de código a global.*R* después de haber leído nuestros datos:



```{r, , message =  FALSE}

all_districts <- c("All", unique(malaria_data$District))

# data frame of location names by district
facility_list <- malaria_data %>%
  group_by(location_name, District) %>%
  summarise() %>% 
  ungroup()

```

Vamos a verlos: 

```{r}
all_districts
```


```{r}
facility_list
```


Podemos pasar estas nuevas variables a la ui sin ningún problema, ya que son visibles globalmente tanto por el servidor como por la ui. Actualicemos nuestra UI:


```{r, eval = FALSE}


ui <- fluidPage(

  titlePanel("Malaria facility visualisation app"),

  sidebarLayout(

    sidebarPanel(
         # selector for district
         selectInput(
              inputId = "select_district",
              label = "Select district",
              choices = all_districts,
              selected = "All",
              multiple = FALSE
         ),
         # selector for age group
         selectInput(
              inputId = "select_agegroup",
              label = "Select age group",
              choices = c(
                   "All ages" = "malaria_tot",
                   "0-4 yrs" = "malaria_rdt_0-4",
                   "5-14 yrs" = "malaria_rdt_5-14",
                   "15+ yrs" = "malaria_rdt_15"
              ), 
              selected = "All",
              multiple = FALSE
         ),
         # selector for facility
         selectInput(
           inputId = "select_facility",
           label = "Select Facility",
           choices = c("All", facility_list$location_name),
           selected = "All"
         ),
         
         # horizontal line
         hr(),
         downloadButton(
           outputId = "download_epicurve",
           label = "Download plot"
         )

    ),

    mainPanel(
      # epicurve goes here
      plotOutput("malaria_epicurve"),
      br(),
      hr(),
      p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
      tags$ul(
        tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
        tags$li(tags$b("data_date"), " - the date the data were collected at"),
        tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
        tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
        tags$li(tags$b("District"), " - the district the data were collected at"),
        tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
        tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
      )
      
    )
    
  )
)


```


Fíjate en que ahora pasamos variables para nuestras elecciones en lugar de codificarlas en la interfaz de usuario. Esto también puede hacer que nuestro código sea más compacto. Por último, tendremos que actualizar el servidor. Será fácil actualizar nuestra función para incorporar nuestra nueva entrada (sólo tenemos que pasarla como argumento a nuestro nuevo parámetro), pero debemos recordar que también queremos que la ui se actualice dinámicamente cuando el usuario cambie el distrito seleccionado. Es importante entender aquí que *podemos cambiar los parámetros y el comportamiento de los widgets* mientras la aplicación se está ejecutando, pero esto debe hacerse *en el servidor*. Tenemos que entender una nueva forma de salida al servidor para aprender a hacer esto.

Las funciones que necesitamos para entender cómo hacer esto se conocen como funciones de *observador*, y son similares a las funciones *reactivas* en cuanto a su comportamiento. Sin embargo, tienen una diferencia clave:

- Las funciones reactivas no afectan directamente a las salidas, y producen objetos que pueden verse en otros lugares del servidor

- Las funciones de los observadores *pueden* afectar a las salidas del servidor, pero lo hacen a través de los efectos secundarios de otras funciones. (También pueden hacer otras cosas, pero esta es su función principal en la práctica)

Al igual que las funciones reactivas, hay dos tipos de funciones de observador, y se dividen por la misma lógica que divide las funciones reactivas:

1.  `observe()` - esta función se ejecuta cada vez que cambian las entradas utilizadas dentro de ella
2.  `observeEvent()` - esta función se ejecuta cuando cambia una entrada *especificada por el usuario*

También necesitamos entender las funciones proporcionadas por Shiny que actualizan los widgets. Estas son bastante sencillas de ejecutar - primero toman el objeto `session` de la función del servidor (esto no necesita ser entendido por ahora), y luego el `inputId` de la función a ser cambiada. Luego pasamos las nuevas versiones de todos los parámetros que ya son tomados por `selectInput()` - estos serán actualizados automáticamente en el widget.

Veamos un ejemplo aislado de cómo podríamos utilizar esto en nuestro servidor. Cuando el usuario cambia de distrito, queremos filtrar nuestra lista de instalaciones por distrito, y actualizar las opciones para que *sólo reflejen las que están disponibles en ese distrito* (y una opción para todas las instalaciones)

```{r, eval = FALSE}

observe({
  
  if (input$select_district == "All") {
    new_choices <- facility_list$location_name
  } else {
    new_choices <- facility_list %>%
      filter(District == input$select_district) %>%
      pull(location_name)
  }
  
  new_choices <- c("All", new_choices)
  
  updateSelectInput(session, inputId = "select_facility",
                    choices = new_choices)
  
})


```

Y ya está, podemos añadirlo a nuestro servidor, y ese comportamiento ya funcionará. Este es el aspecto que debería tener nuestro nuevo servidor:

```{r, eval = FALSE}
server <- function(input, output, session) {
  
  malaria_plot <- reactive({
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup, facility = input$select_facility)
  })
  
  
  
  observe({
    
    if (input$select_district == "All") {
      new_choices <- facility_list$location_name
    } else {
      new_choices <- facility_list %>%
        filter(District == input$select_district) %>%
        pull(location_name)
    }
    
    new_choices <- c("All", new_choices)
    
    updateSelectInput(session, inputId = "select_facility",
                      choices = new_choices)
    
  })
  
  
  output$malaria_epicurve <- renderPlot(
    malaria_plot()
  )
  
  output$download_epicurve <- downloadHandler(
    
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             malaria_plot(),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
  
  
}

```


```{r, out.width=c('100%', '100%'), echo=F, fig.show='hold', fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "shiny", "app_menu_view.gif"))
```







### Añadir otra pestaña con una tabla {.unnumbered}

Ahora pasaremos al último componente que queremos añadir a nuestra aplicación. Querremos separar nuestra ui en dos pestañas, una de las cuales tendrá una tabla interactiva donde el usuario podrá ver los datos con los que está haciendo la curva epidémica. Para ello, podemos utilizar los elementos de ui empaquetados que vienen con shiny relevantes para las pestañas. En un nivel básico, podemos encerrar la mayor parte de nuestro panel principal en esta estructura general:

```{r, eval = FALSE}


# ... the rest of ui

mainPanel(
  
  tabsetPanel(
    type = "tabs",
    tabPanel(
      "Epidemic Curves",
      ...
    ),
    tabPanel(
      "Data",
      ...
    )
  )
)


```

Apliquemos esto a nuestra ui. También vamos a querer utilizar el paquete **DT** aquí - este es un gran paquete para hacer tablas interactivas a partir de datos preexistentes. Podemos ver que se utiliza para `DT::datatableOutput()` en este ejemplo.

```{r, echo = FALSE}
library(DT)
```

```{r, eval = FALSE}
ui <- fluidPage(
     
     titlePanel("Malaria facility visualisation app"),
     
     sidebarLayout(
          
          sidebarPanel(
               # selector for district
               selectInput(
                    inputId = "select_district",
                    label = "Select district",
                    choices = all_districts,
                    selected = "All",
                    multiple = FALSE
               ),
               # selector for age group
               selectInput(
                    inputId = "select_agegroup",
                    label = "Select age group",
                    choices = c(
                         "All ages" = "malaria_tot",
                         "0-4 yrs" = "malaria_rdt_0-4",
                         "5-14 yrs" = "malaria_rdt_5-14",
                         "15+ yrs" = "malaria_rdt_15"
                    ), 
                    selected = "All",
                    multiple = FALSE
               ),
               # selector for facility
               selectInput(
                    inputId = "select_facility",
                    label = "Select Facility",
                    choices = c("All", facility_list$location_name),
                    selected = "All"
               ),
               
               # horizontal line
               hr(),
               downloadButton(
                    outputId = "download_epicurve",
                    label = "Download plot"
               )
               
          ),
          
          mainPanel(
               tabsetPanel(
                    type = "tabs",
                    tabPanel(
                         "Epidemic Curves",
                         plotOutput("malaria_epicurve")
                    ),
                    tabPanel(
                         "Data",
                         DT::dataTableOutput("raw_data")
                    )
               ),
               br(),
               hr(),
               p("Welcome to the malaria facility visualisation app! To use this app, manipulate the widgets on the side to change the epidemic curve according to your preferences! To download a high quality image of the plot you've created, you can also download it with the download button. To see the raw data, use the raw data tab for an interactive form of the table. The data dictionary is as follows:"),
               tags$ul(
                    tags$li(tags$b("location_name"), " - the facility that the data were collected at"),
                    tags$li(tags$b("data_date"), " - the date the data were collected at"),
                    tags$li(tags$b("submitted_daate"), " - the date the data were submitted at"),
                    tags$li(tags$b("Province"), " - the province the data were collected at (all 'North' for this dataset)"),
                    tags$li(tags$b("District"), " - the district the data were collected at"),
                    tags$li(tags$b("age_group"), " - the age group the data were collected for (0-5, 5-14, 15+, and all ages)"),
                    tags$li(tags$b("cases_reported"), " - the number of cases reported for the facility/age group on the given date")
               )
               
               
          )
     )
)


```


Ahora nuestra aplicación está organizada en pestañas! Hagamos también las modificaciones necesarias en el servidor. Dado que no necesitamos manipular nuestro conjunto de datos antes de procesarlo, esto es muy sencillo: ¡sólo tenemos que procesar los datos malaria_data a través de `DT::renderDT()` en la interfaz de usuario!


```{r, eval = FALSE}
server <- function(input, output, session) {
  
  malaria_plot <- reactive({
    plot_epicurve(malaria_data, district = input$select_district, agegroup = input$select_agegroup, facility = input$select_facility)
  })
  
  
  
  observe({
    
    if (input$select_district == "All") {
      new_choices <- facility_list$location_name
    } else {
      new_choices <- facility_list %>%
        filter(District == input$select_district) %>%
        pull(location_name)
    }
    
    new_choices <- c("All", new_choices)
    
    updateSelectInput(session, inputId = "select_facility",
                      choices = new_choices)
    
  })
  
  
  output$malaria_epicurve <- renderPlot(
    malaria_plot()
  )
  
  output$download_epicurve <- downloadHandler(
    
    filename = function() {
      stringr::str_glue("malaria_epicurve_{input$select_district}.png")
    },
    
    content = function(file) {
      ggsave(file, 
             malaria_plot(),
             width = 8, height = 5, dpi = 300)
    }
    
  )
  
  # render data table to ui
  output$raw_data <- DT::renderDT(
    malaria_data
  )
  
  
}


```


```{r, out.width=c('100%', '100%'), fig.show='hold', echo = F, fig.width = 12, fig.height = 9, message=F, warning=F}
knitr::include_graphics(here::here("images", "shiny", "app_table_view.gif"))
```


## Compartir apps Shiny {#sharing-shiny-apps}

Ahora que has desarrollado tu aplicación, probablemente quieras compartirla con los demás, ¡al fin y al cabo esta es la principal ventaja de shiny! Podemos hacerlo compartiendo el código directamente, o podemos publicarlo en un servidor. Si compartimos el código, otros podrán ver lo que has hecho y construir sobre él, pero esto anulará una de las principales ventajas de shiny: *puede eliminar la necesidad de que los usuarios finales mantengan una instalación de R*. Por esta razón, si estás compartiendo tu aplicación con usuarios que no se sienten cómodos con R, es mucho más fácil compartir una aplicación que ha sido publicada en un servidor.

Si prefieres compartir el código, puedes hacer un archivo .zip de la aplicación, o mejor aún, *publicar tu aplicación en github y añadir colaboradores.* Puedes consultar la sección de github para más información aquí.

Sin embargo, si vamos a publicar la aplicación en línea, tenemos que hacer un poco más de trabajo. En última instancia, queremos que se pueda acceder a tu aplicación a través de una URL web para que otros puedan acceder a ella de forma rápida y sencilla. Desafortunadamente, para publicar tu aplicación en un servidor, necesitas tener acceso a un servidor donde publicarla. Hay varias opciones de alojamiento en este sentido:

- _shinyapps.io_: es el lugar más sencillo para publicar aplicaciones shiny, ya que es el que menos trabajo de configuración necesita, y tiene algunas licencias gratuitas, pero limitadas.

- _RStudio Connect_: es una versión mucho más potente de un servidor de R, que puede realizar muchas operaciones, incluida la publicación de aplicaciones Shinys. Sin embargo, es más difícil de usar y menos recomendable para los usuarios noveles.

Para los propósitos de este documento, utilizaremos *shinyapps.io*, ya que es más fácil para los usuarios noveles. Puedes hacer una cuenta gratuita aquí para empezar - también hay diferentes planes de precios para las licecias de los servidores si es necesario. Cuantos más usuarios esperes tener, más caro tendrá que ser tu plan de precios, así que tenlo en cuenta. Si quieres crear algo para un pequeño grupo de personas, una licencia gratuita puede ser perfectamente adecuada, pero una aplicación de cara al público puede necesitar más licencias.

Primero debemos asegurarnos de que nuestra aplicación es adecuada para publicar en un servidor. En tu aplicación, debes reiniciar tu sesión de R, y asegurarte de que se ejecuta sin ejecutar ningún código extra. Esto es importante, ya que una aplicación que requiere la carga de paquetes, o la lectura de datos no definidos en el código de tu aplicación no se ejecutará en un servidor. También ten en cuenta que no puedes tener rutas de archivo *explícitas* en tu aplicación - éstas serán inválidas en la configuración del servidor - el uso del paquete here resuelve muy bien este problema. Por último, si estás leyendo datos de una fuente que requiere autenticación de usuario, como los servidores de tu organización, esto no funcionará generalmente en un servidor. Tendrás que ponerte en contacto con tu departamento de TI para averiguar cómo poner en la lista blanca el Shiny servidor.

*registro de la cuenta*

Una vez que tengas tu cuenta, puedes navegar a la página de tokens en _Accounts_. Aquí querrás añadir un nuevo token, que se utilizará para desplegar tu aplicación.

A partir de aquí, debes tener en cuenta que la url de tu cuenta reflejará el nombre de tu app - así que si tu app se llama *mi_app*, la url se añadirá como *xxx.io/mi_app/*. Elige bien el nombre de tu aplicación. Ahora que está todo listo, clica en desplegar - si tiene éxito esto ejecutará tu aplicación en la url web elegida.

*¿algo sobre la creación de aplicaciones en documentos?*

## Más información {#further-reading}

Hasta ahora, hemos cubierto muchos aspectos de shiny, y apenas hemos arañado la superficie de lo que ofrece shiny. Aunque esta guía sirve de introducción, hay mucho más que aprender para entender completamente shiny. Deberías empezar a crear aplicaciones y añadir gradualmente más y más funcionalidad


## Paquetes de extensión recomendados {#recommended-extension-packages}

A continuación se presenta una selección de extensiones de shiny de alta calidad que pueden ayudarte a sacar mucho más provecho de shiny. Sin ningún orden en particular:

* **shinyWidgets** - este paquete ofrece muchos más widgets que pueden ser utilizados en tu aplicación. Ejecuta  `shinyWidgets::shinyWidgetsGallery()` para ver una selección de los widgets disponibles con este paquete. Mira los ejemplos [aquí](https://github.com/dreamRs/shinyWidgets)

* **shinyjs** - este es un excelente paquete que da al usuario la capacidad de ampliar en gran medida la utilidad de shiny a través de una serie de javascript. Las aplicaciones de este paquete van desde las más sencillas hasta las más avanzadas, pero es posible que quieras utilizarlo primero para manipular la interfaz de usuario de forma sencilla, como ocultar/mostrar elementos, o activar/desactivar botones. Para más información, [consulta aqui](https://deanattali.com/shinyjs/basic)

* **shinydashboard** - este paquete expande masivamente la ui disponible que puede ser usada en shiny, específicamente permitiendo al usuario crear un dashboard complejo con una variedad de diseños complejos. Consulta más [aquí](https://rstudio.github.io/shinydashboard/)

* **shinydashboardPlus**: ¡aún más funciones del marco de trabajo de **shinydashboard**! Puedes ver más  [aquí](https://rinterface.github.io/shinydashboardPlus/articles/shinydashboardPlus.html)

* **shinythemes** - ¡cambia el tema css por defecto de tu app shiny con una amplia gama de plantillas preestablecidas!  [Más aquí](https://rstudio.github.io/shinythemes/)


También hay una serie de paquetes que pueden utilizarse para crear resultados interactivos compatibles con Shiny.

* **DT** está semi-incorporado en shinybásico, pero proporciona un gran conjunto de funciones para crear tablas interactivas.

* **plotly** es un paquete para crear gráficos interactivos que el usuario puede manipular en la aplicación. También puede convertir sus gráficos en versiones interactivas mediante `plotly::ggplotly()`. Como alternativas, **dygraphs** y **highcharter** son también excelentes.

## Recursos recomendados {#recommended-resources}



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/shiny_basics.Rmd-->

# (PART) Miscelánea {.unnumbered}
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/cat_misc.Rmd-->

# Escribir funciones {#writing-functions-1}


<!-- ======================================================= -->



## Preparación {#preparation-37}


### Cargar paquetes {-}

Este trozo de código muestra la carga de los paquetes necesarios para los análisis. En este manual destacamos `p_load()` de **pacman**, que instala el paquete si es necesario *y* lo carga para su uso. También puedes cargar los paquetes instalados con `library()` de R **base.** Consulta la página [fundamentos de R](#r-basics) para obtener más información sobre los paquetes de R.

```{r, echo=F, warning=F, message=F}
pacman::p_load(
  rio,          # File import
  here,         # File locator
  skimr,        # get overview of data
  tidyverse,    # data management + ggplot2 graphics, 
  gtsummary,    # summary statistics and tests
  janitor,      # adding totals and percents to tables
  scales,       # easily convert proportions to percents  
  flextable,     # converting tables to HTML
  purrr,          #makes functional programming easier
  readr,          #to read csv files
  highcharter     #to create highchart object and draw particular plot

  )
```

### Importar datos {-}

Importamos los datos de casos de una epidemia de ébola simulada. Si deseas descargar los datos para seguirlos paso a paso, consulta las instrucciones en la página [Descargar libro y datos]. Los datos se importan mediante la función `import()` del paquete **rio**. Consulta la página sobre [importación y exportación](#import-and-export) para conocer las distintas formas de importar datos.

También utilizaremos en la última parte de esta página algunos datos sobre la gripe H7N9 de 2013.

```{r, echo=F}
# import the linelists into R
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

flu_china <- rio::import(here::here("data", "case_linelists", "fluH7N9_China_2013.csv"))

```


## Funciones {#functions-2}

Las funciones son útiles en la programación, ya que permiten hacer códigos más fáciles de entender, de alguna manera más cortos y menos propensos a errores (dado que no hay errores en la propia función).

Si has llegado hasta este manual, significa que te has encontrado con un sinfín de funciones ya que en R, cada operación es una llamada a una función `+, for, if, [, $, { …`. Por ejemplo, `x + y` es lo mismo que`'+'(x, y)` 

R es uno de los lenguajes que más posibilidades ofrece para trabajar con funciones y da suficientes herramientas al usuario para escribirlas fácilmente. No debemos pensar en las funciones como algo fijo en la cima o al final de la cadena de programación, R ofrece la posibilidad de utilizarlas como si fueran vectores e incluso utilizarlas dentro de otras funciones, listas...

Existen muchos recursos muy avanzados sobre programación funcional y aquí sólo daremos una visión para ayudarte a empezar con la programación de funciones con breves ejemplos prácticos. Te animamos a visitar los enlaces de las referencias para leer más sobre el tema.


## ¿Por qué utilizar una función? {#why-would-you-use-a-function}

Antes de responder a esta pregunta, es importante tener en cuenta que ya has tenido consejos para llegar a escribir tus primeras funciones R en la página sobre [Iteración, bucles y listas](#iteration-loops-and-lists) de este manual. De hecho, el uso de "if/else" y bucles suele ser una parte fundamental de muchas de nuestras funciones, ya que ayudan fácilmente a ampliar la aplicación de nuestro código permitiendo múltiples condiciones o a iterar códigos para repetir tareas.

* ¿Estoy repitiendo varias veces el mismo bloque de código para aplicarlo a una variable o dato diferente?

* Deshacerse de él, ¿acortará sustancialmente mi código general y hará que se ejecute más rápido?

* ¿Es posible que el código que he escrito se utilice de nuevo pero con un valor diferente en muchos lugares del código?

Si la respuesta a una de las preguntas anteriores es "SÍ", es probable que tenga que escribir una función.

## ¿Cómo construye R las funciones? {#how-does-r-build-functions}

Las funciones en R tienen tres componentes principales:

* las `formals()` que es la lista de argumentos que controla cómo podemos llamar a la función

* el `body()` que es el código dentro de la función, es decir, dentro de los paréntesis o después del paréntesis, dependiendo de cómo lo escribamos

y,

* el `environment()` que ayudará a localizar las variables de la función y determina cómo encuentra la función el valor.

Una vez que hayas creado tu función, puedes verificar cada uno de estos componentes llamando a la función asociada.

## Sintaxis y estructura básica {#basic-syntax-and-structure}

* Una función tendrá que ser nombrada adecuadamente para que su trabajo sea fácilmente comprensible tan pronto como leamos su nombre. En realidad, este es el caso de la mayoría de la arquitectura básica de R. Funciones como `mean()`, `print()`, `summary()` tienen nombres muy sencillos

* Una función necesitará argumentos, como los datos sobre los que trabajar y otros objetos que pueden ser valores estáticos entre otras opciones

* Y finalmente una función producirá una salida basada en su tarea principal y en los argumentos que se le han dado. Normalmente utilizaremos las funciones incorporadas como `print()`, `return()`... para producir la salida. La salida puede ser un valor lógico, un número, un carácter, un dataframe... en definitiva cualquier tipo de objeto de R.

Básicamente se trata de la composición de una función:

```{r, eval=FALSE}

function_name <- function(argument_1, argument_2, argument_3){
  
           function_task
  
           return(output)
}


```

Podemos crear nuestra primera función que se llamará `contain_covid19()`.

```{r}

contain_covid19 <- function(barrier_gest, wear_mask, get_vaccine){
  
                            if(barrier_gest == "yes" & wear_mask == "yes" & get_vaccine == "yes" ) 
       
                            return("success")
  
  else("please make sure all are yes, this pandemic has to end!")
}


```

A continuación, podemos verificar los componentes de nuestra función recién creada.

```{r}

formals(contain_covid19)
body(contain_covid19)
environment(contain_covid19)

```


Ahora vamos a probar nuestra función. Para llamar a nuestra función escrita, la usas como usas todas las funciones de R, es decir, escribiendo el nombre de la función y añadiendo los argumentos necesarios.

```{r}

contain_covid19(barrier_gest = "yes", wear_mask = "yes", get_vaccine = "yes")

```

Podemos volver a escribir el nombre de cada argumento por precaución. Pero sin especificarlos, el código debería funcionar ya que R tiene en memoria la posición de cada argumento. Así que mientras pongas los valores de los argumentos en el orden correcto, puedes omitir escribir los nombres de los argumentos al llamar a las funciones.

```{r}

contain_covid19("yes", "yes", "yes")

```

A continuación, veamos qué ocurre si uno de los valores es `"no"` o **no** `"yes"`.

```{r}

contain_covid19(barrier_gest = "yes", wear_mask = "yes", get_vaccine = "no")
```

Si proporcionamos un argumento que no es reconocido, se producirá un error:

```{r, eval=F}
contain_covid19(barrier_gest = "sometimes", wear_mask = "yes", get_vaccine = "no")
```

`Error en contain_covid19(barrier_gest = "sometimes", wear_mask = "yes", : no se pudo encontrar la función "contain_covid19"`


<span style="color: black;">**_NOTA:_** Algunas funciones (la mayoría de las veces muy cortas y sencillas) pueden no necesitar un nombre y pueden ser utilizadas directamente en una línea de código o dentro de otra función para realizar una tarea rápida. Se llaman **funciones anónimas**.</span>

Por ejemplo, a continuación se muestra una primera función anónima que mantiene sólo las variables de carácter de los datos.


```{r, eval=F}
linelist %>% 
  dplyr::slice_head(n=10) %>%  #equivalente a la función de R base "head" que retorna las n primeras observaciones de un conjunto de datos.
  select(function(x) is.character(x)) 
```
  
```{r, echo=F}
linelist %>% 
  dplyr::slice_head(n=10) %>%  #equivalent to R base "head" function and that return first n observation of the  dataset
  select(function(x) is.character(x)) %>%  
DT::datatable(rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )
```


A continuación, otra función que selecciona una de cada dos observaciones de nuestro conjunto de datos (puede ser relevante cuando tenemos datos longitudinales con muchos registros por paciente, por ejemplo, después de haber ordenado por fecha o visita). En este caso, la función adecuada que se escribe fuera de dplyr sería `function (x) (x%2 == 0)` para aplicarla al vector que contiene todos los números de fila.


```{r, eval=F}
linelist %>%   
   slice_head(n=20) %>% 
   tibble::rownames_to_column() %>% # agregue índices de cada obs como rownames para ver claramente la selección final
   filter(row_number() %%2 == 0)
```

```{r, echo=F}
linelist %>%   
   slice_head(n=20) %>% 
   tibble::rownames_to_column() %>%    # add indices of each obs as rownames to clearly see the final selection
   filter(row_number() %%2 == 0) %>% 
DT::datatable(rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )

```


Un posible código para la misma tarea sería:

```{r, eval = F}

linelist_firstobs <- head(linelist, 20)

linelist_firstobs[base::Filter(function(x) (x%%2 == 0), seq(nrow(linelist_firstobs))),]
```

```{r, echo=F}

linelist_firstobs <- head(linelist, 20)

linelist_firstobs[base::Filter(function(x) (x%%2 == 0), seq(nrow(linelist_firstobs))),] %>% 
DT::datatable(rownames = FALSE, filter="top", options = list(pageLength = 10, scrollX=T), class = 'white-space: nowrap' )

```


<span style="color: orange;">**_PRECAUCIÓN:_** Aunque es cierto que el uso de funciones puede ayudarnos con nuestro código, puede llevar mucho tiempo escribir algunas funciones o arreglar una si no ha sido pensada a fondo, escrita adecuadamente y está devolviendo errores como resultado. Por esta razón, a menudo se recomienda escribir primero el código en R, asegurarse de que hace lo que pretendemos, y luego transformarlo en una función con sus tres componentes principales, como se ha indicado anteriormente.</span>

## Ejemplos  {#examples-2}

### Devuelve tablas de proporciones para varias columnas {.unnumbered}  

Sí, ya disponemos de bonitas funciones en muchos paquetes que permiten resumir la información de una manera muy fácil y agradable. Pero aún así intentaremos hacer las nuestras, en nuestros primeros pasos para acostumbrarnos a escribir funciones.

En este ejemplo queremos mostrar cómo la escritura de una función simple te evitaría copiar y pegar el mismo código varias veces.

```{r}

proptab_multiple <- function(my_data, var_to_tab){
  
  #imprime el nombre de cada variable de interés antes de realizar la tabulación
  print(var_to_tab)

  with(my_data,
       rbind( #enlazar por filas los resultados de las siguientes dos funciones 
        #tabular la variable de interés: da solo números
          table(my_data[[var_to_tab]], useNA = "no"),
          #calcular la proporción de cada variable de interés y redondear el valor a 2 decimales
         round(prop.table(table(my_data[[var_to_tab]]))*100,2)
         )
       )
}


proptab_multiple(linelist, "gender")

proptab_multiple(linelist, "age_cat")

proptab_multiple(linelist, "outcome")


```

<span style="color: darkgreen;">**_CONSEJO:_** Como se ha indicado anteriormente, es muy importante comentar las funciones como se haría en la programación general. Ten en cuenta que el objetivo de una función es hacer un código fácil de leer, más corto y más eficiente. Entonces uno debería ser capaz de entender lo que hace la función con sólo leer su nombre y debería tener más detalles leyendo los comentarios.</span>


Una segunda opción es utilizar esta función en otra a través de un bucle para hacer el proceso a la vez:

```{r}


for(var_to_tab in c("gender","age_cat",  "outcome")){
  
  print(proptab_multiple(linelist, var_to_tab))
  
}

```

Una forma más sencilla podría ser utilizar la base R "apply" en lugar de un "bucle for" como se expresa a continuación:

```{r, include= FALSE, eval=FALSE}

base::lapply(linelist[,c("gender","age_cat", "outcome")], table)

```


<span style="color: darkgreen;">**_CONSEJO:_** R se define a menudo como un lenguaje de programación funcional y casi siempre que ejecutas una línea de código estás utilizando algunas funciones incorporadas. Un buen hábito para sentirse más cómodo con la escritura de funciones es echar a menudo un vistazo interno a cómo están construidas las funciones básicas que utiliza a diario. El atajo para hacerlo es seleccionar el nombre de la función y luego clicar en `Ctrl+F2` o `fn+F2` o `Cmd+F2` (dependiendo de tu ordenador).</span>

## Uso de **purrr**: escribir funciones que se pueden aplicar de forma iterativa {#using-purrr-writing-functions-that-can-be-iteratively-applied}

### Modificar el tipo de múltiples columnas en unos datos {.unnumbered}   

Digamos que muchas variables de carácter en los datos originales de `linelist` necesitan ser cambiadas a "factor" para propósitos de análisis y trazado. En lugar de repetir el paso varias veces, podemos utilizar simplemente `lapply()` para realizar la transformación de todas las variables afectadas en una sola línea de código.


<span style="color: orange;">**_PRECAUCIÓN:_** `lapply()` devuelve una lista, por lo que su uso puede requerir una modificación adicional como último paso.</span>


```{r, include=FALSE}

linelist_factor1 <- linelist %>%
      lapply(
          function(x) if(is.character(x)) as.factor(x) else x) %>%
      as.data.frame() %>% 
      glimpse()

```


El mismo paso puede realizarse utilizando la función `map_if()` del paquete **purrr**

```{r}

linelist_factor2 <- linelist %>%
  purrr::map_if(is.character, as.factor)


linelist_factor2 %>%
        glimpse()

```


### Elaborar de forma iterativa gráficos para diferentes niveles de una variable {.unnumbered} 

Produciremos aquí un gráfico circular para ver la distribución del resultado de los pacientes en China durante el brote de H7N9 para cada provincia. En lugar de repetir el código para cada una de ellas, nos limitaremos a aplicar una función que crearemos.

```{r}

#precisar opciones para el uso de highchart
options(highcharter.theme =   highcharter::hc_theme_smpl(tooltip = list(valueDecimals = 2)))


#Cree una función llamada "chart_outcome_province" que tome como argumento el conjunto de datos y el nombre de la provincia para la cual plotear la distribución del resultado.

chart_outcome_province <- function(data_used, prov){
  
  tab_prov <- data_used %>% 
    filter(province == prov,
           !is.na(outcome))%>% 
    group_by(outcome) %>% 
    count() %>%
    adorn_totals(where = "row") %>% 
    adorn_percentages(denominator = "col", )%>%
    mutate(
        perc_outcome= round(n*100,2))
  
  
  tab_prov %>%
    filter(outcome != "Total") %>% 
  highcharter::hchart(
    "pie", hcaes(x = outcome, y = perc_outcome),
    name = paste0("Distibution of the outcome in:", prov)
    )
  
}

chart_outcome_province(flu_china, "Shanghai")
chart_outcome_province(flu_china,"Zhejiang")
chart_outcome_province(flu_china,"Jiangsu")


```



### Producir iterativamente tablas para diferentes niveles de una variable {.unnumbered}  

Aquí crearemos tres indicadores para resumirlos en una tabla y nos gustaría elaborar esta tabla para cada una de las provincias. Nuestros indicadores son el retraso entre el inicio y la hospitalización, el porcentaje de recuperación y la edad media de los casos.

```{r}


indic_1 <- flu_china %>% 
  group_by(province) %>% 
  mutate(
    date_hosp= strptime(date_of_hospitalisation, format = "%m/%d/%Y"),
    date_ons= strptime(date_of_onset, format = "%m/%d/%Y"), 
    delay_onset_hosp= as.numeric(date_hosp - date_ons)/86400,
    mean_delay_onset_hosp = round(mean(delay_onset_hosp, na.rm=TRUE ), 0)) %>%
  select(province, mean_delay_onset_hosp)  %>% 
  distinct()
     

indic_2 <-  flu_china %>% 
            filter(!is.na(outcome)) %>% 
            group_by(province, outcome) %>% 
            count() %>%
            pivot_wider(names_from = outcome, values_from = n) %>% 
    adorn_totals(where = "col") %>% 
    mutate(
        perc_recovery= round((Recover/Total)*100,2))%>% 
  select(province, perc_recovery)
    
    
    
indic_3 <-  flu_china %>% 
            group_by(province) %>% 
            mutate(
                    median_age_cases = median(as.numeric(age), na.rm = TRUE)
            ) %>% 
  select(province, median_age_cases)  %>% 
  distinct()

#unir los indicadores de los tres conjuntos de datos

table_indic_all <- indic_1 %>% 
  dplyr::left_join(indic_2, by = "province") %>% 
        left_join(indic_3, by = "province")


#imprimir los indicadores en una flextable


print_indic_prov <-  function(table_used, prov){
  
  #first transform a bit the dataframe for printing ease
  indic_prov <- table_used %>%
    filter(province==prov) %>%
    pivot_longer(names_to = "Indicateurs", cols = 2:4) %>% 
   mutate( indic_label = factor(Indicateurs,
   levels= c("mean_delay_onset_hosp","perc_recovery","median_age_cases"),
   labels=c("Mean delay onset-hosp","Percentage of recovery", "Median age of the cases"))
   ) %>% 
    ungroup(province) %>% 
    select(indic_label, value)
  

    tab_print <- flextable(indic_prov)  %>%
    theme_vanilla() %>% 
    flextable::fontsize(part = "body", size = 10) 
    
    
     tab_print <- tab_print %>% 
                  autofit()   %>%
                  set_header_labels( 
                indic_label= "Indicateurs", value= "Estimation") %>%
    flextable::bg( bg = "darkblue", part = "header") %>%
    flextable::bold(part = "header") %>%
    flextable::color(color = "white", part = "header") %>% 
    add_header_lines(values = paste0("Indicateurs pour la province de: ", prov)) %>% 
bold(part = "header")
 
 tab_print <- set_formatter_type(tab_print,
   fmt_double = "%.2f",
   na_str = "-")

tab_print 
    
}




print_indic_prov(table_indic_all, "Shanghai")
print_indic_prov(table_indic_all, "Jiangsu")


```


## Consejos y buens prácticas para el buen funcionamiento de las funciones {#tips-and-best-practices-for-well-functioning-functions}

La programación funcional está pensada para aliviar el código y facilitar su lectura. Podría producir lo contrario. Los siguientes consejos le ayudarán a tener un código limpio y fácil de leer.


### Nombres y sintaxis {.unnumbered} 

* Evitar el uso de caracteres que podrían haber sido fácilmente tomados por otras funciones ya existentes en su entorno

* Se recomienda que el nombre de la función sea corto y sencillo de entender para otro lector

* Es preferible utilizar verbos como nombre de la función y sustantivos para los nombres de los argumentos.


### Nombres de columnas y evaluación ordenada {.unnumbered}   

Si quiere saber cómo referenciar *nombres de columnas* que se proporcionan a su código como argumentos, lea esta [guía de programación de tidyverse](https://dplyr.tidyverse.org/articles/programming.html). Entre los temas tratados están la *evaluación ordenada* y el uso del *abrazo* con `{{ }}` "llaves dobles"

Por ejemplo, aquí hay un esqueleto rápido de código de plantilla del tutorial de la página mencionada anteriormente:

```{r, eval=F}

var_summary <- function(data, var) {
  data %>%
    summarise(n = n(), min = min({{ var }}), max = max({{ var }}))
}
mtcars %>% 
  group_by(cyl) %>% 
  var_summary(mpg)

```


### Pruebas y tratamiento de errores {.unnumbered} 

Cuanto más complicada sea la tarea de una función, mayor será la posibilidad de errores. Por lo tanto, a veces es necesario añadir alguna verificación dentro de la función para ayudar a entender rápidamente de dónde proviene el error y encontrar una manera de solucionarlo.

* Puede ser más que recomendable introducir una comprobación sobre la ausencia de un argumento utilizando `missing(argumento)`. Esta simple comprobación puede devolver el valor "TRUE" o "FALSE".

```{r , error=TRUE}

contain_covid19_missing <- function(barrier_gest, wear_mask, get_vaccine){
  
  if (missing(barrier_gest)) (print("please provide arg1"))
  if (missing(wear_mask)) print("please provide arg2")
  if (missing(get_vaccine)) print("please provide arg3")


  if (!barrier_gest == "yes" | wear_mask =="yes" | get_vaccine == "yes" ) 
       
       return ("you can do better")
  
  else("please make sure all are yes, this pandemic has to end!")
}


contain_covid19_missing(get_vaccine = "yes")

```


* Utiliza `stop()` para errores más detectables.

```{r, error=TRUE}

contain_covid19_stop <- function(barrier_gest, wear_mask, get_vaccine){
  
  if(!is.character(barrier_gest)) (stop("arg1 should be a character, please enter the value with `yes`, `no` or `sometimes"))
  
  if (barrier_gest == "yes" & wear_mask =="yes" & get_vaccine == "yes" ) 
       
       return ("success")
  
  else("please make sure all are yes, this pandemic has to end!")
}


contain_covid19_stop(barrier_gest=1, wear_mask="yes", get_vaccine = "no")

```

* Como vemos cuando ejecutamos la mayoría de las funciones incorporadas, hay mensajes y advertencias que pueden aparecer en ciertas condiciones. Podemos integrarlos en nuestras funciones escritas utilizando las funciones `message()` y `warning()`.

* También podemos manejar los errores usando `safely()` que toma una función como argumento y la ejecuta de forma segura. De hecho, la función se ejecutará sin detenerse si encuentra un error. `safely()` devuelve como salida una **lista** con dos objetos que son los resultados y el error que se ha "saltado".

Podemos verificarlo ejecutando primero la `mean()` como función, y luego ejecutarla con `safely()`.


```{r, warning=FALSE}

map(linelist, mean)
```


```{r, warning=FALSE}
safe_mean <- safely(mean)
linelist %>% 
  map(safe_mean)

```


Como se ha dicho anteriormente, comentar bien nuestros códigos ya es una buena forma de tener documentación en nuestro trabajo.


<!-- ======================================================= -->
## Recursos {#resources-36}

[Funciones en R for Data Science en español](https://es.r4ds.hadley.nz/funciones.html)

[Cheatsheet advanzado de programación de R](https://www.rstudio.com/wp-content/uploads/2016/02/advancedR.pdf)

[Cheatsheet del paquete purr](https://purrr.tidyverse.org/)

[Vídeo-ACM charla de Hadley Wickham: La alegría de la programación funcional (cómo funciona map_dbl)](https://youtube.videoken.com/embed/bzUmK0Y07ck)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/writing_functions.Rmd-->


# Interacciones con directorios {#directory-interactions}  


En esta página cubrimos los escenarios comunes en los que se crea, se interactúa, se guarda y se importa con directorios (carpetas).


## Preparación  {#preparation-38}

### Paquete **fs** {.unnumbered}   

El paquete **fs** es un paquete **tidyverse** que facilita las interacciones con los directorios, mejorando algunas de las funciones de R **base**. En las secciones siguientes utilizaremos a menudo funciones de **fs**.

```{r}
pacman::p_load(
  fs,             # file/directory interactions
  rio,            # import/export
  here,           # relative file pathways
  tidyverse)      # data management and visualization
```


### Imprimir el directorio como un árbol de dendrogramas {.unnumbered}   

Utiliza la función `dir_tree()` de **fs**.

Proporciona la ruta de la carpeta a `path = ` y decide si quieres mostrar sólo un nivel (`recurse = FALSE`) o todos los archivos en todos los subniveles (`recurse = TRUE`). A continuación utilizamos `here()` como abreviatura del proyecto R y especificamos su subcarpeta "data", que contiene todos los datos utilizados para este manual de R. Lo configuramos para que muestre todos los archivos dentro de "data" y sus subcarpetas (por ejemplo, "cache", "epidemic models", "population", "shp" y "weather").


```{r}
fs::dir_tree(path = here("data"), recurse = TRUE)
```


## Listar los archivos de un directorio {#list-files-in-a-directory}

Para listar sólo los nombres de los archivos de un directorio puedes utilizar `dir()` de R **base**. Por ejemplo, este comando lista los nombres de los archivos de la subcarpeta "population" de la carpeta "data" en un proyecto R. La ruta relativa de los archivos se proporciona utilizando `here()` (sobre la que puede leer más en la página de [importación y exportación](#import-and-export)).  

```{r}
# file names
dir(here("data", "gis", "population"))
```

Para listar las rutas completas de los archivos del directorio, puedes utilizar `dir_ls()` de **fs**. Una alternativa de R **base** es `list.files()`.

```{r}
# file paths
dir_ls(here("data", "gis", "population"))
```

Para obtener toda la información de los metadatos de cada archivo en un directorio, (por ejemplo, la ruta, la fecha de modificación, etc.) puedes utilizar `dir_info()` de **fs**.

Esto puede ser especialmente útil si quieres extraer la última hora de modificación del archivo, por ejemplo si quieres importar la versión más reciente de un archivo. Para ver un ejemplo de esto, consulta la página de [importación y exportación](#import-and-export).

```{r, eval=F}
# file info
dir_info(here("data", "gis", "population"))
```

Aquí está el dataframe devuelto. Desplázate a la derecha para ver todas las columnas.  

```{r, echo=F}
DT::datatable(dir_info(here("data", "gis", "population")), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

## Información sobre el archivo {#file-information}

Para extraer información de metadatos sobre un archivo específico, puedes utilizar `file_info()` de **fs** (o `file.info()` de R **base**).

```{r, eval=F}
file_info(here("data", "case_linelists", "linelist_cleaned.rds"))
```

```{r, echo=F}
DT::datatable(file_info(here("data", "case_linelists", "linelist_cleaned.rds")), rownames = FALSE, options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Aquí usamos `$` para indexar el resultado y devolver sólo el valor de `modification_time`.

```{r}
file_info(here("data", "case_linelists", "linelist_cleaned.rds"))$modification_time
```




## Comprobar si existe  {#check-if-exists}

### Objetos R {.unnumbered}   

Puedes utilizar `exists()` de R **base** para comprobar si un objeto R existe *dentro* de R (escribe el nombre del objeto entre comillas).

```{r}
exists("linelist")
```

Ten en cuenta que algunos paquetes de R **base** utilizan nombres de objetos genéricos como "data" entre bastidores, que aparecerán como TRUE a menos que se especifique `inherit = FALSE`. Esta es una razón para no nombrar tu conjunto de datos como "data".

```{r}
exists("data")
exists("data", inherit = FALSE)
```

Si estás escribiendo una función, deberías utilizar `missing()` de R **base** para comprobar si un argumento está presente o no, en lugar de `exists()`.



### Directorios {.unnumbered}   

Para comprobar si un directorio existe, escribe la ruta del archivo (y el nombre del archivo) a `is_dir()` de **fs**. Desplázate a la derecha para ver que se imprime `TRUE`.


```{r}
is_dir(here("data"))
```

Una alternativa de R **base** es `file.exists()`.


### Files {.unnumbered} 

Para comprobar si un archivo específico existe, utiliza `is_file()` de **fs**. Desplázate a la derecha para ver que se imprime `TRUE`. 

```{r}
is_file(here("data", "case_linelists", "linelist_cleaned.rds"))
```

Una alternativa de R **base** es `file.exists()`. 


## Crear {#create}

### Directorios {.unnumbered}   

Para crear un nuevo directorio (carpeta) puede utilizar `dir_create()` de **fs**. Si el directorio ya existe, no se sobrescribirá y no se devolverá ningún error.

```{r, eval=F}
dir_create(here("data", "test"))
```

Una alternativa es `dir.create()` de R **base**, que mostrará un error si el directorio ya existe. En cambio, `dir_create()` en este escenario será silencioso.

### Archivos {.unnumbered}   

Puedes crear un archivo (vacío) con `file_create()` de **fs**. Si el archivo ya existe, no se sobreescribirá ni se modificará.


```{r, eval=F}
file_create(here("data", "test.rds"))
```

Una alternativa de R **base** es `file.create()`. Pero si el archivo ya existe, esta opción lo truncará. Si se utiliza `file_create()` el archivo se dejará sin cambios  


### Crear si no existe {.unnumbered}   


EN CONSTRUCCIÓN


## Borrar {#delete}

### Objetos R {.unnumbered}   

Utiliza `rm()` de R **base** para eliminar un objeto R. 

### Directorios {.unnumbered}  

Utiliza `dir_delete()` de **fs**.


### Archivos {.unnumbered}  

Puedes eliminar archivos con `file_delete()` de **fs**.


## Ejecución de otros archivos  {#running-other-files}

### `source()` {.unnumbered}  

Para ejecutar un script de R desde otro script de R, puedes utilizar el comando `source()` (de R **base**).

```{r, eval=F}
source(here("scripts", "cleaning_scripts", "clean_testing_data.R"))
```

Esto equivale a ver el script de R anterior y clicar en el botón "Source" en la parte superior derecha del script. Esto ejecutará el script pero lo hará de forma silenciosa (sin salida a la consola de R) a menos que se pretenda específicamente. Consulta la página sobre [Consola interactiva] para ver ejemplos de uso de `source()` para interactuar con un usuario a través de la consola de R en modo de pregunta y respuesta.

```{r, fig.align = "center", out.height = '300%', echo=F}
knitr::include_graphics(here::here("images", "source_button.png"))
```


### `render()` {.unnumbered}  

`render()` es una variación de `source()` que se utiliza más a menudo para los scripts de R markdown. Tu pescribes `input = ` que es el archivo R markdown, y también `output_format = ` ("html_document", "pdf_document", "word_document", "")

Mira la página sobre [Informes con R Markdown](#reports-with-r-markdown) para más detalles. También consulta la documentación de `render()` [aquí](https://rmarkdown.rstudio.com/docs/reference/render.html) o escribiendo `?render`.



### Ejecutar archivos en un directorio {.unnumbered}

Puedes crear un bucle *for* y utilizarlo para `source()` cada archivo en un directorio, identificado con `dir()`. 

```{r, eval=F}
for(script in dir(here("scripts"), pattern = ".R$")) {   # for each script name in the R Project's "scripts" folder (with .R extension)
  source(here("scripts", script))                        # source the file with the matching name that exists in the scripts folder
}
```

Si sólo quieres ejecutar determinados scripts, puedes identificarlos por su nombre de la siguiente manera: 

```{r, eval=F}

scripts_to_run <- c(
     "epicurves.R",
     "demographic_tables.R",
     "survival_curves.R"
)

for(script in scripts_to_run) {
  source(here("scripts", script))
}

```



Aquí puedes ver una [comparación](https://cran.r-project.org/web/packages/fs/vignettes/function-comparisons.html) de las funciones **fs** y R **base**.

### Importar archivos en un directorio {.unnumbered}

Consulta la página sobre [importación y exportación](#import-and-export) para importar y exportar archivos individuales.

Consulta también la página de [importación y exportación](#import-and-export) para conocer los métodos para importar automáticamente el archivo más reciente, basándose en una fecha del nombre del archivo *o* mirando los metadatos del mismo.

Consulta la página sobre [Iteración, bucles y listas](#iteration-loops-and-lists) para ver un ejemplo con el paquete **purrr** demostrando:

* Dividir un dataframe y guardarlo como múltiples archivos CSV
* Dividir un dataframe y guardar cada parte como una hoja separada dentro de un libro de Excel
* Importar varios archivos CSV y combinarlos en un dataframe
* Importar un libro de Excel con varias hojas y combinarlas en un dataframe




## R **base**  {#base-r-4}

Mira a continuación las funciones `list.files()` y `dir()`, que realizan la misma operación de listar archivos dentro de un directorio especificado. Puedes especificar `ignore.case = ` o un patrón específico para buscar.

```{r, eval=F}
list.files(path = here("data"))

list.files(path = here("data"), pattern = ".csv")
# dir(path = here("data"), pattern = ".csv")

list.files(path = here("data"), pattern = "evd", ignore.case = TRUE)

```

Si un archivo está actualmente "abierto", se mostrará en su carpeta con una tilde delante, como "\~$hospital_linelists.xlsx".  


<!-- ======================================================= -->
## Recursos {#resources-37}

https://cran.r-project.org/web/packages/fs/vignettes/function-comparisons.html



```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/directories.Rmd-->

# Control de versiones y colaboración con Git y Github {#version-control-and-collaboration-with-git-and-github}

Este capítulo presenta una visión general del uso de Git para colaborar con otros. Puedes encontrar tutoriales más extensos al final, en la sección de Recursos.

## ¿Qué es Git? {#what-is-git}

Git es un software de **control de versiones** que permite seguir los cambios realizados en una carpeta. Se puede utilizar como la opción "control de cambios" de Word, LibreOffice o Google docs, pero para todo tipo de archivos. Es una de las opciones más potentes y más utilizadas para el control de versiones.

**¿Por qué nunca he oído hablar de Git? -** Mientras que las personas con formación como desarrollador aprenden habitualmente a utilizar un software de control de versiones (Git, Mercurial, Subversion u otros), a pocas personas de las disciplinas cuantitativas se nos enseñan estas habilidades. En consecuencia, la mayoría de profesionales de la epidemiología nunca hemos oído hablar sobre esto en sus estudios, y tenemos que aprenderlo sobre la marcha.

**Espera, he oído hablar de Github, ¿es lo mismo?** - No exactamente, pero a menudo se utilizan juntos, y veremos aquí cómo hacerlo. En resumen:

-   **Git** es el sistema de control de versiones, una pieza de software. Se puede utilizar localmente en el ordenador o para sincronizar una carpeta con un **sitio web** anfitrión. Por defecto, se utiliza una ventana de terminal para escribir las instrucciones de Git en la línea de comandos.

-   Se puede utilizar un **cliente/interfaz Git** para evitar la línea de comandos y realizar las mismas acciones (al menos para las más sencillas y supercomunes).

-   Si se quiere almacenar una carpeta en un **sitio web** para colaborar con otros, se puede utilizar una cuenta en Github, Gitlab, Bitbucket u otros.

Se puede utilizar el cliente/interfaz **Github Desktop**, que utiliza **Git** en segundo plano para gestionar los archivos, tanto localmente en el ordenador, como remotamente en un servidor de **Github**.

## ¿Por qué utilizar la combinación de Git y Github? {#why-use-the-combo-git-and-github}

El uso de **Git** facilita:

1)  Almacenar versiones de archivos con cambios incrementales de forma que permite volver fácilmente a cualquier estado anterior
2)  Mantener *ramas* paralelas, es decir, versiones de desarrollo/"trabajo" que más adelante pueden integrar los cambios después de su revisión

Esto también se puede hacer localmente en tu ordenador, incluso si no colaboras con otras personas. Alguna vez ....:

-   ¿te has arrepentido de haber eliminado una sección de código, para darte cuenta dos meses después de que realmente la necesitabas?

-   ¿has vuelto a un proyecto que había estado en pausa e intentado recordar si habías hecho esa complicada modificación en uno de los modelos?

-   ¿tenías un archivo *modelo_1.R* y otro archivo *modelo_1_prueba.R* y un archivo *modelo_1_no_funciona.R* para probar las cosas?

-   ¿tenías un archivo report.*Rmd*, un archivo *report_full.Rmd*, un archivo *report_true_final.Rmd,* un archivo *report_final_20210304.Rmd*, un archivo *report_final_20210402.Rmd* y maldecías tus habilidades de almacenamiento?

Git puede ayudar con todo eso, y vale la pena aprenderlo sólo por eso.


Sin embargo, se vuelve aún más potente cuando se utiliza con un repositorio en línea como Github para apoyar **proyectos de colaboración**. Esto facilita:

-   Colaboración: otros pueden revisar, comentar y aceptar o rechazar los cambios

-   Compartir el código, los datos y los resultados, e invitar a hacer comentarios al público (o en privado, con tu equipo)

y evitar:

-   "Uy, me olvidé de enviar la última versión y ahora tienes que rehacer el trabajo de dos días en este nuevo archivo"

-   Mina, Henry y Oumar trabajaron al mismo tiempo en un script y necesitan fusionar manualmente sus cambios

-   Dos personas intentan modificar el mismo archivo en Dropbox y Sharepoint y esto crea un error de sincronización.

### Esto suena complicado, yo no soy un programador {-}

Puede ser. Los ejemplos de usos avanzados pueden ser bastante aterradores. Sin embargo, al igual que ocurre con R, o incluso con Excel, no es necesario convertirse en un experto para aprovechar las ventajas de la herramienta. El aprendizaje de un *pequeño número de funciones y nociones te permite* seguir sus cambios, sincronizar los archivos en un repositorio en línea y colaborar con los colegas en muy poco tiempo.

Debido a la curva de aprendizaje, el contexto de emergencia puede no ser el mejor momento para aprender estas herramientas. Pero el aprendizaje puede hacerse por pasos. Una vez que adquieras un par de nociones, tu flujo de trabajo puede ser bastante eficiente y rápido. Si no estás trabajando en un proyecto en el que la colaboración con personas a través de Git sea una necesidad, ... **en realidad es un buen momento para adquirir confianza en su uso** en solitario antes de sumergirte en ello en un proyecto colaborativo.

## Configuración {#setup}

### Instalar Git {.unnumbered}

*Git* es el motor que está de este control de cambios la computadora; rastrea los cambios, las ramas (versiones), las fusiones y las reversiones. **Primero debes instalar *Git* desde https://git-scm.com/downloads.**

### Instalar una interfaz gráfica (opcional pero recomendable) {.unnumbered}

Git tiene su propio lenguaje de comandos, que se pueden escribir en la línea de comandos de un terminal. Sin embargo, hay muchos clientes/interfaces que proporcionan una buena visualización de las modificaciones de archivos o ramas. Esto es recomendable ya que personas que no son desarrolladoras, en su uso diario, rara vez *necesitarán* interactuar directamente con Git.

Existen muchas opciones, en todos los sistemas operativos, desde las amigables para los principiantes hasta las más complejas. Unas buenas opciones para principiantes son el panel Git de RStudio y [Github Desktop](https://desktop.github.com/), que mostraremos en este capítulo. Las opciones intermedias (más potentes, pero más complejas) incluyen Source Tree, Gitkracken, Smart Git y otras.

Explicación rápida sobre [los clientes Git](https:/happygitwithr.com/git-client.html#git-client).

*Nota: dado que todas las interfaces utilizan Git internamente, puedes probar varias de ellas, cambiar de una a otra en un proyecto determinado, utilizar la consola puntualmente para una acción que tu interfaz no soporta, o incluso realizar una serie de acciones online en Github.*

Como se indica más adelante, es posible que ocasionalmente tengas que escribir comandos Git en un terminal como en la pestaña "terminal" de RStudio (una pestaña adyacente a la consola de R) o la aplicación de terminal Git Bash.


### Cuenta de Github {.unnumbered}

Regístrate para obtener una cuenta gratuita en [github.com](github.com).

Es posible que se te ofrezca configurar la autenticación de dos pasos con una aplicación en tu teléfono. Lee más en [estos documentos de ayuda](https://docs.github.com/es/github/authenticating-to-github/securing-your-account-with-two-factor-authentication-2fa) de Github.

Si usas Github Desktop, puedes introducir tus credenciales de Github después de la instalación siguiendo estos [pasos](https://docs.github.com/es/desktop/installing-and-configuring-github-desktop/authenticating-to-github). Si no lo haces, las credenciales se te pedirán más tarde cuando intentes clonar un proyecto desde Github.

## Vocabulario, conceptos y funciones básicas {#vocabulary-concepts-and-basic-functions}

Al igual que cuando se aprende R, hay que recordar un poco de vocabulario para entender Git. Aquí están los [conceptos básicos para empezar](https://www.freecodecamp.org/news/an-introduction-to-git-for-absolute-beginners-86fa1d32ff71/) / [tutorial interactivo](https:/learngitbranching.js.org). En las próximas secciones, mostraremos cómo usar las interfaces, pero es bueno tener el vocabulario y los conceptos en mente, para construir tu modelo mental, ya que lo necesitarás cuando más tarde, aunque uses las interfaces de los programas.

### Repositorio {.unnumbered}

Un *repositorio* Git ("*repo"*) es una carpeta que contiene todas las subcarpetas y archivos de tu proyecto (datos, código, imágenes, etc.) y sus historiales de revisión. Cuando empieces a seguir los cambios en el repositorio con él, Git creará una carpeta oculta que contiene toda la información de seguimiento. Un repositorio típico de Git es la carpeta de tu proyecto R (ver la página del manual sobre [proyectos R](#r-projects)).

Mostraremos cómo crear (*inicializar)* un repositorio Git desde Github, Github Desktop o Rstudio en las siguientes secciones.
sections.

### Commits (**Consolidaciones**) {.unnumbered}

Cuando realices un cambio en el proyecto, hay que ejecutar commit para consolidar estos cambios (el delta) realizados en tus archivos. Por ejemplo, quizás hayas editado algunas líneas de código y actualizado unos datos relacionados. Una vez guardados los cambios, puedes agrupar y confirmar estos cambios en un solo "commit".

Cada consolidación (commit) tiene un ID único (un *hash*). Para el control de versiones, puedes revertir tu proyecto hacia atrás en base a estas Consolidaciones, así que es mejor mantenerlas relativamente pequeñas y coherentes. También realizarás una breve descripción de los cambios llamada "commit message (mensaje de consolidación)". En cierto modo, cada *commit* es una **instantánea** del proyecto en un momento dado.

*¿Cambios por etapas (staged)*? Poner etapas en los cambios es añadirlos a la *zona de* preparación para la siguiente consolidación. La idea es que puedas decidir con precisión qué cambios incluir en un determinado commit. Por ejemplo, si trabajas en la especificación del modelo en un script, y más tarde en una figura en otro script, tendría sentido tener dos commits diferentes (sería más fácil en caso de que quisieras revertir los cambios en la figura pero no en el modelo).


### Ramas (Branches) {.unnumbered}

Una rama representa una *línea independiente* de cambios en su repo, una versión paralela y alternativa de los archivos del proyecto.

Las ramas son útiles para probar los cambios antes de incorporarlos a la rama *principal (main, master),* que suele ser la versión primaria/final/"viva" de tu proyecto. Cuando termines de experimentar en una rama, puedes incorporar los cambios a tu rama *principal*, *fusionándola*, o eliminarla, si los cambios no fueron tan exitosos.

*Nota: no es necesario colaborar con otras personas para utilizar las ramas, ni es necesario tener un repositorio remoto en línea.*


### Repositorios locales y remotos {.unnumbered}

-  el repositorio *LOCAL* en el ordenador físico. Aquí es donde se hacen los cambios reales a los archivos/código.

-  el repositorio *REMOTO*, en línea: las versiones de los archivos del proyecto en el repositorio Github (o en cualquier otro alojamiento web).

Para sincronizar estos repositorios, utilizaremos más funciones. En efecto, a diferencia de Sharepoint, Dropbox u otro software de sincronización, Git no actualiza automáticamente el repositorio local en base a lo que está en línea, o viceversa. Tú eliges cuándo y cómo sincronizarlo.

-   `FETCH`: `git fetch` descarga los cambios realizados en el repositorio remoto pero no cambia el repositorio local. Piensa en ello para una comprobación del estado del repositorio remoto.

-   `PULL`:`git pull` descarga archivos cambiados en los repositorios remotos y actualiza el repositorio local.

-   `PUSH`: Actualiza el repositorio remoto. Cuando hayas hecho uno o varios commits localmente, puedes hacer `git push` de los commits al repositorio remoto. Esto envía tus cambios a Github para actualizar el repositorio y que otras personas puedan verlos y extraerlos si lo desean.


## Empezar: crear un nuevo repositorio {#get-started-create-a-new-repository}

Hay muchas formas de crear nuevos repositorios. Puedes hacerlo desde la consola/terminal, desde Github, desde una interfaz gráfica, como Github Desktop o Rstudio-\>Git.

Hay dos enfoques generales para la puesta en marcha:

-   Crear un nuevo proyecto R a partir de un repositorio de Github existente o nuevo (*preferible para los principiantes*), o
-   Crear un repositorio Github para un proyecto R existente


### Archivos de inicio {.unnumbered}

When you create a new repository, you can optionally create 
all of the below files, or you can add them to your repository at a later stage.
They would typically live in the "root" folder of the repository.

-   Un archivo *README* es un archivo que alguien puede leer para entender por qué existe tu proyecto y qué más deben saber para usarlo. Al principio estará vacío, pero deberías completarlo más adelante.

-   Un archivo *.gitignore* es un archivo de texto donde cada línea contendría carpetas o archivos que Git debería ignorar (no rastrear los cambios). Lee más sobre esto y mira ejemplos [aquí](https://www.freecodecamp.org/news/gitignore-what-is-it-and-how-to-add-to-repo/).

-   Puedes elegir un tipo de *licencia* para el trabajo, de modo que otras personas sepan en qué condiciones pueden utilizar o reproducir tu obra. Para más información, consulta las [licencias Creative Commons](https://creativecommons.org/licenses/).

### Crear un nuevo repositorio en Github {.unnumbered}

Para crear un nuevo repositorio, entra en Github y busca el botón verde para crear un nuevo repositorio. Este repositorio, ahora vacío, puede ser clonado localmente en tu ordenador (ver la siguiente sección).

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_new.png"))
```

Debes elegir si quieres que tu repositorio sea **público** (visible para todo el mundo en Internet) o **privado** (sólo visible para aquellos con permiso). Esto tiene importantes implicaciones si tus datos son sensibles. Si tu repositorio es privado te encontrarás con algunos límites en circunstancias especiales avanzadas, como por ejemplo si estás usando *actions de* Github para ejecutar automáticamente tu código en la nube.
 
### Clonar desde un repositorio de Github {.unnumbered}

Puedes *clonar* un repositorio de Github existente para crear un nuevo proyecto R local en tu ordenador.

El repositorio de Github puede ser uno que ya existe y tiene contenido, o puede ser un repositorio vacío que acabas de crear. En este último caso, básicamente estás creando el repositorio de Github y el proyecto local de R al mismo tiempo (ver las instrucciones anteriores).

_Nota_: si no tienes derechos de contribución en un repositorio de Github, es posible primero _bifurcar (fork)_ el repositorio hacia tu perfil, y luego proceder con las otras acciones. La bifurcación se explica al final de este capítulo, pero recomendamos que leas primero las otras secciones.

Paso 1: Navega en Github hasta el repositorio, clica en el botón verde "**Code**" y copia la **HTTPS clon URL** (ver imagen inferior)

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_clone.png"))
```

El siguiente paso se puede realizar en cualquier interfaz. Lo ilustraremos con Rstudio y Github desktop.

#### En Rstudio {.unnumbered}

En RStudio, inicia un nuevo proyecto R clicando en *File\>New project \> Version control \> Git)* (Archivo \> Nuevo proyecto \> Control de versiones \> Git)

-   Cuando te pida la "URL del repositorio", pega la URL HTTPS de Github

-   Asigna al proyecto R un nombre corto e informativo

-   Elige dónde se guardará el nuevo proyecto R localmente

-   Marca "Abrir en una nueva sesión" y clica en "Crear proyecto".

Ahora estás en un nuevo proyecto local de RStudio que es un clon del repositorio de Github. Este proyecto local y el repositorio de Github están ahora vinculados.

#### En Github Desktop {.unnumbered}

-   Clica en *File\>Clone repository* (Archivo \> Clonar un repositorio)

-   Selecciona la pestaña URL

-   Pega la URL HTTPS de Github en la primera casilla

-   Selecciona la carpeta en la que deseas tener tu repositorio local

-   Clica en "CLONE"

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_clone_desktop.png"))
```

### Nuevo repositorio de Github a partir de un proyecto R existente {.unnumbered}

Un escenario alternativo de configuración es que ya tengas un proyecto R con contenido, y quieras crear un repositorio Github para él.

1)  Crear un nuevo repositorio de Github vacío para el proyecto (ver instrucciones anteriores)

2)  Clona este repositorio localmente (ver las instrucciones de HTTPS más arriba)

3)  Copia todo el contenido de tu proyecto R preexistente (códigos, datos, etc.) en este nuevo repositorio local vacío (por ejemplo, utiliza copiar y pegar).

4)  Abre tu nuevo proyecto en RStudio, y ve al panel Git. Los nuevos archivos deberían registrarse como cambios de archivo, ahora rastreados por Git. Por lo tanto, puedes agrupar estos cambios bajo un *commit* y *push* a Github. Una vez hecho *push*, el repositorio en Github reflejará todos los archivos.

Consulta la sección de flujo de trabajo de Github para obtener más detalles sobre este proceso.

### ¿Qué aspecto tiene ahora? {.unnumbered}

#### En RStudio {-}

Una vez que hayas clonado un repositorio de Github a un nuevo proyecto R, ahora verás en RStudio una pestaña "Git". Esta pestaña aparece en el mismo panel de RStudio que Environment:

```{r echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "Git_console.png"))
```

-   Botón *commit* para consolidar los cambios del archivo guardado en local (se abrirá una nueva ventana para añadir la descripción y confirmarlo)

-   Flecha azul *pull* (descarga los cambios realizados en la versión remota/Github de esa rama y actualiza tu versión local de la rama)

-   Flecha verde *push* (enviar cualquier commits/cambio de tu versión local de la rama y actualiza la versión remota/Github de esa rama)

-   La pestaña Git en RStudio

-   Botón para crear una rama NUEVA dependiente de la rama que se muestra a la derecha como base. *Casi siempre querrá bifurcarse desde la rama principal (después de haber tirado primero para actualizar la rama principal)*

-   La Rama en la que trabajas actualmente

-   A continuación aparecerán los cambios que haya realizado en el código o en otros archivos

#### En Github Desktop {.unnumbered}

Github Desktop es una aplicación independiente que te permite gestionar todos tus repositorios. Cuando la abres, la interfaz te permite elegir el repositorio en el que quieres trabajar, y luego realizar acciones básicas de Git desde allí.

```{r echo=F, out.width = "75%", out.height="75%", fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_interface.png"))
```


## Flujo de trabajo Git + Github {#git-github-workflow}

### Resumen del proceso {.unnumbered}

Una vez que hayas completado la configuración (descrita anteriormente), tendrás un repo de Github que está conectado (*clonado*) a un proyecto local de R. La rama principal (*main* por defecto) es la llamada versión "viva" de *todos* los archivos. Cuando quieras hacer modificaciones, es una buena práctica crear una *nueva rama* a partir de la rama *principal* (como "Hacer una copia"). Este es un flujo de trabajo típico en Git porque crear una rama es fácil y rápido.

Un flujo de trabajo típico es el siguiente:

1.  Asegúrate de que tu repositorio local está actualizado, actualízalo si no es así

2.  Ve a la rama en la que estabas trabajando anteriormente, o crea una nueva rama para probar algunas cosas

3.  Trabaja en los archivos localmente en tu ordenador, haz uno o varios commits en esta rama

4.  Actualiza la versión remota de la rama con tus cambios (push)

5.  Cuando estés satisfecho con tu rama, puedes fusionar la versión en línea de la rama de trabajo con la rama "principal" en línea para transferir los cambios

Otros miembros del equipo pueden estar haciendo lo mismo con sus propias ramas, o quizás contribuyendo con commits en su rama de trabajo también.

A continuación, repasamos el proceso anterior paso a paso con más detalle. Es un esquema que hemos desarrollado - está en el formato de una tabla de dos x dos, por lo que debería ayudarnos a entenderlo.

```{r echo=F, out.height='150%', out.width='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_table.png"))
```

Aquí hay [otro diagrama](https://build5nines.com/introduction-to-git-version-control-workflow/).

*Nota: hasta hace poco, se utilizaba el término rama "master" (maestra), pero ahora se denomina rama "main" (principal).*

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "GitHub-Flow.png"))
```

[Fuente](https://build5nines.com/introduction-to-git-version-control-workflow/) de la imagen

## Crear una nueva rama {#create-a-new-branch}

Cuando seleccionas una rama para trabajar, **Git restablece tu directorio de trabajo tal y como estaba la última vez que estuviste en esta rama**.

### En el panel Git de Rstudio {.unnumbered}

Asegúrate que te encuentras en la rama "main" (master, principal) y, a continuación, clica en el icono morado para crear una nueva rama (véase la imagen anterior).

-   Pedirá un nombre descriptivo para esa rama, de una palabra (se pueden usar barras bajas si es necesario).
-   Verás que localmente, sigues en el mismo proyecto R, pero ya no estás trabajando en la rama "main "(principal).
-   Una vez creada, la nueva rama también aparecerá en el sitio web de Github como una rama.

Puedes visualizar las ramas en el panel Git de Rstudio tras clicar en "History"

```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_rstudio_branchs.png"))
```


### En Github Desktop {-}

El proceso es muy similar, se pide que des un nombre a tu rama. Después, pedirá que "publique su rama en Github" para que la nueva rama aparezca también en el repositorio remoto.


```{r echo=F, out.width = '100%', out.height='100%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_new_branch.png"))
```

### En la consola {.unnumbered}

Lo que realmente ocurre entre bastidores es que creas una nueva rama con `git branch`, y luego vas a la rama con `git checkout` (_es decir_, le dices a Git que tus próximos commits se producirán allí). Desde tu repositorio git:

```{bash, eval = FALSE}
git branch my-new-branch  # Crea la nueva rama my-new-branch
git checkout my-new-branch # Va a la rama
git checkout -b my-new-branch # Ambos a la vez (satajo)
```


Para más información sobre el uso de la consola, consulta la sección sobre comandos Git al final.

## Consolidar los cambios (Commit) {#commit-changes}

Ahora puedes editar el código, añadir nuevos archivos, actualizar conjuntos de datos, etc.

Cada uno de tus cambios es rastreado, *una vez que el archivo respectivo es guardado*. Los archivos modificados aparecerán en la pestaña Git de RStudio, en Github Desktop, o utilizando el comando `git status` en el terminal (ver más abajo).

Siempre que hagas cambios sustanciales (por ejemplo, añadir o actualizar una sección de código), haz una pausa y *consolida* esos cambios (Commit). Piensa en una Consolidación como un "lote" de cambios relacionados con un propósito común. Siempre puedes seguir revisando un archivo después de haber confirmado los cambios en él.

*Consejo sobre los commits*: en general, es mejor hacer Consolidaciones pequeñas, que puedan revertirse fácilmente si surge un problema, y Consolidar juntas modificaciones relacionadas con un propósito común. Para lograr esto, verás que *debes hacer commits a menudo*. Al principio, es probable que te olvides de hacer commits a menudo, pero luego el hábito se impone.

### En Rstudio {.unnumbered}

El ejemplo siguiente muestra que, desde la última consolidación, el script de R Markdown "collaboration.Rmd" ha cambiado, y se han añadido varias imágenes PNG.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_tracking2.png"))
```

Puede que te preguntes qué representan los cuadrados amarillo, azul, verde y rojo que aparecen junto a los nombres de los archivos. Aquí hay una captura de la [hoja de trucos de RStudio](https://www.rstudio.com/wp-content/uploads/2016/01/rstudio-IDE-cheatsheet.pdf) que explica su significado. Ten en cuenta que los cambios con el amarillo "?" aún pueden ser puestos en escena, confirmados y enviados.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_tracking.png"))
```

-   Clica el botón "Commit" en la pestaña Git, que abrirá una nueva ventana (mostrada a continuación)

-   Clica en un nombre de archivo en el cuadro superior izquierdo

-   Revisa los cambios que ha realizado en ese archivo (resaltados en verde o rojo)

-   "Stage" (Poner en etapas) el archivo , lo que incluirá esos cambios en la consolidación. Para ello, marca la casilla situada junto al nombre del archivo. También puedes marcar varios nombres de archivo y clicar en "Stage".

-   Escribe un mensaje de consolidación breve pero descriptivo (obligatorio)

-   Clica el botón "Commit". Aparecerá un cuadro emergente mostrando el éxito o un mensaje de error.

Ahora puedes hacer más cambios y más commits, tantas veces como quieras

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_commit.png"))
```

### En Github Desktop {.unnumbered}

Puedes ver la lista de los archivos que se han modificado a la izquierda. Si seleccionas un archivo de texto, verás en el panel derecho un resumen de las modificaciones que se han hecho (la vista no funcionará en archivos más complejos como .docs o .xlsx).

Para añadir los cambios, basta con marcar la pequeña casilla situada junto a los nombres de los archivos. Cuando hayas seleccionado los archivos que quieres añadir a esta consolidación, dale un nombre a la consolidación, opcionalmente una descripción y luego clica en el botón de **commit**.
button.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_commit.png"))
```

### En la consola {.unnumbered}

Las dos funciones que se utilizan entre bastidores son `git add` para seleccionar/poner en escena los archivos y `git commit` para hacer realmente el commit.

```{bash, eval = FALSE}
git status # see the changes 

git add new_pages/collaboration.Rmd  # selecciona los ficheros a (= stage los cambioss)

git commit -m "Describe commit from Github Desktop" # commit los cambios con un mensaje

git log  # ver información sobre los commits anteriores
```


### Modificar una consolidación anterior {.unnumbered}

¿Qué sucede si confirmas algunos cambios, sigues trabajando y te das cuenta de que hiciste cambios que deberían "pertenecer" a la consolidación anterior (en tu opinión)? No temas! Puedes añadir estos cambios a tu consolidación anterior.

En Rstudio, debería ser bastante obvio, ya que hay una casilla "Amend previous commit" (modificar una consolidación anterior) en la misma línea que el botón COMMIT.

Por alguna razón poco clara, la funcionalidad no se ha implementado como tal en Github Desktop, pero hay una forma (conceptualmente incómoda pero fácil) de hacerlo. Si has confirmado **pero** aún **no has enviado** tus cambios, aparece un botón "UNDO" justo debajo del botón COMMIT. Clica en él y revertirá tu consolidación (pero mantendrá sus archivos en etapa y tu mensaje de consolidación). Guarda los cambios, añade nuevos archivos a la consolidación si es necesario y vuelva a confirmar.

En la consola:

```{bash, eval = FALSE}
git add [YOUR FILES] # Stage your new changes

git commit --amend  # Amend the previous commit

git commit --amend -m "An updated commit message"  # Amend the previous commit AND update the commit message
```


_Note: think before modifying commits that are already public and shared with your collaborators_.


## Actualizar los cambios con Github {#pull-and-push-changes-up-to-github}

"Primero PULL (actualizar local), luego PUSH (actualizar reomto)"

Es una buena práctica *fetch* y *pull* antes de empezar a trabajar en tu proyecto, para actualizar la versión de la rama en tu equipo local con los cambios que se han hecho en la versión remota/Github.

Pull a menudo. No dudes. *Pull siempre antes de Push*.

Cuando los cambios estén hechos y confirmados y estés contento con el estado de tu proyecto, puedes *enviar* (push) tus consolidaciones a la versión remota/Github de tu rama.

Repite la operación mientras trabajas en el repositorio.

**Nota:** es mucho más fácil revertir los cambios que fueron confirmados pero no empujados (es decir, siguen siendo locales) que revertir los cambios que fueron empujados al repositorio remoto (y tal vez ya sacados por otra persona), por lo que es mejor empujar cuando haya terminado de introducir cambios en la tarea en la que estaba trabajando.


#### En Rstudio {.unnumbered}

*PULL* - En primer lugar, clica en el icono "Pull" (flecha hacia abajo) que busca y tira al mismo tiempo.

*PUSH --* Clicando en el icono verde "Push" (flecha hacia arriba). Es posible que pida que introduzcas tu nombre de usuario y contraseña de Github. La primera vez que la pida, es posible que tenga que introducir dos líneas de comando Git en el *Terminal*:

-   **git config --global user.email
    "[you\@example.com](mailto:you@example.com){.email}"** (your Github
    email address), and\
-   **git config --global user.name "Your Github username"**

Para saber más sobre cómo introducir estos comandos, consulta la sección siguiente sobre comandos Git.

***SUGERENCIA:*** ¿Te piden la contraseña muy a menudo? Consulta los capítulos 10 y 11 de este [tutorial](https://happygitwithr.com/credential-caching.html#credential-caching) para conectarse a un repositorio usando una clave SSH (más complicado)


#### En Github Desktop {.unnumbered}

Clica en el botón "Fetch origin" para comprobar si hay nuevos commits en el repositorio remoto.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_fetch_button.png"))
```

Si Git encuentra nuevos commits en el repositorio remoto, el botón cambiará a un botón "Pull". Dado que el mismo botón se utiliza para Pull y Push, no puedes enviar tus cambios si no descargas y actualizas antes.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_pull_button.png"))
```

Puedes ir a la pestaña "History" (cerca de la pestaña "Changes") para ver todos los commits (los tuyos y los de los demás). Esta es una buena manera de conocer lo que hicieron tus colaboradores. Puedes leer el mensaje de consolidación, la descripción si la hay, y comparar el código de los dos archivos usando el panel *diff*.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_history.png"))
```

Una vez que se han extraído todos los cambios remotos y se ha consignado al menos un cambio local, se puede empujar clicando en el mismo botón.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_push_button.png"))
```

#### Consola {.unnumbered}

Sin sorpresas, las órdenes son *fetch*, *pull* y *push*.

```{bash, eval = FALSE}
git fetch  # ¿hay nuevos commits en el directorio remoto?
git pull   # Trae los commits remotos a tu rama local y la actualiza
git push   # Envía los commits locales de esta rama a la rama remota
```


### Quiero actualizarme pero tengo trabajo local {.unnumbered}

Esto puede ocurrir a veces: has hecho algunos cambios en tu repositorio local, pero el repositorio remoto tiene consolidaciones que no has descargado.


Git rechazará hacer un pull porque podría sobrescribir tus cambios. Hay varias estrategias para guardar tus cambios, bien descritas en [Happy Git with R](https://happygitwithr.com/pull-tricky.html), entre las cuales las dos principales son:
- Confirmar tus cambios, obtener los cambios remotos, extraerlos, resolver los conflictos si es necesario (ver la sección más abajo), y consolidar todo en línea 
- stash tus cambios, lo que en cierto modo los guarda a un lado, pull, unstash (restaurar), y luego confirmar, resolver cualquier conflicto, y push.

Si los archivos afectados por los cambios remotos y los archivos afectados por tus cambios locales no se solapan, Git puede resolver los conflictos automáticamente.

En Github Desktop, esto se puede hacer con botones. Para almacenar, ve a *Branch \> Stash all changes.*

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_stash.png"))
```


## Combinar la rama con la principal {#merge-branch-into-main}

Si has terminado de hacer cambios, puedes comenzar el proceso de fusión de esos cambios en la rama principal. Dependiendo de su situación, esto puede ser rápido, o puede tener pasos deliberados de revisión y aprobación que involucren a compañeros de equipo.

### Localmente en Github Desktop {.unnumbered}

Se pueden fusionar ramas localmente usando Github Desktop. Primero, ve a (checkout) la rama que será la destinataria de los commits, es decir, la rama que quieres actualizar. A continuación, clica en el menú *Branch \> Merge into current branch*. Un cuadro te permitirá seleccionar la rama desde la que quieres importar.

```{r echo=F, fig.align = "center"}
knitr::include_graphics(here::here("images", "github_desktop_merge.png"))
```

### En la consola {.unnumbered}
Primero, vuelve a la rama que será la destinataria de los cambios. Normalmente es *la rama maestra* (main), pero puede ser otra rama. Luego fusiona tu rama de trabajo con la maestra.

```{bash, eval = FALSE}
git checkout master  # Go back to master (or to the branch you want to move your )
git merge this_fancy_new_branch
```

[Esta página](https://git-scm.com/book/en/v2/Git-Branching-Basic-Branching-and-Merging) muestra un ejemplo más avanzado de bifurcación y explica un poco lo que ocurre entre bastidores.

### En Github: envío de pull requests {.unnumbered}


Aunque es totalmente posible fusionar dos ramas localmente, o sin informar a nadie, una fusión puede ser discutida o investigada por varias personas antes de ser integrada en la rama maestra. Para ayudar en el proceso, Github ofrece algunas funciones de discusión en torno a la fusión: el **pull request**.

Un pull request (un "PR") es una solicitud para fusionar una rama con otra (en otras palabras, una solicitud para que *tu rama de trabajo se incorpore a la rama "principal"*). Una solicitud de extracción suele incluir varias consolidaciones. Un pull request suele iniciar un proceso de conversación y revisión antes de que sea aceptado y la rama sea fusionada. Por ejemplo, puedes leer las discusiones sobre pull requests en [el github de dplyr](https://github.com/tidyverse/dplyr/pulls).


Puedes enviar una solicitud de extracción (PR) directamente desde el sitio web (como se ilustra a continuación) o desde Github Desktop.

-   Ir al repositorio Github (en línea)
-   Ve a la pestaña "Pull Requests" y clica en el botón "New pull request".
-   Selecciona en el menú desplegable para fusionar su rama en la principal
-   Escribe un comentario detallado sobre la solicitud de extracción y clica en "Crear solicitud de extracción".

En la imagen siguiente, se ha seleccionado la rama "forests" para fusionarla con la "principal":

```{r echo=F, out.width = '100%', out.height='150%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_pull_request2.png"))
```

Ahora se debería poder ver el pull request (imagen de ejemplo abajo):

* Revisa la pestaña "Files changed" (Archivos cambiados) para ver cómo cambiaría la rama "principal" si se fusionara la rama.\
* A la derecha, puedes solicitar una revisión a los miembros de tu equipo etiquetando su ID de Github. Si quieres, puedes configurar el repositorio para que se requiera una revisión de aprobación para poder fusionarlo con el principal.\
* Una vez aprobada la solicitud de extracción, se activará un botón para "Merge pull request" (fusionar la solicitud de extracción). Clica en él.\
* Una vez completado, elimina tu rama como se explica a continuación.

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_pull_request.png"))
```

### Resolución de conflictos {.unnumbered}

Cuando dos personas modifican la(s) misma(s) línea(s) al mismo tiempo, surge un conflicto de fusión. De hecho, Git se niega a tomar una decisión sobre qué versión mantener, pero te ayuda a encontrar dónde está el conflicto. **NO TE ASUSTES**. La mayoría de las veces, es bastante sencillo de resolver.

Por ejemplo, en Github:

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_conflict2.png"))
```


Después de que la fusión haya planteado un conflicto, abre el archivo en tu editor favorito. El conflicto se indicará con una serie de caracteres:

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_conflict3.png"))
```

El texto entre *\<\<\<\<\<\<\< HEAD* y *=======* proviene de *tu* repositorio local, y el que está entre ======= y \>\>\>\>\>\>\> *de* la *otra rama* (que puede ser origin, master o cualquier rama de tu elección).

Tienes que decidir qué versión del código prefieres (o incluso escribir una tercera, incluyendo los cambios de ambas partes si es pertinente), borrar el resto y eliminar todas las marcas que Git ha añadido *(\<\<\<\<\<\<\< HEAD, =======, \>\>\>\>\>\>\> origin/master/tu_nombre_de_rama).*

A continuación, guarda el archivo, estadíalo y haz un commit: este es el commit que hace que la versión fusionada sea "oficial". No te olvides de hacer push después.

Cuanto más a menudo hagáis pull y push tú y tus colaboradores, menores serán los conflictos.

*Nota: Si te sientes cómodo con la consola, existen [opciones avanzadas de fusión](https://git-scm.com/book/en/v2/Git-Tools-Advanced-Merging) (por ejemplo, ignorar los espacios en blanco, dar prioridad a un colaborador, etc.).*


### Borrar tu rama {.unnumbered}

Una vez que una rama se ha fusionado con la maestra y ya no es necesaria, puedes eliminarla.

#### Github + Rstudio {.unnumbered}

Ve al repositorio en Github y clica en el botón para ver todas las ramas (junto al desplegable para seleccionar ramas). Ahora busca tu rama y clica en el icono de la papelera junto a ella. Lee más detalles sobre cómo eliminar una rama [aquí](https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/creating-and-deleting-branches-within-your-repository#deleting-a-branch).

Asegúrate de eliminar también la rama localmente en tu ordenador. Esto no ocurrirá automáticamente.

-   Desde RStudio, asegúrese de estar en la rama principal
-   Cambia para escribir los comandos Git en la "Terminal" de RStudio (la pestaña adyacente a la consola de R), y escribe: **git branch -d nombre_de_rama**, donde "nombre_de_rama" es el nombre de la rama a eliminar
-   Actualiza tu pestaña Git y la rama debería desaparecer


#### En Github Desktop {.unnumbered}

Sólo tienes que comprobar la rama que quieres eliminar, e ir al menú
*Branch \> Delete*.


### Bifurcación {.unnumbered}

Puedes bifurcar (fork) un proyecto si quieres contribuir a él pero no tienes los derechos para hacerlo, o si sólo quieres modificarlo para tu uso personal. Puedes encontrar una breve descripción de la bifurcación [aquí](https://guides.github.com/activities/forking/).

En Github, clica en el botón "Fork":

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_1.png"))
```

Esto clonará el repositorio original, pero en tu propio perfil. Así que ahora hay dos versiones del repositorio **en Github**: la original, que no puedes modificar, y la versión clonada en tu perfil.

Entonces, puedes clonar tu versión del repositorio en línea localmente en tu ordenador, utilizando cualquiera de los métodos descritos en las secciones anteriores. Luego, puede crear una nueva rama, hacer cambios, confirmarlos y empujarlos _a tu repositorio remoto_.

Una vez que estés contento con el resultado, puedes crear un Pull Request desde Github o Github Desktop para iniciar la conversación con los propietarios/mantenedores del repositorio original.


**¿Y si necesitas algunos commits más recientes del repositorio oficial?**

Imagina que alguien hace una modificación crítica en el repositorio oficial, que quieres incluir en tu versión clonada. Es posible sincronizar tu fork con el repositorio oficial. Implica usar el terminal, pero no es demasiado complicado. Principalmente necesitas recordar que - _upstream_ = el repositorio oficial, el que no has podido modificar - _origin_ = tu versión del repositorio en tu perfil de Github

Puedes leer [este tutorial](https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/syncing-a-fork) o seguirlo a continuación:

Primero, escribe en tu terminal Git (dentro de tu repo):

```{bash, eval = FALSE}
git remote -v
```
 
Si aún no has configurado el repositorio upstream deberías ver dos líneas, que comienzan por _origin_. Muestran el repositorio remoto al que apuntan fetch y push. Recuerda que _origin_ es el apodo convencional para tu propia versión del repositorio en Github. Por ejemplo: 

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_2.png"))
```

Ahora, añade un nuevo repositorio remoto:

```{bash, eval = FALSE}
git remote add upstream https://github.com/epirhandbook/Epi_R_handbook.git
```
 
Aquí la dirección es la que genera Github cuando clonas un repositorio (ver sección de clonación). Ahora tendrás cuatro punteros remotos:

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_3.png"))
```

Ahora que la configuración está hecha, siempre que quieras obtener los cambios del repositorio original (*upstream)*, sólo tienes que ir (*checkout*) a la rama que quieres actualizar y teclear:

```{bash, eval = FALSE}
git fetch upstream # Obtener los nuevos commits del repositorio remoto
git checkout the_branch_you_want_to_update
git merge upstream/the_branch_you_want_to_update  # Fusiona la rama de upstream en tu rama..
git push # Actualiza tu propia versión del repositorio remoto
```

Si hay conflictos, tendrá que resolverlos, tal y como se explica en la sección Resolución de conflictos.

**Resumen**: forking es clonación, pero en el lado del servidor de Github. El resto de las acciones son las típicas del flujo de trabajo de colaboración (clonar, empujar, tirar, confirmar, fusionar, enviar solicitudes de extracción...).

_Nota: aunque la bifurcación es un concepto, no un comando de Git, también existe en otros hosts web, como [Bitbucket](https://www.atlassian.com/git/tutorials/comparing-workflows/forking-workflow)._


```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_fork_4.png"))
```


## Lo que hemos aprendido {#what-we-learned}

Has aprendido a:

- Configurar Git para rastrear las modificaciones en tus carpetas,
- Conectar tu repositorio local a un repositorio remoto en línea,
- Confirmar los cambios,
- Sincronizar tus repositorios locales y remotos. 

Todo esto debería ayudar a ponerte en marcha y ser suficiente para la mayoría de tus necesidades de análisis epidemiológico. Normalmente no tenemos un uso tan avanzado como los desarrolladores.

Sin embargo, debes saber que si quieres (o necesitas) ir más allá, Git ofrece más potencia para simplificar los historiales de commit, revertir uno o varios commits, hacer cherry-pick de commits, etc. Algunas cosas pueden parecer pura magia, pero ahora que tienes los fundamentos, es más fácil construir sobre ellos.

Ten en cuenta que mientras el panel Git en Rstudio y Github Desktop son buenos para los principiantes / uso diario en nuestra línea de trabajo, no ofrecen una interfaz para algunas de las funciones intermedias / avanzadas de Git. Algunas interfaces más completas permiten hacer más cosas con apuntar y clicar (normalmente a costa de un diseño más complejo).

Recuerda que, dado que puedes utilizar cualquier herramienta en cualquier momento para realizar el seguimiento de tu repositorio, puedes instalar muy fácilmente una interfaz para probarla a veces, o para realizar alguna tarea compleja menos común ocasionalmente, mientras prefieres una interfaz simplificada para el resto del tiempo (por ejemplo, utilizando Github Desktop la mayor parte del tiempo, y cambiando a SourceTree o Gitbash para algunas tareas específicas).


## Comandos Git {#git}


### Aprendizaje recomendado {.unnumbered}

Para aprender los comandos de Git en un tutorial interactivo, consulta [este sitio web](https://learngitbranching.js.org/).

### ¿Dónde escribir los comandos? {.unnumbered}

Se introducen comandos en un entorno Git.

*Opción 1* Puedes abrir una nueva Terminal en RStudio. Esta pestaña está al lado de la Consola R. Si no puedes escribir ningún texto en ella, clica en el menú desplegable debajo de "Terminal" y selecciona "Nueva terminal". Escribe los comandos en el espacio parpadeante delante del signo de dólar "\$".

```{r echo=F, out.width = '100%', out.height='200%', fig.align = "center"}
knitr::include_graphics(here::here("images", "github_terminal.png"))
```

*Opción 2* También puede abrir un *shell* (un terminal para introducir comandos) clicando en el icono azul de "engranajes" en la pestaña Git (cerca del entorno de RStudio). Selecciona "Shell" en el menú desplegable. Se abrirá una nueva ventana en la que puedes escribir los comandos después del signo de dólar "$".

*Opción 3* Clica con el botón derecho para abrir "Git Bash here" que abrirá el mismo tipo de terminal, o abra *Git Bash* desde tu lista de aplicaciones. [Más información para principiantes sobre Git Bash](https://happygitwithr.com/shell.html), cómo encontrarlo y algunos comandos bash que necesitarás.

### Ejemplos de comandos {.unnumbered}

A continuación presentamos algunos comandos git comunes. Cuando los uses, ten en cuenta qué rama está activa (check-out), ¡ya que eso cambiará la acción!

En los comandos de abajo, representa un nombre de rama. representa el hash ID de un commit específico. representa un número. No escriba los símbolos \< o \>.

| Comando Git              | Acción                                                                   |
|--------------------------|--------------------------------------------------------------------------|
| `git branch <name>`      | Crear una nueva rama con el nombre <name>                                 |
| `git checkout <name>`    | Cambiar la rama actual a <name>                                          |
| `git checkout -b <name>` | Atajo para crear una nueva rama *y* cambiar a ella                         |
| `git status`             | Ver los cambios no rastreados                                                   |
| `git add <file>`         | Preparar un archivo (estadiarlo)                                          |
| `git commit -m <message>`| Confirmar los cambios preparados a la rama actual con el mensaje |
| `git fetch`              | Obtener los commits del repositorio remoto.                                     |
| `git pull`               | Actualizar desde el repositorio remoto en la rama actual                  |
| `git push`               | Enviar los commits locales al directorio remoto                           |
| `git switch`             | Una alternativa a `git checkout`                  |
| `git merge <name>`       | Fusionar la rama <name> en la rama actual                         |
| `git rebase <name>`      | Añadir los commits de la rama actual a la rama <name>          |



<!-- ======================================================= -->

## Recursos {#resources-6}

Gran parte de esta página está inspirada en [this "Happy Git with R"
website](https://happygitwithr.com/) by Jenny Bryan. Hay una sección muy útil de este sitio web que te ayuda a solucionar errores comunes relacionados con Git y R.

La [documentación y guía de inicio de Github.com.](https://docs.github.com/en/github).

La hoja de trucos de RStudio ["IDE"
cheatsheet](https://https://raw.githubusercontent.com/rstudio/cheatsheets/main/rstudio-ide.pdf)
 que incluye consejos sobre Git con RStudio.

<https://ohi-science.org/news/github-going-back-in-time>

**Comandos Git para principiantes**

Un [tutorial interactivo](https://learngitbranching.js.org) para aprender los comandos de Git.

<https://www.freecodecamp.org/news/an-introduction-to-git-for-absolute-beginners-86fa1d32ff71/>:
bueno para aprender los fundamentos absolutos para rastrear los cambios en una carpeta en
en tu propio ordenador.

Buen esquema para entender las ramas:
<https://speakerdeck.com/alicebartlett/git-for-humans>


**Tutoriales que cubren temas básicos y más avanzados***

<https://tutorialzine.com/2016/06/learn-git-in-30-minutes>

<https://dzone.com/articles/git-tutorial-commands-and-operations-in-git>
<https://swcarpentry.github.io/git-novice/> (short course)
<https://rsjakob.gitbooks.io/git/content/chapter1.html>

El libro [Pro Git](https://git-scm.com/book/en/v2) está considerado como una referencia oficial. 
Aunque algunos capítulos están bien, suele ser un poco _técnico_. Probablemente es un buen recurso 
una vez que hayas usado un poco Git y quieras aprender con un poco más de precisión 
lo que sucede y cómo ir más allá.
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/collaboration.Rmd-->


# Errores comunes {#common-errors}

Esta página incluye una lista actualizada de los errores más comunes y sugiere soluciones para solucionarlos.


## Interpretación de los mensajes de error {#interpreting-error-messages}

Los mensajes de error de R pueden ser crípticos a veces, así que Google es tu amigo. Busca el mensaje de error con "R" y busca publicaciones recientes en [StackExchange.com](https://stackexchange.com/), [stackoverflow.com](https://stackoverflow.com/), [community.rstudio.com](https://community.rstudio.com/), twitter (#rstats) y otros foros utilizados por los programadores para archivar preguntas y respuestas. Intenta encontrar publicaciones recientes que hayan resuelto problemas similares.

Si después de mucho buscar no encuentras una respuesta a tu problema, considera la posibilidad de crear un ejemplo *reproducible* ("reprex") y publicar tú mismo la pregunta. Consulta la página sobre [Cómo obtener ayuda](#getting-help) para obtener consejos sobre cómo crear y publicar un ejemplo reproducible en los foros.


## Errores comunes  {#common-errors-1}

A continuación, enumeramos algunos errores comunes y posibles explicaciones/soluciones. Algunos de ellos se han tomado prestados de Noam Ross, que analizó los mensajes más comunes del foro en Stack Overflow sobre los mensajes de error de R (véase el análisis [aquí](https://github.com/noamross/zero-dependency-problems/blob/master/misc/stack-overflow-common-r-errors.md)) 


### Errores tipográficos {.unnumbered} 

```
Error: unexpected symbol in:
"  geom_histogram(stat = "identity")+
  tidyquant::geom_ma(n=7, size = 2, color = "red" lty"
```
Si aparece "unexpected symbol" (símbolo inesperado), comprueba si faltan comas


### Errores del paquete{.unnumbered}

```
could not find function "x"...
```
Esto probablemente significa que has escrito mal el nombre de la función, o que has olvidado instalar o cargar un paquete.


```
Error in select(data, var) : unused argument (var)
```
Crees que estás usando dplyr::select() pero la función select() ha sido enmascarada por MASS::select() - especifica dplyr:: o reordena la carga de tu paquete para que dplyr esté después de todos los demás.

Otros errores de enmascaramiento comunes provienen de: plyr::summarise() y stats::filter(). Considere la posibilidad de utilizar el [paquete **conflicted**](https://www.tidyverse.org/blog/2018/06/conflicted/).


```
Error in install.packages : ERROR: failed to lock directory ‘C:\Users\Name\Documents\R\win-library\4.0’ for modifying
Try removing ‘C:\Users\Name\Documents\R\win-library\4.0/00LOCK’
```

Si recibes un error diciendo que necesitas eliminar un archivo "00LOCK", ve a tu biblioteca "R" en el directorio de tu ordenador (por ejemplo, R/win-library/) y busca una carpeta llamada "00LOCK". Elimínala manualmente e intenta instalar el paquete de nuevo. Es probable que un proceso de instalación anterior se haya interrumpido, provocando este error.




### Errores en los objetos {.unnumbered}  

```
No such file or directory:
```
Si ves un error como este cuando intentas exportar o importar: Comprueba la ortografía del archivo y de la ruta de acceso, y si la ruta contiene barras inclinadas, asegúrese de que son hacia delante `/` y no hacia atrás `\`. Asegúrate también de que has utilizado la extensión de archivo correcta (por ejemplo, .csv, .xlsx).


```
object 'x' not found 
```
Esto significa que el objeto al que se hace referencia no existe. ¿Quizá el código anterior no se ha ejecutado correctamente? 


```
Error in 'x': subscript out of bounds
```
Esto significa que has intentado acceder a algo (un elemento de un vector o una lista) que no estaba allí.  




### Errores de sintaxis de las funciones {.unnumbered}

```
# ejecuta recode sin reiniciar la variable x en mutate(x = recode(x, OLD = NEW)
Error: Problem with `mutate()` input `hospital`.
x argument ".x" is missing, with no default
i Input `hospital` is `recode(...)`.
```
Este error de arriba (`argument .x is missing, with no default`) es común en `mutate()` si estás suministrando una función como `recode()` o `replace_na()` donde se espera que proporciones el nombre de la columna como primer argumento. Esto es fácil de olvidar. 



### Errores lógicos {.unnumbered}  

```
Error in if
```

Esto probablemente significa que se aplicó una sentencia `if` a algo que no era TRUE o FALSE.


### Errores de los factores {.unnumbered}  

```
#Trató de añadir un valor ("Missing") a un factor (con replace_na operando en un factor)
Problem with `mutate()` input `age_cat`.
i invalid factor level, NA generated
i Input `age_cat` is `replace_na(age_cat, "Missing")`.invalid factor level, NA generated
```
Si ves este error sobre niveles de factor no válidos, es probable que tengas una columna de tipo Factor (que contiene niveles predefinidos) y hayas intentado añadirle un nuevo valor. Conviértela al tipo Carácter antes de añadir un nuevo valor. 


### Errores de trazado {.unnumbered} 

`Error: Insufficient values in manual scale. 3 needed but only 2 provided.`
ggplot() scale_fill_manual() values = c("orange", "purple") ... insuficiente para el número de niveles del factor ... considera si NA es ahora un nivel del factor...

```
Can't add x object
```
Probablemente tienes un `+` extra al final de un comando ggplot que necesitas eliminar.


### Errores de R Markdown {.unnumbered} 

Si el mensaje de error contiene algo como `Error en options[sprintf("fig.%s", i)]]`, comprueba que tus opciones knitr en la parte superior de cada chunk utilizan correctamente ` out.width = ` o `out.height = `y *no* `fig.width= ` y `fig.height= `.

### Miscelánea {.unnumbered}

Comprueba si has reordenado los verbos **dplyr** y no has reemplazado un pipe en el medio, o no has eliminado un pipe del final después de reordenar.

 


<!-- ======================================================= -->
## Recursos {#resources-39}

Esta es otra entrada del blog que enumera los [errores comunes de programación en R a los que se enfrentan los principiantes](https://www.r-bloggers.com/2016/06/common-r-programming-errors-faced-by-beginners/) 
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/errors.Rmd-->


# Cómo obtener ayuda {#getting-help}

This page covers how to get help by posting a Github issue or by posting a reproducible example ("reprex") to an online forum.  




## Issues en Github  {#github-issues}

Muchos paquetes y proyectos de R tienen su código alojado en el sitio web Github.com. Puedes comunicarte directamente con los autores a través de este sitio web publicando un "Issue".

Lee más sobre cómo almacenar tu trabajo en Github en la página [Colaboración y Github]{#version-control-and-collaboration-with-git-and-github}.

En Github, cada proyecto está contenido en un *repositorio*. Cada repositorio contiene código, datos, resultados, documentación de ayuda, etc. También hay un vehículo para comunicarse con los autores llamado "Issues".

Mira a continuación la página de Github del paquete **incidence2** (utilizado para hacer curvas epidémicas). Puedes ver la pestaña "Issues" resaltada en amarillo. Puedes ver que hay 5 temas abiertos.

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "errors_Github_issues.png"))
```

Una vez en la pestaña de problemas, podrás ver los problemas abiertos. Revísalas para asegurarte de que tu problema no ha sido ya tratado. Puedes abrir una nueva incidencia clicando en el botón verde de la derecha. Necesitarás una cuenta de Github para hacerlo.

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "errors_Github_issues2.png"))
```


  
En tu Issue, sigue las instrucciones que aparecen a continuación para proporcionar un ejemplo mínimo y reproducible. Y, por favor, ¡se cortés! La mayoría de las personas que desarrollan paquetes y proyectos de R lo hacen en su tiempo libre (¡como este manual!).

Para leer más materiales avanzados sobre el manejo de problemas en tu propio repositorio de Github, consulta su documentación [sobre Problemas](https://guides.github.com/features/issues/).



## Ejemplo reproducible  {#reproducible-example}

Proporcionar un ejemplo reproducible ("reprex") es la clave para obtener ayuda cuando se publica en un foro o en un Issue de Github. La gente quiere ayudarte, pero tienes que darles un ejemplo con el que puedan trabajar en su propio ordenador. El ejemplo debe:

* Demostrar el problema que has encontrado
* Ser *mínimo*, en el sentido de que incluya sólo los datos y el código necesarios para reproducir el problema
* Ser *reproducible*, de manera que se incluyan todos los objetos (por ejemplo, los datos), las llamadas al paquete (por ejemplo, `library()` o `p_load()`)

*Además, ¡asegúrese de no publicar ningún dato sensible con el reprex!* Puedes crear dataframes de ejemplo, o utilizar uno de los dataframes incorporados en R (escribe `data()` para ver una lista de estos set de datos).



### El paquete **reprex** {.unnumbered}  


El paquete **reprex** puede ayudarte a crear un ejemplo reproducible:

1.  **reprex** se instala con **tidyverse**, así que carga cualquiera de los dos paquetes

```{r, eval=F}
# install/load tidyverse (which includes reprex)
pacman::p_load(tidyverse)
```

2.  Inicia un script de R que cree el problema, paso a paso, empezando por la carga de paquetes y datos.

```{r, eval=F}
# cargar paquetes
pacman::p_load(
     tidyverse,  # gestión y visualización de datos
     outbreaks)  # datos de ejemplo de brotes

#  lista de casos del brote de gripe
outbreak_raw <- outbreaks::fluH7N9_china_2013  # obtener datos del paquete de brotes

# Limpiar los datos
outbreak <- outbreak_raw %>% 
     mutate(across(contains("date"), as.Date))

# Graficar el brote

ggplot(data = outbreak)+
     geom_histogram(
          mapping = aes(x = date_of_onset),
          binwidth = 7
     )+
  scale_x_date(
    date_format = "%d %m"
  )

```
*Copia* todo el código en tu portapapeles y ejecuta el siguiente comando:

```{r, eval=F}
reprex::reprex()
```

Verás que aparece una salida HTML en el panel del visor de RStudio. Contendrá todo tu código y cualquier advertencia, error o salida de gráficos. Esta salida también se copia en el portapapeles, por lo que puedes publicarla directamente en un Issue de Github o en un mensaje del foro. 

```{r, out.width=c('100%', '100%'), warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "errors_reprex_RStudio1.png"))
```


* Si estableces `session_info = TRUE` se incluirá la salida de `sessioninfo::session_info()` con tus versiones de R y del paquete utilizado
* Puedes proporcionar un directorio de trabajo con ` wd = `
* Puedes leer más sobre los argumentos y las posibles variaciones en esta página o introduciendo ?reprex

En el ejemplo anterior, el comando `ggplot()` no se ejecutó porque el argumento `date_format = ` no es correcto - debería ser `date_labels = `.


### Datos mínimos {.unnumbered}  

Los revisores tienen que ser capaces de utilizar tus datos - idealmente tienen que ser capaces de crearlos *con código*.

Para crear unos datos mínimos, considera la posibilidad de anonimizarlos y utilizar sólo un subconjunto de las observaciones.

EN CONSTRUCCIÓN - también puede utilizar la función `dput()` para crear unos datos mínimo.




## Publicar en un foro  {#posting-to-a-forum}

Lee muchos mensajes de foros. Comprende qué mensajes están bien escritos y cuáles no.

1)  En primer lugar, decide si vas a formular la pregunta. Has revisado *a fondo* el sitio web del foro, probando con varios términos de búsqueda, para ver si tu pregunta ya ha sido formulada?

2)  Dale a tu pregunta un título informativo (no "¡Ayuda! esto no funciona").

3)  Escribe tu pregunta:

* Presenta la situación y tu problema
* Enlaza con posts de temas similares y explica cómo no responden a tu pregunta
* Incluye cualquier información relevante para ayudar a alguien que no conozca el contexto de tu trabajo
* Da un ejemplo mínimo reproducible con la información de tu sesión de R
* Utiliza la ortografía, la gramática y la puntuación adecuadas, y divide tu pregunta en párrafos para que sea más fácil de leer

4)  Supervisa tu pregunta una vez publicada para responder a cualquier solicitud de aclaración. Se cortés y amable: a menudo las personas que responden están ofreciendo su tiempo para ayudarte. Si tienes una pregunta de seguimiento, piensa si debe ser una pregunta publicada por separado.

5)  Marca la pregunta como respondida, *si* obtienes una respuesta que satisfaga la petición *original*. Esto ayuda a que otros reconozcan más tarde rápidamente la solución.

Lee estos posts sobre [cómo hacer una buena pregunta](https://stackoverflow.com/help/how-to-ask) el [código de conducta de Stack overflow](https://stackoverflow.com/conduct).


<!-- ======================================================= -->
## Recursos {#resources-40}

Página de Tidyverse sobre cómo [obtener ayuda](https://www.tidyverse.org/help/#:~:text=When%20you%20want%20to%20make,to%20load%20the%20reprex%20package.&text=Enter%20reprex()%20in%20the,preview%20of%20your%20rendered%20reprex.)

Consejos para [elaborar unos datos mínimos](https://xiangxing98.github.io/R_Learning/R_Reproducible.nb.html#producing-a-minimal-dataset)

Documentación de la [función dput](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/dput)
```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/help.Rmd-->


# R en redes locales {#r-on-network-drives}

<!-- ======================================================= -->
## Resumen {#overview-9}

El uso de R en unidades compartidas de la red o de la "empresa" puede presentar desafíos adicionales. Esta página contiene enfoques, errores comunes y sugerencias sobre la solución de problemas obtenidas a partir de nuestra experiencia trabajando con estos problemas. Se incluyen consejos para las situaciones especialmente delicadas relacionadas con R Markdown.


**Uso de R en unidades de red: Principios generales**

1) Debes tener acceso de administrador a tu ordenador. Configura RStudio específicamente para que se ejecute como administrador.  
2) Guarda los paquetes en una biblioteca en una unidad con letras (por ejemplo, "C:") cuando sea posible. Uiliza lo menos posible una biblioteca de paquetes cuya ruta comience por "\\\".  
3) El paquete **rmarkdown** debe **no** estar en una librería de paquetes "\\\", ya que entonces no puede conectarse a TinyTex o Pandoc. 





## RStudio como administrador  {#rstudio-as-administrator}

Cuando clicas en el icono de RStudio para abrirlo, hazlo clicando con el botón derecho. Dependiendo de tu máquina, puedes ver una opción para "Ejecutar como administrador". O si no,  puedes ver una opción para seleccionar Propiedades (entonces debería aparecer una ventana con la opción "Compatibilidad", y selecciona una casilla de verificación "Ejecutar como administrador").




## Comandos útiles {#useful-commands}

A continuación se presentan algunos comandos útiles cuando se trata de solucionar problemas utilizando R en unidades de red.

Puedes devolver la(s) ruta(s) a las bibliotecas de paquetes que R está utilizando. Serán listadas en el orden que R está usando para instalar/cargar/buscar paquetes. Por lo tanto, si quieres que R utilice una biblioteca diferente por defecto, puedes cambiar el orden de estas rutas (ver más abajo). 

```{r, eval=F}
# Find libraries
.libPaths()                   # Your library paths, listed in order that R installs/searches. 
                              # Note: all libraries will be listed, but to install to some (e.g. C:) you 
                              # may need to be running RStudio as an administrator (it won't appear in the 
                              # install packages library drop-down menu) 
```

Es posible que desees cambiar el orden de las bibliotecas de paquetes utilizados por R. Por ejemplo, si R está recogiendo una ubicación de la biblioteca que comienza con "\\\" y uno que comienza con una letra, por ejemplo, "D:". Puedes ajustar el orden de `.libPaths()` con el siguiente código. 

```{r, eval=F}
# Switch order of libraries
# this can effect the priority of R finding a package. E.g. you may want your C: library to be listed first
myPaths <- .libPaths() # get the paths
myPaths <- c(myPaths[2], myPaths[1]) # switch them
.libPaths(myPaths) # reassign them
```

Si tienes dificultades para que R Markdown se conecte a Pandoc, comienza con este código para averiguar dónde cree RStudio que está tu instalación de Pandoc. 

```{r, eval=F}
# Find Pandoc
Sys.getenv("RSTUDIO_PANDOC")  # Find where RStudio thinks your Pandoc installation is
```

Si quieres ver de qué biblioteca se está cargando un paquete, prueba con el siguiente código:

```{r, eval=F}
# Find a package
# gives first location of package (note order of your libraries)
find.package("rmarkdown", lib.loc = NULL, quiet = FALSE, verbose = getOption("verbose")) 
```



<!-- ======================================================= -->
## Solución de errores comunes {#troubleshooting-common-errors}


**"Fallo al compilar...tex en rmarkdown "**  

* Comprueba la instalación de TinyTex, o instala TinyTex en la ubicación C:. Consulta la página de [fundamentos de R](#r-basics) sobre cómo instalar TinyTex. 

```{r, eval=F}
# check/install tinytex, to C: location
tinytex::install_tinytex()
tinytex:::is_tinytex() # should return TRUE (note three colons)
```


**No se pueden cargar las rutinas de Internet.**  

Por ejemplo, `Error in tools::startDynamicHelp() : internet routines cannot be loaded`  

* Intenta seleccionar la versión de 32 bits de RStudio a través de Herramientas/Opciones Globales.
  * nota: si la versión de 32 bits no aparece en el menú, asegúrate que no está utilizando RStudio v1.2.
* Alternativamente, intenta desinstalar R y volver a instalarlo con una versión de bits diferente (32 en lugar de 64)


**C: la biblioteca no aparece como opción cuando intento instalar los paquetes manualmente**

* Ejecuta RStudio como administrador, entonces aparecerá esta opción.
* Para configurar RStudio para que se ejecute siempre como administrador (lo que resulta ventajoso cuando se utiliza un proyecto R en el que no se clica en el icono de RStudio para abrirlo)... clica con el botón derecho en el icono de Rstudio

La imagen siguiente muestra cómo puedes seleccionar manualmente la biblioteca en la que instalar un paquete. Esta ventana aparece cuando se abre el panel de paquetes de RStudio y se clica en "Install".

```{r, warning=F, message=F, echo=F}
knitr::include_graphics(here::here("images", "network_install.png"))
```

**Error Pandoc 1**  

Si aparece el error "pandoc error 1" al ejecutar R Markdowns scripts en unidades de red:

* De las múltiples ubicaciones de las bibliotecas, que aparezca en primer lugar la que tenga una unidad de disco con letras (véanse los códigos anteriores)
* La solución anterior funciona en una unidad de red local, si establece la conexión a Internet en la red
* Mira más consejos aquí: [https://ciser.cornell.edu/rmarkdown-knit-to-html-word-pdf/](https://ciser.cornell.edu/rmarkdown-knit-to-html-word-pdf/)

**Error Pandoc 83** 

El error será algo así: `can't find file...rmarkdown...lua...`. Esto significa que no se ha podido encontrar este archivo.

[Ver https://stackoverflow.com/questions/58830927/rmarkdown-unable-to-locate-lua-filter-when-knitting-to-word](https://stackoverflow.com/questions/58830927/rmarkdown-unable-to-locate-lua-filter-when-knitting-to-word)

Posibilidades:

1.  El paquete Rmarkdown no está instalado
2.  El paquete Rmarkdown no se encuentra
3.  Un problema de derechos de administración.

Es posible que R no sea capaz de encontrar el archivo del paquete rmarkdown, así que comprueba en qué biblioteca está el paquete **rmarkdown** (vearel código anterior). Si el paquete está instalado en una biblioteca inaccesible (por ejemplo, comienza con "\\\") considera moverlo manualmente a C: o a otra biblioteca con nombre. Ten en cuenta que el paquete **rmarkdown** tiene que ser capaz de conectarse a la instalación de TinyTex, por lo que no puede valojarse en una biblioteca en una unidad de red.


**Error Pandoc 61**

Por ejemplo: `Error: pandoc document conversion failed with error 61`  o `Could not fetch...`  

* Prueba a ejecutar RStudio como administrador (clica con el botón derecho en el icono, selecciona ejecutar como administrador, vea las instrucciones anteriores)
* Ver también si el paquete específico que no pudo ser alcanzado puede ser movido a la biblioteca C:.


**Error de LaTex (ver más abajo)**

Un error como: `! Package pdftex.def Error: File 'cict_qm2_2020-06-29_files/figure-latex/unnamed-chunk-5-1.png' not found: using draft setting.` o `Error: LaTeX failed to compile file_name.tex.`  

* Consulta https://yihui.org/tinytex/r/#debugging para obtener consejos de depuración.
* Ver file_name.log para más información.

**Error Pandoc 127**

Podría tratarse de un problema de RAM (espacio). Reinicia tu sesión de R e inténtelo de nuevo.


**Asignación de unidades de red**

Mapear una unidad de red puede ser arriesgado. Consulta con tu departamento de TI antes de intentarlo.

Un consejo tomado de este [foro de discusión](https://stackoverflow.com/questions/48161177/r-markdown-openbinaryfile-does-not-exist-no-such-file-or-directory/55616529?noredirect=1#comment97966859_55616529):

¿Cómo se abre un archivo "a través de una unidad de red asignada"?

* En primer lugar, tendrás que conocer la ubicación de la red a la que intentas acceder.
* A continuación, en el administrador de archivos de Windows, deberás clicar con el botón derecho en "Este PC" en el panel de la derecha, y seleccionar "Asignar una unidad de red".
* Asigna la ubicación de red como una letra de unidad.
* Ahora tienes dos maneras de llegar al archivo que estás abriendo. Usar la ruta de la letra de la unidad debería funcionar.


**Error in install.packages()**  

Si obtienes un error que incluya la mención de un directorio de "bloqueo", por ejemplo `Error in install.packages : ERROR: failed to lock directory...`

Busca en tu biblioteca de paquetes y verás una carpeta cuyo nombre empieza por "00LOCK". Prueba los siguientes consejos:

* Elimina manualmente el directorio de la carpeta "00LOCK" de tu biblioteca de paquetes. Intenta instalar el paquete de nuevo.
* También puedes probar el comando `pacman::p_unlock()` (también puedes poner este comando en el Rprofile para que se ejecute cada vez que se abra el proyecto). Luego intenta instalar el paquete de nuevo. Puedes necesitar varios intentos.
* Prueba a ejecutar RStudio en modo de administrador e intenta instalar los paquetes uno por uno.
* Si todo lo demás falla, instala el paquete en otra biblioteca o carpeta (por ejemplo, Temp) y luego copia manualmente la carpeta del paquete en la biblioteca deseada.  






```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/network_drives.Rmd-->


# Data Table {#data-table}

El manual se centra en las funciones "verbales" de **dplyr** y en el operador pipe `%>%` de **magrittr** como método para limpiar y agrupar datos, pero el paquete **data.table** ofrece un método alternativo que puedes encontrar en tu recorrido con R.



<!-- ======================================================= -->
## Introducción a data.table {#intro-to-data-tables}

Una tabla de datos es una estructura de datos bidimensional como un dataframe que permite realizar operaciones de agrupación complejas. La sintaxis de data.table está estructurada de forma que se puedan realizar operaciones sobre filas, columnas y grupos.

La estructura es **DT[i, j, by]**, separada por 3 partes; los argumentos **i**, **j** y **by**. El argumento **i** permite subconjuntar las filas necesarias, el argumento **j** permite operar sobre las columnas y el argumento **by** permite operar sobre las columnas por grupos.

En esta página se tratarán los siguientes temas:

* Importación de datos y uso de `fread()` y `fwrite()`
* Selección y filtrado de filas mediante el argumento **i**
* Uso de las funciones de ayuda `%like%`, `%chin%`, `%between%`
* Selección y cálculo de columnas con el argumento **j**
* Cálculo por grupos utilizando el argumento **by**
* Añadir y actualizar datos a las tablas de datos utilizando `:=`



<!-- ======================================================= -->
## Cargar paquetes e importar datos {#load-packages-and-import-data}

### Cargar paquetes {.unnumbered}  

Utilizando la función `p_load()` de **pacman**, cargamos (e instalamos si es necesario) los paquetes necesarios para este análisis.
     
     
```{r}
pacman::p_load(
  rio,        # to import data
  data.table, # to group and clean data
  tidyverse,  # allows use of pipe (%>%) function in this chapter
  here 
  ) 
```


### Importar datos {.unnumbered}

Esta página explorará algunas de las funciones principales de **data.table** utilizando la lista de casos referenciados a lo largo del manual.

Importamos los datos de casos de una epidemia de ébola simulada. Si deseas descargar los datos para seguirlos paso a paso, consulta las instrucciones en la página [Descargar libro y datos]. Los datos se importan mediante la función `import()` del paquete **rio**. Consulta la página sobre [importación y exportación](#import-and-export) para conocer las distintas formas de importar datos. A partir de aquí utilizamos `data.table()` para convertir el dataframe en una data.table.

```{r}
linelist <- rio::import(here("data", "linelist_cleaned.xlsx")) %>% data.table()
```

La función `fread()` se utiliza para importar directamente archivos delimitados regulares, como los archivos .csv, directamente a un formato de tabla de datos. Esta función, y su homóloga, `fwrite()`, utilizada para escribir tablas de datos como archivos delimitados regulares, son opciones muy rápidas y eficientes desde el punto de vista computacional para bases de datos de gran tamaño.


Las primeras 20 filas de `linelist`:  

```{r message=FALSE, echo=F, eval=FALSE}
DT::datatable(head(linelist,20), rownames = FALSE, filter="top", options = list(pageLength = 5, scrollX=T), class = 'white-space: nowrap' )
```

Los comandos de R **base**, como `dim()`, que se utilizan para los dataframes, también pueden utilizarse para las tablas de datos

```{r}
dim(linelist) #gives the number of rows and columns in the data table
```


<!-- ======================================================= -->
## El argumento i: seleccionar y filtrar filas {#the-i-argument-selecting-and-filtering-rows}
     
Recordando la estructura **DT*[i, j, by]**, podemos filtrar filas utilizando números de fila o expresiones lógicas. El argumento i es el primero; por tanto, se puede utilizar la sintaxis **DT[i]** o **DT[i,]**.

El primer ejemplo muestra las 5 primeras filas de la tabla de datos, el segundo ejemplo los casos de 18 años o más, y el tercer ejemplo los casos de 18 años o más pero no diagnosticados en el Central Hospital:


```{r, eval=F}
linelist[1:5] #returns the 1st to 5th row
linelist[age >= 18] #subsets cases are equal to or over 18 years
linelist[age >= 18 & hospital != "Central Hospital"] #subsets cases equal to or over 18 years old but not diagnosed at the Central Hospital

```

El uso de .N en el argumento i representa el número total de filas en la tabla de datos. Esto se puede utilizar para subconjuntar los números de las filas:

```{r, eval=F}
linelist[.N] #returns the last row
linelist[15:.N] #returns the 15th to the last row
```


### Uso de funciones de ayuda para el filtrado {.unnumbered}  

Data table utiliza funciones de ayuda que facilitan el subconjunto de filas. La función %like% se utiliza para coincidir con un patrón en una columna, `%chin%` se utiliza para coincidir con un carácter específico, y la función de ayuda `%between%` se utiliza para coincidir con columnas numéricas dentro de un rango preestablecido.

En los siguientes ejemplos: 
* filtramos las filas en las que la variable hospital contiene "Hospital" 
* filtramos las filas en las que el resultado es "Recover" o "Death" 
* filtramos las filas en el rango de edad 40-60

```{r, eval=F}
linelist[hospital %like% "Hospital"] #filter rows where the hospital variable contains “Hospital”
linelist[outcome %chin% c("Recover", "Death")] #filter rows where the outcome is “Recover” or “Death”
linelist[age %between% c(40, 60)] #filter rows in the age range 40-60

#%between% must take a vector of length 2, whereas %chin% can take vectors of length >= 1

```

## El argumento j: seleccionar y calcular en columnas {#the-j-argument-selecting-and-computing-on-columns}

Utilizando la estructura DT**[i, j, by]**, podemos seleccionar columnas utilizando números o nombres. El argumento **j** es el segundo; por lo tanto, se utiliza la sintaxis **DT[, j]**. Para facilitar los cálculos sobre el argumento **j**, la columna se envuelve utilizando `list()` o `.()`.


### Selección de columnas {.unnumbered} 

El primer ejemplo recupera la primera, tercera y quinta columnas de la tabla de datos, el segundo ejemplo selecciona todas las columnas excepto las de altura, peso y sexo. El tercer ejemplo utiliza la envoltura `.()` para seleccionar las columnas **case_id** y **outcome**.


```{r, eval=F}
linelist[ , c(1,3,5)]
linelist[ , -c("gender", "age", "wt_kg", "ht_cm")]
linelist[ , list(case_id, outcome)] #linelist[ , .(case_id, outcome)] works just as well

```

### Cálculo en columnas {.unnumbered} 

Combinando los argumentos **i** y **j** es posible filtrar filas y calcular en sus columnas. El uso de **.N** en el argumento **j** también representa el número total de filas en la tabla de datos y puede ser útil para devolver el número de filas después del filtrado de filas.

En los siguientes ejemplos: 
* Contar el número de casos que permanecieron más de 7 días en el hospital 
* Calcular la edad media de los casos que murieron en el hospital militar 
* Calcular la desviación estándar, la mediana, la edad media de los casos que se recuperaron en el central hospital


```{r}
linelist[days_onset_hosp > 7 , .N]
linelist[hospital %like% "Military" & outcome %chin% "Death", .(mean(age, na.rm = T))] #na.rm = T removes N/A values
linelist[hospital == "Central Hospital" & outcome == "Recover", 
                 .(mean_age = mean(age, na.rm = T),
                   median_age = median(age, na.rm = T),
                   sd_age = sd(age, na.rm = T))] #this syntax does not use the helper functions but works just as well

```

Recuerda que el uso de `.()` en el argumento j facilita el cálculo, devuelve una tabla de datos y permite nombrar las columnas.

## El argumento by: cálculo por grupos {#the-by-argument-computing-by-groups}

El argumento **by** es el tercer argumento de la estructura **DT[i, j, by]**. El argumento **by** acepta tanto un vector de caracteres como la sintaxis `list()` o `.()`. El uso de la sintaxis `.()` en el argumento **by** permite renombrar las columnas sobre la marcha.

En los siguientes ejemplos:
* agrupamos el número de casos por hospital 
* en los casos de 18 años o más, calculamos la media de altura y peso de los casos según el sexo y si se recuperaron o murieron 
* en los ingresos que duraron más de 7 días, contamos el número de casos según el mes en que ingresaron y el hospital en el que lo hicieron


```{r}
linelist[, .N, .(hospital)] #the number of cases by hospital
linelist[age > 18, .(mean_wt = mean(wt_kg, na.rm = T),
                             mean_ht = mean(ht_cm, na.rm = T)), .(gender, outcome)] #NAs represent the categories where the data is missing
linelist[days_onset_hosp > 7, .N, .(month = month(date_hospitalisation), hospital)]

```

Data.table también permite encadenar expresiones de la siguiente manera:

```{r}

linelist[, .N, .(hospital)][order(-N)][1:3] #1st selects all cases by hospital, 2nd orders the cases in descending order, 3rd subsets the 3 hospitals with the largest caseload


```

En estos ejemplos estamos siguiendo la suposición de que una fila en la tabla de datos es igual a un nuevo caso, y por lo tanto podemos utilizar el **.N** para representar el número de filas en la tabla de datos. Otra función útil para representar el número de casos únicos es `uniqueN()`, que devuelve el número de valores únicos en una entrada dada. Esto se ilustra aquí:

```{r}

linelist[, .(uniqueN(gender))] #remember .() in the j argument returns a data table

```

La respuesta es 3, ya que los valores únicos de la columna de género son m, f y N/A. Compárelo con la función R base `unique()`, que devuelve todos los valores únicos en una entrada dada:

```{r}

linelist[, .(unique(gender))]
```

Para hallar el número de casos únicos en un mes determinado escribiríamos lo siguiente:

```{r}

linelist[, .(uniqueN(case_id)), .(month = month(date_hospitalisation))]

```

## Añadir y actualizar a las tablas de datos {#adding-and-updating-to-data-tables}

El operador `:=` se utiliza para añadir o actualizar datos en una tabla de datos. La adición de columnas a la tabla de datos puede hacerse de las siguientes maneras:

```{r}

linelist[, adult := age >= 18] #adds one column
linelist[, c("child", "wt_lbs") := .(age < 18, wt_kg*2.204)] #to add multiple columns requires c("") and list() or .() syntax
linelist[, `:=` (bmi_in_range = (bmi > 16 & bmi < 40),
                         no_infector_source_data = is.na(infector) | is.na(source))] #this method uses := as a functional operator `:=`
linelist[, adult := NULL] #deletes the column

```


Las agregaciones más complejas están fuera del alcance de este capítulo introductorio, pero la idea es proporcionar una alternativa popular y viable a **dplyr** para agrupar y limpiar datos. El paquete **data.table** es un gran paquete que permite un código ordenado y legible.


## Recursos {#resources-36}

A continuación, algunos recursos útiles para obtener más información:
* https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html
* https://github.com/Rdatatable/data.table
* https://s3.amazonaws.com/assets.datacamp.com/img/blog/data+table+cheat+sheet.pdf
* https://www.machinelearningplus.com/data-manipulation/datatable-in-r-complete-guide/
* https://www.datacamp.com/community/tutorials/data-table-r-tutorial

Puedes realizar cualquier función de resumen sobre datos agrupados; consulta la hoja de trucos [aquí](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/datatable_Cheat_Sheet_R.pdf) para obtener más información:




```{r include=FALSE, cache=FALSE}

# clear workspace
rm(list = ls(all = TRUE))

# clear all packages except base
#lapply(names(sessionInfo()$loadedOnly), require, character.only = TRUE)
#invisible(lapply(paste0('package:', names(sessionInfo()$otherPkgs)), detach, character.only=TRUE, unload=TRUE, force=TRUE))

# to ensure that tidyverse packages prevail
filter <- dplyr::filter
select <- dplyr::select
summarise <- dplyr::summarise
summary <- base::summary
incidence <- incidence2::incidence
clean_names <- janitor::clean_names

#load core packages
pacman::p_load(
     rio,
     here,
     DT,
     stringr,
     lubridate,
     tidyverse
)

# import the cleaned ebola linelist
linelist <- rio::import(here::here("data", "case_linelists", "linelist_cleaned.rds"))

# import the count data - facility level
#count_data <- rio::import(here::here("data", "facility_count_data.rds"))

# Settings

options(scipen=1, digits=7)

# print only text (not code)
#library(knitr)
#opts_chunk$set(list(echo = FALSE, eval = FALSE))
```

<!--chapter:end:new_pages/data_table.Rmd-->

